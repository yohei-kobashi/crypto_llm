
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0345
    HostName g0345
    Port 2222
    StrictHostKeyChecking no

Host g0346
    HostName g0346
    Port 2222
    StrictHostKeyChecking no

Host g0347
    HostName g0347
    Port 2222
    StrictHostKeyChecking no

Host g0352
    HostName g0352
    Port 2222
    StrictHostKeyChecking no

Host g0358
    HostName g0358
    Port 2222
    StrictHostKeyChecking no

Host g0362
    HostName g0362
    Port 2222
    StrictHostKeyChecking no

Host g0363
    HostName g0363
    Port 2222
    StrictHostKeyChecking no

Host g0364
    HostName g0364
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42771619
g0345 slots=4
g0346 slots=4
g0347 slots=4
g0352 slots=4
g0358 slots=4
g0362 slots=4
g0363 slots=4
g0364 slots=4

[2024-08-02 18:13:09,265] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 18:13:14,898] [INFO] [runner.py:463:main] Using IP address of 10.1.11.5 for node g0345
[2024-08-02 18:13:14,901] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0345,g0346,g0347,g0352,g0358,g0362,g0363,g0364
[2024-08-02 18:13:14,901] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0345,g0346,g0347,g0352,g0358,g0362,g0363,g0364 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDM0NSI6IFswLCAxLCAyLCAzXSwgImcwMzQ2IjogWzAsIDEsIDIsIDNdLCAiZzAzNDciOiBbMCwgMSwgMiwgM10sICJnMDM1MiI6IFswLCAxLCAyLCAzXSwgImcwMzU4IjogWzAsIDEsIDIsIDNdLCAiZzAzNjIiOiBbMCwgMSwgMiwgM10sICJnMDM2MyI6IFswLCAxLCAyLCAzXSwgImcwMzY0IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.11.5 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True' --wandb_tag 'other_gpu'
g0345: [2024-08-02 18:13:18,351] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0345: [2024-08-02 18:13:20,555] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0345: [2024-08-02 18:13:20,555] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0345: [2024-08-02 18:13:20,555] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0345: [2024-08-02 18:13:20,555] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0345: [2024-08-02 18:13:20,555] [INFO] [launch.py:163:main] dist_world_size=32
g0345: [2024-08-02 18:13:20,555] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0345: [2024-08-02 18:13:23,650] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0345: [2024-08-02 18:13:23,650] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0345: [2024-08-02 18:13:23,708] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0345: [2024-08-02 18:13:23,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0362: [2024-08-02 18:13:25,266] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0352: [2024-08-02 18:13:25,467] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0363: [2024-08-02 18:13:25,561] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0346: [2024-08-02 18:13:25,956] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0347: [2024-08-02 18:13:25,997] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0358: [2024-08-02 18:13:26,076] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0364: [2024-08-02 18:13:26,380] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0345: --------------------------------------------------
g0345: DeepSpeed C++/CUDA extension op report
g0345: --------------------------------------------------
g0345: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0345:       runtime if needed. Op compatibility means that your system
g0345:       meet the required dependencies to JIT install the op.
g0345: --------------------------------------------------
g0345: JIT compiled ops requires ninja
g0345: --------------------------------------------------
g0345: DeepSpeed C++/CUDA extension op report
g0345: --------------------------------------------------
g0345: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0345:       runtime if needed. Op compatibility means that your system
g0345:       meet the required dependencies to JIT install the op.
g0345: --------------------------------------------------
g0345: JIT compiled ops requires ninja
g0345: --------------------------------------------------
g0345: DeepSpeed C++/CUDA extension op report
g0345: --------------------------------------------------
g0345: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0345:       runtime if needed. Op compatibility means that your system
g0345:       meet the required dependencies to JIT install the op.
g0345: --------------------------------------------------
g0345: JIT compiled ops requires ninja
g0345: --------------------------------------------------
g0345: DeepSpeed C++/CUDA extension op report
g0345: --------------------------------------------------
g0345: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0345:       runtime if needed. Op compatibility means that your system
g0345:       meet the required dependencies to JIT install the op.
g0345: --------------------------------------------------
g0345: JIT compiled ops requires ninja
g0345: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0345: 
g0345: ----------------------------------------------------------------------------------------------------
g0345: 
g0345: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0345: 
g0345: ----------------------------------------------------------------------------------------------------
g0345: 
g0345: ninja .................. [92m[OKAY][0m
g0345: --------------------------------------------------
g0345: op name ................ installed .. compatible
g0345: --------------------------------------------------
g0345: ninja .................. [92m[OKAY][0m
g0345: --------------------------------------------------
g0345: op name ................ installed .. compatible
g0345: --------------------------------------------------
g0345: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0345: async_ioevoformer_attn  ........................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0345: 
g0345: fused_lamb .............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0345:  [92m[OKAY][0m
g0345: cpu_adam ............... [92m[YES][0mfused_lion  ...................  [92m[OKAY][0m[92m[YES][0m
g0345:  ...... cpu_adagrad[92m[OKAY][0m 
g0345: ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0345: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0345: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0345: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0345: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: async_io fused_lion...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0345: [92m[OKAY][0m
g0345: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0345: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0345: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0345: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0345: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0345: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0345: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0345: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: cutlass_ops ............ cutlass_ops[92m[YES][0m  ..................  [92m[OKAY][0m[92m[YES][0m
g0345:  ...... [92m[OKAY][0mquantizer
g0345:  .............. [92m[YES][0mquantizer  ....................  [92m[OKAY][0m[92m[YES][0m
g0345:  ...... [92m[OKAY][0m
g0345: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0345: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0345: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0mragged_device_ops
g0345:  ...... [92m[YES][0m ...... [92m[OKAY][0m
g0345: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0345: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0345: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0345: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0345: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0345: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0345: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0345: ragged_opsragged_ops  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0345: 
g0345: random_ltdrandom_ltd  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0345: 
g0345: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0345: 
g0345: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0345: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatiblesparse_attn 
g0345: ............ [93m[NO][0msparse_attn .......  ............[93m[NO][0m 
g0345: [93m[NO][0m ....... [93m[NO][0m
g0345: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0345: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0345: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0345: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0345: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0345: spatial_inferencespatial_inference  ............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0345: 
g0345: transformertransformer  ........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0345: 
g0345: stochastic_transformerstochastic_transformer  ..  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0345: 
g0345: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0345: --------------------------------------------------
g0345: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0345: --------------------------------------------------
g0345: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0345: --------------------------------------------------
g0345: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0345: --------------------------------------------------
g0345: DeepSpeed general environment info:
g0345: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0345: torch version .................... 2.0.1+cu118
g0345: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0345: deepspeed info ................... 0.12.4, unknown, unknown
g0345: torch cuda version ............... 11.8
g0345: torch hip version ................ None
g0345: nvcc version ..................... 11.8
g0345: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0345: shared memory (/dev/shm) size .... 188.13 GB
g0345: DeepSpeed general environment info:
g0345: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0345: torch version .................... 2.0.1+cu118
g0345: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0345: deepspeed info ................... 0.12.4, unknown, unknown
g0345: torch cuda version ............... 11.8
g0345: torch hip version ................ None
g0345: nvcc version ..................... 11.8
g0345: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0345: shared memory (/dev/shm) size .... 188.13 GB
g0345: DeepSpeed general environment info:
g0345: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0345: torch version .................... 2.0.1+cu118
g0345: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0345: deepspeed info ................... 0.12.4, unknown, unknown
g0345: torch cuda version ............... 11.8
g0345: torch hip version ................ None
g0345: nvcc version ..................... 11.8
g0345: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0345: shared memory (/dev/shm) size .... 188.13 GB
g0345: DeepSpeed general environment info:
g0345: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0345: torch version .................... 2.0.1+cu118
g0345: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0345: deepspeed info ................... 0.12.4, unknown, unknown
g0345: torch cuda version ............... 11.8
g0345: torch hip version ................ None
g0345: nvcc version ..................... 11.8
g0345: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0345: shared memory (/dev/shm) size .... 188.13 GB
g0345: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0345: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0345: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0345: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0345: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0345: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0345: using torch.float32 for parameters ...
g0345: ------------------------ arguments ------------------------
g0345:   accumulate_allreduce_grads_in_fp32 .............. False
g0345:   adam_beta1 ...................................... 0.9
g0345:   adam_beta2 ...................................... 0.95
g0345:   adam_eps ........................................ 1e-08
g0345:   add_bias_linear ................................. False
g0345:   add_position_embedding .......................... False
g0345:   adlr_autoresume ................................. False
g0345:   adlr_autoresume_interval ........................ 1000
g0345:   aml_data_download_path .......................... None
g0345:   apply_layernorm_1p .............................. False
g0345:   apply_query_key_layer_scaling ................... False
g0345:   apply_residual_connection_post_layernorm ........ False
g0345:   async_tensor_model_parallel_allreduce ........... False
g0345:   attention_dropout ............................... 0.0
g0345:   attention_softmax_in_fp32 ....................... False
g0345:   barrier_with_L1_time ............................ True
g0345:   bert_binary_head ................................ True
g0345:   bert_embedder_type .............................. megatron
g0345:   bert_load ....................................... None
g0345:   bf16 ............................................ False
g0345:   bias_dropout_fusion ............................. True
g0345:   bias_gelu_fusion ................................ False
g0345:   biencoder_projection_dim ........................ 0
g0345:   biencoder_shared_query_context_model ............ False
g0345:   block_data_path ................................. None
g0345:   checkpoint_activations .......................... False
g0345:   checkpoint_in_cpu ............................... False
g0345:   checkpoint_num_layers ........................... 1
g0345:   classes_fraction ................................ 1.0
g0345:   clip_grad ....................................... 1.0
g0345:   compression_training ............................ False
g0345:   consumed_train_samples .......................... 0
g0345:   consumed_train_tokens ........................... 0
g0345:   consumed_valid_samples .......................... 0
g0345:   contigious_checkpointing ........................ False
g0345:   cpu_optimizer ................................... False
g0345:   cpu_torch_adam .................................. False
g0345:   create_moe_param_group .......................... False
g0345:   curriculum_learning_legacy ...................... False
g0345:   data_cache_path ................................. None
g0345:   data_efficiency_curriculum_learning ............. False
g0345:   data_impl ....................................... mmap
g0345:   data_parallel_random_init ....................... False
g0345:   data_parallel_size .............................. 4
g0345:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document']
g0345:   data_per_class_fraction ......................... 1.0
g0345:   data_sharding ................................... True
g0345:   dataloader_type ................................. single
g0345:   DDP_impl ........................................ local
g0345:   decoder_num_layers .............................. None
g0345:   decoder_seq_length .............................. None
g0345:   deepscale ....................................... False
g0345:   deepscale_config ................................ None
g0345:   deepspeed ....................................... True
g0345:   deepspeed_activation_checkpointing .............. False
g0345:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0345:   deepspeed_mpi ................................... False
g0345:   dino_bottleneck_size ............................ 256
g0345:   dino_freeze_last_layer .......................... 1
g0345:   dino_head_hidden_size ........................... 2048
g0345:   dino_local_crops_number ......................... 10
g0345:   dino_local_img_size ............................. 96
g0345:   dino_norm_last_layer ............................ False
g0345:   dino_teacher_temp ............................... 0.07
g0345:   dino_warmup_teacher_temp ........................ 0.04
g0345:   dino_warmup_teacher_temp_epochs ................. 30
g0345:   distribute_checkpointed_activations ............. False
g0345:   distribute_saved_activations .................... False
g0345:   distributed_backend ............................. nccl
g0345:   distributed_timeout_minutes ..................... 10
g0345:   ds_fused_adam ................................... False
g0345:   ds_inference .................................... False
g0345:   ds_pipeline_enabled ............................. True
g0345:   ds_sequence_parallel_size ....................... 1
g0345:   embedding_path .................................. None
g0345:   embedding_weights_in_fp32 ....................... False
g0345:   empty_unused_memory_level ....................... 0
g0345:   enable_expert_tensor_parallelism ................ False
g0345:   encoder_num_layers .............................. 22
g0345:   encoder_seq_length .............................. 2048
g0345:   end_weight_decay ................................ 0.1
g0345:   eod_mask_loss ................................... False
g0345:   eval_interval ................................... 1000
g0345:   eval_iters ...................................... 100
g0345:   evidence_data_path .............................. None
g0345:   exit_duration_in_mins ........................... 30000000
g0345:   exit_interval ................................... None
g0345:   exit_on_missing_checkpoint ...................... False
g0345:   exit_signal_handler ............................. False
g0345:   expert_interval ................................. 2
g0345:   ffn_hidden_size ................................. 5632
g0345:   finetune ........................................ False
g0345:   force_ds_sequence_parallel ...................... False
g0345:   fp16 ............................................ False
g0345:   fp16_lm_cross_entropy ........................... False
g0345:   fp32_residual_connection ........................ False
g0345:   fp8_amax_compute_algo ........................... most_recent
g0345:   fp8_amax_history_len ............................ 1
g0345:   fp8_e4m3 ........................................ False
g0345:   fp8_hybrid ...................................... False
g0345:   fp8_interval .................................... 1
g0345:   fp8_margin ...................................... 0
g0345:   fp8_wgrad ....................................... True
g0345:   global_batch_size ............................... 128
g0345:   gradient_accumulation_fusion .................... True
g0345:   head_lr_mult .................................... 1.0
g0345:   hidden_dropout .................................. 0.0
g0345:   hidden_size ..................................... 2048
g0345:   hidden_size_teacher ............................. None
g0345:   hysteresis ...................................... 2
g0345:   ict_head_size ................................... None
g0345:   ict_load ........................................ None
g0345:   img_h ........................................... 224
g0345:   img_w ........................................... 224
g0345:   indexer_batch_size .............................. 128
g0345:   indexer_log_interval ............................ 1000
g0345:   inference ....................................... False
g0345:   inference_batch_times_seqlen_threshold .......... 512
g0345:   init_method_std ................................. 0.013
g0345:   init_method_xavier_uniform ...................... False
g0345:   initial_loss_scale .............................. 4294967296
g0345:   iter_per_epoch .................................. 1250
g0345:   kd .............................................. False
g0345:   kd_alpha_ce ..................................... 1
g0345:   kd_beta_ce ...................................... 1
g0345:   kd_temp ......................................... 1.0
g0345:   kv_channels ..................................... 128
g0345:   layernorm_epsilon ............................... 1e-05
g0345:   lazy_mpu_init ................................... None
g0345:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345:   load_teacher .................................... None
g0345:   local_rank ...................................... 0
g0345:   log_batch_size_to_tensorboard ................... True
g0345:   log_interval .................................... 10
g0345:   log_learning_rate_to_tensorboard ................ True
g0345:   log_loss_scale_to_tensorboard ................... True
g0345:   log_memory_to_tensorboard ....................... False
g0345:   log_num_zeros_in_grad ........................... False
g0345:   log_optimizer_states_to_tensorboard ............. True
g0345:   log_params_norm ................................. False
g0345:   log_timers_to_tensorboard ....................... True
g0345:   log_validation_ppl_to_tensorboard ............... True
g0345:   log_world_size_to_tensorboard ................... False
g0345:   loss_scale ...................................... None
g0345:   loss_scale_window ............................... 1000
g0345:   lr .............................................. 0.0002
g0345:   lr_decay_iters .................................. None
g0345:   lr_decay_samples ................................ None
g0345:   lr_decay_style .................................. cosine
g0345:   lr_decay_tokens ................................. 300000000000
g0345:   lr_warmup_fraction .............................. None
g0345:   lr_warmup_iters ................................. 0
g0345:   lr_warmup_samples ............................... 0
g0345:   lr_warmup_tokens ................................ 3000000000
g0345:   make_vocab_size_divisible_by .................... 128
g0345:   mask_factor ..................................... 1.0
g0345:   mask_prob ....................................... 0.15
g0345:   mask_type ....................................... random
g0345:   masked_softmax_fusion ........................... True
g0345:   max_position_embeddings ......................... 2048
g0345:   max_tokens_to_oom ............................... 12000
g0345:   mem_efficient_ln ................................ True
g0345:   memory_centric_tiled_linear ..................... False
g0345:   merge_file ...................................... None
g0345:   micro_batch_size ................................ 1
g0345:   min_loss_scale .................................. 1.0
g0345:   min_lr .......................................... 1e-05
g0345:   mlp_type ........................................ standard
g0345:   mmap_warmup ..................................... False
g0345:   moe_eval_capacity_factor ........................ 1.0
g0345:   moe_expert_parallel_size ........................ 1
g0345:   moe_loss_coeff .................................. 0.1
g0345:   moe_min_capacity ................................ 4
g0345:   moe_token_dropping .............................. True
g0345:   moe_train_capacity_factor ....................... 1.0
g0345:   mos ............................................. False
g0345:   no_load_lr_state ................................ False
g0345:   no_load_optim ................................... None
g0345:   no_load_rng ..................................... None
g0345:   no_persist_layer_norm ........................... False
g0345:   no_pipeline_parallel ............................ False
g0345:   no_save_optim ................................... None
g0345:   no_save_rng ..................................... None
g0345:   normalization ................................... rmsnorm
g0345:   num_attention_heads ............................. 16
g0345:   num_attention_heads_teacher ..................... None
g0345:   num_channels .................................... 3
g0345:   num_classes ..................................... 1000
g0345:   num_experts ..................................... [1]
g0345:   num_experts_switch .............................. None
g0345:   num_experts_teacher ............................. [1]
g0345:   num_key_value_heads ............................. 4
g0345:   num_layers ...................................... 22
g0345:   num_layers_per_virtual_pipeline_stage ........... None
g0345:   num_layers_teacher .............................. None
g0345:   num_workers ..................................... 0
g0345:   onnx_safe ....................................... None
g0345:   openai_gelu ..................................... False
g0345:   optimizer ....................................... adam
g0345:   output_bert_embeddings .......................... False
g0345:   overlap_p2p_comm ................................ False
g0345:   override_opt_param_scheduler .................... True
g0345:   params_dtype .................................... torch.float32
g0345:   partition_activations ........................... False
g0345:   patch_dim ....................................... 16
g0345:   perform_initialization .......................... True
g0345:   pipeline_model_parallel_size .................... 8
g0345:   pipeline_model_parallel_split_rank .............. None
g0345:   profile_backward ................................ False
g0345:   query_in_block_prob ............................. 0.1
g0345:   rampup_batch_size ............................... None
g0345:   random_ltd ...................................... False
g0345:   rank ............................................ 0
g0345:   recompute_granularity ........................... None
g0345:   recompute_method ................................ None
g0345:   recompute_num_layers ............................ 1
g0345:   remote_device ................................... none
g0345:   repeated_dataloader ............................. False
g0345:   reset_attention_mask ............................ False
g0345:   reset_iteration ................................. False
g0345:   reset_position_ids .............................. False
g0345:   retriever_report_topk_accuracies ................ []
g0345:   retriever_score_scaling ......................... False
g0345:   retriever_seq_length ............................ 256
g0345:   retro_add_retriever ............................. False
g0345:   retro_cyclic_train_iters ........................ None
g0345:   retro_encoder_attention_dropout ................. 0.1
g0345:   retro_encoder_hidden_dropout .................... 0.1
g0345:   retro_encoder_layers ............................ 2
g0345:   retro_num_neighbors ............................. 2
g0345:   retro_num_retrieved_chunks ...................... 2
g0345:   retro_return_doc_ids ............................ False
g0345:   retro_workdir ................................... None
g0345:   return_data_index ............................... False
g0345:   rotary_percent .................................. 1.0
g0345:   sample_rate ..................................... 1.0
g0345:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345:   save_interval ................................... 1000
g0345:   scatter_gather_tensors_in_pipeline .............. True
g0345:   scattered_embeddings ............................ False
g0345:   seed ............................................ 1234
g0345:   seq_length ...................................... 2048
g0345:   sequence_parallel ............................... False
g0345:   sgd_momentum .................................... 0.9
g0345:   short_seq_prob .................................. 0.1
g0345:   skip_train ...................................... False
g0345:   split ........................................... 949,50,1
g0345:   split_transformers .............................. False
g0345:   squared_relu .................................... False
g0345:   standalone_embedding_stage ...................... False
g0345:   start_weight_decay .............................. 0.1
g0345:   swiglu .......................................... True
g0345:   swin_backbone_type .............................. tiny
g0345:   synchronize_each_layer .......................... False
g0345:   tensor_model_parallel_size ...................... 1
g0345:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True
g0345:   tensorboard_log_interval ........................ 1
g0345:   tensorboard_queue_size .......................... 1
g0345:   test_data_path .................................. None
g0345:   tf32 ............................................ False
g0345:   tile_factor ..................................... 1
g0345:   timing_log_level ................................ 0
g0345:   timing_log_option ............................... minmax
g0345:   titles_data_path ................................ None
g0345:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
g0345:   tokenizer_type .................................. SentencePieceTokenizer
g0345:   topk ............................................ 1
g0345:   train_data_exact_num_epochs ..................... 1
g0345:   train_data_path ................................. None
g0345:   train_desc_path ................................. None
g0345:   train_doc_idx_path .............................. None
g0345:   train_idx_path .................................. None
g0345:   train_iters ..................................... None
g0345:   train_sample_idx_path ........................... None
g0345:   train_samples ................................... 1280000000
g0345:   train_shuffle_idx_path .......................... None
g0345:   train_tokens .................................... 2621440000000
g0345:   transformer_impl ................................ local
g0345:   transformer_pipeline_model_parallel_size ........ 8
g0345:   universal_checkpoint ............................ False
g0345:   untie_embeddings_and_output_weights ............. True
g0345:   use_checkpoint_args ............................. False
g0345:   use_checkpoint_opt_param_scheduler .............. False
g0345:   use_contiguous_buffers_in_local_ddp ............. True
g0345:   use_cpu_initialization .......................... None
g0345:   use_dataset_only ................................ False
g0345:   use_distributed_optimizer ....................... False
g0345:   use_flash_attn .................................. False
g0345:   use_flash_attn_triton ........................... False
g0345:   use_flash_attn_v1 ............................... False
g0345:   use_flash_attn_v2 ............................... False
g0345:   use_one_sent_docs ............................... False
g0345:   use_pin_memory .................................. False
g0345:   use_ring_exchange_p2p ........................... False
g0345:   use_rotary_position_embeddings .................. True
g0345:   use_tutel ....................................... False
g0345:   use_wandb ....................................... True
g0345:   valid_data_path ................................. None
g0345:   variable_seq_lengths ............................ False
g0345:   virtual_pipeline_model_parallel_size ............ None
g0345:   vision_backbone_type ............................ vit
g0345:   vision_pretraining .............................. False
g0345:   vision_pretraining_type ......................... classify
g0345:   vocab_extra_ids ................................. 0
g0345:   vocab_file ...................................... None
g0345:   vocab_size ...................................... None
g0345:   wandb_entity .................................... yohei-kobashi
g0345:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True
g0345:   wandb_project ................................... encrypted_data_LLM
g0345:   wandb_tag ....................................... other_gpu
g0345:   weight_decay .................................... 0.1
g0345:   weight_decay_incr_style ......................... constant
g0345:   world_size ...................................... 32
g0345:   zero_allgather_bucket_size ...................... 0.0
g0345:   zero_contigious_gradients ....................... False
g0345:   zero_reduce_bucket_size ......................... 0.0
g0345:   zero_reduce_scatter ............................. False
g0345:   zero_stage ...................................... 0
g0345: -------------------- end of arguments ---------------------
g0345: setting number of micro-batches to constant 32
g0345: > building SentencePieceTokenizer tokenizer ...
g0345: [2024-08-02 18:13:28,414] [INFO] [comm.py:637:init_distributed] cdb=None
g0345: [2024-08-02 18:13:28,415] [INFO] [comm.py:637:init_distributed] cdb=None
g0345: [2024-08-02 18:13:28,415] [INFO] [comm.py:637:init_distributed] cdb=None
g0345:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0345: > initializing torch distributed ...
g0345: [2024-08-02 18:13:28,417] [INFO] [comm.py:637:init_distributed] cdb=None
g0345: [2024-08-02 18:13:28,417] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0345: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [2024-08-02 18:13:29,362] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0362: [2024-08-02 18:13:29,362] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0362: [2024-08-02 18:13:29,362] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0362: [2024-08-02 18:13:29,362] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0362: [2024-08-02 18:13:29,362] [INFO] [launch.py:163:main] dist_world_size=32
g0362: [2024-08-02 18:13:29,362] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0352: [2024-08-02 18:13:29,419] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0352: [2024-08-02 18:13:29,419] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0352: [2024-08-02 18:13:29,419] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0352: [2024-08-02 18:13:29,419] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0352: [2024-08-02 18:13:29,419] [INFO] [launch.py:163:main] dist_world_size=32
g0352: [2024-08-02 18:13:29,419] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0363: [2024-08-02 18:13:29,670] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0363: [2024-08-02 18:13:29,670] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0363: [2024-08-02 18:13:29,670] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0363: [2024-08-02 18:13:29,670] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0363: [2024-08-02 18:13:29,670] [INFO] [launch.py:163:main] dist_world_size=32
g0363: [2024-08-02 18:13:29,670] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0347: [2024-08-02 18:13:29,958] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0347: [2024-08-02 18:13:29,958] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0347: [2024-08-02 18:13:29,958] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0347: [2024-08-02 18:13:29,958] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0347: [2024-08-02 18:13:29,958] [INFO] [launch.py:163:main] dist_world_size=32
g0347: [2024-08-02 18:13:29,958] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0346: [2024-08-02 18:13:29,986] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0346: [2024-08-02 18:13:29,986] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0346: [2024-08-02 18:13:29,986] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0346: [2024-08-02 18:13:29,986] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0346: [2024-08-02 18:13:29,986] [INFO] [launch.py:163:main] dist_world_size=32
g0346: [2024-08-02 18:13:29,986] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0358: [2024-08-02 18:13:30,103] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0358: [2024-08-02 18:13:30,104] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0358: [2024-08-02 18:13:30,104] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0358: [2024-08-02 18:13:30,104] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0358: [2024-08-02 18:13:30,104] [INFO] [launch.py:163:main] dist_world_size=32
g0358: [2024-08-02 18:13:30,104] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0364: [2024-08-02 18:13:30,498] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0364: [2024-08-02 18:13:30,498] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0345': [0, 1, 2, 3], 'g0346': [0, 1, 2, 3], 'g0347': [0, 1, 2, 3], 'g0352': [0, 1, 2, 3], 'g0358': [0, 1, 2, 3], 'g0362': [0, 1, 2, 3], 'g0363': [0, 1, 2, 3], 'g0364': [0, 1, 2, 3]}
g0364: [2024-08-02 18:13:30,498] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0364: [2024-08-02 18:13:30,498] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0345': [0, 1, 2, 3], 'g0346': [4, 5, 6, 7], 'g0347': [8, 9, 10, 11], 'g0352': [12, 13, 14, 15], 'g0358': [16, 17, 18, 19], 'g0362': [20, 21, 22, 23], 'g0363': [24, 25, 26, 27], 'g0364': [28, 29, 30, 31]})
g0364: [2024-08-02 18:13:30,498] [INFO] [launch.py:163:main] dist_world_size=32
g0364: [2024-08-02 18:13:30,498] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0362: [2024-08-02 18:13:32,447] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0362: [2024-08-02 18:13:32,447] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0362: [2024-08-02 18:13:32,447] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0352: [2024-08-02 18:13:32,544] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0352: [2024-08-02 18:13:32,544] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0352: [2024-08-02 18:13:32,594] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0352: [2024-08-02 18:13:32,673] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0362: [2024-08-02 18:13:32,718] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0363: [2024-08-02 18:13:32,763] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0363: [2024-08-02 18:13:32,764] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0363: [2024-08-02 18:13:32,834] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0363: [2024-08-02 18:13:32,861] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0347: [2024-08-02 18:13:33,036] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0347: [2024-08-02 18:13:33,036] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0347: [2024-08-02 18:13:33,110] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0347: [2024-08-02 18:13:33,110] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0346: [2024-08-02 18:13:33,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0346: [2024-08-02 18:13:33,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0346: [2024-08-02 18:13:33,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0358: [2024-08-02 18:13:33,217] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0346: [2024-08-02 18:13:33,251] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0358: [2024-08-02 18:13:33,271] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0358: [2024-08-02 18:13:33,326] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0358: [2024-08-02 18:13:33,326] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0364: [2024-08-02 18:13:33,550] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0364: [2024-08-02 18:13:33,550] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0364: [2024-08-02 18:13:33,551] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0364: [2024-08-02 18:13:33,745] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0362: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0362: 
g0362: 
g0362: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0362: 
g0362: 
g0362: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0362: 
g0362: 
g0362: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0362:       runtime if needed. Op compatibility means that your system
g0362:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0362:       runtime if needed. Op compatibility means that your system
g0362:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0362:       runtime if needed. Op compatibility means that your system
g0362:       meet the required dependencies to JIT install the op.
g0362: 
g0362: 
g0362: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0362: 
g0362: 
g0362: JIT compiled ops requires ninjaJIT compiled ops requires ninjaJIT compiled ops requires ninja
g0362: 
g0362: 
g0362: --------------------------------------------------
g0362: DeepSpeed C++/CUDA extension op report
g0362: --------------------------------------------------
g0362: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0362:       runtime if needed. Op compatibility means that your system
g0362:       meet the required dependencies to JIT install the op.
g0362: --------------------------------------------------
g0362: JIT compiled ops requires ninja
g0362: ninjaninjaninjaninja    ......................................................  .................. [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0362: 
g0362: 
g0362: 
g0362: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0362: --------------------------------------------------
g0362: 
g0362: op name
g0362:  op nameop name................ op name  ................installed................    installedinstalled..................    ....compatible installed compatible
g0362:  
g0362: compatible--------------------------------------------------
g0362: ..
g0362: ----------------------------------------------------------------------------------------------------
g0362:  
g0362: compatible
g0362: --------------------------------------------------
g0362: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0362: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0362: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0362: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0362: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0masync_io
g0362:  ............... [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0362: ...... evoformer_attn [92m[OKAY][0m......... 
g0362: [93m[NO][0m ....... [93m[NO][0m
g0362: fused_lamb .............fused_adam [92m[YES][0m  ...................  [92m[OKAY][0m
g0362: [92m[YES][0m ...... [92m[OKAY][0m
g0362: fused_lion ............. [92m[YES][0m cpu_adam......  [92m[OKAY][0m...............
g0362:  [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0362: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0362: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0362: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0362:  [92m[OKAY][0m
g0362: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0362: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0362: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0362: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0362: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0362: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0362: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0362: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0362: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: ragged_ops .............[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0 
g0362: [92m[YES][0m ...... [92m[OKAY][0m
g0362: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0362: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0362: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0362: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0362: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0362: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0362: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0362: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0362: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0362: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0362: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0362: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0362: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0362: --------------------------------------------------
g0362: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0362: --------------------------------------------------
g0362: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0362: --------------------------------------------------
g0362: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0362: --------------------------------------------------
g0362: DeepSpeed general environment info:
g0362: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0362: torch version .................... 2.0.1+cu118
g0362: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0362: deepspeed info ................... 0.12.4, unknown, unknown
g0362: torch cuda version ............... 11.8
g0362: torch hip version ................ None
g0362: nvcc version ..................... 11.8
g0362: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0362: shared memory (/dev/shm) size .... 188.13 GB
g0362: DeepSpeed general environment info:
g0362: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0362: torch version .................... 2.0.1+cu118
g0362: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0362: deepspeed info ................... 0.12.4, unknown, unknown
g0362: torch cuda version ............... 11.8
g0362: torch hip version ................ None
g0362: nvcc version ..................... 11.8
g0362: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0362: shared memory (/dev/shm) size .... 188.13 GB
g0362: DeepSpeed general environment info:
g0362: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0362: torch version .................... 2.0.1+cu118
g0362: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0362: deepspeed info ................... 0.12.4, unknown, unknown
g0362: torch cuda version ............... 11.8
g0362: torch hip version ................ None
g0362: nvcc version ..................... 11.8
g0362: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0362: shared memory (/dev/shm) size .... 188.13 GB
g0362: DeepSpeed general environment info:
g0362: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0362: torch version .................... 2.0.1+cu118
g0362: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0362: deepspeed info ................... 0.12.4, unknown, unknown
g0362: torch cuda version ............... 11.8
g0362: torch hip version ................ None
g0362: nvcc version ..................... 11.8
g0362: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0362: shared memory (/dev/shm) size .... 188.13 GB
g0362: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0362: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0362: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0362: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0352: --------------------------------------------------
g0352: DeepSpeed C++/CUDA extension op report
g0352: --------------------------------------------------
g0352: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0352:       runtime if needed. Op compatibility means that your system
g0352:       meet the required dependencies to JIT install the op.
g0352: --------------------------------------------------
g0352: JIT compiled ops requires ninja
g0352: ----------------------------------------------------------------------------------------------------
g0352: DeepSpeed C++/CUDA extension op report
g0352: 
g0352: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0352: 
g0352: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0352:       runtime if needed. Op compatibility means that your system
g0352:       meet the required dependencies to JIT install the op.
g0352: 
g0352: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0352:       runtime if needed. Op compatibility means that your system
g0352:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0352: 
g0352: --------------------------------------------------JIT compiled ops requires ninja
g0352: 
g0352: JIT compiled ops requires ninja
g0352: --------------------------------------------------
g0352: DeepSpeed C++/CUDA extension op report
g0352: --------------------------------------------------
g0352: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0352:       runtime if needed. Op compatibility means that your system
g0352:       meet the required dependencies to JIT install the op.
g0352: --------------------------------------------------
g0352: JIT compiled ops requires ninja
g0352: ninjaninjaninja  ninja....................................    ..................[92m[OKAY][0m..................[92m[OKAY][0m 
g0352:  
g0352: [92m[OKAY][0m[92m[OKAY][0m--------------------------------------------------
g0352: 
g0352: --------------------------------------------------
g0352: 
g0352: ----------------------------------------------------------------------------------------------------op name
g0352: op name
g0352:   op name................................op name    ................installedinstalled................    installed....installed    ..compatiblecompatible.. 
g0352: 
g0352:  compatible----------------------------------------------------------------------------------------------------
g0352: compatible
g0352: 
g0352: 
g0352: --------------------------------------------------
g0352: --------------------------------------------------
g0362: [2024-08-02 18:13:36,869] [INFO] [comm.py:637:init_distributed] cdb=None
g0362: [2024-08-02 18:13:36,869] [INFO] [comm.py:637:init_distributed] cdb=None
g0362: [2024-08-02 18:13:36,869] [INFO] [comm.py:637:init_distributed] cdb=None
g0362: [2024-08-02 18:13:36,869] [INFO] [comm.py:637:init_distributed] cdb=None
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0362: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0352: async_io ............... [92m[YES][0masync_io ......  ...............[92m[OKAY][0m 
g0352: [92m[YES][0m ...... [92m[OKAY][0m
g0352: fused_adamasync_io .............  ...............fused_adam [92m[YES][0m [92m[YES][0m ............. ...... ...... [92m[YES][0m [92m[OKAY][0m [92m[OKAY][0m......
g0352:  
g0352: [92m[OKAY][0m
g0352: cpu_adam ...............cpu_adam  [92m[YES][0mfused_adam...............   ...................[92m[YES][0m   [92m[OKAY][0m[92m[YES][0m......
g0352:   ......[92m[OKAY][0m 
g0352: [92m[OKAY][0mcpu_adagrad
g0352:  cpu_adagrad............  ............[92m[YES][0mcpu_adam   [92m[YES][0m.....................   ......[92m[OKAY][0m[92m[YES][0m 
g0352:  [92m[OKAY][0m......
g0352:  [92m[OKAY][0mcpu_lion
g0352:  cpu_lion...............  ...............cpu_adagrad[92m[YES][0m   [92m[YES][0m..................   ......[92m[YES][0m[92m[OKAY][0m  
g0352: [92m[OKAY][0m......
g0352:  [92m[OKAY][0m
g0352: cpu_lion ............... [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0352:  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0352: evoformer_attn
g0352:  evoformer_attn.........  .........[93m[NO][0m  [93m[NO][0m.......  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.......[93m[NO][0m
g0352:  
g0352: [93m[NO][0mevoformer_attn
g0352:  fused_lamb.........  fused_lamb.............[93m[NO][0m   .............[92m[YES][0m.......  [92m[YES][0m ...... [93m[NO][0m ......
g0352: [92m[OKAY][0m 
g0352: [92m[OKAY][0m
g0352: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: fused_lion fused_lion.............  .............[92m[YES][0m  [92m[YES][0m...... fused_lion ...... [92m[OKAY][0m .............
g0352:  [92m[OKAY][0m[92m[YES][0m
g0352:  ...... [92m[OKAY][0m
g0352: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0352: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0352: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0352: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0352: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0352: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0352: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: ----------------------------------------------------------------------------------------------------
g0363: 
g0363: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0363: 
g0363: ----------------------------------------------------------------------------------------------------
g0363: 
g0363: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0363:       runtime if needed. Op compatibility means that your system
g0363:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0363:       runtime if needed. Op compatibility means that your system
g0363:       meet the required dependencies to JIT install the op.
g0363: 
g0363: ----------------------------------------------------------------------------------------------------
g0363: 
g0363: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0363: 
g0363: --------------------------------------------------
g0363: DeepSpeed C++/CUDA extension op report
g0363: --------------------------------------------------
g0363: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0363:       runtime if needed. Op compatibility means that your system
g0363:       meet the required dependencies to JIT install the op.
g0363: --------------------------------------------------
g0363: JIT compiled ops requires ninja
g0363: --------------------------------------------------
g0363: DeepSpeed C++/CUDA extension op report
g0363: --------------------------------------------------
g0363: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0363:       runtime if needed. Op compatibility means that your system
g0363:       meet the required dependencies to JIT install the op.
g0363: --------------------------------------------------
g0363: JIT compiled ops requires ninja
g0352: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0352: inference_core_ops ..... inference_core_ops[92m[YES][0m  ...........  [92m[OKAY][0m[92m[YES][0m
g0352:  ...... [92m[OKAY][0m
g0352: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0352: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: ninjaninja  ninja....................................   [92m[OKAY][0m..................[92m[OKAY][0m
g0352: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0352: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0352: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0363:  
g0363: [92m[OKAY][0m--------------------------------------------------
g0363: --------------------------------------------------
g0363: 
g0363: --------------------------------------------------op nameop name
g0363:   ................................op name   installedinstalled................  .. .. installed compatible compatible
g0363: ..
g0363:  ----------------------------------------------------------------------------------------------------compatible
g0363: 
g0363: 
g0352: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: --------------------------------------------------
g0352: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: ninja .................. [92m[OKAY][0m
g0363: --------------------------------------------------
g0363: op name ................ installed .. compatible
g0363: --------------------------------------------------
g0352: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0352: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0352: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0352: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0352: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatibleragged_ops
g0352:  .............sparse_attn  [92m[YES][0m............  ......[93m[NO][0m  [92m[OKAY][0m.......
g0352:  [93m[NO][0m
g0352: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0352: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0352: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0352: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0352: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0352: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0352: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0352: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0352: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0352: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0352: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0352: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0352: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0352: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0352: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0352: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0352: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0352: --------------------------------------------------
g0352: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0352: --------------------------------------------------
g0352: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0352: --------------------------------------------------
g0352: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0352: --------------------------------------------------
g0352: DeepSpeed general environment info:
g0352: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0352: torch version .................... 2.0.1+cu118
g0352: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0352: deepspeed info ................... 0.12.4, unknown, unknown
g0352: torch cuda version ............... 11.8
g0352: torch hip version ................ None
g0352: nvcc version ..................... 11.8
g0352: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0352: shared memory (/dev/shm) size .... 188.13 GB
g0352: DeepSpeed general environment info:
g0352: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0352: torch version .................... 2.0.1+cu118
g0352: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0352: deepspeed info ................... 0.12.4, unknown, unknown
g0352: torch cuda version ............... 11.8
g0352: torch hip version ................ None
g0352: nvcc version ..................... 11.8
g0352: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0352: shared memory (/dev/shm) size .... 188.13 GB
g0352: DeepSpeed general environment info:
g0352: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0352: torch version .................... 2.0.1+cu118
g0352: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0352: deepspeed info ................... 0.12.4, unknown, unknown
g0352: torch cuda version ............... 11.8
g0352: torch hip version ................ None
g0352: nvcc version ..................... 11.8
g0352: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0352: shared memory (/dev/shm) size .... 188.13 GB
g0352: DeepSpeed general environment info:
g0352: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0352: torch version .................... 2.0.1+cu118
g0352: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0352: deepspeed info ................... 0.12.4, unknown, unknown
g0352: torch cuda version ............... 11.8
g0352: torch hip version ................ None
g0352: nvcc version ..................... 11.8
g0352: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0352: shared memory (/dev/shm) size .... 188.13 GB
g0352: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0352: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0352: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0352: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0363: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0363: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0363: cpu_adagrad ............ [92m[YES][0m ......async_io [92m[OKAY][0m 
g0363: ............... [92m[YES][0mcpu_lion  ..................... [92m[YES][0m ......  [92m[OKAY][0m[92m[OKAY][0m
g0363: 
g0363: fused_adam ............. [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0363: [92m[OKAY][0m
g0363: evoformer_attn .........cpu_adam  [93m[NO][0m...............  .......[92m[YES][0m  [93m[NO][0m......
g0363:  [92m[OKAY][0m
g0363: fused_lamb cpu_adagrad.............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0363: [92m[OKAY][0m
g0363: cpu_lion ............... [92m[YES][0m ......fused_lion  [92m[OKAY][0m.............
g0363:  [92m[YES][0m ...... [92m[OKAY][0m
g0363: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0363: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0363: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0363: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0363: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: async_iocpu_lion  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0363: 
g0363: fused_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0363: ............. [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0363:  ....... [93m[NO][0mcpu_adam
g0363:  ...............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0363:  [92m[OKAY][0m
g0363: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: fused_lion ............. cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0363: ...... [92m[OKAY][0m
g0363: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0363: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0363: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0363: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0363: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0363: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0363: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0363: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0363: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0363: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0363: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0363: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0363: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0363: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0363: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0363: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0363: sparse_attn ............ [93m[NO][0m .......[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0363: [93m[NO][0m
g0363: ragged_opssparse_attn  .........................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0363: 
g0363: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0363: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0363: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0363: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0363: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0363: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0363: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0363: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0363: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0363: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0363: transformer spatial_inference............  [92m[YES][0m......  ......[92m[YES][0m  [92m[OKAY][0m......
g0363:  [92m[OKAY][0m
g0363: stochastic_transformer . transformer[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0363: ...... [92m[OKAY][0m
g0363: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0363: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0363: --------------------------------------------------
g0363: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0363: --------------------------------------------------
g0363: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0363: --------------------------------------------------
g0363: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0363: --------------------------------------------------
g0363: DeepSpeed general environment info:
g0363: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0363: torch version .................... 2.0.1+cu118
g0363: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0363: deepspeed info ................... 0.12.4, unknown, unknown
g0363: torch cuda version ............... 11.8
g0363: torch hip version ................ None
g0363: nvcc version ..................... 11.8
g0363: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0363: shared memory (/dev/shm) size .... 188.13 GB
g0363: DeepSpeed general environment info:
g0363: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0363: torch version .................... 2.0.1+cu118
g0363: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0363: deepspeed info ................... 0.12.4, unknown, unknown
g0363: torch cuda version ............... 11.8
g0363: torch hip version ................ None
g0363: nvcc version ..................... 11.8
g0363: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0363: shared memory (/dev/shm) size .... 188.13 GB
g0363: DeepSpeed general environment info:
g0363: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0363: torch version .................... 2.0.1+cu118
g0363: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0363: deepspeed info ................... 0.12.4, unknown, unknown
g0363: torch cuda version ............... 11.8
g0363: torch hip version ................ None
g0363: nvcc version ..................... 11.8
g0363: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0363: shared memory (/dev/shm) size .... 188.13 GB
g0363: DeepSpeed general environment info:
g0363: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0363: torch version .................... 2.0.1+cu118
g0363: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0363: deepspeed info ................... 0.12.4, unknown, unknown
g0363: torch cuda version ............... 11.8
g0363: torch hip version ................ None
g0363: nvcc version ..................... 11.8
g0363: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0363: shared memory (/dev/shm) size .... 188.13 GB
g0352: [2024-08-02 18:13:37,123] [INFO] [comm.py:637:init_distributed] cdb=None
g0352: [2024-08-02 18:13:37,124] [INFO] [comm.py:637:init_distributed] cdb=None
g0352: [2024-08-02 18:13:37,124] [INFO] [comm.py:637:init_distributed] cdb=None
g0352: [2024-08-02 18:13:37,124] [INFO] [comm.py:637:init_distributed] cdb=None
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0363: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0363: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0363: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0352: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [2024-08-02 18:13:37,224] [INFO] [comm.py:637:init_distributed] cdb=None
g0363: [2024-08-02 18:13:37,224] [INFO] [comm.py:637:init_distributed] cdb=None
g0363: [2024-08-02 18:13:37,226] [INFO] [comm.py:637:init_distributed] cdb=None
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [2024-08-02 18:13:37,234] [INFO] [comm.py:637:init_distributed] cdb=None
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0363: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0347: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0347: 
g0347: 
g0347: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0347: 
g0347: 
g0347: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0347: 
g0347: 
g0347: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0347:       runtime if needed. Op compatibility means that your system
g0347:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0347:       runtime if needed. Op compatibility means that your system
g0347:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0347:       runtime if needed. Op compatibility means that your system
g0347:       meet the required dependencies to JIT install the op.
g0347: 
g0347: 
g0347: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0347: 
g0347: 
g0347: JIT compiled ops requires ninjaJIT compiled ops requires ninjaJIT compiled ops requires ninja
g0347: 
g0347: 
g0347: --------------------------------------------------
g0347: DeepSpeed C++/CUDA extension op report
g0347: --------------------------------------------------
g0347: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0347:       runtime if needed. Op compatibility means that your system
g0347:       meet the required dependencies to JIT install the op.
g0347: --------------------------------------------------
g0347: JIT compiled ops requires ninja
g0347: ninjaninjaninjaninja   .................. .................................... ..................  [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m
g0347: [92m[OKAY][0m
g0347: 
g0347: 
g0347: ----------------------------------------------------------------------------------------------------
g0347: ----------------------------------------------------------------------------------------------------
g0347: 
g0347: op name
g0347: op nameop name   ................op name................................    installed................installedinstalled    ..installed....    compatible..compatible
g0347: compatible 
g0347: 
g0347: --------------------------------------------------compatible--------------------------------------------------
g0347: --------------------------------------------------
g0347: 
g0347: 
g0347: --------------------------------------------------
g0346: --------------------------------------------------
g0346: DeepSpeed C++/CUDA extension op report
g0346: --------------------------------------------------
g0346: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0346:       runtime if needed. Op compatibility means that your system
g0346:       meet the required dependencies to JIT install the op.
g0346: --------------------------------------------------
g0346: JIT compiled ops requires ninja
g0346: --------------------------------------------------
g0346: DeepSpeed C++/CUDA extension op report
g0346: --------------------------------------------------
g0346: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0346:       runtime if needed. Op compatibility means that your system
g0346:       meet the required dependencies to JIT install the op.
g0346: --------------------------------------------------
g0346: JIT compiled ops requires ninja
g0346: --------------------------------------------------
g0346: DeepSpeed C++/CUDA extension op report
g0346: --------------------------------------------------
g0346: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0346:       runtime if needed. Op compatibility means that your system
g0346:       meet the required dependencies to JIT install the op.
g0346: ----------------------------------------------------------------------------------------------------
g0346: JIT compiled ops requires ninja
g0346: 
g0346: DeepSpeed C++/CUDA extension op report
g0346: --------------------------------------------------
g0346: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0346:       runtime if needed. Op compatibility means that your system
g0346:       meet the required dependencies to JIT install the op.
g0346: --------------------------------------------------
g0346: JIT compiled ops requires ninja
g0346: ninjaninjaninja ninja .................. .................. ..................  [92m[OKAY][0m ..................[92m[OKAY][0m
g0346: [92m[OKAY][0m 
g0346: 
g0346: [92m[OKAY][0m--------------------------------------------------
g0346: --------------------------------------------------
g0346: --------------------------------------------------
g0346: 
g0346: --------------------------------------------------op name
g0346: op name op name ................ op name................ ................  installed installed................ installed  .. ..installed ..  compatible.. compatible
g0346:  compatible
g0346: compatible
g0346: --------------------------------------------------
g0346: --------------------------------------------------
g0346: --------------------------------------------------
g0346: --------------------------------------------------
g0346: 
g0347: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0347: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0347: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0347: async_io ............... [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0347: 
g0347: evoformer_attn ......... [93m[NO][0mfused_adam  ....................  [93m[NO][0m[92m[YES][0m
g0347:  ...... fused_lamb[92m[OKAY][0m 
g0347: ............. [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0347: [92m[YES][0m ...... [92m[OKAY][0m
g0347: cpu_adagrad fused_lion............  async_io.............[92m[YES][0m   [92m[YES][0m.....................   ......[92m[OKAY][0m[92m[YES][0m
g0347:   [92m[OKAY][0m......
g0347:  cpu_lion[92m[OKAY][0m ...............
g0347:  [92m[YES][0m ...... [92m[OKAY][0m
g0347: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0347: cpu_adamevoformer_attn  ........................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0347: 
g0347: fused_lambcpu_adagrad  .........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0347: 
g0347: cpu_lion ............... [92m[YES][0m ......fused_lion  [92m[OKAY][0m.............
g0347:  [92m[YES][0m ...... [92m[OKAY][0m
g0347: async_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0347: ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0347: ....... [93m[NO][0m
g0347: fused_lamb fused_adam.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0347: [92m[OKAY][0m
g0347: cpu_adam ............... [92m[YES][0m fused_lion......  [92m[OKAY][0m
g0347: .............cpu_adagrad  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0347:  [92m[OKAY][0m
g0347: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0347: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0347: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0347: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0346: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0346: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0346: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0346: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0346: fused_lamb ............. [92m[YES][0m async_io......  [92m[OKAY][0m...............
g0346:  [92m[YES][0m ...... [92m[OKAY][0m
g0346: async_iofused_lion  ............................ fused_adam [92m[YES][0m [92m[YES][0m ............. ...... ...... [92m[YES][0m [92m[OKAY][0m [92m[OKAY][0m......
g0346: 
g0346:  [92m[OKAY][0m
g0346: fused_adamcpu_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0346: 
g0346: async_iocpu_adagradcpu_adam   ...........................  ...............[92m[YES][0m[92m[YES][0m   [92m[YES][0m............   ......[92m[OKAY][0m[92m[OKAY][0m 
g0346: 
g0346: [92m[OKAY][0m
g0346: cpu_lioncpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ......fused_adam......   [92m[OKAY][0m.............[92m[OKAY][0m
g0346:  
g0346: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0346:  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0346:  cpu_adam...... evoformer_attn ............... [92m[OKAY][0m .........
g0346: [92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0346: [93m[NO][0m
g0346: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_adagrad
g0346:  fused_lamb............ evoformer_attn ............. [92m[YES][0m ......... [92m[YES][0m ...... [93m[NO][0m ...... [92m[OKAY][0m .......
g0346: [92m[OKAY][0m 
g0346: [93m[NO][0m
g0346: cpu_lion ............... fused_lamb[92m[YES][0m  ...................  [92m[YES][0mfused_lion[92m[OKAY][0m  
g0346: ...................  [92m[OKAY][0m[92m[YES][0m
g0346:  ...... [92m[OKAY][0m
g0346: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0346: evoformer_attnfused_lion  ......................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0346: 
g0346: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0347: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0347: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0347: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0347: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0347: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0347: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0347: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0347: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0347: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0347: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: ragged_ops ............. [92m[YES][0m ......[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0 
g0347: [92m[OKAY][0m
g0347: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0347: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0347: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0347: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0347: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0346: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0346: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0346: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0346: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0347: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0347: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0347: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0347: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0347: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0347: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0347: --------------------------------------------------
g0346: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0347: --------------------------------------------------
g0346: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0347: --------------------------------------------------
g0347: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0347: --------------------------------------------------
g0347: DeepSpeed general environment info:
g0347: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0347: torch version .................... 2.0.1+cu118
g0347: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0347: deepspeed info ................... 0.12.4, unknown, unknown
g0347: torch cuda version ............... 11.8
g0347: torch hip version ................ None
g0347: nvcc version ..................... 11.8
g0347: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0347: shared memory (/dev/shm) size .... 188.13 GB
g0346: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0346: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0346: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0346: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0347: DeepSpeed general environment info:
g0347: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0347: torch version .................... 2.0.1+cu118
g0347: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0347: deepspeed info ................... 0.12.4, unknown, unknown
g0347: torch cuda version ............... 11.8
g0347: torch hip version ................ None
g0347: nvcc version ..................... 11.8
g0347: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0347: shared memory (/dev/shm) size .... 188.13 GB
g0347: DeepSpeed general environment info:
g0347: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0347: torch version .................... 2.0.1+cu118
g0347: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0347: deepspeed info ................... 0.12.4, unknown, unknown
g0347: torch cuda version ............... 11.8
g0347: torch hip version ................ None
g0347: nvcc version ..................... 11.8
g0347: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0347: shared memory (/dev/shm) size .... 188.13 GB
g0347: DeepSpeed general environment info:
g0347: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0347: torch version .................... 2.0.1+cu118
g0347: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0347: deepspeed info ................... 0.12.4, unknown, unknown
g0347: torch cuda version ............... 11.8
g0347: torch hip version ................ None
g0347: nvcc version ..................... 11.8
g0347: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0347: shared memory (/dev/shm) size .... 188.13 GB
g0346: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0346: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0346: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0346: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0346: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0346: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0346: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0346: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0346: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0346: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0346: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0346: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0346: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0346: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0346: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0346: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0346: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0346: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0346: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0346: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0346: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0346: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0346: --------------------------------------------------
g0346: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0346: --------------------------------------------------
g0346: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0346: --------------------------------------------------
g0346: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0346: --------------------------------------------------
g0346: DeepSpeed general environment info:
g0346: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0346: torch version .................... 2.0.1+cu118
g0346: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0346: deepspeed info ................... 0.12.4, unknown, unknown
g0346: torch cuda version ............... 11.8
g0346: torch hip version ................ None
g0346: nvcc version ..................... 11.8
g0346: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0346: shared memory (/dev/shm) size .... 188.13 GB
g0346: DeepSpeed general environment info:
g0346: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0346: torch version .................... 2.0.1+cu118
g0346: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0346: deepspeed info ................... 0.12.4, unknown, unknown
g0346: torch cuda version ............... 11.8
g0346: torch hip version ................ None
g0346: nvcc version ..................... 11.8
g0346: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0346: shared memory (/dev/shm) size .... 188.13 GB
g0346: DeepSpeed general environment info:
g0346: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0346: torch version .................... 2.0.1+cu118
g0346: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0346: deepspeed info ................... 0.12.4, unknown, unknown
g0346: torch cuda version ............... 11.8
g0346: torch hip version ................ None
g0346: nvcc version ..................... 11.8
g0346: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0346: shared memory (/dev/shm) size .... 188.13 GB
g0346: DeepSpeed general environment info:
g0346: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0346: torch version .................... 2.0.1+cu118
g0346: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0346: deepspeed info ................... 0.12.4, unknown, unknown
g0346: torch cuda version ............... 11.8
g0346: torch hip version ................ None
g0346: nvcc version ..................... 11.8
g0346: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0346: shared memory (/dev/shm) size .... 188.13 GB
g0347: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0347: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0347: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0347: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0346: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0346: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0346: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0346: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0358: --------------------------------------------------
g0358: DeepSpeed C++/CUDA extension op report
g0358: --------------------------------------------------
g0358: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0358:       runtime if needed. Op compatibility means that your system
g0358:       meet the required dependencies to JIT install the op.
g0358: --------------------------------------------------
g0358: JIT compiled ops requires ninja
g0358: --------------------------------------------------
g0358: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0358: 
g0358: --------------------------------------------------
g0358: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0358:       runtime if needed. Op compatibility means that your system
g0358:       meet the required dependencies to JIT install the op.
g0358: 
g0358: ----------------------------------------------------------------------------------------------------
g0358: 
g0358: JIT compiled ops requires ninjaNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0358:       runtime if needed. Op compatibility means that your system
g0358:       meet the required dependencies to JIT install the op.
g0358: 
g0358: --------------------------------------------------
g0358: JIT compiled ops requires ninja
g0358: --------------------------------------------------
g0358: DeepSpeed C++/CUDA extension op report
g0358: --------------------------------------------------
g0358: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0358:       runtime if needed. Op compatibility means that your system
g0358:       meet the required dependencies to JIT install the op.
g0358: --------------------------------------------------
g0358: JIT compiled ops requires ninja
g0358: ninjaninjaninja   ninja......................................................   [92m[OKAY][0m ..................[92m[OKAY][0m
g0358: [92m[OKAY][0m 
g0358: 
g0358: [92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0358: 
g0358: --------------------------------------------------
g0358: 
g0358: op nameop name--------------------------------------------------op name  
g0358:  ................................................  op nameinstalled  installed installed..................    ....installedcompatible   
g0358: compatible..compatible
g0358:  --------------------------------------------------
g0358: compatible--------------------------------------------------
g0358: 
g0358: 
g0358: --------------------------------------------------
g0358: --------------------------------------------------
g0347: [2024-08-02 18:13:37,570] [INFO] [comm.py:637:init_distributed] cdb=None
g0347: [2024-08-02 18:13:37,570] [INFO] [comm.py:637:init_distributed] cdb=None
g0347: [2024-08-02 18:13:37,571] [INFO] [comm.py:637:init_distributed] cdb=None
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0347: [2024-08-02 18:13:37,582] [INFO] [comm.py:637:init_distributed] cdb=None
g0358: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0358: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0358: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: async_io ...............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m...... 
g0358: [92m[OKAY][0m
g0358: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0358: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0358: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHasync_io
g0358:  evoformer_attn...............  .........[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0358: [93m[NO][0m
g0358: fused_lambfused_adam  ..........................  [92m[YES][0m[92m[YES][0m  ...... ......[92m[OKAY][0m 
g0358: [92m[OKAY][0m
g0358: cpu_adam ............... fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0358: ...... [92m[OKAY][0mcpu_adagrad
g0358:  ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0358: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0358: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0346: [2024-08-02 18:13:37,586] [INFO] [comm.py:637:init_distributed] cdb=None
g0346: [2024-08-02 18:13:37,587] [INFO] [comm.py:637:init_distributed] cdb=None
g0347: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0346: [2024-08-02 18:13:37,588] [INFO] [comm.py:637:init_distributed] cdb=None
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0358: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0358: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0358: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0346: [2024-08-02 18:13:37,599] [INFO] [comm.py:637:init_distributed] cdb=None
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0346: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0358: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0358: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0358: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0358: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0358: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0358: sparse_attn ............ [93m[NO][0m .......[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0358: [93m[NO][0m
g0358: sparse_attn ............ [93m[NO][0mragged_ops .......  .............[93m[NO][0m 
g0358: [92m[YES][0m ...... [92m[OKAY][0m
g0358: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0358: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0358: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0358: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0358: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0358: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0358: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0358: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0358: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0358: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0358: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0358: --------------------------------------------------
g0358: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0358: --------------------------------------------------
g0358: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0358: --------------------------------------------------
g0358: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0358: --------------------------------------------------
g0358: DeepSpeed general environment info:
g0358: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0358: torch version .................... 2.0.1+cu118
g0358: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0358: deepspeed info ................... 0.12.4, unknown, unknown
g0358: torch cuda version ............... 11.8
g0358: torch hip version ................ None
g0358: nvcc version ..................... 11.8
g0358: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0358: shared memory (/dev/shm) size .... 188.13 GB
g0358: DeepSpeed general environment info:
g0358: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0358: torch version .................... 2.0.1+cu118
g0358: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0358: deepspeed info ................... 0.12.4, unknown, unknown
g0358: torch cuda version ............... 11.8
g0358: torch hip version ................ None
g0358: nvcc version ..................... 11.8
g0358: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0358: shared memory (/dev/shm) size .... 188.13 GB
g0358: DeepSpeed general environment info:
g0358: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0358: torch version .................... 2.0.1+cu118
g0358: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0358: deepspeed info ................... 0.12.4, unknown, unknown
g0358: torch cuda version ............... 11.8
g0358: torch hip version ................ None
g0358: nvcc version ..................... 11.8
g0358: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0358: shared memory (/dev/shm) size .... 188.13 GB
g0358: DeepSpeed general environment info:
g0358: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0358: torch version .................... 2.0.1+cu118
g0358: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0358: deepspeed info ................... 0.12.4, unknown, unknown
g0358: torch cuda version ............... 11.8
g0358: torch hip version ................ None
g0358: nvcc version ..................... 11.8
g0358: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0358: shared memory (/dev/shm) size .... 188.13 GB
g0358: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0358: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0358: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0358: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0358: [2024-08-02 18:13:37,756] [INFO] [comm.py:637:init_distributed] cdb=None
g0358: [2024-08-02 18:13:37,756] [INFO] [comm.py:637:init_distributed] cdb=None
g0358: [2024-08-02 18:13:37,756] [INFO] [comm.py:637:init_distributed] cdb=None
g0358: [2024-08-02 18:13:37,757] [INFO] [comm.py:637:init_distributed] cdb=None
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0358: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0364: 
g0364: 
g0364: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0364: DeepSpeed C++/CUDA extension op report
g0364: 
g0364: ----------------------------------------------------------------------------------------------------
g0364: --------------------------------------------------
g0364: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0364:       runtime if needed. Op compatibility means that your system
g0364:       meet the required dependencies to JIT install the op.
g0364: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0364:       runtime if needed. Op compatibility means that your system
g0364:       meet the required dependencies to JIT install the op.
g0364: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0364:       runtime if needed. Op compatibility means that your system
g0364:       meet the required dependencies to JIT install the op.
g0364: --------------------------------------------------
g0364: 
g0364: ----------------------------------------------------------------------------------------------------
g0364: JIT compiled ops requires ninja
g0364: JIT compiled ops requires ninja
g0364: JIT compiled ops requires ninja
g0364: 
g0364: --------------------------------------------------
g0364: DeepSpeed C++/CUDA extension op report
g0364: --------------------------------------------------
g0364: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0364:       runtime if needed. Op compatibility means that your system
g0364:       meet the required dependencies to JIT install the op.
g0364: --------------------------------------------------
g0364: JIT compiled ops requires ninja
g0364: ninjaninjaninja ninja  .................................... ..................  .................. [92m[OKAY][0m[92m[OKAY][0m [92m[OKAY][0m
g0364: 
g0364: [92m[OKAY][0m
g0364: ----------------------------------------------------------------------------------------------------
g0364: --------------------------------------------------
g0364: 
g0364: 
g0364: --------------------------------------------------op nameop name
g0364: op name   op name................................................    ................installedinstalledinstalled    installed....  .. ..compatible  compatiblecompatible
g0364: compatible
g0364: 
g0364: 
g0364: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0364: 
g0364: 
g0364: 
g0364: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0364: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0364: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0364: async_io ............... [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0364: 
g0364: evoformer_attn ......... [93m[NO][0m fused_adam.......  .............[93m[NO][0m 
g0364: [92m[YES][0m ...... fused_lamb[92m[OKAY][0m 
g0364: ............. [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0364: [92m[YES][0m ...... [92m[OKAY][0m
g0364: cpu_adagrad ............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0364:  [92m[OKAY][0m
g0364: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0364: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0364: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0364: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0364: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: async_iocpu_adam  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0364: 
g0364: cpu_adagrad ............ [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0364:  ...... cpu_lion[92m[OKAY][0m 
g0364: ............... [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0364: [92m[YES][0m ...... [92m[OKAY][0m
g0364: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_adagrad
g0364:  ............ evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0364: ....... [93m[NO][0m
g0364: cpu_lion ...............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0364:  [92m[OKAY][0m
g0364: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0364: fused_lion evoformer_attn.............  .........[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0364: [93m[NO][0m
g0364: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0364: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0364: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0364: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0364: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0364: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0364: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0364: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0364: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0364: sparse_attn ............ [93m[NO][0m ....... ragged_ops[93m[NO][0m
g0364:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0364: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0364: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0364: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0364: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0364: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0364: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0364: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0364: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0364: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0364: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0364: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0364: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0364: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0364: --------------------------------------------------
g0364: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0364: --------------------------------------------------
g0364: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0364: --------------------------------------------------
g0364: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0364: --------------------------------------------------
g0364: DeepSpeed general environment info:
g0364: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0364: torch version .................... 2.0.1+cu118
g0364: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0364: deepspeed info ................... 0.12.4, unknown, unknown
g0364: torch cuda version ............... 11.8
g0364: torch hip version ................ None
g0364: nvcc version ..................... 11.8
g0364: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0364: shared memory (/dev/shm) size .... 188.13 GB
g0364: DeepSpeed general environment info:
g0364: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0364: torch version .................... 2.0.1+cu118
g0364: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0364: deepspeed info ................... 0.12.4, unknown, unknown
g0364: torch cuda version ............... 11.8
g0364: torch hip version ................ None
g0364: nvcc version ..................... 11.8
g0364: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0364: shared memory (/dev/shm) size .... 188.13 GB
g0364: DeepSpeed general environment info:
g0364: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0364: torch version .................... 2.0.1+cu118
g0364: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0364: deepspeed info ................... 0.12.4, unknown, unknown
g0364: torch cuda version ............... 11.8
g0364: torch hip version ................ None
g0364: nvcc version ..................... 11.8
g0364: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0364: shared memory (/dev/shm) size .... 188.13 GB
g0364: DeepSpeed general environment info:
g0364: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0364: torch version .................... 2.0.1+cu118
g0364: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0364: deepspeed info ................... 0.12.4, unknown, unknown
g0364: torch cuda version ............... 11.8
g0364: torch hip version ................ None
g0364: nvcc version ..................... 11.8
g0364: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0364: shared memory (/dev/shm) size .... 188.13 GB
g0364: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0364: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0364: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0364: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0364: [2024-08-02 18:13:38,109] [INFO] [comm.py:637:init_distributed] cdb=None
g0364: [2024-08-02 18:13:38,110] [INFO] [comm.py:637:init_distributed] cdb=None
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: [2024-08-02 18:13:38,120] [INFO] [comm.py:637:init_distributed] cdb=None
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: > setting tensorboard ...
g0364: [2024-08-02 18:13:38,438] [INFO] [comm.py:637:init_distributed] cdb=None
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0364: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0345-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0345: > initialized tensor model parallel with size 1
g0345: > initialized pipeline model parallel with size 8
g0345: > setting random seeds to 1234 ...
g0345: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0345: > compiling dataset index builder ...
g0345: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0345: make: Nothing to be done for 'default'.
g0345: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0345: >>> done with dataset index builder. Compilation time: 0.080 seconds
g0345: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0345: > compiling and loading fused kernels ...
g0345: Detected CUDA files, patching ldflags
g0345: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0345: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0345: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0345: ninja: no work to do.
g0345: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0345: Detected CUDA files, patching ldflags
g0345: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0345: Building extension module scaled_masked_softmax_cuda...
g0345: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0345: ninja: no work to do.
g0345: Loading extension module scaled_masked_softmax_cuda...
g0345: Detected CUDA files, patching ldflags
g0345: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0345: Building extension module scaled_softmax_cuda...
g0345: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0345: ninja: no work to do.
g0345: Loading extension module scaled_softmax_cuda...
g0345: >>> done with compiling and loading fused kernels. Compilation time: 7.404 seconds
g0345: time to initialize megatron (seconds): 22.171
g0345: [after megatron is initialized] datetime: 2024-08-02 18:13:48 
g0347: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0352: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0362: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0345: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0363: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0346: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0364: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0358: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0345: wandb: Tracking run with wandb version 0.17.5
g0345: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-cga2soa1
g0345: wandb: Run `wandb offline` to turn off syncing.
g0345: wandb: Syncing run g0345.abci.local
g0345: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0345: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/cga2soa1
g0352: wandb: Tracking run with wandb version 0.17.5
g0352: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-oecjktpo
g0352: wandb: Run `wandb offline` to turn off syncing.
g0352: wandb: Syncing run g0352.abci.local
g0352: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0352: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/oecjktpo
g0347: wandb: Tracking run with wandb version 0.17.5
g0347: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-npme3lce
g0347: wandb: Run `wandb offline` to turn off syncing.
g0347: wandb: Syncing run g0347.abci.local
g0347: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0347: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/npme3lce
g0362: wandb: Tracking run with wandb version 0.17.5
g0362: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-b78onmlj
g0362: wandb: Run `wandb offline` to turn off syncing.
g0362: wandb: Syncing run g0362.abci.local
g0362: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0362: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/b78onmlj
g0363: wandb: Tracking run with wandb version 0.17.5
g0363: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-98i5kgab
g0363: wandb: Run `wandb offline` to turn off syncing.
g0363: wandb: Syncing run g0363.abci.local
g0363: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0363: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/98i5kgab
g0364: wandb: Tracking run with wandb version 0.17.5
g0364: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-mwy2xl07
g0364: wandb: Run `wandb offline` to turn off syncing.
g0364: wandb: Syncing run g0364.abci.local
g0364: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0364: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/mwy2xl07
g0358: wandb: Tracking run with wandb version 0.17.5
g0358: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-7o4yxyzv
g0358: wandb: Run `wandb offline` to turn off syncing.
g0358: wandb: Syncing run g0358.abci.local
g0358: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0358: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/7o4yxyzv
g0346: wandb: Tracking run with wandb version 0.17.5
g0346: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181350-fpfx1y5t
g0346: wandb: Run `wandb offline` to turn off syncing.
g0346: wandb: Syncing run g0346.abci.local
g0346: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0346: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/fpfx1y5t
g0345: building GPT model ...
g0345: [2024-08-02 18:13:51,159] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0345: [2024-08-02 18:13:51,160] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0345: [2024-08-02 18:13:51,161] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 42.47 GB, percent = 11.3%
g0345: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0345: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0345: [2024-08-02 18:13:51,693] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0345: stage=0 layers=5
g0345:      0: _to_float16
g0345:      1: EmbeddingPipe
g0345:      2: ParallelTransformerLayerPipe
g0345:      3: ParallelTransformerLayerPipe
g0345:      4: ParallelTransformerLayerPipe
g0345: stage=1 layers=3
g0345:      5: ParallelTransformerLayerPipe
g0345:      6: ParallelTransformerLayerPipe
g0345:      7: ParallelTransformerLayerPipe
g0345: stage=2 layers=3
g0345:      8: ParallelTransformerLayerPipe
g0345:      9: ParallelTransformerLayerPipe
g0345:     10: ParallelTransformerLayerPipe
g0345: stage=3 layers=3
g0345:     11: ParallelTransformerLayerPipe
g0345:     12: ParallelTransformerLayerPipe
g0345:     13: ParallelTransformerLayerPipe
g0345: stage=4 layers=3
g0345:     14: ParallelTransformerLayerPipe
g0345:     15: ParallelTransformerLayerPipe
g0345:     16: ParallelTransformerLayerPipe
g0345: stage=5 layers=3
g0345:     17: ParallelTransformerLayerPipe
g0345:     18: ParallelTransformerLayerPipe
g0345:     19: ParallelTransformerLayerPipe
g0345: stage=6 layers=3
g0345:     20: ParallelTransformerLayerPipe
g0345:     21: ParallelTransformerLayerPipe
g0345:     22: ParallelTransformerLayerPipe
g0345: stage=7 layers=3
g0345:     23: ParallelTransformerLayerPipe
g0345:     24: MixedFusedRMSNorm
g0345:     25: LMHeadPipe
g0345:   loss: CrossEntropy
g0362:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0363:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0364:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0352:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0358:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0347:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0346:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0345: [2024-08-02 18:13:52,179] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0345: [2024-08-02 18:13:52,180] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0345: [2024-08-02 18:13:52,180] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 42.53 GB, percent = 11.3%
g0345:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0345: setting training iterations to 10000000
g0345: > learning rate decay style: cosine
g0345: DeepSpeed is enabled.
g0345: [2024-08-02 18:13:52,181] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0362: [2024-08-02 18:13:52,320] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0362: [2024-08-02 18:13:52,320] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0362: [2024-08-02 18:13:52,320] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0362: [2024-08-02 18:13:52,320] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0346: [2024-08-02 18:13:52,321] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0346: [2024-08-02 18:13:52,321] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0346: [2024-08-02 18:13:52,321] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0346: [2024-08-02 18:13:52,321] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0352: [2024-08-02 18:13:52,325] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0352: [2024-08-02 18:13:52,325] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0352: [2024-08-02 18:13:52,325] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0352: [2024-08-02 18:13:52,326] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0364: [2024-08-02 18:13:52,330] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0364: [2024-08-02 18:13:52,330] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0364: [2024-08-02 18:13:52,330] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0364: [2024-08-02 18:13:52,330] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0363: [2024-08-02 18:13:52,334] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0363: [2024-08-02 18:13:52,334] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0363: [2024-08-02 18:13:52,334] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0363: [2024-08-02 18:13:52,334] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0358: [2024-08-02 18:13:52,340] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0358: [2024-08-02 18:13:52,340] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0358: [2024-08-02 18:13:52,341] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0358: [2024-08-02 18:13:52,341] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0347: [2024-08-02 18:13:52,365] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0347: [2024-08-02 18:13:52,365] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0347: [2024-08-02 18:13:52,365] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0347: [2024-08-02 18:13:52,365] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0345: [2024-08-02 18:13:52,395] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0345: [2024-08-02 18:13:52,397] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0345: [2024-08-02 18:13:52,397] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0345: [2024-08-02 18:13:52,397] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0345: [2024-08-02 18:13:52,398] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0345: [2024-08-02 18:13:52,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0345: [2024-08-02 18:13:52,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0345: [2024-08-02 18:13:52,425] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0345: [2024-08-02 18:13:52,425] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0345: [2024-08-02 18:13:52,425] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0345: [2024-08-02 18:13:52,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fb1aa22fd90>
g0345: [2024-08-02 18:13:52,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: [2024-08-02 18:13:52,426] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0345: [2024-08-02 18:13:52,427] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0345:     "partition_activations": false, 
g0345:     "contiguous_memory_optimization": false, 
g0345:     "cpu_checkpointing": false, 
g0345:     "number_checkpoints": null, 
g0345:     "synchronize_checkpoint_boundary": false, 
g0345:     "profile": false
g0345: }
g0345: [2024-08-02 18:13:52,427] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0345: [2024-08-02 18:13:52,427] [INFO] [config.py:983:print]   amp_enabled .................. False
g0345: [2024-08-02 18:13:52,427] [INFO] [config.py:983:print]   amp_params ................... False
g0345: [2024-08-02 18:13:52,428] [INFO] [config.py:983:print]   autotuning_config ............ {
g0345:     "enabled": false, 
g0345:     "start_step": null, 
g0345:     "end_step": null, 
g0345:     "metric_path": null, 
g0345:     "arg_mappings": null, 
g0345:     "metric": "throughput", 
g0345:     "model_info": null, 
g0345:     "results_dir": "autotuning_results", 
g0345:     "exps_dir": "autotuning_exps", 
g0345:     "overwrite": true, 
g0345:     "fast": true, 
g0345:     "start_profile_step": 3, 
g0345:     "end_profile_step": 5, 
g0345:     "tuner_type": "gridsearch", 
g0345:     "tuner_early_stopping": 5, 
g0345:     "tuner_num_trials": 50, 
g0345:     "model_info_path": null, 
g0345:     "mp_size": 1, 
g0345:     "max_train_batch_size": null, 
g0345:     "min_train_batch_size": 1, 
g0345:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0345:     "min_train_micro_batch_size_per_gpu": 1, 
g0345:     "num_tuning_micro_batch_sizes": 3
g0345: }
g0345: [2024-08-02 18:13:52,428] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0345: [2024-08-02 18:13:52,428] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0345: [2024-08-02 18:13:52,428] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0345: [2024-08-02 18:13:52,428] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0345: [2024-08-02 18:13:52,429] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb1a80ab810>
g0345: [2024-08-02 18:13:52,429] [INFO] [config.py:983:print]   communication_data_type ...... None
g0345: [2024-08-02 18:13:52,429] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0345: [2024-08-02 18:13:52,429] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0345: [2024-08-02 18:13:52,429] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0345: [2024-08-02 18:13:52,429] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0345: [2024-08-02 18:13:52,430] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0345: [2024-08-02 18:13:52,430] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0345: [2024-08-02 18:13:52,430] [INFO] [config.py:983:print]   disable_allgather ............ False
g0345: [2024-08-02 18:13:52,430] [INFO] [config.py:983:print]   dump_state ................... False
g0345: [2024-08-02 18:13:52,430] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0345: [2024-08-02 18:13:52,430] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0345: [2024-08-02 18:13:52,431] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0345: [2024-08-02 18:13:52,431] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0345: [2024-08-02 18:13:52,431] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0345: [2024-08-02 18:13:52,431] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0345: [2024-08-02 18:13:52,431] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0345: [2024-08-02 18:13:52,431] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0345: [2024-08-02 18:13:52,432] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0345: [2024-08-02 18:13:52,432] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0345: [2024-08-02 18:13:52,432] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0345:     "enabled": false, 
g0345:     "recompute_fwd_factor": 0.0, 
g0345:     "profile_step": 1, 
g0345:     "module_depth": -1, 
g0345:     "top_modules": 1, 
g0345:     "detailed": true, 
g0345:     "output_file": null
g0345: }
g0345: [2024-08-02 18:13:52,432] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0345: [2024-08-02 18:13:52,432] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0345: [2024-08-02 18:13:52,432] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0345: [2024-08-02 18:13:52,433] [INFO] [config.py:983:print]   global_rank .................. 0
g0345: [2024-08-02 18:13:52,433] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0345: [2024-08-02 18:13:52,433] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0345: [2024-08-02 18:13:52,433] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0345: [2024-08-02 18:13:52,433] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0345: [2024-08-02 18:13:52,433] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0345: [2024-08-02 18:13:52,434] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0345: [2024-08-02 18:13:52,434] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0345: [2024-08-02 18:13:52,434] [INFO] [config.py:983:print]   loss_scale ................... 0
g0345: [2024-08-02 18:13:52,434] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0345: [2024-08-02 18:13:52,434] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0345: [2024-08-02 18:13:52,434] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0345: [2024-08-02 18:13:52,435] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0345: [2024-08-02 18:13:52,435] [INFO] [config.py:983:print]   nebula_config ................ {
g0345:     "enabled": false, 
g0345:     "persistent_storage_path": null, 
g0345:     "persistent_time_interval": 100, 
g0345:     "num_of_version_in_retention": 2, 
g0345:     "enable_nebula_load": true, 
g0345:     "load_path": null
g0345: }
g0345: [2024-08-02 18:13:52,435] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0345: [2024-08-02 18:13:52,435] [INFO] [config.py:983:print]   optimizer_name ............... None
g0345: [2024-08-02 18:13:52,435] [INFO] [config.py:983:print]   optimizer_params ............. None
g0345: [2024-08-02 18:13:52,436] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0345: [2024-08-02 18:13:52,436] [INFO] [config.py:983:print]   pld_enabled .................. False
g0345: [2024-08-02 18:13:52,436] [INFO] [config.py:983:print]   pld_params ................... False
g0345: [2024-08-02 18:13:52,436] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0345: [2024-08-02 18:13:52,436] [INFO] [config.py:983:print]   scheduler_name ............... None
g0345: [2024-08-02 18:13:52,436] [INFO] [config.py:983:print]   scheduler_params ............. None
g0345: [2024-08-02 18:13:52,437] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0345: [2024-08-02 18:13:52,437] [INFO] [config.py:983:print]   sparse_attention ............. None
g0345: [2024-08-02 18:13:52,437] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0345: [2024-08-02 18:13:52,437] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0345: [2024-08-02 18:13:52,437] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0345: [2024-08-02 18:13:52,437] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0345: [2024-08-02 18:13:52,437] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0345: [2024-08-02 18:13:52,438] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0345: [2024-08-02 18:13:52,438] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0345: [2024-08-02 18:13:52,438] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0345: [2024-08-02 18:13:52,438] [INFO] [config.py:983:print]   world_size ................... 4
g0345: [2024-08-02 18:13:52,438] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0345: [2024-08-02 18:13:52,438] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0345: [2024-08-02 18:13:52,439] [INFO] [config.py:983:print]   zero_enabled ................. False
g0345: [2024-08-02 18:13:52,439] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0345: [2024-08-02 18:13:52,439] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0345: [2024-08-02 18:13:52,439] [INFO] [config.py:969:print_user_config]   json = {
g0345:     "train_batch_size": 128, 
g0345:     "train_micro_batch_size_per_gpu": 1, 
g0345:     "steps_per_print": 10, 
g0345:     "zero_optimization": {
g0345:         "stage": 0
g0345:     }, 
g0345:     "gradient_clipping": 1.0, 
g0345:     "prescale_gradients": true, 
g0345:     "fp16": {
g0345:         "enabled": true, 
g0345:         "loss_scale": 0, 
g0345:         "loss_scale_window": 500, 
g0345:         "hysteresis": 2, 
g0345:         "min_loss_scale": 1, 
g0345:         "initial_scale_power": 11
g0345:     }, 
g0345:     "wall_clock_breakdown": false
g0345: }
g0345: [2024-08-02 18:13:52,439] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0345: [2024-08-02 18:13:52,440] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0345: [2024-08-02 18:13:53,134] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0363: [2024-08-02 18:13:53,135] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0347: [2024-08-02 18:13:53,135] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0358: [2024-08-02 18:13:53,135] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0362: [2024-08-02 18:13:53,136] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0346: [2024-08-02 18:13:53,136] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0352: [2024-08-02 18:13:53,136] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0364: [2024-08-02 18:13:53,136] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0363: [2024-08-02 18:13:53,843] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0364: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0363: [2024-08-02 18:13:53,843] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0364: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0364: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0362: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0363: [2024-08-02 18:13:53,843] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0358: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0358: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0362: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0358: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0358: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0363: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0345: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0352: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0352: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0352: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0347: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0346: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0347: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0346: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0347: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0346: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0347: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0346: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0352: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0364: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0362: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0362: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0345: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0345: [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0345: WARNING: could not find the metadata file /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase [2024-08-02 18:13:53,844] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0345: 
g0345:     will not load any checkpoints and will start from random
g0364: (min, max) time across ranks (ms):
g0364:     load-checkpoint ................................: (2.65, 3.20)
g0345: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-02 18:13:53 
g0345: > building train, validation, and test datasets ...
g0345:  > datasets target sizes (minimum size):
g0345:     train:      1280000000
g0345:     validation: 128012800
g0345:     test:       12800
g0345: > building train, validation, and test datasets for GPT ...
g0345: Single data path provided for train, valid & test
g0345:  > building dataset index ...
g0345:     reading sizes...
g0345:     reading pointers...
g0345:     reading document index...
g0345:     creating numpy buffer of mmap...
g0345:     creating memory view of numpy buffer...
g0345:  > finished creating indexed dataset in 0.039441 seconds
g0345:     number of documents: 2237032
g0345:  > dataset split:
g0345:     train:
g0345:      document indices in [0, 2122943) total of 2122943 documents
g0345:     validation:
g0345:      document indices in [2122943, 2234795) total of 111852 documents
g0345:     test:
g0345:      document indices in [2234795, 2237032) total of 2237 documents
g0345:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0345:  > only one epoch required, setting separate_last_epoch to False
g0345:  > elasped time to build and save doc-idx mapping (seconds): 0.081810
g0345:     using:
g0345:      number of documents:       2122943
g0345:      number of epochs:          1
g0345:      sequence length:           2048
g0345:      total number of samples:   10749554
g0345:  > elasped time to build and save sample-idx mapping (seconds): 0.336322
g0345:  > building shuffle index with split [0, 10749554) and [10749554, 10749554) ...
g0345:  > elasped time to build and save shuffle-idx mapping (seconds): 0.454657
g0345:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3596e0161783fa35db8c4808febe221a_doc_idx.npy
g0345:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3596e0161783fa35db8c4808febe221a_sample_idx.npy
g0345:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3596e0161783fa35db8c4808febe221a_shuffle_idx.npy
g0345:     loaded indexed file in 0.014 seconds
g0345:     total number of samples: 10749555
g0345:     total number of epochs: 1
g0345:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0345:  > last epoch number of samples (49488) is smaller than 80% of number of samples per epoch (563715), setting separate_last_epoch to True
g0345:  > elasped time to build and save doc-idx mapping (seconds): 1.593942
g0345:     using:
g0345:      number of documents:       111852
g0345:      number of epochs:          228
g0345:      sequence length:           2048
g0345:      total number of samples:   128527027
g0345:  > elasped time to build and save sample-idx mapping (seconds): 2.488844
g0345:  > building shuffle index with split [0, 127963312) and [127963312, 128527027) ...
g0345:  > elasped time to build and save shuffle-idx mapping (seconds): 7.628053
g0345:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/f33b588cfa0bd03d2d3e7adf89988baa_doc_idx.npy
g0345:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/f33b588cfa0bd03d2d3e7adf89988baa_sample_idx.npy
g0345:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/f33b588cfa0bd03d2d3e7adf89988baa_shuffle_idx.npy
g0345:     loaded indexed file in 0.019 seconds
g0345:     total number of samples: 128527028
g0345:     total number of epochs: 228
g0345:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0345:  > last epoch number of samples (5567) is smaller than 80% of number of samples per epoch (7233), setting separate_last_epoch to True
g0345:  > elasped time to build and save doc-idx mapping (seconds): 0.001569
g0345:     using:
g0345:      number of documents:       2237
g0345:      number of epochs:          2
g0345:      sequence length:           2048
g0345:      total number of samples:   14467
g0345:  > elasped time to build and save sample-idx mapping (seconds): 0.001706
g0345:  > building shuffle index with split [0, 7233) and [7233, 14467) ...
g0345:  > elasped time to build and save shuffle-idx mapping (seconds): 0.001799
g0345:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_doc_idx.npy
g0345:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_sample_idx.npy
g0345:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_shuffle_idx.npy
g0345:     loaded indexed file in 0.003 seconds
g0345:     total number of samples: 14468
g0345:     total number of epochs: 2
g0345: > finished creating GPT datasets ...
g0345: [after dataloaders are built] datetime: 2024-08-02 18:14:08 
g0345: done with setup ...
g0345: training ...
g0364: (min, max) time across ranks (ms):
g0364:     model-and-optimizer-setup ......................: (2857.25, 2863.11)
g0364:     train/valid/test-data-iterators-setup ..........: (15023.06, 15117.82)
g0345: [before the start of training step] datetime: 2024-08-02 18:14:09 
g0345: [2024-08-02 18:15:00,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.5728640000000002e-07, 1.5728640000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 10 loss: 10.5452 iter time (s): 5.064 samples/sec: 25.278
g0364:  iteration       10/10000000 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 5100.6 | learning rate: 1.573E-07 | global batch size:   128 | lm loss: 1.055340E+01 | loss scale: 2048.0 | grad norm: 7.831 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.095 | tokens per gpu per second (tgs): 1606.099 | TFLOPs: 12.92 |
g0364: [Rank 28] (after 10 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0345: [Rank 0] (after 10 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11650.0 | max reserved: 11650.0
g0352: [Rank 12] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7198.0 | max reserved: 7198.0
g0362: [Rank 20] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5410.0 | max reserved: 5410.0
g0346: [Rank 4] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8986.0 | max reserved: 8986.0
g0363: [Rank 24] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 4662.0 | max reserved: 4662.0
g0358: [Rank 16] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6304.0 | max reserved: 6304.0
g0347: [Rank 8] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8092.0 | max reserved: 8092.0
g0345: [2024-08-02 18:15:42,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[3.3204906666666666e-07, 3.3204906666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 20 loss: 10.4319 iter time (s): 4.041 samples/sec: 31.672
g0364:  iteration       20/10000000 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 4215.9 | learning rate: 3.320E-07 | global batch size:   128 | lm loss: 1.049183E+01 | loss scale: 2048.0 | grad norm: 8.587 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.361 | tokens per gpu per second (tgs): 1943.103 | TFLOPs: 15.64 |
g0345: [2024-08-02 18:16:22,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[5.068117333333334e-07, 5.068117333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 30 loss: 10.1846 iter time (s): 3.964 samples/sec: 32.293
g0364:  iteration       30/10000000 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 3996.6 | learning rate: 5.068E-07 | global batch size:   128 | lm loss: 1.031337E+01 | loss scale: 2048.0 | grad norm: 11.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.027 | tokens per gpu per second (tgs): 2049.735 | TFLOPs: 16.49 |
g0345: [2024-08-02 18:17:00,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[6.815744000000001e-07, 6.815744000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 40 loss: 9.7851 iter time (s): 3.819 samples/sec: 33.519
g0364:  iteration       40/10000000 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 3863.7 | learning rate: 6.816E-07 | global batch size:   128 | lm loss: 9.960386E+00 | loss scale: 2048.0 | grad norm: 7.431 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.128 | tokens per gpu per second (tgs): 2120.221 | TFLOPs: 17.06 |
g0345: [2024-08-02 18:17:40,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[8.563370666666667e-07, 8.563370666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 50 loss: 9.4983 iter time (s): 3.891 samples/sec: 32.899
g0364:  iteration       50/10000000 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 3923.5 | learning rate: 8.563E-07 | global batch size:   128 | lm loss: 9.609592E+00 | loss scale: 2048.0 | grad norm: 3.943 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.624 | tokens per gpu per second (tgs): 2087.906 | TFLOPs: 16.80 |
g0345: [2024-08-02 18:18:18,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0310997333333332e-06, 1.0310997333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 60 loss: 9.3292 iter time (s): 3.835 samples/sec: 33.381
g0364:  iteration       60/10000000 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 3867.5 | learning rate: 1.031E-06 | global batch size:   128 | lm loss: 9.393959E+00 | loss scale: 2048.0 | grad norm: 2.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.097 | tokens per gpu per second (tgs): 2118.182 | TFLOPs: 17.05 |
g0345: [2024-08-02 18:18:59,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.2058624000000002e-06, 1.2058624000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 70 loss: 9.2416 iter time (s): 4.041 samples/sec: 31.676
g0364:  iteration       70/10000000 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 4073.8 | learning rate: 1.206E-06 | global batch size:   128 | lm loss: 9.271092E+00 | loss scale: 2048.0 | grad norm: 2.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.420 | tokens per gpu per second (tgs): 2010.910 | TFLOPs: 16.18 |
g0345: [2024-08-02 18:19:39,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.3806250666666669e-06, 1.3806250666666669e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 80 loss: 9.1528 iter time (s): 3.966 samples/sec: 32.277
g0364:  iteration       80/10000000 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 3998.6 | learning rate: 1.381E-06 | global batch size:   128 | lm loss: 9.186479E+00 | loss scale: 2048.0 | grad norm: 1.999 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.011 | tokens per gpu per second (tgs): 2048.704 | TFLOPs: 16.49 |
g0345: [2024-08-02 18:20:20,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.5553877333333333e-06, 1.5553877333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 90 loss: 9.0818 iter time (s): 4.047 samples/sec: 31.625
g0364:  iteration       90/10000000 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 4079.8 | learning rate: 1.555E-06 | global batch size:   128 | lm loss: 9.108409E+00 | loss scale: 2048.0 | grad norm: 1.917 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.374 | tokens per gpu per second (tgs): 2007.961 | TFLOPs: 16.16 |
g0345: [2024-08-02 18:21:00,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.7301504e-06, 1.7301504e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 100 loss: 9.0115 iter time (s): 4.026 samples/sec: 31.790
g0364:  iteration      100/10000000 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 4058.9 | learning rate: 1.730E-06 | global batch size:   128 | lm loss: 9.038265E+00 | loss scale: 2048.0 | grad norm: 1.884 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.536 | tokens per gpu per second (tgs): 2018.272 | TFLOPs: 16.24 |
g0345: [2024-08-02 18:21:41,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.9049130666666667e-06, 1.9049130666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 110 loss: 8.9463 iter time (s): 3.987 samples/sec: 32.107
g0364:  iteration      110/10000000 | consumed samples:        14080 | consumed tokens:     28835840 | elapsed time per iteration (ms): 4019.0 | learning rate: 1.905E-06 | global batch size:   128 | lm loss: 8.978908E+00 | loss scale: 2048.0 | grad norm: 1.870 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.849 | tokens per gpu per second (tgs): 2038.315 | TFLOPs: 16.40 |
g0345: [2024-08-02 18:22:21,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[2.0796757333333334e-06, 2.0796757333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 120 loss: 8.8853 iter time (s): 4.017 samples/sec: 31.864
g0364:  iteration      120/10000000 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 4049.3 | learning rate: 2.080E-06 | global batch size:   128 | lm loss: 8.913850E+00 | loss scale: 2048.0 | grad norm: 1.864 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.610 | tokens per gpu per second (tgs): 2023.056 | TFLOPs: 16.28 |
g0345: [2024-08-02 18:23:02,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[2.2544384e-06, 2.2544384e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 130 loss: 8.8149 iter time (s): 4.024 samples/sec: 31.810
g0364:  iteration      130/10000000 | consumed samples:        16640 | consumed tokens:     34078720 | elapsed time per iteration (ms): 4056.3 | learning rate: 2.254E-06 | global batch size:   128 | lm loss: 8.847561E+00 | loss scale: 2048.0 | grad norm: 1.834 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.556 | tokens per gpu per second (tgs): 2019.589 | TFLOPs: 16.25 |
g0345: [2024-08-02 18:23:44,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[2.429201066666667e-06, 2.429201066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 140 loss: 8.7340 iter time (s): 4.154 samples/sec: 30.814
g0364:  iteration      140/10000000 | consumed samples:        17920 | consumed tokens:     36700160 | elapsed time per iteration (ms): 4186.6 | learning rate: 2.429E-06 | global batch size:   128 | lm loss: 8.775456E+00 | loss scale: 2048.0 | grad norm: 1.864 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.574 | tokens per gpu per second (tgs): 1956.708 | TFLOPs: 15.75 |
g0345: [2024-08-02 18:24:25,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[2.6039637333333333e-06, 2.6039637333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 150 loss: 8.6538 iter time (s): 4.119 samples/sec: 31.077
g0364:  iteration      150/10000000 | consumed samples:        19200 | consumed tokens:     39321600 | elapsed time per iteration (ms): 4151.2 | learning rate: 2.604E-06 | global batch size:   128 | lm loss: 8.697501E+00 | loss scale: 2048.0 | grad norm: 1.860 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.834 | tokens per gpu per second (tgs): 1973.384 | TFLOPs: 15.88 |
g0345: [2024-08-02 18:25:07,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[2.7787264000000002e-06, 2.7787264000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 160 loss: 8.5762 iter time (s): 4.180 samples/sec: 30.626
g0364:  iteration      160/10000000 | consumed samples:        20480 | consumed tokens:     41943040 | elapsed time per iteration (ms): 4211.8 | learning rate: 2.779E-06 | global batch size:   128 | lm loss: 8.613799E+00 | loss scale: 2048.0 | grad norm: 1.790 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.391 | tokens per gpu per second (tgs): 1945.011 | TFLOPs: 15.65 |
g0345: [2024-08-02 18:25:49,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[2.953489066666667e-06, 2.953489066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 170 loss: 8.4825 iter time (s): 4.125 samples/sec: 31.033
g0364:  iteration      170/10000000 | consumed samples:        21760 | consumed tokens:     44564480 | elapsed time per iteration (ms): 4158.6 | learning rate: 2.953E-06 | global batch size:   128 | lm loss: 8.521075E+00 | loss scale: 2048.0 | grad norm: 1.771 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.780 | tokens per gpu per second (tgs): 1969.910 | TFLOPs: 15.85 |
g0345: [2024-08-02 18:26:31,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[3.128251733333333e-06, 3.128251733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 180 loss: 8.3697 iter time (s): 4.157 samples/sec: 30.788
g0364:  iteration      180/10000000 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 4190.4 | learning rate: 3.128E-06 | global batch size:   128 | lm loss: 8.426257E+00 | loss scale: 2048.0 | grad norm: 1.910 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.546 | tokens per gpu per second (tgs): 1954.938 | TFLOPs: 15.73 |
g0345: [2024-08-02 18:27:12,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[3.3030144e-06, 3.3030144e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 190 loss: 8.2782 iter time (s): 4.098 samples/sec: 31.235
g0364:  iteration      190/10000000 | consumed samples:        24320 | consumed tokens:     49807360 | elapsed time per iteration (ms): 4130.4 | learning rate: 3.303E-06 | global batch size:   128 | lm loss: 8.328364E+00 | loss scale: 2048.0 | grad norm: 1.703 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.990 | tokens per gpu per second (tgs): 1983.361 | TFLOPs: 15.96 |
g0345: [2024-08-02 18:27:54,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[3.477777066666667e-06, 3.477777066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 200 loss: 8.1797 iter time (s): 4.183 samples/sec: 30.601
g0364:  iteration      200/10000000 | consumed samples:        25600 | consumed tokens:     52428800 | elapsed time per iteration (ms): 4215.4 | learning rate: 3.478E-06 | global batch size:   128 | lm loss: 8.229436E+00 | loss scale: 2048.0 | grad norm: 1.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.365 | tokens per gpu per second (tgs): 1943.345 | TFLOPs: 15.64 |
g0345: [2024-08-02 18:28:37,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[3.6525397333333335e-06, 3.6525397333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 210 loss: 8.0610 iter time (s): 4.232 samples/sec: 30.249
g0364:  iteration      210/10000000 | consumed samples:        26880 | consumed tokens:     55050240 | elapsed time per iteration (ms): 4264.1 | learning rate: 3.653E-06 | global batch size:   128 | lm loss: 8.117262E+00 | loss scale: 2048.0 | grad norm: 1.672 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.018 | tokens per gpu per second (tgs): 1921.147 | TFLOPs: 15.46 |
g0345: [2024-08-02 18:29:18,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[3.8273024e-06, 3.8273024e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 220 loss: 7.9610 iter time (s): 4.144 samples/sec: 30.892
g0364:  iteration      220/10000000 | consumed samples:        28160 | consumed tokens:     57671680 | elapsed time per iteration (ms): 4176.2 | learning rate: 3.827E-06 | global batch size:   128 | lm loss: 8.009213E+00 | loss scale: 2048.0 | grad norm: 1.584 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.650 | tokens per gpu per second (tgs): 1961.612 | TFLOPs: 15.79 |
g0345: [2024-08-02 18:29:58,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[4.002065066666667e-06, 4.002065066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 230 loss: 7.8433 iter time (s): 3.941 samples/sec: 32.477
g0364:  iteration      230/10000000 | consumed samples:        29440 | consumed tokens:     60293120 | elapsed time per iteration (ms): 3974.1 | learning rate: 4.002E-06 | global batch size:   128 | lm loss: 7.895856E+00 | loss scale: 2048.0 | grad norm: 1.537 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.209 | tokens per gpu per second (tgs): 2061.355 | TFLOPs: 16.59 |
g0345: [2024-08-02 18:30:40,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[4.176827733333334e-06, 4.176827733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 240 loss: 7.7249 iter time (s): 4.165 samples/sec: 30.735
g0364:  iteration      240/10000000 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 4197.2 | learning rate: 4.177E-06 | global batch size:   128 | lm loss: 7.778384E+00 | loss scale: 2048.0 | grad norm: 1.498 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.497 | tokens per gpu per second (tgs): 1951.786 | TFLOPs: 15.71 |
g0345: [2024-08-02 18:31:21,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[4.351590400000001e-06, 4.351590400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 250 loss: 7.6062 iter time (s): 4.092 samples/sec: 31.284
g0364:  iteration      250/10000000 | consumed samples:        32000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 4124.1 | learning rate: 4.352E-06 | global batch size:   128 | lm loss: 7.658145E+00 | loss scale: 2048.0 | grad norm: 1.437 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.037 | tokens per gpu per second (tgs): 1986.379 | TFLOPs: 15.98 |
g0345: [2024-08-02 18:32:03,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[4.526353066666667e-06, 4.526353066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 260 loss: 7.4866 iter time (s): 4.127 samples/sec: 31.018
g0364:  iteration      260/10000000 | consumed samples:        33280 | consumed tokens:     68157440 | elapsed time per iteration (ms): 4159.1 | learning rate: 4.526E-06 | global batch size:   128 | lm loss: 7.540765E+00 | loss scale: 2048.0 | grad norm: 1.403 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.776 | tokens per gpu per second (tgs): 1969.633 | TFLOPs: 15.85 |
g0345: [2024-08-02 18:32:46,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[4.701115733333334e-06, 4.701115733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 270 loss: 7.3872 iter time (s): 4.226 samples/sec: 30.292
g0364:  iteration      270/10000000 | consumed samples:        34560 | consumed tokens:     70778880 | elapsed time per iteration (ms): 4258.2 | learning rate: 4.701E-06 | global batch size:   128 | lm loss: 7.426646E+00 | loss scale: 2048.0 | grad norm: 1.307 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.059 | tokens per gpu per second (tgs): 1923.804 | TFLOPs: 15.48 |
g0345: [2024-08-02 18:33:28,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[4.875878400000001e-06, 4.875878400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 280 loss: 7.2733 iter time (s): 4.196 samples/sec: 30.508
g0364:  iteration      280/10000000 | consumed samples:        35840 | consumed tokens:     73400320 | elapsed time per iteration (ms): 4228.3 | learning rate: 4.876E-06 | global batch size:   128 | lm loss: 7.317844E+00 | loss scale: 2048.0 | grad norm: 1.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.272 | tokens per gpu per second (tgs): 1937.409 | TFLOPs: 15.59 |
g0345: [2024-08-02 18:34:09,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[5.050641066666667e-06, 5.050641066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 290 loss: 7.1561 iter time (s): 4.102 samples/sec: 31.204
g0364:  iteration      290/10000000 | consumed samples:        37120 | consumed tokens:     76021760 | elapsed time per iteration (ms): 4134.8 | learning rate: 5.051E-06 | global batch size:   128 | lm loss: 7.211963E+00 | loss scale: 2048.0 | grad norm: 1.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.957 | tokens per gpu per second (tgs): 1981.234 | TFLOPs: 15.94 |
g0345: [2024-08-02 18:34:50,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[5.225403733333334e-06, 5.225403733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 300 loss: 7.0510 iter time (s): 4.051 samples/sec: 31.595
g0364:  iteration      300/10000000 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 4083.8 | learning rate: 5.225E-06 | global batch size:   128 | lm loss: 7.102456E+00 | loss scale: 2048.0 | grad norm: 1.127 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.343 | tokens per gpu per second (tgs): 2005.975 | TFLOPs: 16.14 |
g0345: [2024-08-02 18:35:32,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[5.4001664e-06, 5.4001664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 310 loss: 6.9565 iter time (s): 4.138 samples/sec: 30.933
g0364:  iteration      310/10000000 | consumed samples:        39680 | consumed tokens:     81264640 | elapsed time per iteration (ms): 4170.5 | learning rate: 5.400E-06 | global batch size:   128 | lm loss: 7.008646E+00 | loss scale: 2048.0 | grad norm: 1.001 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.692 | tokens per gpu per second (tgs): 1964.294 | TFLOPs: 15.81 |
g0345: [2024-08-02 18:36:13,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[5.574929066666667e-06, 5.574929066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 320 loss: 6.8986 iter time (s): 4.084 samples/sec: 31.345
g0364:  iteration      320/10000000 | consumed samples:        40960 | consumed tokens:     83886080 | elapsed time per iteration (ms): 4116.0 | learning rate: 5.575E-06 | global batch size:   128 | lm loss: 6.915192E+00 | loss scale: 2048.0 | grad norm: 0.911 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.098 | tokens per gpu per second (tgs): 1990.301 | TFLOPs: 16.02 |
g0345: [2024-08-02 18:36:55,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[5.7496917333333335e-06, 5.7496917333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 330 loss: 6.8190 iter time (s): 4.201 samples/sec: 30.469
g0364:  iteration      330/10000000 | consumed samples:        42240 | consumed tokens:     86507520 | elapsed time per iteration (ms): 4233.8 | learning rate: 5.750E-06 | global batch size:   128 | lm loss: 6.827944E+00 | loss scale: 2048.0 | grad norm: 0.871 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.233 | tokens per gpu per second (tgs): 1934.923 | TFLOPs: 15.57 |
g0345: [2024-08-02 18:37:37,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[5.9244543999999995e-06, 5.9244543999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 340 loss: 6.7272 iter time (s): 4.143 samples/sec: 30.893
g0364:  iteration      340/10000000 | consumed samples:        43520 | consumed tokens:     89128960 | elapsed time per iteration (ms): 4175.8 | learning rate: 5.924E-06 | global batch size:   128 | lm loss: 6.758569E+00 | loss scale: 2048.0 | grad norm: 0.777 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.778 | TFLOPs: 15.79 |
g0345: [2024-08-02 18:38:19,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[6.0992170666666664e-06, 6.0992170666666664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 350 loss: 6.6845 iter time (s): 4.201 samples/sec: 30.470
g0364:  iteration      350/10000000 | consumed samples:        44800 | consumed tokens:     91750400 | elapsed time per iteration (ms): 4233.8 | learning rate: 6.099E-06 | global batch size:   128 | lm loss: 6.691272E+00 | loss scale: 2048.0 | grad norm: 0.706 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.233 | tokens per gpu per second (tgs): 1934.925 | TFLOPs: 15.57 |
g0345: [2024-08-02 18:39:02,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[6.273979733333333e-06, 6.273979733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 360 loss: 6.6186 iter time (s): 4.251 samples/sec: 30.111
g0364:  iteration      360/10000000 | consumed samples:        46080 | consumed tokens:     94371840 | elapsed time per iteration (ms): 4284.7 | learning rate: 6.274E-06 | global batch size:   128 | lm loss: 6.631891E+00 | loss scale: 2048.0 | grad norm: 0.813 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.873 | tokens per gpu per second (tgs): 1911.898 | TFLOPs: 15.39 |
g0345: [2024-08-02 18:39:44,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[6.4487424e-06, 6.4487424e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 370 loss: 6.5671 iter time (s): 4.132 samples/sec: 30.978
g0364:  iteration      370/10000000 | consumed samples:        47360 | consumed tokens:     96993280 | elapsed time per iteration (ms): 4164.5 | learning rate: 6.449E-06 | global batch size:   128 | lm loss: 6.584326E+00 | loss scale: 2048.0 | grad norm: 0.661 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.736 | tokens per gpu per second (tgs): 1967.094 | TFLOPs: 15.83 |
g0345: [2024-08-02 18:40:26,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[6.623505066666667e-06, 6.623505066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 380 loss: 6.5243 iter time (s): 4.210 samples/sec: 30.404
g0364:  iteration      380/10000000 | consumed samples:        48640 | consumed tokens:     99614720 | elapsed time per iteration (ms): 4242.7 | learning rate: 6.624E-06 | global batch size:   128 | lm loss: 6.542853E+00 | loss scale: 2048.0 | grad norm: 0.641 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.169 | tokens per gpu per second (tgs): 1930.841 | TFLOPs: 15.54 |
g0345: [2024-08-02 18:41:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[6.798267733333334e-06, 6.798267733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 390 loss: 6.4969 iter time (s): 4.102 samples/sec: 31.204
g0364:  iteration      390/10000000 | consumed samples:        49920 | consumed tokens:    102236160 | elapsed time per iteration (ms): 4134.7 | learning rate: 6.798E-06 | global batch size:   128 | lm loss: 6.503143E+00 | loss scale: 2048.0 | grad norm: 0.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.958 | tokens per gpu per second (tgs): 1981.294 | TFLOPs: 15.94 |
g0345: [2024-08-02 18:41:50,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[6.973030400000001e-06, 6.973030400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 400 loss: 6.4714 iter time (s): 4.157 samples/sec: 30.793
g0364:  iteration      400/10000000 | consumed samples:        51200 | consumed tokens:    104857600 | elapsed time per iteration (ms): 4189.4 | learning rate: 6.973E-06 | global batch size:   128 | lm loss: 6.478048E+00 | loss scale: 2048.0 | grad norm: 0.642 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.553 | tokens per gpu per second (tgs): 1955.405 | TFLOPs: 15.74 |
g0345: [2024-08-02 18:42:32,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[7.147793066666666e-06, 7.147793066666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 410 loss: 6.4323 iter time (s): 4.216 samples/sec: 30.360
g0364:  iteration      410/10000000 | consumed samples:        52480 | consumed tokens:    107479040 | elapsed time per iteration (ms): 4248.4 | learning rate: 7.148E-06 | global batch size:   128 | lm loss: 6.453884E+00 | loss scale: 2048.0 | grad norm: 0.499 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.129 | tokens per gpu per second (tgs): 1928.270 | TFLOPs: 15.52 |
g0345: [2024-08-02 18:43:13,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[7.322555733333333e-06, 7.322555733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 420 loss: 6.4113 iter time (s): 4.082 samples/sec: 31.358
g0364:  iteration      420/10000000 | consumed samples:        53760 | consumed tokens:    110100480 | elapsed time per iteration (ms): 4114.2 | learning rate: 7.323E-06 | global batch size:   128 | lm loss: 6.422516E+00 | loss scale: 2048.0 | grad norm: 0.484 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.112 | tokens per gpu per second (tgs): 1991.155 | TFLOPs: 16.02 |
g0345: [2024-08-02 18:43:55,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[7.4973184e-06, 7.4973184e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 430 loss: 6.3966 iter time (s): 4.153 samples/sec: 30.824
g0364:  iteration      430/10000000 | consumed samples:        55040 | consumed tokens:    112721920 | elapsed time per iteration (ms): 4186.4 | learning rate: 7.497E-06 | global batch size:   128 | lm loss: 6.409657E+00 | loss scale: 2048.0 | grad norm: 0.616 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.576 | tokens per gpu per second (tgs): 1956.836 | TFLOPs: 15.75 |
g0345: [2024-08-02 18:44:37,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[7.672081066666667e-06, 7.672081066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 440 loss: 6.3954 iter time (s): 4.182 samples/sec: 30.609
g0364:  iteration      440/10000000 | consumed samples:        56320 | consumed tokens:    115343360 | elapsed time per iteration (ms): 4214.1 | learning rate: 7.672E-06 | global batch size:   128 | lm loss: 6.393593E+00 | loss scale: 2048.0 | grad norm: 0.472 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.374 | tokens per gpu per second (tgs): 1943.946 | TFLOPs: 15.64 |
g0345: [2024-08-02 18:45:19,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[7.846843733333333e-06, 7.846843733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 450 loss: 6.3790 iter time (s): 4.162 samples/sec: 30.753
g0364:  iteration      450/10000000 | consumed samples:        57600 | consumed tokens:    117964800 | elapsed time per iteration (ms): 4194.7 | learning rate: 7.847E-06 | global batch size:   128 | lm loss: 6.378075E+00 | loss scale: 2048.0 | grad norm: 0.478 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.515 | tokens per gpu per second (tgs): 1952.951 | TFLOPs: 15.72 |
g0345: [2024-08-02 18:46:01,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[8.0216064e-06, 8.0216064e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 460 loss: 6.3853 iter time (s): 4.205 samples/sec: 30.439
g0364:  iteration      460/10000000 | consumed samples:        58880 | consumed tokens:    120586240 | elapsed time per iteration (ms): 4237.4 | learning rate: 8.022E-06 | global batch size:   128 | lm loss: 6.371702E+00 | loss scale: 2048.0 | grad norm: 0.896 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.207 | tokens per gpu per second (tgs): 1933.266 | TFLOPs: 15.56 |
g0345: [2024-08-02 18:46:43,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[8.196369066666667e-06, 8.196369066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 470 loss: 6.3539 iter time (s): 4.165 samples/sec: 30.734
g0364:  iteration      470/10000000 | consumed samples:        60160 | consumed tokens:    123207680 | elapsed time per iteration (ms): 4197.1 | learning rate: 8.196E-06 | global batch size:   128 | lm loss: 6.356767E+00 | loss scale: 2048.0 | grad norm: 0.514 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.497 | tokens per gpu per second (tgs): 1951.810 | TFLOPs: 15.71 |
g0345: [2024-08-02 18:47:25,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[8.371131733333335e-06, 8.371131733333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 480 loss: 6.3498 iter time (s): 4.160 samples/sec: 30.769
g0364:  iteration      480/10000000 | consumed samples:        61440 | consumed tokens:    125829120 | elapsed time per iteration (ms): 4192.6 | learning rate: 8.371E-06 | global batch size:   128 | lm loss: 6.347995E+00 | loss scale: 2048.0 | grad norm: 0.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.530 | tokens per gpu per second (tgs): 1953.898 | TFLOPs: 15.72 |
g0345: [2024-08-02 18:48:08,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[8.5458944e-06, 8.5458944e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 490 loss: 6.3471 iter time (s): 4.212 samples/sec: 30.392
g0364:  iteration      490/10000000 | consumed samples:        62720 | consumed tokens:    128450560 | elapsed time per iteration (ms): 4244.0 | learning rate: 8.546E-06 | global batch size:   128 | lm loss: 6.343028E+00 | loss scale: 2048.0 | grad norm: 0.810 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.160 | tokens per gpu per second (tgs): 1930.258 | TFLOPs: 15.53 |
g0345: [2024-08-02 18:48:49,844] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[8.720657066666667e-06, 8.720657066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 500 loss: 6.3284 iter time (s): 4.119 samples/sec: 31.079
g0364:  iteration      500/10000000 | consumed samples:        64000 | consumed tokens:    131072000 | elapsed time per iteration (ms): 4150.9 | learning rate: 8.721E-06 | global batch size:   128 | lm loss: 6.338305E+00 | loss scale: 2048.0 | grad norm: 0.705 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.837 | tokens per gpu per second (tgs): 1973.554 | TFLOPs: 15.88 |
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0362: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0362: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0362: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0362: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0347: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0364: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0364: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0364: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0352: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0345: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0346: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0363: [2024-08-02 18:48:53,961] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0364: [2024-08-02 18:48:53,962] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0362: [2024-08-02 18:48:53,962] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0345: [2024-08-02 18:49:32,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[8.895419733333333e-06, 8.895419733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 510 loss: 6.3144 iter time (s): 4.187 samples/sec: 30.572
g0364:  iteration      510/10000000 | consumed samples:        65280 | consumed tokens:    133693440 | elapsed time per iteration (ms): 4218.8 | learning rate: 8.895E-06 | global batch size:   128 | lm loss: 6.330696E+00 | loss scale: 4096.0 | grad norm: 0.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.340 | tokens per gpu per second (tgs): 1941.773 | TFLOPs: 15.63 |
g0345: [2024-08-02 18:50:14,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[9.0701824e-06, 9.0701824e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 520 loss: 6.3220 iter time (s): 4.187 samples/sec: 30.568
g0364:  iteration      520/10000000 | consumed samples:        66560 | consumed tokens:    136314880 | elapsed time per iteration (ms): 4219.7 | learning rate: 9.070E-06 | global batch size:   128 | lm loss: 6.325331E+00 | loss scale: 4096.0 | grad norm: 0.661 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.334 | tokens per gpu per second (tgs): 1941.370 | TFLOPs: 15.62 |
g0345: [2024-08-02 18:50:56,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[9.244945066666667e-06, 9.244945066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 530 loss: 6.3199 iter time (s): 4.174 samples/sec: 30.663
g0364:  iteration      530/10000000 | consumed samples:        67840 | consumed tokens:    138936320 | elapsed time per iteration (ms): 4206.6 | learning rate: 9.245E-06 | global batch size:   128 | lm loss: 6.325024E+00 | loss scale: 4096.0 | grad norm: 1.010 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.428 | tokens per gpu per second (tgs): 1947.423 | TFLOPs: 15.67 |
g0345: [2024-08-02 18:51:38,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[9.419707733333334e-06, 9.419707733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 540 loss: 6.3189 iter time (s): 4.222 samples/sec: 30.316
g0364:  iteration      540/10000000 | consumed samples:        69120 | consumed tokens:    141557760 | elapsed time per iteration (ms): 4254.8 | learning rate: 9.420E-06 | global batch size:   128 | lm loss: 6.317002E+00 | loss scale: 4096.0 | grad norm: 0.655 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.083 | tokens per gpu per second (tgs): 1925.337 | TFLOPs: 15.49 |
g0345: [2024-08-02 18:52:21,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[9.5944704e-06, 9.5944704e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 550 loss: 6.3233 iter time (s): 4.219 samples/sec: 30.342
g0364:  iteration      550/10000000 | consumed samples:        70400 | consumed tokens:    144179200 | elapsed time per iteration (ms): 4251.0 | learning rate: 9.594E-06 | global batch size:   128 | lm loss: 6.316348E+00 | loss scale: 4096.0 | grad norm: 0.954 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.110 | tokens per gpu per second (tgs): 1927.069 | TFLOPs: 15.51 |
g0345: [2024-08-02 18:53:03,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[9.769233066666668e-06, 9.769233066666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 560 loss: 6.3122 iter time (s): 4.179 samples/sec: 30.627
g0364:  iteration      560/10000000 | consumed samples:        71680 | consumed tokens:    146800640 | elapsed time per iteration (ms): 4211.6 | learning rate: 9.769E-06 | global batch size:   128 | lm loss: 6.313811E+00 | loss scale: 4096.0 | grad norm: 0.925 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.392 | tokens per gpu per second (tgs): 1945.092 | TFLOPs: 15.65 |
g0345: [2024-08-02 18:53:45,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[9.943995733333334e-06, 9.943995733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 570 loss: 6.3017 iter time (s): 4.193 samples/sec: 30.530
g0364:  iteration      570/10000000 | consumed samples:        72960 | consumed tokens:    149422080 | elapsed time per iteration (ms): 4224.7 | learning rate: 9.944E-06 | global batch size:   128 | lm loss: 6.306602E+00 | loss scale: 4096.0 | grad norm: 0.936 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.298 | tokens per gpu per second (tgs): 1939.062 | TFLOPs: 15.60 |
g0345: [2024-08-02 18:54:28,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[1.01187584e-05, 1.01187584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 580 loss: 6.2997 iter time (s): 4.246 samples/sec: 30.147
g0364:  iteration      580/10000000 | consumed samples:        74240 | consumed tokens:    152043520 | elapsed time per iteration (ms): 4278.3 | learning rate: 1.012E-05 | global batch size:   128 | lm loss: 6.297309E+00 | loss scale: 4096.0 | grad norm: 0.605 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.919 | tokens per gpu per second (tgs): 1914.792 | TFLOPs: 15.41 |
g0345: [2024-08-02 18:55:10,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[1.0293521066666666e-05, 1.0293521066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 590 loss: 6.2988 iter time (s): 4.197 samples/sec: 30.497
g0364:  iteration      590/10000000 | consumed samples:        75520 | consumed tokens:    154664960 | elapsed time per iteration (ms): 4229.8 | learning rate: 1.029E-05 | global batch size:   128 | lm loss: 6.300487E+00 | loss scale: 4096.0 | grad norm: 0.626 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.262 | tokens per gpu per second (tgs): 1936.742 | TFLOPs: 15.59 |
g0345: [2024-08-02 18:55:53,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[1.0468283733333334e-05, 1.0468283733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 600 loss: 6.3104 iter time (s): 4.234 samples/sec: 30.229
g0364:  iteration      600/10000000 | consumed samples:        76800 | consumed tokens:    157286400 | elapsed time per iteration (ms): 4266.6 | learning rate: 1.047E-05 | global batch size:   128 | lm loss: 6.297145E+00 | loss scale: 4096.0 | grad norm: 1.048 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.001 | tokens per gpu per second (tgs): 1920.045 | TFLOPs: 15.45 |
g0345: [2024-08-02 18:56:37,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[1.06430464e-05, 1.06430464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 610 loss: 6.2928 iter time (s): 4.367 samples/sec: 29.309
g0364:  iteration      610/10000000 | consumed samples:        78080 | consumed tokens:    159907840 | elapsed time per iteration (ms): 4399.8 | learning rate: 1.064E-05 | global batch size:   128 | lm loss: 6.291422E+00 | loss scale: 4096.0 | grad norm: 0.744 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.092 | tokens per gpu per second (tgs): 1861.911 | TFLOPs: 14.98 |
g0345: [2024-08-02 18:57:20,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[1.0817809066666668e-05, 1.0817809066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 620 loss: 6.2929 iter time (s): 4.294 samples/sec: 29.806
g0364:  iteration      620/10000000 | consumed samples:        79360 | consumed tokens:    162529280 | elapsed time per iteration (ms): 4326.6 | learning rate: 1.082E-05 | global batch size:   128 | lm loss: 6.287740E+00 | loss scale: 4096.0 | grad norm: 0.679 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.584 | tokens per gpu per second (tgs): 1893.397 | TFLOPs: 15.24 |
g0345: [2024-08-02 18:58:03,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[1.0992571733333332e-05, 1.0992571733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 630 loss: 6.2963 iter time (s): 4.257 samples/sec: 30.071
g0364:  iteration      630/10000000 | consumed samples:        80640 | consumed tokens:    165150720 | elapsed time per iteration (ms): 4291.0 | learning rate: 1.099E-05 | global batch size:   128 | lm loss: 6.285532E+00 | loss scale: 4096.0 | grad norm: 1.653 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.830 | tokens per gpu per second (tgs): 1909.102 | TFLOPs: 15.36 |
g0345: [2024-08-02 18:58:47,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[1.11673344e-05, 1.11673344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 640 loss: 6.2860 iter time (s): 4.331 samples/sec: 29.557
g0364:  iteration      640/10000000 | consumed samples:        81920 | consumed tokens:    167772160 | elapsed time per iteration (ms): 4363.0 | learning rate: 1.117E-05 | global batch size:   128 | lm loss: 6.281885E+00 | loss scale: 4096.0 | grad norm: 0.992 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.338 | tokens per gpu per second (tgs): 1877.606 | TFLOPs: 15.11 |
g0345: [2024-08-02 18:59:31,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[1.1342097066666666e-05, 1.1342097066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 650 loss: 6.2847 iter time (s): 4.435 samples/sec: 28.863
g0364:  iteration      650/10000000 | consumed samples:        83200 | consumed tokens:    170393600 | elapsed time per iteration (ms): 4467.0 | learning rate: 1.134E-05 | global batch size:   128 | lm loss: 6.285075E+00 | loss scale: 4096.0 | grad norm: 0.724 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.655 | tokens per gpu per second (tgs): 1833.910 | TFLOPs: 14.76 |
g0345: [2024-08-02 19:00:15,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[1.1516859733333334e-05, 1.1516859733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 660 loss: 6.2732 iter time (s): 4.330 samples/sec: 29.558
g0364:  iteration      660/10000000 | consumed samples:        84480 | consumed tokens:    173015040 | elapsed time per iteration (ms): 4362.7 | learning rate: 1.152E-05 | global batch size:   128 | lm loss: 6.279141E+00 | loss scale: 4096.0 | grad norm: 0.588 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.340 | tokens per gpu per second (tgs): 1877.745 | TFLOPs: 15.11 |
g0345: [2024-08-02 19:00:59,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[1.16916224e-05, 1.16916224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 670 loss: 6.2635 iter time (s): 4.323 samples/sec: 29.606
g0364:  iteration      670/10000000 | consumed samples:        85760 | consumed tokens:    175636480 | elapsed time per iteration (ms): 4355.7 | learning rate: 1.169E-05 | global batch size:   128 | lm loss: 6.271849E+00 | loss scale: 4096.0 | grad norm: 1.286 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.387 | tokens per gpu per second (tgs): 1880.772 | TFLOPs: 15.13 |
g0345: [2024-08-02 19:01:43,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[1.1866385066666668e-05, 1.1866385066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 680 loss: 6.2771 iter time (s): 4.430 samples/sec: 28.893
g0364:  iteration      680/10000000 | consumed samples:        87040 | consumed tokens:    178257920 | elapsed time per iteration (ms): 4462.5 | learning rate: 1.187E-05 | global batch size:   128 | lm loss: 6.271728E+00 | loss scale: 4096.0 | grad norm: 1.103 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.684 | tokens per gpu per second (tgs): 1835.755 | TFLOPs: 14.77 |
g0345: [2024-08-02 19:02:26,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[1.2041147733333334e-05, 1.2041147733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 690 loss: 6.2651 iter time (s): 4.256 samples/sec: 30.076
g0364:  iteration      690/10000000 | consumed samples:        88320 | consumed tokens:    180879360 | elapsed time per iteration (ms): 4288.7 | learning rate: 1.204E-05 | global batch size:   128 | lm loss: 6.264289E+00 | loss scale: 4096.0 | grad norm: 0.849 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.846 | tokens per gpu per second (tgs): 1910.146 | TFLOPs: 15.37 |
g0345: [2024-08-02 19:03:08,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[1.22159104e-05, 1.22159104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 700 loss: 6.2598 iter time (s): 4.157 samples/sec: 30.793
g0364:  iteration      700/10000000 | consumed samples:        89600 | consumed tokens:    183500800 | elapsed time per iteration (ms): 4189.4 | learning rate: 1.222E-05 | global batch size:   128 | lm loss: 6.263806E+00 | loss scale: 4096.0 | grad norm: 0.944 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.553 | tokens per gpu per second (tgs): 1955.391 | TFLOPs: 15.74 |
g0345: [2024-08-02 19:03:52,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[1.2390673066666668e-05, 1.2390673066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 710 loss: 6.2703 iter time (s): 4.360 samples/sec: 29.358
g0364:  iteration      710/10000000 | consumed samples:        90880 | consumed tokens:    186122240 | elapsed time per iteration (ms): 4394.0 | learning rate: 1.239E-05 | global batch size:   128 | lm loss: 6.263698E+00 | loss scale: 4096.0 | grad norm: 1.010 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.131 | tokens per gpu per second (tgs): 1864.362 | TFLOPs: 15.00 |
g0345: [2024-08-02 19:04:36,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[1.2565435733333334e-05, 1.2565435733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 720 loss: 6.2592 iter time (s): 4.402 samples/sec: 29.075
g0364:  iteration      720/10000000 | consumed samples:        92160 | consumed tokens:    188743680 | elapsed time per iteration (ms): 4435.1 | learning rate: 1.257E-05 | global batch size:   128 | lm loss: 6.257483E+00 | loss scale: 4096.0 | grad norm: 2.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.861 | tokens per gpu per second (tgs): 1847.085 | TFLOPs: 14.86 |
g0345: [2024-08-02 19:05:21,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[1.2740198400000001e-05, 1.2740198400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 730 loss: 6.2626 iter time (s): 4.415 samples/sec: 28.995
g0364:  iteration      730/10000000 | consumed samples:        93440 | consumed tokens:    191365120 | elapsed time per iteration (ms): 4448.7 | learning rate: 1.274E-05 | global batch size:   128 | lm loss: 6.256378E+00 | loss scale: 4096.0 | grad norm: 1.046 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.773 | tokens per gpu per second (tgs): 1841.442 | TFLOPs: 14.82 |
g0345: [2024-08-02 19:06:04,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[1.2914961066666667e-05, 1.2914961066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 740 loss: 6.2393 iter time (s): 4.251 samples/sec: 30.111
g0364:  iteration      740/10000000 | consumed samples:        94720 | consumed tokens:    193986560 | elapsed time per iteration (ms): 4283.6 | learning rate: 1.291E-05 | global batch size:   128 | lm loss: 6.248988E+00 | loss scale: 4096.0 | grad norm: 0.911 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.881 | tokens per gpu per second (tgs): 1912.399 | TFLOPs: 15.39 |
g0345: [2024-08-02 19:06:48,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[1.3089723733333335e-05, 1.3089723733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 750 loss: 6.2533 iter time (s): 4.429 samples/sec: 28.900
g0364:  iteration      750/10000000 | consumed samples:        96000 | consumed tokens:    196608000 | elapsed time per iteration (ms): 4461.3 | learning rate: 1.309E-05 | global batch size:   128 | lm loss: 6.246029E+00 | loss scale: 4096.0 | grad norm: 0.699 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.691 | tokens per gpu per second (tgs): 1836.220 | TFLOPs: 14.78 |
g0345: [2024-08-02 19:07:33,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[1.3264486400000001e-05, 1.3264486400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 760 loss: 6.2305 iter time (s): 4.414 samples/sec: 29.001
g0364:  iteration      760/10000000 | consumed samples:        97280 | consumed tokens:    199229440 | elapsed time per iteration (ms): 4445.9 | learning rate: 1.326E-05 | global batch size:   128 | lm loss: 6.237285E+00 | loss scale: 4096.0 | grad norm: 1.019 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.790 | tokens per gpu per second (tgs): 1842.589 | TFLOPs: 14.83 |
g0345: [2024-08-02 19:08:14,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[1.3439249066666669e-05, 1.3439249066666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 770 loss: 6.2290 iter time (s): 4.140 samples/sec: 30.916
g0364:  iteration      770/10000000 | consumed samples:        98560 | consumed tokens:    201850880 | elapsed time per iteration (ms): 4172.4 | learning rate: 1.344E-05 | global batch size:   128 | lm loss: 6.241388E+00 | loss scale: 4096.0 | grad norm: 1.152 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.678 | tokens per gpu per second (tgs): 1963.368 | TFLOPs: 15.80 |
g0345: [2024-08-02 19:08:57,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[1.3614011733333333e-05, 1.3614011733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 780 loss: 6.2263 iter time (s): 4.251 samples/sec: 30.110
g0364:  iteration      780/10000000 | consumed samples:        99840 | consumed tokens:    204472320 | elapsed time per iteration (ms): 4283.3 | learning rate: 1.361E-05 | global batch size:   128 | lm loss: 6.227568E+00 | loss scale: 4096.0 | grad norm: 0.733 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.884 | tokens per gpu per second (tgs): 1912.566 | TFLOPs: 15.39 |
g0345: [2024-08-02 19:09:40,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[1.37887744e-05, 1.37887744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 790 loss: 6.2202 iter time (s): 4.204 samples/sec: 30.447
g0364:  iteration      790/10000000 | consumed samples:       101120 | consumed tokens:    207093760 | elapsed time per iteration (ms): 4236.5 | learning rate: 1.379E-05 | global batch size:   128 | lm loss: 6.228790E+00 | loss scale: 4096.0 | grad norm: 1.007 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.214 | tokens per gpu per second (tgs): 1933.678 | TFLOPs: 15.56 |
g0345: [2024-08-02 19:10:22,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[1.3963537066666667e-05, 1.3963537066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 800 loss: 6.2185 iter time (s): 4.191 samples/sec: 30.541
g0364:  iteration      800/10000000 | consumed samples:       102400 | consumed tokens:    209715200 | elapsed time per iteration (ms): 4224.2 | learning rate: 1.396E-05 | global batch size:   128 | lm loss: 6.218912E+00 | loss scale: 4096.0 | grad norm: 1.108 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.301 | tokens per gpu per second (tgs): 1939.284 | TFLOPs: 15.61 |
g0345: [2024-08-02 19:11:04,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[1.4138299733333333e-05, 1.4138299733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 810 loss: 6.1907 iter time (s): 4.181 samples/sec: 30.616
g0364:  iteration      810/10000000 | consumed samples:       103680 | consumed tokens:    212336640 | elapsed time per iteration (ms): 4213.8 | learning rate: 1.414E-05 | global batch size:   128 | lm loss: 6.215450E+00 | loss scale: 4096.0 | grad norm: 1.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.376 | tokens per gpu per second (tgs): 1944.077 | TFLOPs: 15.64 |
g0345: [2024-08-02 19:11:46,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[1.43130624e-05, 1.43130624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 820 loss: 6.2102 iter time (s): 4.182 samples/sec: 30.611
g0364:  iteration      820/10000000 | consumed samples:       104960 | consumed tokens:    214958080 | elapsed time per iteration (ms): 4213.8 | learning rate: 1.431E-05 | global batch size:   128 | lm loss: 6.212835E+00 | loss scale: 4096.0 | grad norm: 1.071 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.376 | tokens per gpu per second (tgs): 1944.068 | TFLOPs: 15.64 |
g0345: [2024-08-02 19:12:29,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[1.4487825066666667e-05, 1.4487825066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 830 loss: 6.1802 iter time (s): 4.223 samples/sec: 30.314
g0364:  iteration      830/10000000 | consumed samples:       106240 | consumed tokens:    217579520 | elapsed time per iteration (ms): 4254.9 | learning rate: 1.449E-05 | global batch size:   128 | lm loss: 6.205508E+00 | loss scale: 4096.0 | grad norm: 1.109 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.083 | tokens per gpu per second (tgs): 1925.313 | TFLOPs: 15.49 |
g0345: [2024-08-02 19:13:11,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[1.4662587733333333e-05, 1.4662587733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 840 loss: 6.1792 iter time (s): 4.148 samples/sec: 30.859
g0364:  iteration      840/10000000 | consumed samples:       107520 | consumed tokens:    220200960 | elapsed time per iteration (ms): 4180.6 | learning rate: 1.466E-05 | global batch size:   128 | lm loss: 6.195138E+00 | loss scale: 4096.0 | grad norm: 1.126 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.617 | tokens per gpu per second (tgs): 1959.507 | TFLOPs: 15.77 |
g0345: [2024-08-02 19:13:53,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[1.4837350400000001e-05, 1.4837350400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 850 loss: 6.1815 iter time (s): 4.214 samples/sec: 30.377
g0364:  iteration      850/10000000 | consumed samples:       108800 | consumed tokens:    222822400 | elapsed time per iteration (ms): 4246.1 | learning rate: 1.484E-05 | global batch size:   128 | lm loss: 6.191581E+00 | loss scale: 4096.0 | grad norm: 1.054 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.145 | tokens per gpu per second (tgs): 1929.308 | TFLOPs: 15.53 |
g0345: [2024-08-02 19:14:34,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.5012113066666667e-05, 1.5012113066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 860 loss: 6.1845 iter time (s): 4.104 samples/sec: 31.192
g0364:  iteration      860/10000000 | consumed samples:       110080 | consumed tokens:    225443840 | elapsed time per iteration (ms): 4136.1 | learning rate: 1.501E-05 | global batch size:   128 | lm loss: 6.189524E+00 | loss scale: 4096.0 | grad norm: 1.974 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.947 | tokens per gpu per second (tgs): 1980.612 | TFLOPs: 15.94 |
g0345: [2024-08-02 19:15:16,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.5186875733333335e-05, 1.5186875733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 870 loss: 6.1706 iter time (s): 4.169 samples/sec: 30.699
g0364:  iteration      870/10000000 | consumed samples:       111360 | consumed tokens:    228065280 | elapsed time per iteration (ms): 4201.9 | learning rate: 1.519E-05 | global batch size:   128 | lm loss: 6.181608E+00 | loss scale: 4096.0 | grad norm: 1.594 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.462 | tokens per gpu per second (tgs): 1949.596 | TFLOPs: 15.69 |
g0345: [2024-08-02 19:15:59,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.5361638400000003e-05, 1.5361638400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 880 loss: 6.1566 iter time (s): 4.222 samples/sec: 30.320
g0364:  iteration      880/10000000 | consumed samples:       112640 | consumed tokens:    230686720 | elapsed time per iteration (ms): 4254.3 | learning rate: 1.536E-05 | global batch size:   128 | lm loss: 6.174018E+00 | loss scale: 4096.0 | grad norm: 1.756 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.087 | tokens per gpu per second (tgs): 1925.594 | TFLOPs: 15.50 |
g0345: [2024-08-02 19:16:42,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.553640106666667e-05, 1.553640106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 890 loss: 6.1688 iter time (s): 4.229 samples/sec: 30.269
g0364:  iteration      890/10000000 | consumed samples:       113920 | consumed tokens:    233308160 | elapsed time per iteration (ms): 4261.3 | learning rate: 1.554E-05 | global batch size:   128 | lm loss: 6.163602E+00 | loss scale: 4096.0 | grad norm: 1.496 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.038 | tokens per gpu per second (tgs): 1922.423 | TFLOPs: 15.47 |
g0345: [2024-08-02 19:17:23,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.5711163733333335e-05, 1.5711163733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 900 loss: 6.1485 iter time (s): 4.126 samples/sec: 31.022
g0364:  iteration      900/10000000 | consumed samples:       115200 | consumed tokens:    235929600 | elapsed time per iteration (ms): 4160.9 | learning rate: 1.571E-05 | global batch size:   128 | lm loss: 6.155320E+00 | loss scale: 4096.0 | grad norm: 1.452 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.763 | tokens per gpu per second (tgs): 1968.826 | TFLOPs: 15.84 |
g0345: [2024-08-02 19:18:05,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.58859264e-05, 1.58859264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 910 loss: 6.1377 iter time (s): 4.178 samples/sec: 30.636
g0364:  iteration      910/10000000 | consumed samples:       116480 | consumed tokens:    238551040 | elapsed time per iteration (ms): 4210.9 | learning rate: 1.589E-05 | global batch size:   128 | lm loss: 6.149080E+00 | loss scale: 4096.0 | grad norm: 1.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.397 | tokens per gpu per second (tgs): 1945.430 | TFLOPs: 15.66 |
g0345: [2024-08-02 19:18:47,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[1.6060689066666667e-05, 1.6060689066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 920 loss: 6.1431 iter time (s): 4.160 samples/sec: 30.766
g0364:  iteration      920/10000000 | consumed samples:       117760 | consumed tokens:    241172480 | elapsed time per iteration (ms): 4193.1 | learning rate: 1.606E-05 | global batch size:   128 | lm loss: 6.131353E+00 | loss scale: 4096.0 | grad norm: 1.961 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.526 | tokens per gpu per second (tgs): 1953.666 | TFLOPs: 15.72 |
g0345: [2024-08-02 19:19:29,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[1.6235451733333336e-05, 1.6235451733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 930 loss: 6.1276 iter time (s): 4.132 samples/sec: 30.981
g0364:  iteration      930/10000000 | consumed samples:       119040 | consumed tokens:    243793920 | elapsed time per iteration (ms): 4178.3 | learning rate: 1.624E-05 | global batch size:   128 | lm loss: 6.134153E+00 | loss scale: 4096.0 | grad norm: 2.069 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.634 | tokens per gpu per second (tgs): 1960.593 | TFLOPs: 15.78 |
g0345: [2024-08-02 19:20:11,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[1.6410214400000002e-05, 1.6410214400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 940 loss: 6.0963 iter time (s): 4.171 samples/sec: 30.685
g0364:  iteration      940/10000000 | consumed samples:       120320 | consumed tokens:    246415360 | elapsed time per iteration (ms): 4205.0 | learning rate: 1.641E-05 | global batch size:   128 | lm loss: 6.114042E+00 | loss scale: 4096.0 | grad norm: 2.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.440 | tokens per gpu per second (tgs): 1948.156 | TFLOPs: 15.68 |
g0345: [2024-08-02 19:20:53,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[1.6584977066666665e-05, 1.6584977066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 950 loss: 6.0935 iter time (s): 4.161 samples/sec: 30.764
g0364:  iteration      950/10000000 | consumed samples:       121600 | consumed tokens:    249036800 | elapsed time per iteration (ms): 4192.9 | learning rate: 1.658E-05 | global batch size:   128 | lm loss: 6.112182E+00 | loss scale: 4096.0 | grad norm: 1.765 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.765 | TFLOPs: 15.72 |
g0345: [2024-08-02 19:21:35,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[1.6759739733333334e-05, 1.6759739733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 960 loss: 6.1026 iter time (s): 4.159 samples/sec: 30.779
g0364:  iteration      960/10000000 | consumed samples:       122880 | consumed tokens:    251658240 | elapsed time per iteration (ms): 4190.9 | learning rate: 1.676E-05 | global batch size:   128 | lm loss: 6.088235E+00 | loss scale: 4096.0 | grad norm: 1.978 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.542 | tokens per gpu per second (tgs): 1954.691 | TFLOPs: 15.73 |
g0345: [2024-08-02 19:22:17,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[1.69345024e-05, 1.69345024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 970 loss: 6.0582 iter time (s): 4.196 samples/sec: 30.505
g0364:  iteration      970/10000000 | consumed samples:       124160 | consumed tokens:    254279680 | elapsed time per iteration (ms): 4228.6 | learning rate: 1.693E-05 | global batch size:   128 | lm loss: 6.084806E+00 | loss scale: 4096.0 | grad norm: 1.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.270 | tokens per gpu per second (tgs): 1937.291 | TFLOPs: 15.59 |
g0345: [2024-08-02 19:22:59,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[1.7109265066666667e-05, 1.7109265066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 980 loss: 6.0698 iter time (s): 4.122 samples/sec: 31.053
g0364:  iteration      980/10000000 | consumed samples:       125440 | consumed tokens:    256901120 | elapsed time per iteration (ms): 4154.7 | learning rate: 1.711E-05 | global batch size:   128 | lm loss: 6.060508E+00 | loss scale: 4096.0 | grad norm: 2.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.808 | tokens per gpu per second (tgs): 1971.737 | TFLOPs: 15.87 |
g0345: [2024-08-02 19:23:41,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[1.7284027733333333e-05, 1.7284027733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 990 loss: 6.0326 iter time (s): 4.155 samples/sec: 30.803
g0364:  iteration      990/10000000 | consumed samples:       126720 | consumed tokens:    259522560 | elapsed time per iteration (ms): 4187.9 | learning rate: 1.728E-05 | global batch size:   128 | lm loss: 6.053566E+00 | loss scale: 4096.0 | grad norm: 1.848 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.564 | tokens per gpu per second (tgs): 1956.108 | TFLOPs: 15.74 |
g0345: [2024-08-02 19:24:22,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.7458790400000002e-05, 1.7458790400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1000 loss: 6.0531 iter time (s): 4.144 samples/sec: 30.886
g0364:  iteration     1000/10000000 | consumed samples:       128000 | consumed tokens:    262144000 | elapsed time per iteration (ms): 4176.6 | learning rate: 1.746E-05 | global batch size:   128 | lm loss: 6.036452E+00 | loss scale: 4096.0 | grad norm: 2.361 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.647 | tokens per gpu per second (tgs): 1961.413 | TFLOPs: 15.78 |
g0364: ------------------------------------------------------------------------------------------------
g0364:  validation loss at iteration 1000 | lm loss value: 6.019457E+00 | lm loss PPL: 4.113551E+02 | 
g0364: ------------------------------------------------------------------------------------------------
g0345: saving checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: [2024-08-02 19:30:51,249] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
g0345: [2024-08-02 19:30:51,255] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0345: [2024-08-02 19:30:51,255] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0345: [2024-08-02 19:30:51,255] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0364: [2024-08-02 19:30:51,256] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0364: [2024-08-02 19:30:51,256] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0364: [2024-08-02 19:30:51,256] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0346: [2024-08-02 19:30:51,257] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0346: [2024-08-02 19:30:51,257] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0346: [2024-08-02 19:30:51,257] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0352: [2024-08-02 19:30:51,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0352: [2024-08-02 19:30:51,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0352: [2024-08-02 19:30:51,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0363: [2024-08-02 19:30:51,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0363: [2024-08-02 19:30:51,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0347: [2024-08-02 19:30:51,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0347: [2024-08-02 19:30:51,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0347: [2024-08-02 19:30:51,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0363: [2024-08-02 19:30:51,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0358: [2024-08-02 19:30:51,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0358: [2024-08-02 19:30:51,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0358: [2024-08-02 19:30:51,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0362: [2024-08-02 19:30:51,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0362: [2024-08-02 19:30:51,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0362: [2024-08-02 19:30:51,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0364: [2024-08-02 19:30:51,283] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt...
g0346: [2024-08-02 19:30:51,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt...
g0358: [2024-08-02 19:30:51,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt...
g0362: [2024-08-02 19:30:51,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt...
g0347: [2024-08-02 19:30:51,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt...
g0352: [2024-08-02 19:30:51,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt...
g0363: [2024-08-02 19:30:51,296] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt...
g0345: [2024-08-02 19:30:51,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt...
g0347: [2024-08-02 19:30:51,426] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt.
g0362: [2024-08-02 19:30:51,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt.
g0363: [2024-08-02 19:30:51,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt.
g0346: [2024-08-02 19:30:51,459] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt.
g0358: [2024-08-02 19:30:51,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt.
g0364: [2024-08-02 19:30:51,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt.
g0364: [2024-08-02 19:30:51,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt...
g0347: [2024-08-02 19:30:51,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt...
g0364: [2024-08-02 19:30:51,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt.
g0362: [2024-08-02 19:30:51,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt...
g0363: [2024-08-02 19:30:51,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt...
g0346: [2024-08-02 19:30:51,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt...
g0358: [2024-08-02 19:30:51,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt...
g0364: [2024-08-02 19:30:51,509] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt...
g0352: [2024-08-02 19:30:51,531] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt.
g0345: [2024-08-02 19:30:51,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt.
g0352: [2024-08-02 19:30:51,570] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt...
g0345: [2024-08-02 19:30:51,583] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt...
g0347: [2024-08-02 19:30:51,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt.
g0347: [2024-08-02 19:30:51,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt...
g0362: [2024-08-02 19:30:51,645] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt.
g0352: [2024-08-02 19:30:51,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt.
g0362: [2024-08-02 19:30:51,678] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt...
g0364: [2024-08-02 19:30:51,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt.
g0364: [2024-08-02 19:30:51,682] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt...
g0352: [2024-08-02 19:30:51,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt...
g0362: [2024-08-02 19:30:51,775] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt.
g0362: [2024-08-02 19:30:51,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt...
g0347: [2024-08-02 19:30:51,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt.
g0347: [2024-08-02 19:30:51,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt...
g0358: [2024-08-02 19:30:51,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt.
g0352: [2024-08-02 19:30:51,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt.
g0352: [2024-08-02 19:30:51,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt...
g0346: [2024-08-02 19:30:51,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt.
g0358: [2024-08-02 19:30:51,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt...
g0346: [2024-08-02 19:30:51,864] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt...
g0345: [2024-08-02 19:30:51,887] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt.
g0345: [2024-08-02 19:30:51,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt...
g0363: [2024-08-02 19:30:51,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt.
g0358: [2024-08-02 19:30:51,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt.
g0358: [2024-08-02 19:30:51,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt...
g0363: [2024-08-02 19:30:51,952] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt...
g0346: [2024-08-02 19:30:51,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt.
g0346: [2024-08-02 19:30:51,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt...
g0345: [2024-08-02 19:30:52,010] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt.
g0345: [2024-08-02 19:30:52,035] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt...
g0363: [2024-08-02 19:30:52,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt.
g0363: [2024-08-02 19:30:52,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt...
g0345: [2024-08-02 19:30:52,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt.
g0345: [2024-08-02 19:30:52,231] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt
g0345: [2024-08-02 19:30:52,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt...
g0364: [2024-08-02 19:30:53,524] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt.
g0364: [2024-08-02 19:30:53,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0347: [2024-08-02 19:30:54,078] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt.
g0347: [2024-08-02 19:30:54,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0352: [2024-08-02 19:30:54,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt.
g0352: [2024-08-02 19:30:54,186] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0358: [2024-08-02 19:30:54,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt.
g0358: [2024-08-02 19:30:54,249] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0346: [2024-08-02 19:30:54,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt.
g0346: [2024-08-02 19:30:54,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0363: [2024-08-02 19:30:54,526] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt.
g0363: [2024-08-02 19:30:54,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0362: [2024-08-02 19:30:55,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt.
g0362: [2024-08-02 19:30:55,867] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0345: [2024-08-02 19:30:56,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt.
g0345: [2024-08-02 19:30:56,120] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0345:   successfully saved checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: Checkpoint Save GB: 22.521, GB/Sec: 4.61, Latency(second): 4.886
g0364: (min, max) time across ranks (ms):
g0364:     save-checkpoint ................................: (4885.60, 4885.80)
g0347: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0347: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0345: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0358: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0358: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0358: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0345: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0363: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0347: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0358: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0347: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0358: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0363: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0362: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0362: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0362: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0346: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0352: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0363: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0364: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0346: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0346: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0352: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0352: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0363: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0352: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0364: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0364: [2024-08-02 19:31:00,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0364: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0364: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0362: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0352: [2024-08-02 19:31:00,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0345: [2024-08-02 19:31:37,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[1.7633553066666668e-05, 1.7633553066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1010 loss: 6.0512 iter time (s): 4.104 samples/sec: 31.188
g0364:  iteration     1010/10000000 | consumed samples:       129280 | consumed tokens:    264765440 | elapsed time per iteration (ms): 43463.8 | learning rate: 1.763E-05 | global batch size:   128 | lm loss: 6.029456E+00 | loss scale: 8192.0 | grad norm: 1.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.945 | tokens per gpu per second (tgs): 188.479 | TFLOPs: 1.52 |
g0345: [2024-08-02 19:32:18,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[1.7808315733333334e-05, 1.7808315733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1020 loss: 5.9404 iter time (s): 4.075 samples/sec: 31.409
g0364:  iteration     1020/10000000 | consumed samples:       130560 | consumed tokens:    267386880 | elapsed time per iteration (ms): 4107.7 | learning rate: 1.781E-05 | global batch size:   128 | lm loss: 5.998033E+00 | loss scale: 8192.0 | grad norm: 2.548 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.161 | tokens per gpu per second (tgs): 1994.288 | TFLOPs: 16.05 |
g0345: [2024-08-02 19:33:00,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[1.79830784e-05, 1.79830784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1030 loss: 5.9540 iter time (s): 4.208 samples/sec: 30.421
g0364:  iteration     1030/10000000 | consumed samples:       131840 | consumed tokens:    270008320 | elapsed time per iteration (ms): 4239.9 | learning rate: 1.798E-05 | global batch size:   128 | lm loss: 5.987583E+00 | loss scale: 8192.0 | grad norm: 2.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.189 | tokens per gpu per second (tgs): 1932.115 | TFLOPs: 15.55 |
g0345: [2024-08-02 19:33:42,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[1.8157841066666666e-05, 1.8157841066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1040 loss: 5.9488 iter time (s): 4.169 samples/sec: 30.705
g0364:  iteration     1040/10000000 | consumed samples:       133120 | consumed tokens:    272629760 | elapsed time per iteration (ms): 4201.0 | learning rate: 1.816E-05 | global batch size:   128 | lm loss: 5.964307E+00 | loss scale: 8192.0 | grad norm: 3.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.469 | tokens per gpu per second (tgs): 1949.991 | TFLOPs: 15.69 |
g0345: [2024-08-02 19:34:23,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[1.8332603733333336e-05, 1.8332603733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1050 loss: 5.9394 iter time (s): 4.056 samples/sec: 31.557
g0364:  iteration     1050/10000000 | consumed samples:       134400 | consumed tokens:    275251200 | elapsed time per iteration (ms): 4088.0 | learning rate: 1.833E-05 | global batch size:   128 | lm loss: 5.950660E+00 | loss scale: 8192.0 | grad norm: 2.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.311 | tokens per gpu per second (tgs): 2003.908 | TFLOPs: 16.13 |
g0345: [2024-08-02 19:35:04,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[1.8507366400000002e-05, 1.8507366400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1060 loss: 5.8839 iter time (s): 4.083 samples/sec: 31.350
g0364:  iteration     1060/10000000 | consumed samples:       135680 | consumed tokens:    277872640 | elapsed time per iteration (ms): 4115.9 | learning rate: 1.851E-05 | global batch size:   128 | lm loss: 5.923167E+00 | loss scale: 8192.0 | grad norm: 3.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.099 | tokens per gpu per second (tgs): 1990.307 | TFLOPs: 16.02 |
g0345: [2024-08-02 19:35:45,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[1.8682129066666668e-05, 1.8682129066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1070 loss: 5.8984 iter time (s): 3.993 samples/sec: 32.056
g0364:  iteration     1070/10000000 | consumed samples:       136960 | consumed tokens:    280494080 | elapsed time per iteration (ms): 4025.2 | learning rate: 1.868E-05 | global batch size:   128 | lm loss: 5.911370E+00 | loss scale: 8192.0 | grad norm: 3.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.799 | tokens per gpu per second (tgs): 2035.155 | TFLOPs: 16.38 |
g0345: [2024-08-02 19:36:27,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[1.8856891733333334e-05, 1.8856891733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1080 loss: 5.8540 iter time (s): 4.161 samples/sec: 30.758
g0364:  iteration     1080/10000000 | consumed samples:       138240 | consumed tokens:    283115520 | elapsed time per iteration (ms): 4193.7 | learning rate: 1.886E-05 | global batch size:   128 | lm loss: 5.883961E+00 | loss scale: 8192.0 | grad norm: 2.500 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.522 | tokens per gpu per second (tgs): 1953.396 | TFLOPs: 15.72 |
g0345: [2024-08-02 19:37:07,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[1.9031654400000003e-05, 1.9031654400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1090 loss: 5.9000 iter time (s): 4.040 samples/sec: 31.683
g0364:  iteration     1090/10000000 | consumed samples:       139520 | consumed tokens:    285736960 | elapsed time per iteration (ms): 4072.8 | learning rate: 1.903E-05 | global batch size:   128 | lm loss: 5.856219E+00 | loss scale: 8192.0 | grad norm: 2.535 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.428 | tokens per gpu per second (tgs): 2011.395 | TFLOPs: 16.19 |
g0345: [2024-08-02 19:37:49,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[1.920641706666667e-05, 1.920641706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1100 loss: 5.8058 iter time (s): 4.120 samples/sec: 31.069
g0364:  iteration     1100/10000000 | consumed samples:       140800 | consumed tokens:    288358400 | elapsed time per iteration (ms): 4151.8 | learning rate: 1.921E-05 | global batch size:   128 | lm loss: 5.816917E+00 | loss scale: 8192.0 | grad norm: 1.896 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.830 | tokens per gpu per second (tgs): 1973.103 | TFLOPs: 15.88 |
g0345: [2024-08-02 19:38:30,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[1.9381179733333332e-05, 1.9381179733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1110 loss: 5.7554 iter time (s): 4.043 samples/sec: 31.658
g0364:  iteration     1110/10000000 | consumed samples:       142080 | consumed tokens:    290979840 | elapsed time per iteration (ms): 4075.7 | learning rate: 1.938E-05 | global batch size:   128 | lm loss: 5.804633E+00 | loss scale: 8192.0 | grad norm: 2.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.406 | tokens per gpu per second (tgs): 2009.956 | TFLOPs: 16.17 |
g0345: [2024-08-02 19:39:12,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[1.95559424e-05, 1.95559424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1120 loss: 5.7410 iter time (s): 4.161 samples/sec: 30.765
g0364:  iteration     1120/10000000 | consumed samples:       143360 | consumed tokens:    293601280 | elapsed time per iteration (ms): 4193.2 | learning rate: 1.956E-05 | global batch size:   128 | lm loss: 5.785170E+00 | loss scale: 8192.0 | grad norm: 4.312 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.526 | tokens per gpu per second (tgs): 1953.634 | TFLOPs: 15.72 |
g0345: [2024-08-02 19:39:53,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[1.9730705066666668e-05, 1.9730705066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1130 loss: 5.7579 iter time (s): 4.112 samples/sec: 31.128
g0364:  iteration     1130/10000000 | consumed samples:       144640 | consumed tokens:    296222720 | elapsed time per iteration (ms): 4144.5 | learning rate: 1.973E-05 | global batch size:   128 | lm loss: 5.777556E+00 | loss scale: 8192.0 | grad norm: 3.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.884 | tokens per gpu per second (tgs): 1976.604 | TFLOPs: 15.91 |
g0345: [2024-08-02 19:40:34,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[1.9905467733333334e-05, 1.9905467733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1140 loss: 5.7519 iter time (s): 4.050 samples/sec: 31.606
g0364:  iteration     1140/10000000 | consumed samples:       145920 | consumed tokens:    298844160 | elapsed time per iteration (ms): 4082.3 | learning rate: 1.991E-05 | global batch size:   128 | lm loss: 5.753416E+00 | loss scale: 8192.0 | grad norm: 3.846 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.355 | tokens per gpu per second (tgs): 2006.720 | TFLOPs: 16.15 |
g0345: [2024-08-02 19:41:17,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[2.00802304e-05, 2.00802304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1150 loss: 5.6969 iter time (s): 4.262 samples/sec: 30.031
g0364:  iteration     1150/10000000 | consumed samples:       147200 | consumed tokens:    301465600 | elapsed time per iteration (ms): 4294.8 | learning rate: 2.008E-05 | global batch size:   128 | lm loss: 5.728183E+00 | loss scale: 8192.0 | grad norm: 2.917 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.804 | tokens per gpu per second (tgs): 1907.445 | TFLOPs: 15.35 |
g0345: [2024-08-02 19:41:59,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[2.0254993066666666e-05, 2.0254993066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1160 loss: 5.6656 iter time (s): 4.155 samples/sec: 30.803
g0364:  iteration     1160/10000000 | consumed samples:       148480 | consumed tokens:    304087040 | elapsed time per iteration (ms): 4187.8 | learning rate: 2.025E-05 | global batch size:   128 | lm loss: 5.696117E+00 | loss scale: 8192.0 | grad norm: 4.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.565 | tokens per gpu per second (tgs): 1956.169 | TFLOPs: 15.74 |
g0345: [2024-08-02 19:42:40,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[2.0429755733333335e-05, 2.0429755733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1170 loss: 5.6066 iter time (s): 4.089 samples/sec: 31.306
g0364:  iteration     1170/10000000 | consumed samples:       149760 | consumed tokens:    306708480 | elapsed time per iteration (ms): 4121.1 | learning rate: 2.043E-05 | global batch size:   128 | lm loss: 5.662688E+00 | loss scale: 8192.0 | grad norm: 2.513 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.060 | tokens per gpu per second (tgs): 1987.827 | TFLOPs: 16.00 |
g0345: [2024-08-02 19:43:20,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[2.06045184e-05, 2.06045184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1180 loss: 5.6399 iter time (s): 4.000 samples/sec: 31.997
g0364:  iteration     1180/10000000 | consumed samples:       151040 | consumed tokens:    309329920 | elapsed time per iteration (ms): 4032.5 | learning rate: 2.060E-05 | global batch size:   128 | lm loss: 5.620867E+00 | loss scale: 8192.0 | grad norm: 3.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.742 | tokens per gpu per second (tgs): 2031.469 | TFLOPs: 16.35 |
g0345: [2024-08-02 19:44:01,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[2.0779281066666667e-05, 2.0779281066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1190 loss: 5.5466 iter time (s): 4.055 samples/sec: 31.569
g0364:  iteration     1190/10000000 | consumed samples:       152320 | consumed tokens:    311951360 | elapsed time per iteration (ms): 4086.8 | learning rate: 2.078E-05 | global batch size:   128 | lm loss: 5.585067E+00 | loss scale: 8192.0 | grad norm: 2.930 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.320 | tokens per gpu per second (tgs): 2004.504 | TFLOPs: 16.13 |
g0345: [2024-08-02 19:44:41,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[2.0954043733333333e-05, 2.0954043733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1200 loss: 5.5641 iter time (s): 4.007 samples/sec: 31.941
g0364:  iteration     1200/10000000 | consumed samples:       153600 | consumed tokens:    314572800 | elapsed time per iteration (ms): 4039.6 | learning rate: 2.095E-05 | global batch size:   128 | lm loss: 5.571687E+00 | loss scale: 8192.0 | grad norm: 2.742 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.686 | tokens per gpu per second (tgs): 2027.904 | TFLOPs: 16.32 |
g0345: [2024-08-02 19:45:24,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[2.1128806400000003e-05, 2.1128806400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1210 loss: 5.5694 iter time (s): 4.253 samples/sec: 30.098
g0364:  iteration     1210/10000000 | consumed samples:       154880 | consumed tokens:    317194240 | elapsed time per iteration (ms): 4285.1 | learning rate: 2.113E-05 | global batch size:   128 | lm loss: 5.579029E+00 | loss scale: 8192.0 | grad norm: 2.514 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.871 | tokens per gpu per second (tgs): 1911.726 | TFLOPs: 15.38 |
g0345: [2024-08-02 19:46:07,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[2.130356906666667e-05, 2.130356906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1220 loss: 5.4857 iter time (s): 4.239 samples/sec: 30.193
g0364:  iteration     1220/10000000 | consumed samples:       156160 | consumed tokens:    319815680 | elapsed time per iteration (ms): 4271.9 | learning rate: 2.130E-05 | global batch size:   128 | lm loss: 5.513334E+00 | loss scale: 8192.0 | grad norm: 3.543 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.963 | tokens per gpu per second (tgs): 1917.662 | TFLOPs: 15.43 |
g0345: [2024-08-02 19:46:48,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[2.1478331733333335e-05, 2.1478331733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1230 loss: 5.5046 iter time (s): 4.102 samples/sec: 31.202
g0364:  iteration     1230/10000000 | consumed samples:       157440 | consumed tokens:    322437120 | elapsed time per iteration (ms): 4134.5 | learning rate: 2.148E-05 | global batch size:   128 | lm loss: 5.521269E+00 | loss scale: 8192.0 | grad norm: 2.778 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.959 | tokens per gpu per second (tgs): 1981.382 | TFLOPs: 15.94 |
g0345: [2024-08-02 19:47:30,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[2.16530944e-05, 2.16530944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1240 loss: 5.4097 iter time (s): 4.102 samples/sec: 31.204
g0364:  iteration     1240/10000000 | consumed samples:       158720 | consumed tokens:    325058560 | elapsed time per iteration (ms): 4134.3 | learning rate: 2.165E-05 | global batch size:   128 | lm loss: 5.475774E+00 | loss scale: 8192.0 | grad norm: 3.784 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.961 | tokens per gpu per second (tgs): 1981.475 | TFLOPs: 15.95 |
g0345: [2024-08-02 19:48:11,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[2.1827857066666667e-05, 2.1827857066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1250 loss: 5.3133 iter time (s): 4.109 samples/sec: 31.149
g0364:  iteration     1250/10000000 | consumed samples:       160000 | consumed tokens:    327680000 | elapsed time per iteration (ms): 4141.9 | learning rate: 2.183E-05 | global batch size:   128 | lm loss: 5.385834E+00 | loss scale: 8192.0 | grad norm: 2.665 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.903 | tokens per gpu per second (tgs): 1977.818 | TFLOPs: 15.92 |
g0345: [2024-08-02 19:48:52,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[2.2002619733333337e-05, 2.2002619733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1260 loss: 5.3894 iter time (s): 4.058 samples/sec: 31.539
g0364:  iteration     1260/10000000 | consumed samples:       161280 | consumed tokens:    330301440 | elapsed time per iteration (ms): 4091.1 | learning rate: 2.200E-05 | global batch size:   128 | lm loss: 5.375024E+00 | loss scale: 8192.0 | grad norm: 3.097 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.288 | tokens per gpu per second (tgs): 2002.418 | TFLOPs: 16.11 |
g0345: [2024-08-02 19:49:34,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[2.2177382400000003e-05, 2.2177382400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1270 loss: 5.3515 iter time (s): 4.143 samples/sec: 30.893
g0364:  iteration     1270/10000000 | consumed samples:       162560 | consumed tokens:    332922880 | elapsed time per iteration (ms): 4175.8 | learning rate: 2.218E-05 | global batch size:   128 | lm loss: 5.372059E+00 | loss scale: 8192.0 | grad norm: 2.695 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.769 | TFLOPs: 15.79 |
g0345: [2024-08-02 19:50:15,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[2.235214506666667e-05, 2.235214506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1280 loss: 5.3187 iter time (s): 4.040 samples/sec: 31.685
g0364:  iteration     1280/10000000 | consumed samples:       163840 | consumed tokens:    335544320 | elapsed time per iteration (ms): 4072.2 | learning rate: 2.235E-05 | global batch size:   128 | lm loss: 5.367287E+00 | loss scale: 8192.0 | grad norm: 3.087 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.432 | tokens per gpu per second (tgs): 2011.667 | TFLOPs: 16.19 |
g0345: [2024-08-02 19:50:57,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[2.2526907733333335e-05, 2.2526907733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1290 loss: 5.2175 iter time (s): 4.207 samples/sec: 30.426
g0364:  iteration     1290/10000000 | consumed samples:       165120 | consumed tokens:    338165760 | elapsed time per iteration (ms): 4241.2 | learning rate: 2.253E-05 | global batch size:   128 | lm loss: 5.346322E+00 | loss scale: 8192.0 | grad norm: 3.040 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.180 | tokens per gpu per second (tgs): 1931.511 | TFLOPs: 15.54 |
g0345: [2024-08-02 19:51:38,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[2.2701670400000004e-05, 2.2701670400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1300 loss: 5.2350 iter time (s): 4.059 samples/sec: 31.538
g0364:  iteration     1300/10000000 | consumed samples:       166400 | consumed tokens:    340787200 | elapsed time per iteration (ms): 4091.0 | learning rate: 2.270E-05 | global batch size:   128 | lm loss: 5.269186E+00 | loss scale: 8192.0 | grad norm: 3.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.288 | tokens per gpu per second (tgs): 2002.438 | TFLOPs: 16.11 |
g0345: [2024-08-02 19:52:19,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[2.287643306666667e-05, 2.287643306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1310 loss: 5.2629 iter time (s): 4.126 samples/sec: 31.025
g0364:  iteration     1310/10000000 | consumed samples:       167680 | consumed tokens:    343408640 | elapsed time per iteration (ms): 4158.5 | learning rate: 2.288E-05 | global batch size:   128 | lm loss: 5.238373E+00 | loss scale: 8192.0 | grad norm: 2.928 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.780 | tokens per gpu per second (tgs): 1969.932 | TFLOPs: 15.85 |
g0345: [2024-08-02 19:53:01,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[2.3051195733333336e-05, 2.3051195733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1320 loss: 5.2299 iter time (s): 4.151 samples/sec: 30.832
g0364:  iteration     1320/10000000 | consumed samples:       168960 | consumed tokens:    346030080 | elapsed time per iteration (ms): 4183.9 | learning rate: 2.305E-05 | global batch size:   128 | lm loss: 5.211580E+00 | loss scale: 8192.0 | grad norm: 2.994 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.593 | tokens per gpu per second (tgs): 1957.970 | TFLOPs: 15.76 |
g0345: [2024-08-02 19:53:43,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[2.3225958400000002e-05, 2.3225958400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1330 loss: 5.2069 iter time (s): 4.150 samples/sec: 30.843
g0364:  iteration     1330/10000000 | consumed samples:       170240 | consumed tokens:    348651520 | elapsed time per iteration (ms): 4182.4 | learning rate: 2.323E-05 | global batch size:   128 | lm loss: 5.218366E+00 | loss scale: 8192.0 | grad norm: 4.145 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.604 | tokens per gpu per second (tgs): 1958.679 | TFLOPs: 15.76 |
g0345: [2024-08-02 19:54:25,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[2.340072106666667e-05, 2.340072106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1340 loss: 5.1657 iter time (s): 4.117 samples/sec: 31.087
g0364:  iteration     1340/10000000 | consumed samples:       171520 | consumed tokens:    351272960 | elapsed time per iteration (ms): 4149.9 | learning rate: 2.340E-05 | global batch size:   128 | lm loss: 5.193626E+00 | loss scale: 8192.0 | grad norm: 3.126 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.844 | tokens per gpu per second (tgs): 1974.037 | TFLOPs: 15.89 |
g0345: [2024-08-02 19:55:07,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[2.3575483733333338e-05, 2.3575483733333338e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1350 loss: 5.2114 iter time (s): 4.181 samples/sec: 30.614
g0364:  iteration     1350/10000000 | consumed samples:       172800 | consumed tokens:    353894400 | elapsed time per iteration (ms): 4213.6 | learning rate: 2.358E-05 | global batch size:   128 | lm loss: 5.168342E+00 | loss scale: 8192.0 | grad norm: 2.712 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.378 | tokens per gpu per second (tgs): 1944.160 | TFLOPs: 15.64 |
g0345: [2024-08-02 19:55:49,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[2.3750246399999997e-05, 2.3750246399999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1360 loss: 5.1783 iter time (s): 4.165 samples/sec: 30.732
g0364:  iteration     1360/10000000 | consumed samples:       174080 | consumed tokens:    356515840 | elapsed time per iteration (ms): 4197.6 | learning rate: 2.375E-05 | global batch size:   128 | lm loss: 5.135759E+00 | loss scale: 8192.0 | grad norm: 2.774 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.493 | tokens per gpu per second (tgs): 1951.572 | TFLOPs: 15.70 |
g0345: [2024-08-02 19:56:30,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[2.3925009066666667e-05, 2.3925009066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1370 loss: 5.0464 iter time (s): 4.058 samples/sec: 31.541
g0364:  iteration     1370/10000000 | consumed samples:       175360 | consumed tokens:    359137280 | elapsed time per iteration (ms): 4090.8 | learning rate: 2.393E-05 | global batch size:   128 | lm loss: 5.055160E+00 | loss scale: 8192.0 | grad norm: 3.366 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.290 | tokens per gpu per second (tgs): 2002.533 | TFLOPs: 16.11 |
g0345: [2024-08-02 19:57:12,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[2.4099771733333333e-05, 2.4099771733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1380 loss: 5.2092 iter time (s): 4.190 samples/sec: 30.547
g0364:  iteration     1380/10000000 | consumed samples:       176640 | consumed tokens:    361758720 | elapsed time per iteration (ms): 4222.6 | learning rate: 2.410E-05 | global batch size:   128 | lm loss: 5.072968E+00 | loss scale: 8192.0 | grad norm: 2.976 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.313 | tokens per gpu per second (tgs): 1940.038 | TFLOPs: 15.61 |
g0345: [2024-08-02 19:57:54,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[2.42745344e-05, 2.42745344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1390 loss: 5.0692 iter time (s): 4.185 samples/sec: 30.588
g0364:  iteration     1390/10000000 | consumed samples:       177920 | consumed tokens:    364380160 | elapsed time per iteration (ms): 4216.8 | learning rate: 2.427E-05 | global batch size:   128 | lm loss: 5.007437E+00 | loss scale: 8192.0 | grad norm: 3.370 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.355 | tokens per gpu per second (tgs): 1942.699 | TFLOPs: 15.63 |
g0345: [2024-08-02 19:58:36,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[2.4449297066666665e-05, 2.4449297066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1400 loss: 4.9909 iter time (s): 4.184 samples/sec: 30.593
g0364:  iteration     1400/10000000 | consumed samples:       179200 | consumed tokens:    367001600 | elapsed time per iteration (ms): 4216.3 | learning rate: 2.445E-05 | global batch size:   128 | lm loss: 4.999440E+00 | loss scale: 8192.0 | grad norm: 2.847 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.359 | tokens per gpu per second (tgs): 1942.949 | TFLOPs: 15.64 |
g0345: [2024-08-02 19:59:18,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[2.4624059733333334e-05, 2.4624059733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1410 loss: 4.8071 iter time (s): 4.150 samples/sec: 30.840
g0364:  iteration     1410/10000000 | consumed samples:       180480 | consumed tokens:    369623040 | elapsed time per iteration (ms): 4183.3 | learning rate: 2.462E-05 | global batch size:   128 | lm loss: 4.992890E+00 | loss scale: 8192.0 | grad norm: 2.990 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.598 | tokens per gpu per second (tgs): 1958.242 | TFLOPs: 15.76 |
g0345: [2024-08-02 20:00:00,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[2.47988224e-05, 2.47988224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1420 loss: 5.0768 iter time (s): 4.199 samples/sec: 30.480
g0364:  iteration     1420/10000000 | consumed samples:       181760 | consumed tokens:    372244480 | elapsed time per iteration (ms): 4231.8 | learning rate: 2.480E-05 | global batch size:   128 | lm loss: 4.974914E+00 | loss scale: 8192.0 | grad norm: 2.910 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.248 | tokens per gpu per second (tgs): 1935.840 | TFLOPs: 15.58 |
g0345: [2024-08-02 20:00:42,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[2.4973585066666666e-05, 2.4973585066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1430 loss: 5.0544 iter time (s): 4.123 samples/sec: 31.046
g0364:  iteration     1430/10000000 | consumed samples:       183040 | consumed tokens:    374865920 | elapsed time per iteration (ms): 4154.9 | learning rate: 2.497E-05 | global batch size:   128 | lm loss: 4.921203E+00 | loss scale: 8192.0 | grad norm: 3.328 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.807 | tokens per gpu per second (tgs): 1971.637 | TFLOPs: 15.87 |
g0345: [2024-08-02 20:01:24,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[2.5148347733333333e-05, 2.5148347733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1440 loss: 4.9195 iter time (s): 4.187 samples/sec: 30.567
g0364:  iteration     1440/10000000 | consumed samples:       184320 | consumed tokens:    377487360 | elapsed time per iteration (ms): 4219.6 | learning rate: 2.515E-05 | global batch size:   128 | lm loss: 4.930381E+00 | loss scale: 8192.0 | grad norm: 3.027 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.335 | tokens per gpu per second (tgs): 1941.423 | TFLOPs: 15.62 |
g0345: [2024-08-02 20:02:05,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[2.53231104e-05, 2.53231104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1450 loss: 4.8797 iter time (s): 4.090 samples/sec: 31.295
g0364:  iteration     1450/10000000 | consumed samples:       185600 | consumed tokens:    380108800 | elapsed time per iteration (ms): 4122.2 | learning rate: 2.532E-05 | global batch size:   128 | lm loss: 4.871530E+00 | loss scale: 8192.0 | grad norm: 2.691 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.052 | tokens per gpu per second (tgs): 1987.308 | TFLOPs: 15.99 |
g0345: [2024-08-02 20:02:47,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[2.5497873066666668e-05, 2.5497873066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1460 loss: 4.7499 iter time (s): 4.162 samples/sec: 30.757
g0364:  iteration     1460/10000000 | consumed samples:       186880 | consumed tokens:    382730240 | elapsed time per iteration (ms): 4193.8 | learning rate: 2.550E-05 | global batch size:   128 | lm loss: 4.823995E+00 | loss scale: 8192.0 | grad norm: 3.123 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.521 | tokens per gpu per second (tgs): 1953.338 | TFLOPs: 15.72 |
g0345: [2024-08-02 20:03:29,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[2.5672635733333334e-05, 2.5672635733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1470 loss: 4.8175 iter time (s): 4.169 samples/sec: 30.699
g0364:  iteration     1470/10000000 | consumed samples:       188160 | consumed tokens:    385351680 | elapsed time per iteration (ms): 4201.6 | learning rate: 2.567E-05 | global batch size:   128 | lm loss: 4.840955E+00 | loss scale: 8192.0 | grad norm: 2.838 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.465 | tokens per gpu per second (tgs): 1949.747 | TFLOPs: 15.69 |
g0345: [2024-08-02 20:04:11,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[2.58473984e-05, 2.58473984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1480 loss: 4.7834 iter time (s): 4.114 samples/sec: 31.110
g0364:  iteration     1480/10000000 | consumed samples:       189440 | consumed tokens:    387973120 | elapsed time per iteration (ms): 4146.6 | learning rate: 2.585E-05 | global batch size:   128 | lm loss: 4.821747E+00 | loss scale: 8192.0 | grad norm: 2.835 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.869 | tokens per gpu per second (tgs): 1975.591 | TFLOPs: 15.90 |
g0345: [2024-08-02 20:04:52,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[2.6022161066666666e-05, 2.6022161066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1490 loss: 4.8342 iter time (s): 4.088 samples/sec: 31.309
g0364:  iteration     1490/10000000 | consumed samples:       190720 | consumed tokens:    390594560 | elapsed time per iteration (ms): 4120.2 | learning rate: 2.602E-05 | global batch size:   128 | lm loss: 4.779800E+00 | loss scale: 8192.0 | grad norm: 3.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.067 | tokens per gpu per second (tgs): 1988.269 | TFLOPs: 16.00 |
g0345: [2024-08-02 20:05:34,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[2.6196923733333336e-05, 2.6196923733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1500 loss: 4.6953 iter time (s): 4.174 samples/sec: 30.667
g0364:  iteration     1500/10000000 | consumed samples:       192000 | consumed tokens:    393216000 | elapsed time per iteration (ms): 4206.2 | learning rate: 2.620E-05 | global batch size:   128 | lm loss: 4.793014E+00 | loss scale: 8192.0 | grad norm: 2.607 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.431 | tokens per gpu per second (tgs): 1947.586 | TFLOPs: 15.67 |
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0347: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0345: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0347: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0358: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0362: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0363: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0352: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0364: [2024-08-02 20:05:38,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0347: [2024-08-02 20:05:38,796] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0345: [2024-08-02 20:06:16,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[2.6371686400000002e-05, 2.6371686400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1510 loss: 4.5947 iter time (s): 4.177 samples/sec: 30.647
g0364:  iteration     1510/10000000 | consumed samples:       193280 | consumed tokens:    395837440 | elapsed time per iteration (ms): 4209.2 | learning rate: 2.637E-05 | global batch size:   128 | lm loss: 4.712776E+00 | loss scale: 16384.0 | grad norm: 2.798 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.410 | tokens per gpu per second (tgs): 1946.223 | TFLOPs: 15.66 |
g0345: [2024-08-02 20:06:57,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[2.6546449066666668e-05, 2.6546449066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1520 loss: 4.6616 iter time (s): 4.072 samples/sec: 31.431
g0364:  iteration     1520/10000000 | consumed samples:       194560 | consumed tokens:    398458880 | elapsed time per iteration (ms): 4104.7 | learning rate: 2.655E-05 | global batch size:   128 | lm loss: 4.728779E+00 | loss scale: 16384.0 | grad norm: 3.328 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.184 | tokens per gpu per second (tgs): 1995.748 | TFLOPs: 16.06 |
g0345: [2024-08-02 20:07:38,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[2.6721211733333334e-05, 2.6721211733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1530 loss: 4.6627 iter time (s): 4.049 samples/sec: 31.616
g0364:  iteration     1530/10000000 | consumed samples:       195840 | consumed tokens:    401080320 | elapsed time per iteration (ms): 4081.4 | learning rate: 2.672E-05 | global batch size:   128 | lm loss: 4.735503E+00 | loss scale: 16384.0 | grad norm: 2.886 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.362 | tokens per gpu per second (tgs): 2007.146 | TFLOPs: 16.15 |
g0345: [2024-08-02 20:08:19,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[2.68959744e-05, 2.68959744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1540 loss: 4.7028 iter time (s): 4.111 samples/sec: 31.137
g0364:  iteration     1540/10000000 | consumed samples:       197120 | consumed tokens:    403701760 | elapsed time per iteration (ms): 4143.0 | learning rate: 2.690E-05 | global batch size:   128 | lm loss: 4.714780E+00 | loss scale: 16384.0 | grad norm: 3.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.896 | tokens per gpu per second (tgs): 1977.317 | TFLOPs: 15.91 |
g0345: [2024-08-02 20:09:02,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[2.707073706666667e-05, 2.707073706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1550 loss: 4.6076 iter time (s): 4.187 samples/sec: 30.572
g0364:  iteration     1550/10000000 | consumed samples:       198400 | consumed tokens:    406323200 | elapsed time per iteration (ms): 4219.1 | learning rate: 2.707E-05 | global batch size:   128 | lm loss: 4.631854E+00 | loss scale: 16384.0 | grad norm: 2.912 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.338 | tokens per gpu per second (tgs): 1941.645 | TFLOPs: 15.62 |
g0345: [2024-08-02 20:09:44,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[2.7245499733333335e-05, 2.7245499733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1560 loss: 4.7462 iter time (s): 4.175 samples/sec: 30.658
g0364:  iteration     1560/10000000 | consumed samples:       199680 | consumed tokens:    408944640 | elapsed time per iteration (ms): 4207.9 | learning rate: 2.725E-05 | global batch size:   128 | lm loss: 4.667469E+00 | loss scale: 16384.0 | grad norm: 3.529 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.419 | tokens per gpu per second (tgs): 1946.797 | TFLOPs: 15.67 |
g0345: [2024-08-02 20:10:25,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[2.74202624e-05, 2.74202624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1570 loss: 4.6133 iter time (s): 4.130 samples/sec: 30.991
g0364:  iteration     1570/10000000 | consumed samples:       200960 | consumed tokens:    411566080 | elapsed time per iteration (ms): 4163.0 | learning rate: 2.742E-05 | global batch size:   128 | lm loss: 4.607314E+00 | loss scale: 16384.0 | grad norm: 2.798 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.747 | tokens per gpu per second (tgs): 1967.796 | TFLOPs: 15.84 |
g0345: [2024-08-02 20:11:08,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[2.7595025066666668e-05, 2.7595025066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1580 loss: 4.5991 iter time (s): 4.189 samples/sec: 30.557
g0364:  iteration     1580/10000000 | consumed samples:       202240 | consumed tokens:    414187520 | elapsed time per iteration (ms): 4221.4 | learning rate: 2.760E-05 | global batch size:   128 | lm loss: 4.593649E+00 | loss scale: 16384.0 | grad norm: 3.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.322 | tokens per gpu per second (tgs): 1940.584 | TFLOPs: 15.62 |
g0345: [2024-08-02 20:11:49,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[2.7769787733333337e-05, 2.7769787733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1590 loss: 4.5496 iter time (s): 4.151 samples/sec: 30.839
g0364:  iteration     1590/10000000 | consumed samples:       203520 | consumed tokens:    416808960 | elapsed time per iteration (ms): 4183.2 | learning rate: 2.777E-05 | global batch size:   128 | lm loss: 4.569610E+00 | loss scale: 16384.0 | grad norm: 2.601 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.598 | tokens per gpu per second (tgs): 1958.296 | TFLOPs: 15.76 |
g0345: [2024-08-02 20:12:31,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[2.7944550400000003e-05, 2.7944550400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1600 loss: 4.4970 iter time (s): 4.165 samples/sec: 30.735
g0364:  iteration     1600/10000000 | consumed samples:       204800 | consumed tokens:    419430400 | elapsed time per iteration (ms): 4197.3 | learning rate: 2.794E-05 | global batch size:   128 | lm loss: 4.575426E+00 | loss scale: 16384.0 | grad norm: 2.568 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.496 | tokens per gpu per second (tgs): 1951.745 | TFLOPs: 15.71 |
g0345: [2024-08-02 20:13:13,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[2.811931306666667e-05, 2.811931306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1610 loss: 4.5398 iter time (s): 4.165 samples/sec: 30.729
g0364:  iteration     1610/10000000 | consumed samples:       206080 | consumed tokens:    422051840 | elapsed time per iteration (ms): 4197.5 | learning rate: 2.812E-05 | global batch size:   128 | lm loss: 4.546713E+00 | loss scale: 16384.0 | grad norm: 3.140 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.495 | tokens per gpu per second (tgs): 1951.652 | TFLOPs: 15.71 |
g0345: [2024-08-02 20:13:55,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[2.8294075733333335e-05, 2.8294075733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1620 loss: 4.5921 iter time (s): 4.149 samples/sec: 30.853
g0364:  iteration     1620/10000000 | consumed samples:       207360 | consumed tokens:    424673280 | elapsed time per iteration (ms): 4181.1 | learning rate: 2.829E-05 | global batch size:   128 | lm loss: 4.586426E+00 | loss scale: 16384.0 | grad norm: 3.066 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.614 | tokens per gpu per second (tgs): 1959.292 | TFLOPs: 15.77 |
g0345: [2024-08-02 20:14:36,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[2.84688384e-05, 2.84688384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1630 loss: 4.5124 iter time (s): 4.106 samples/sec: 31.174
g0364:  iteration     1630/10000000 | consumed samples:       208640 | consumed tokens:    427294720 | elapsed time per iteration (ms): 4138.4 | learning rate: 2.847E-05 | global batch size:   128 | lm loss: 4.468456E+00 | loss scale: 16384.0 | grad norm: 2.294 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.930 | tokens per gpu per second (tgs): 1979.522 | TFLOPs: 15.93 |
g0345: [2024-08-02 20:15:18,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[2.864360106666667e-05, 2.864360106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1640 loss: 4.5401 iter time (s): 4.142 samples/sec: 30.907
g0364:  iteration     1640/10000000 | consumed samples:       209920 | consumed tokens:    429916160 | elapsed time per iteration (ms): 4173.6 | learning rate: 2.864E-05 | global batch size:   128 | lm loss: 4.519920E+00 | loss scale: 16384.0 | grad norm: 2.841 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.669 | tokens per gpu per second (tgs): 1962.790 | TFLOPs: 15.79 |
g0345: [2024-08-02 20:16:00,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[2.8818363733333337e-05, 2.8818363733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1650 loss: 4.5121 iter time (s): 4.161 samples/sec: 30.761
g0364:  iteration     1650/10000000 | consumed samples:       211200 | consumed tokens:    432537600 | elapsed time per iteration (ms): 4193.0 | learning rate: 2.882E-05 | global batch size:   128 | lm loss: 4.525433E+00 | loss scale: 16384.0 | grad norm: 2.490 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.527 | tokens per gpu per second (tgs): 1953.727 | TFLOPs: 15.72 |
g0345: [2024-08-02 20:16:42,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[2.8993126400000003e-05, 2.8993126400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1660 loss: 4.5075 iter time (s): 4.160 samples/sec: 30.771
g0364:  iteration     1660/10000000 | consumed samples:       212480 | consumed tokens:    435159040 | elapsed time per iteration (ms): 4191.9 | learning rate: 2.899E-05 | global batch size:   128 | lm loss: 4.475714E+00 | loss scale: 16384.0 | grad norm: 3.075 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.535 | tokens per gpu per second (tgs): 1954.251 | TFLOPs: 15.73 |
g0345: [2024-08-02 20:17:24,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[2.916788906666667e-05, 2.916788906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1670 loss: 4.5982 iter time (s): 4.181 samples/sec: 30.616
g0364:  iteration     1670/10000000 | consumed samples:       213760 | consumed tokens:    437780480 | elapsed time per iteration (ms): 4213.1 | learning rate: 2.917E-05 | global batch size:   128 | lm loss: 4.470010E+00 | loss scale: 16384.0 | grad norm: 2.558 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.382 | tokens per gpu per second (tgs): 1944.429 | TFLOPs: 15.65 |
g0345: [2024-08-02 20:18:06,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[2.934265173333334e-05, 2.934265173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1680 loss: 4.4486 iter time (s): 4.177 samples/sec: 30.647
g0364:  iteration     1680/10000000 | consumed samples:       215040 | consumed tokens:    440401920 | elapsed time per iteration (ms): 4208.6 | learning rate: 2.934E-05 | global batch size:   128 | lm loss: 4.475397E+00 | loss scale: 16384.0 | grad norm: 2.725 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.414 | tokens per gpu per second (tgs): 1946.503 | TFLOPs: 15.66 |
g0345: [2024-08-02 20:18:48,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[2.9517414399999998e-05, 2.9517414399999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1690 loss: 4.4949 iter time (s): 4.148 samples/sec: 30.862
g0364:  iteration     1690/10000000 | consumed samples:       216320 | consumed tokens:    443023360 | elapsed time per iteration (ms): 4179.4 | learning rate: 2.952E-05 | global batch size:   128 | lm loss: 4.482229E+00 | loss scale: 16384.0 | grad norm: 3.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.626 | tokens per gpu per second (tgs): 1960.081 | TFLOPs: 15.77 |
g0345: [2024-08-02 20:19:30,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[2.9692177066666667e-05, 2.9692177066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1700 loss: 4.4148 iter time (s): 4.113 samples/sec: 31.120
g0364:  iteration     1700/10000000 | consumed samples:       217600 | consumed tokens:    445644800 | elapsed time per iteration (ms): 4145.4 | learning rate: 2.969E-05 | global batch size:   128 | lm loss: 4.362238E+00 | loss scale: 16384.0 | grad norm: 2.427 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.878 | tokens per gpu per second (tgs): 1976.167 | TFLOPs: 15.90 |
g0345: [2024-08-02 20:20:13,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[2.9866939733333333e-05, 2.9866939733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1710 loss: 4.4459 iter time (s): 4.265 samples/sec: 30.015
g0364:  iteration     1710/10000000 | consumed samples:       218880 | consumed tokens:    448266240 | elapsed time per iteration (ms): 4296.4 | learning rate: 2.987E-05 | global batch size:   128 | lm loss: 4.463565E+00 | loss scale: 16384.0 | grad norm: 4.090 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.792 | tokens per gpu per second (tgs): 1906.707 | TFLOPs: 15.34 |
g0345: [2024-08-02 20:20:54,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[3.00417024e-05, 3.00417024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1720 loss: 4.2642 iter time (s): 4.154 samples/sec: 30.811
g0364:  iteration     1720/10000000 | consumed samples:       220160 | consumed tokens:    450887680 | elapsed time per iteration (ms): 4186.5 | learning rate: 3.004E-05 | global batch size:   128 | lm loss: 4.416843E+00 | loss scale: 16384.0 | grad norm: 2.487 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.574 | tokens per gpu per second (tgs): 1956.763 | TFLOPs: 15.75 |
g0345: [2024-08-02 20:21:36,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[3.0216465066666665e-05, 3.0216465066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1730 loss: 4.3680 iter time (s): 4.179 samples/sec: 30.626
g0364:  iteration     1730/10000000 | consumed samples:       221440 | consumed tokens:    453509120 | elapsed time per iteration (ms): 4211.7 | learning rate: 3.022E-05 | global batch size:   128 | lm loss: 4.384261E+00 | loss scale: 16384.0 | grad norm: 2.585 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.392 | tokens per gpu per second (tgs): 1945.064 | TFLOPs: 15.65 |
g0345: [2024-08-02 20:22:19,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[3.0391227733333335e-05, 3.0391227733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1740 loss: 4.2989 iter time (s): 4.186 samples/sec: 30.577
g0364:  iteration     1740/10000000 | consumed samples:       222720 | consumed tokens:    456130560 | elapsed time per iteration (ms): 4218.3 | learning rate: 3.039E-05 | global batch size:   128 | lm loss: 4.350655E+00 | loss scale: 16384.0 | grad norm: 2.944 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.344 | tokens per gpu per second (tgs): 1942.023 | TFLOPs: 15.63 |
g0345: [2024-08-02 20:23:01,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[3.05659904e-05, 3.05659904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1750 loss: 4.2835 iter time (s): 4.225 samples/sec: 30.296
g0364:  iteration     1750/10000000 | consumed samples:       224000 | consumed tokens:    458752000 | elapsed time per iteration (ms): 4257.4 | learning rate: 3.057E-05 | global batch size:   128 | lm loss: 4.317467E+00 | loss scale: 16384.0 | grad norm: 2.839 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.065 | tokens per gpu per second (tgs): 1924.185 | TFLOPs: 15.48 |
g0345: [2024-08-02 20:23:44,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[3.0740753066666664e-05, 3.0740753066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1760 loss: 4.2221 iter time (s): 4.256 samples/sec: 30.078
g0364:  iteration     1760/10000000 | consumed samples:       225280 | consumed tokens:    461373440 | elapsed time per iteration (ms): 4294.0 | learning rate: 3.074E-05 | global batch size:   128 | lm loss: 4.297002E+00 | loss scale: 16384.0 | grad norm: 2.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.809 | tokens per gpu per second (tgs): 1907.765 | TFLOPs: 15.35 |
g0345: [2024-08-02 20:24:27,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[3.0915515733333336e-05, 3.0915515733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1770 loss: 4.2058 iter time (s): 4.221 samples/sec: 30.327
g0364:  iteration     1770/10000000 | consumed samples:       226560 | consumed tokens:    463994880 | elapsed time per iteration (ms): 4266.5 | learning rate: 3.092E-05 | global batch size:   128 | lm loss: 4.311033E+00 | loss scale: 16384.0 | grad norm: 2.702 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.002 | tokens per gpu per second (tgs): 1920.097 | TFLOPs: 15.45 |
g0345: [2024-08-02 20:25:09,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[3.10902784e-05, 3.10902784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1780 loss: 4.4460 iter time (s): 4.149 samples/sec: 30.854
g0364:  iteration     1780/10000000 | consumed samples:       227840 | consumed tokens:    466616320 | elapsed time per iteration (ms): 4180.9 | learning rate: 3.109E-05 | global batch size:   128 | lm loss: 4.279720E+00 | loss scale: 16384.0 | grad norm: 6.461 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.615 | tokens per gpu per second (tgs): 1959.380 | TFLOPs: 15.77 |
g0345: [2024-08-02 20:25:52,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[3.126504106666667e-05, 3.126504106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1790 loss: 4.2691 iter time (s): 4.267 samples/sec: 30.000
g0364:  iteration     1790/10000000 | consumed samples:       229120 | consumed tokens:    469237760 | elapsed time per iteration (ms): 4303.0 | learning rate: 3.127E-05 | global batch size:   128 | lm loss: 4.266777E+00 | loss scale: 16384.0 | grad norm: 3.136 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.747 | tokens per gpu per second (tgs): 1903.800 | TFLOPs: 15.32 |
g0345: [2024-08-02 20:26:33,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[3.1439803733333335e-05, 3.1439803733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1800 loss: 4.1936 iter time (s): 4.146 samples/sec: 30.871
g0364:  iteration     1800/10000000 | consumed samples:       230400 | consumed tokens:    471859200 | elapsed time per iteration (ms): 4178.2 | learning rate: 3.144E-05 | global batch size:   128 | lm loss: 4.233995E+00 | loss scale: 16384.0 | grad norm: 2.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.635 | tokens per gpu per second (tgs): 1960.644 | TFLOPs: 15.78 |
g0345: [2024-08-02 20:27:16,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[3.16145664e-05, 3.16145664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1810 loss: 4.1402 iter time (s): 4.186 samples/sec: 30.577
g0364:  iteration     1810/10000000 | consumed samples:       231680 | consumed tokens:    474480640 | elapsed time per iteration (ms): 4218.0 | learning rate: 3.161E-05 | global batch size:   128 | lm loss: 4.205049E+00 | loss scale: 16384.0 | grad norm: 2.588 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.159 | TFLOPs: 15.63 |
g0345: [2024-08-02 20:27:58,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[3.178932906666667e-05, 3.178932906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1820 loss: 4.3804 iter time (s): 4.173 samples/sec: 30.675
g0364:  iteration     1820/10000000 | consumed samples:       232960 | consumed tokens:    477102080 | elapsed time per iteration (ms): 4204.7 | learning rate: 3.179E-05 | global batch size:   128 | lm loss: 4.281586E+00 | loss scale: 16384.0 | grad norm: 2.938 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.442 | tokens per gpu per second (tgs): 1948.285 | TFLOPs: 15.68 |
g0345: [2024-08-02 20:28:39,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[3.196409173333333e-05, 3.196409173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1830 loss: 4.1518 iter time (s): 4.094 samples/sec: 31.262
g0364:  iteration     1830/10000000 | consumed samples:       234240 | consumed tokens:    479723520 | elapsed time per iteration (ms): 4127.0 | learning rate: 3.196E-05 | global batch size:   128 | lm loss: 4.247710E+00 | loss scale: 16384.0 | grad norm: 3.035 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.015 | tokens per gpu per second (tgs): 1984.983 | TFLOPs: 15.97 |
g0345: [2024-08-02 20:29:21,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[3.21388544e-05, 3.21388544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1840 loss: 4.2501 iter time (s): 4.170 samples/sec: 30.692
g0364:  iteration     1840/10000000 | consumed samples:       235520 | consumed tokens:    482344960 | elapsed time per iteration (ms): 4202.5 | learning rate: 3.214E-05 | global batch size:   128 | lm loss: 4.268322E+00 | loss scale: 16384.0 | grad norm: 2.644 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.458 | tokens per gpu per second (tgs): 1949.325 | TFLOPs: 15.69 |
g0345: [2024-08-02 20:30:03,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[3.2313617066666665e-05, 3.2313617066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1850 loss: 4.2181 iter time (s): 4.117 samples/sec: 31.089
g0364:  iteration     1850/10000000 | consumed samples:       236800 | consumed tokens:    484966400 | elapsed time per iteration (ms): 4152.4 | learning rate: 3.231E-05 | global batch size:   128 | lm loss: 4.183793E+00 | loss scale: 16384.0 | grad norm: 2.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.825 | tokens per gpu per second (tgs): 1972.825 | TFLOPs: 15.88 |
g0345: [2024-08-02 20:30:45,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[3.248837973333334e-05, 3.248837973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1860 loss: 4.2231 iter time (s): 4.254 samples/sec: 30.088
g0364:  iteration     1860/10000000 | consumed samples:       238080 | consumed tokens:    487587840 | elapsed time per iteration (ms): 4286.6 | learning rate: 3.249E-05 | global batch size:   128 | lm loss: 4.214798E+00 | loss scale: 16384.0 | grad norm: 2.512 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.861 | tokens per gpu per second (tgs): 1911.081 | TFLOPs: 15.38 |
g0345: [2024-08-02 20:31:26,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[3.2663142400000004e-05, 3.2663142400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1870 loss: 4.2592 iter time (s): 4.034 samples/sec: 31.728
g0364:  iteration     1870/10000000 | consumed samples:       239360 | consumed tokens:    490209280 | elapsed time per iteration (ms): 4066.7 | learning rate: 3.266E-05 | global batch size:   128 | lm loss: 4.181367E+00 | loss scale: 16384.0 | grad norm: 2.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.475 | tokens per gpu per second (tgs): 2014.416 | TFLOPs: 16.21 |
g0345: [2024-08-02 20:32:08,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[3.283790506666667e-05, 3.283790506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1880 loss: 4.2289 iter time (s): 4.207 samples/sec: 30.425
g0364:  iteration     1880/10000000 | consumed samples:       240640 | consumed tokens:    492830720 | elapsed time per iteration (ms): 4239.7 | learning rate: 3.284E-05 | global batch size:   128 | lm loss: 4.160172E+00 | loss scale: 16384.0 | grad norm: 2.372 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.191 | tokens per gpu per second (tgs): 1932.225 | TFLOPs: 15.55 |
g0345: [2024-08-02 20:32:50,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[3.3012667733333336e-05, 3.3012667733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1890 loss: 4.1001 iter time (s): 4.161 samples/sec: 30.761
g0364:  iteration     1890/10000000 | consumed samples:       241920 | consumed tokens:    495452160 | elapsed time per iteration (ms): 4193.4 | learning rate: 3.301E-05 | global batch size:   128 | lm loss: 4.168748E+00 | loss scale: 16384.0 | grad norm: 2.093 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.524 | tokens per gpu per second (tgs): 1953.562 | TFLOPs: 15.72 |
g0345: [2024-08-02 20:33:32,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[3.31874304e-05, 3.31874304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1900 loss: 4.2073 iter time (s): 4.120 samples/sec: 31.072
g0364:  iteration     1900/10000000 | consumed samples:       243200 | consumed tokens:    498073600 | elapsed time per iteration (ms): 4152.7 | learning rate: 3.319E-05 | global batch size:   128 | lm loss: 4.196496E+00 | loss scale: 16384.0 | grad norm: 2.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.823 | tokens per gpu per second (tgs): 1972.698 | TFLOPs: 15.87 |
g0345: [2024-08-02 20:34:13,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[3.336219306666667e-05, 3.336219306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1910 loss: 4.2483 iter time (s): 4.084 samples/sec: 31.346
g0364:  iteration     1910/10000000 | consumed samples:       244480 | consumed tokens:    500695040 | elapsed time per iteration (ms): 4116.5 | learning rate: 3.336E-05 | global batch size:   128 | lm loss: 4.136604E+00 | loss scale: 16384.0 | grad norm: 2.333 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.094 | tokens per gpu per second (tgs): 1990.037 | TFLOPs: 16.01 |
g0345: [2024-08-02 20:34:54,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[3.3536955733333334e-05, 3.3536955733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1920 loss: 4.2814 iter time (s): 4.087 samples/sec: 31.319
g0364:  iteration     1920/10000000 | consumed samples:       245760 | consumed tokens:    503316480 | elapsed time per iteration (ms): 4120.2 | learning rate: 3.354E-05 | global batch size:   128 | lm loss: 4.150647E+00 | loss scale: 16384.0 | grad norm: 2.521 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.067 | tokens per gpu per second (tgs): 1988.264 | TFLOPs: 16.00 |
g0345: [2024-08-02 20:35:35,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[3.37117184e-05, 3.37117184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1930 loss: 4.0640 iter time (s): 4.069 samples/sec: 31.459
g0364:  iteration     1930/10000000 | consumed samples:       247040 | consumed tokens:    505937920 | elapsed time per iteration (ms): 4102.3 | learning rate: 3.371E-05 | global batch size:   128 | lm loss: 4.118637E+00 | loss scale: 16384.0 | grad norm: 2.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.202 | tokens per gpu per second (tgs): 1996.923 | TFLOPs: 16.07 |
g0345: [2024-08-02 20:36:17,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[3.388648106666667e-05, 3.388648106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1940 loss: 4.0672 iter time (s): 4.118 samples/sec: 31.085
g0364:  iteration     1940/10000000 | consumed samples:       248320 | consumed tokens:    508559360 | elapsed time per iteration (ms): 4151.0 | learning rate: 3.389E-05 | global batch size:   128 | lm loss: 4.079294E+00 | loss scale: 16384.0 | grad norm: 2.357 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.836 | tokens per gpu per second (tgs): 1973.507 | TFLOPs: 15.88 |
g0345: [2024-08-02 20:36:59,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[3.406124373333334e-05, 3.406124373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1950 loss: 4.1110 iter time (s): 4.197 samples/sec: 30.496
g0364:  iteration     1950/10000000 | consumed samples:       249600 | consumed tokens:    511180800 | elapsed time per iteration (ms): 4229.9 | learning rate: 3.406E-05 | global batch size:   128 | lm loss: 4.080944E+00 | loss scale: 16384.0 | grad norm: 2.661 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.261 | tokens per gpu per second (tgs): 1936.689 | TFLOPs: 15.58 |
g0345: [2024-08-02 20:37:41,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[3.4236006400000005e-05, 3.4236006400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1960 loss: 4.1542 iter time (s): 4.202 samples/sec: 30.459
g0364:  iteration     1960/10000000 | consumed samples:       250880 | consumed tokens:    513802240 | elapsed time per iteration (ms): 4234.8 | learning rate: 3.424E-05 | global batch size:   128 | lm loss: 4.088367E+00 | loss scale: 16384.0 | grad norm: 2.404 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.226 | tokens per gpu per second (tgs): 1934.452 | TFLOPs: 15.57 |
g0345: [2024-08-02 20:38:24,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[3.441076906666667e-05, 3.441076906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1970 loss: 4.0330 iter time (s): 4.200 samples/sec: 30.479
g0364:  iteration     1970/10000000 | consumed samples:       252160 | consumed tokens:    516423680 | elapsed time per iteration (ms): 4232.0 | learning rate: 3.441E-05 | global batch size:   128 | lm loss: 4.128379E+00 | loss scale: 16384.0 | grad norm: 2.533 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.246 | tokens per gpu per second (tgs): 1935.730 | TFLOPs: 15.58 |
g0345: [2024-08-02 20:39:06,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[3.458553173333334e-05, 3.458553173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1980 loss: 4.1842 iter time (s): 4.205 samples/sec: 30.442
g0364:  iteration     1980/10000000 | consumed samples:       253440 | consumed tokens:    519045120 | elapsed time per iteration (ms): 4236.9 | learning rate: 3.459E-05 | global batch size:   128 | lm loss: 4.103463E+00 | loss scale: 16384.0 | grad norm: 3.031 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.211 | tokens per gpu per second (tgs): 1933.486 | TFLOPs: 15.56 |
g0345: [2024-08-02 20:39:48,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=0, lr=[3.47602944e-05, 3.47602944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 1990 loss: 4.0397 iter time (s): 4.158 samples/sec: 30.784
g0364:  iteration     1990/10000000 | consumed samples:       254720 | consumed tokens:    521666560 | elapsed time per iteration (ms): 4190.8 | learning rate: 3.476E-05 | global batch size:   128 | lm loss: 4.034057E+00 | loss scale: 16384.0 | grad norm: 2.024 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.543 | tokens per gpu per second (tgs): 1954.754 | TFLOPs: 15.73 |
g0345: [2024-08-02 20:40:29,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=0, lr=[3.493505706666667e-05, 3.493505706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2000 loss: 4.3013 iter time (s): 4.050 samples/sec: 31.607
g0364:  iteration     2000/10000000 | consumed samples:       256000 | consumed tokens:    524288000 | elapsed time per iteration (ms): 4082.2 | learning rate: 3.494E-05 | global batch size:   128 | lm loss: 4.051364E+00 | loss scale: 16384.0 | grad norm: 2.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.356 | tokens per gpu per second (tgs): 2006.759 | TFLOPs: 16.15 |
g0364: ------------------------------------------------------------------------------------------------
g0364:  validation loss at iteration 2000 | lm loss value: 4.026967E+00 | lm loss PPL: 5.609051E+01 | 
g0364: ------------------------------------------------------------------------------------------------
g0345: saving checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: [2024-08-02 20:46:49,633] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
g0364: [2024-08-02 20:46:49,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0345: [2024-08-02 20:46:49,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0345: [2024-08-02 20:46:49,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0345: [2024-08-02 20:46:49,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0364: [2024-08-02 20:46:49,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0364: [2024-08-02 20:46:49,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0358: [2024-08-02 20:46:49,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0358: [2024-08-02 20:46:49,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0358: [2024-08-02 20:46:49,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0347: [2024-08-02 20:46:49,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0347: [2024-08-02 20:46:49,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0347: [2024-08-02 20:46:49,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0363: [2024-08-02 20:46:49,642] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0363: [2024-08-02 20:46:49,642] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0362: [2024-08-02 20:46:49,642] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0362: [2024-08-02 20:46:49,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0362: [2024-08-02 20:46:49,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0352: [2024-08-02 20:46:49,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0363: [2024-08-02 20:46:49,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0352: [2024-08-02 20:46:49,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0352: [2024-08-02 20:46:49,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0346: [2024-08-02 20:46:49,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0346: [2024-08-02 20:46:49,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0346: [2024-08-02 20:46:49,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0364: [2024-08-02 20:46:49,655] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt...
g0358: [2024-08-02 20:46:49,674] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt...
g0362: [2024-08-02 20:46:49,676] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt...
g0347: [2024-08-02 20:46:49,678] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt...
g0346: [2024-08-02 20:46:49,679] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt...
g0363: [2024-08-02 20:46:49,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt...
g0352: [2024-08-02 20:46:49,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt...
g0345: [2024-08-02 20:46:49,686] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt...
g0364: [2024-08-02 20:46:49,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt.
g0364: [2024-08-02 20:46:49,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt...
g0364: [2024-08-02 20:46:49,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt.
g0362: [2024-08-02 20:46:49,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt.
g0364: [2024-08-02 20:46:49,822] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt...
g0362: [2024-08-02 20:46:49,850] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt...
g0346: [2024-08-02 20:46:49,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt.
g0346: [2024-08-02 20:46:49,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt...
g0345: [2024-08-02 20:46:49,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt.
g0362: [2024-08-02 20:46:49,975] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt.
g0345: [2024-08-02 20:46:49,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt...
g0362: [2024-08-02 20:46:50,005] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt...
g0364: [2024-08-02 20:46:50,008] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt.
g0364: [2024-08-02 20:46:50,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt...
g0346: [2024-08-02 20:46:50,051] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt.
g0346: [2024-08-02 20:46:50,077] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt...
g0362: [2024-08-02 20:46:50,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt.
g0362: [2024-08-02 20:46:50,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt...
g0347: [2024-08-02 20:46:50,121] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt.
g0363: [2024-08-02 20:46:50,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt.
g0345: [2024-08-02 20:46:50,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt.
g0347: [2024-08-02 20:46:50,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt...
g0345: [2024-08-02 20:46:50,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt...
g0346: [2024-08-02 20:46:50,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt.
g0346: [2024-08-02 20:46:50,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt...
g0363: [2024-08-02 20:46:50,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt...
g0352: [2024-08-02 20:46:50,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt.
g0358: [2024-08-02 20:46:50,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt.
g0352: [2024-08-02 20:46:50,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt...
g0358: [2024-08-02 20:46:50,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt...
g0345: [2024-08-02 20:46:50,269] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt.
g0345: [2024-08-02 20:46:50,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt...
g0347: [2024-08-02 20:46:50,313] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt.
g0363: [2024-08-02 20:46:50,327] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt.
g0358: [2024-08-02 20:46:50,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt.
g0347: [2024-08-02 20:46:50,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt...
g0352: [2024-08-02 20:46:50,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt.
g0363: [2024-08-02 20:46:50,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt...
g0358: [2024-08-02 20:46:50,368] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt...
g0352: [2024-08-02 20:46:50,391] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt...
g0345: [2024-08-02 20:46:50,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt.
g0345: [2024-08-02 20:46:50,411] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt
g0345: [2024-08-02 20:46:50,411] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt...
g0347: [2024-08-02 20:46:50,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt.
g0347: [2024-08-02 20:46:50,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt...
g0358: [2024-08-02 20:46:50,488] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt.
g0358: [2024-08-02 20:46:50,490] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt...
g0363: [2024-08-02 20:46:50,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt.
g0363: [2024-08-02 20:46:50,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt...
g0352: [2024-08-02 20:46:50,532] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt.
g0352: [2024-08-02 20:46:50,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt...
g0364: [2024-08-02 20:46:52,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt.
g0364: [2024-08-02 20:46:52,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0362: [2024-08-02 20:46:52,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt.
g0362: [2024-08-02 20:46:52,331] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0346: [2024-08-02 20:46:52,454] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt.
g0346: [2024-08-02 20:46:52,454] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0358: [2024-08-02 20:46:52,950] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt.
g0358: [2024-08-02 20:46:52,950] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0352: [2024-08-02 20:46:52,965] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt.
g0352: [2024-08-02 20:46:52,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0363: [2024-08-02 20:46:53,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt.
g0363: [2024-08-02 20:46:53,115] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0347: [2024-08-02 20:46:53,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt.
g0347: [2024-08-02 20:46:53,449] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0345: [2024-08-02 20:46:54,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt.
g0345: [2024-08-02 20:46:54,107] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0345:   successfully saved checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: Checkpoint Save GB: 22.521, GB/Sec: 5.0, Latency(second): 4.503
g0364: (min, max) time across ranks (ms):
g0364:     save-checkpoint ................................: (4503.15, 4503.48)
g0345: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0347: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0358: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0363: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0363: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0364: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0346: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0352: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0352: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0358: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0358: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0346: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0347: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0347: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0352: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:46:58,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0352: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0346: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0362: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0345: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0363: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0364: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0364: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0363: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0345: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0358: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0362: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0363: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0346: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0352: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0364: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0345: [2024-08-02 20:46:58,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0364: [2024-08-02 20:46:58,542] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0345: [2024-08-02 20:47:35,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=0, lr=[3.5109819733333335e-05, 3.5109819733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2010 loss: 3.9624 iter time (s): 4.105 samples/sec: 31.185
g0364:  iteration     2010/10000000 | consumed samples:       257280 | consumed tokens:    526909440 | elapsed time per iteration (ms): 42606.3 | learning rate: 3.511E-05 | global batch size:   128 | lm loss: 4.035773E+00 | loss scale: 32768.0 | grad norm: 1.894 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.004 | tokens per gpu per second (tgs): 192.272 | TFLOPs: 1.55 |
g0345: [2024-08-02 20:48:16,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=0, lr=[3.52845824e-05, 3.52845824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2020 loss: 3.9401 iter time (s): 4.068 samples/sec: 31.464
g0364:  iteration     2020/10000000 | consumed samples:       258560 | consumed tokens:    529530880 | elapsed time per iteration (ms): 4100.7 | learning rate: 3.528E-05 | global batch size:   128 | lm loss: 3.976950E+00 | loss scale: 32768.0 | grad norm: 3.151 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.214 | tokens per gpu per second (tgs): 1997.701 | TFLOPs: 16.08 |
g0345: [2024-08-02 20:48:57,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=0, lr=[3.545934506666667e-05, 3.545934506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2030 loss: 4.1020 iter time (s): 4.052 samples/sec: 31.590
g0364:  iteration     2030/10000000 | consumed samples:       259840 | consumed tokens:    532152320 | elapsed time per iteration (ms): 4084.9 | learning rate: 3.546E-05 | global batch size:   128 | lm loss: 4.053253E+00 | loss scale: 32768.0 | grad norm: 2.327 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.335 | tokens per gpu per second (tgs): 2005.447 | TFLOPs: 16.14 |
g0345: [2024-08-02 20:49:38,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=0, lr=[3.5634107733333334e-05, 3.5634107733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2040 loss: 4.1509 iter time (s): 4.064 samples/sec: 31.492
g0364:  iteration     2040/10000000 | consumed samples:       261120 | consumed tokens:    534773760 | elapsed time per iteration (ms): 4098.1 | learning rate: 3.563E-05 | global batch size:   128 | lm loss: 3.982050E+00 | loss scale: 32768.0 | grad norm: 2.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.234 | tokens per gpu per second (tgs): 1998.974 | TFLOPs: 16.09 |
g0345: [2024-08-02 20:50:19,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=0, lr=[3.58088704e-05, 3.58088704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2050 loss: 4.0437 iter time (s): 4.073 samples/sec: 31.423
g0364:  iteration     2050/10000000 | consumed samples:       262400 | consumed tokens:    537395200 | elapsed time per iteration (ms): 4105.9 | learning rate: 3.581E-05 | global batch size:   128 | lm loss: 4.032866E+00 | loss scale: 32768.0 | grad norm: 2.082 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.174 | tokens per gpu per second (tgs): 1995.160 | TFLOPs: 16.06 |
g0345: [2024-08-02 20:51:00,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=0, lr=[3.5983633066666666e-05, 3.5983633066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2060 loss: 4.0197 iter time (s): 4.128 samples/sec: 31.011
g0364:  iteration     2060/10000000 | consumed samples:       263680 | consumed tokens:    540016640 | elapsed time per iteration (ms): 4159.8 | learning rate: 3.598E-05 | global batch size:   128 | lm loss: 3.965166E+00 | loss scale: 32768.0 | grad norm: 2.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.771 | tokens per gpu per second (tgs): 1969.325 | TFLOPs: 15.85 |
g0345: [2024-08-02 20:51:41,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=0, lr=[3.615839573333333e-05, 3.615839573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2070 loss: 3.9310 iter time (s): 4.061 samples/sec: 31.521
g0364:  iteration     2070/10000000 | consumed samples:       264960 | consumed tokens:    542638080 | elapsed time per iteration (ms): 4093.1 | learning rate: 3.616E-05 | global batch size:   128 | lm loss: 3.957486E+00 | loss scale: 32768.0 | grad norm: 2.143 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.272 | tokens per gpu per second (tgs): 2001.436 | TFLOPs: 16.11 |
g0345: [2024-08-02 20:52:23,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=0, lr=[3.63331584e-05, 3.63331584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2080 loss: 3.9409 iter time (s): 4.168 samples/sec: 30.712
g0364:  iteration     2080/10000000 | consumed samples:       266240 | consumed tokens:    545259520 | elapsed time per iteration (ms): 4200.0 | learning rate: 3.633E-05 | global batch size:   128 | lm loss: 3.971053E+00 | loss scale: 32768.0 | grad norm: 2.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.476 | tokens per gpu per second (tgs): 1950.457 | TFLOPs: 15.70 |
g0345: [2024-08-02 20:53:04,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=0, lr=[3.6507921066666664e-05, 3.6507921066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2090 loss: 4.1832 iter time (s): 4.048 samples/sec: 31.620
g0364:  iteration     2090/10000000 | consumed samples:       267520 | consumed tokens:    547880960 | elapsed time per iteration (ms): 4080.5 | learning rate: 3.651E-05 | global batch size:   128 | lm loss: 4.054659E+00 | loss scale: 32768.0 | grad norm: 2.310 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.369 | tokens per gpu per second (tgs): 2007.602 | TFLOPs: 16.16 |
g0345: [2024-08-02 20:53:45,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=0, lr=[3.668268373333334e-05, 3.668268373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2100 loss: 3.9353 iter time (s): 4.093 samples/sec: 31.270
g0364:  iteration     2100/10000000 | consumed samples:       268800 | consumed tokens:    550502400 | elapsed time per iteration (ms): 4126.0 | learning rate: 3.668E-05 | global batch size:   128 | lm loss: 3.958201E+00 | loss scale: 32768.0 | grad norm: 2.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.023 | tokens per gpu per second (tgs): 1985.456 | TFLOPs: 15.98 |
g0345: [2024-08-02 20:54:28,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=0, lr=[3.68574464e-05, 3.68574464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2110 loss: 3.9962 iter time (s): 4.235 samples/sec: 30.227
g0364:  iteration     2110/10000000 | consumed samples:       270080 | consumed tokens:    553123840 | elapsed time per iteration (ms): 4267.6 | learning rate: 3.686E-05 | global batch size:   128 | lm loss: 3.905976E+00 | loss scale: 32768.0 | grad norm: 1.907 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.994 | tokens per gpu per second (tgs): 1919.600 | TFLOPs: 15.45 |
g0345: [2024-08-02 20:55:10,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=0, lr=[3.703220906666667e-05, 3.703220906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2120 loss: 3.8784 iter time (s): 4.122 samples/sec: 31.049
g0364:  iteration     2120/10000000 | consumed samples:       271360 | consumed tokens:    555745280 | elapsed time per iteration (ms): 4154.8 | learning rate: 3.703E-05 | global batch size:   128 | lm loss: 3.984486E+00 | loss scale: 32768.0 | grad norm: 1.896 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.808 | tokens per gpu per second (tgs): 1971.711 | TFLOPs: 15.87 |
g0345: [2024-08-02 20:55:51,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=0, lr=[3.7206971733333335e-05, 3.7206971733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2130 loss: 3.9627 iter time (s): 4.123 samples/sec: 31.044
g0364:  iteration     2130/10000000 | consumed samples:       272640 | consumed tokens:    558366720 | elapsed time per iteration (ms): 4156.2 | learning rate: 3.721E-05 | global batch size:   128 | lm loss: 3.908770E+00 | loss scale: 32768.0 | grad norm: 2.079 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.797 | tokens per gpu per second (tgs): 1971.030 | TFLOPs: 15.86 |
g0345: [2024-08-02 20:56:33,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=0, lr=[3.73817344e-05, 3.73817344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2140 loss: 3.8965 iter time (s): 4.148 samples/sec: 30.860
g0364:  iteration     2140/10000000 | consumed samples:       273920 | consumed tokens:    560988160 | elapsed time per iteration (ms): 4180.0 | learning rate: 3.738E-05 | global batch size:   128 | lm loss: 3.958657E+00 | loss scale: 32768.0 | grad norm: 2.071 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.622 | tokens per gpu per second (tgs): 1959.792 | TFLOPs: 15.77 |
g0345: [2024-08-02 20:57:14,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=0, lr=[3.755649706666667e-05, 3.755649706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2150 loss: 3.9023 iter time (s): 4.098 samples/sec: 31.231
g0364:  iteration     2150/10000000 | consumed samples:       275200 | consumed tokens:    563609600 | elapsed time per iteration (ms): 4130.8 | learning rate: 3.756E-05 | global batch size:   128 | lm loss: 3.905900E+00 | loss scale: 32768.0 | grad norm: 2.086 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.986 | tokens per gpu per second (tgs): 1983.133 | TFLOPs: 15.96 |
g0345: [2024-08-02 20:57:56,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=0, lr=[3.773125973333333e-05, 3.773125973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2160 loss: 3.9992 iter time (s): 4.113 samples/sec: 31.118
g0364:  iteration     2160/10000000 | consumed samples:       276480 | consumed tokens:    566231040 | elapsed time per iteration (ms): 4145.8 | learning rate: 3.773E-05 | global batch size:   128 | lm loss: 3.926057E+00 | loss scale: 32768.0 | grad norm: 2.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.874 | tokens per gpu per second (tgs): 1975.965 | TFLOPs: 15.90 |
g0345: [2024-08-02 20:58:38,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=0, lr=[3.79060224e-05, 3.79060224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2170 loss: 3.8405 iter time (s): 4.169 samples/sec: 30.700
g0364:  iteration     2170/10000000 | consumed samples:       277760 | consumed tokens:    568852480 | elapsed time per iteration (ms): 4201.7 | learning rate: 3.791E-05 | global batch size:   128 | lm loss: 3.903855E+00 | loss scale: 32768.0 | grad norm: 1.936 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.464 | tokens per gpu per second (tgs): 1949.684 | TFLOPs: 15.69 |
g0345: [2024-08-02 20:59:20,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=0, lr=[3.8080785066666665e-05, 3.8080785066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2180 loss: 3.8489 iter time (s): 4.185 samples/sec: 30.582
g0364:  iteration     2180/10000000 | consumed samples:       279040 | consumed tokens:    571473920 | elapsed time per iteration (ms): 4218.8 | learning rate: 3.808E-05 | global batch size:   128 | lm loss: 3.968074E+00 | loss scale: 32768.0 | grad norm: 2.082 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.341 | tokens per gpu per second (tgs): 1941.794 | TFLOPs: 15.63 |
g0345: [2024-08-02 21:00:02,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=0, lr=[3.825554773333334e-05, 3.825554773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2190 loss: 3.9088 iter time (s): 4.123 samples/sec: 31.047
g0364:  iteration     2190/10000000 | consumed samples:       280320 | consumed tokens:    574095360 | elapsed time per iteration (ms): 4155.4 | learning rate: 3.826E-05 | global batch size:   128 | lm loss: 3.918970E+00 | loss scale: 32768.0 | grad norm: 1.972 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.803 | tokens per gpu per second (tgs): 1971.405 | TFLOPs: 15.86 |
g0345: [2024-08-02 21:00:43,812] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=0, lr=[3.8430310400000004e-05, 3.8430310400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2200 loss: 3.9318 iter time (s): 4.144 samples/sec: 30.891
g0364:  iteration     2200/10000000 | consumed samples:       281600 | consumed tokens:    576716800 | elapsed time per iteration (ms): 4176.2 | learning rate: 3.843E-05 | global batch size:   128 | lm loss: 3.928310E+00 | loss scale: 32768.0 | grad norm: 2.353 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.650 | tokens per gpu per second (tgs): 1961.609 | TFLOPs: 15.79 |
g0345: [2024-08-02 21:01:25,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=0, lr=[3.860507306666667e-05, 3.860507306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2210 loss: 3.8762 iter time (s): 4.121 samples/sec: 31.061
g0364:  iteration     2210/10000000 | consumed samples:       282880 | consumed tokens:    579338240 | elapsed time per iteration (ms): 4153.6 | learning rate: 3.861E-05 | global batch size:   128 | lm loss: 3.905473E+00 | loss scale: 32768.0 | grad norm: 2.133 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.817 | tokens per gpu per second (tgs): 1972.275 | TFLOPs: 15.87 |
g0345: [2024-08-02 21:02:07,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=0, lr=[3.8779835733333336e-05, 3.8779835733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2220 loss: 3.8121 iter time (s): 4.140 samples/sec: 30.917
g0364:  iteration     2220/10000000 | consumed samples:       284160 | consumed tokens:    581959680 | elapsed time per iteration (ms): 4172.2 | learning rate: 3.878E-05 | global batch size:   128 | lm loss: 3.852299E+00 | loss scale: 32768.0 | grad norm: 1.888 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.679 | tokens per gpu per second (tgs): 1963.460 | TFLOPs: 15.80 |
g0345: [2024-08-02 21:02:49,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=0, lr=[3.89545984e-05, 3.89545984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2230 loss: 3.9444 iter time (s): 4.193 samples/sec: 30.525
g0364:  iteration     2230/10000000 | consumed samples:       285440 | consumed tokens:    584581120 | elapsed time per iteration (ms): 4225.4 | learning rate: 3.895E-05 | global batch size:   128 | lm loss: 3.862141E+00 | loss scale: 32768.0 | grad norm: 2.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.293 | tokens per gpu per second (tgs): 1938.741 | TFLOPs: 15.60 |
g0345: [2024-08-02 21:03:31,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=0, lr=[3.912936106666667e-05, 3.912936106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2240 loss: 3.8918 iter time (s): 4.184 samples/sec: 30.590
g0364:  iteration     2240/10000000 | consumed samples:       286720 | consumed tokens:    587202560 | elapsed time per iteration (ms): 4216.7 | learning rate: 3.913E-05 | global batch size:   128 | lm loss: 3.936086E+00 | loss scale: 32768.0 | grad norm: 2.560 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.356 | tokens per gpu per second (tgs): 1942.769 | TFLOPs: 15.63 |
g0345: [2024-08-02 21:04:13,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=0, lr=[3.9304123733333334e-05, 3.9304123733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2250 loss: 4.0202 iter time (s): 4.155 samples/sec: 30.804
g0364:  iteration     2250/10000000 | consumed samples:       288000 | consumed tokens:    589824000 | elapsed time per iteration (ms): 4187.7 | learning rate: 3.930E-05 | global batch size:   128 | lm loss: 3.894067E+00 | loss scale: 32768.0 | grad norm: 1.761 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.565 | tokens per gpu per second (tgs): 1956.183 | TFLOPs: 15.74 |
g0345: [2024-08-02 21:04:55,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=0, lr=[3.94788864e-05, 3.94788864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2260 loss: 3.8738 iter time (s): 4.161 samples/sec: 30.763
g0364:  iteration     2260/10000000 | consumed samples:       289280 | consumed tokens:    592445440 | elapsed time per iteration (ms): 4193.4 | learning rate: 3.948E-05 | global batch size:   128 | lm loss: 3.856451E+00 | loss scale: 32768.0 | grad norm: 1.972 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.524 | tokens per gpu per second (tgs): 1953.558 | TFLOPs: 15.72 |
g0345: [2024-08-02 21:05:36,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=0, lr=[3.965364906666667e-05, 3.965364906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2270 loss: 3.8208 iter time (s): 4.072 samples/sec: 31.434
g0364:  iteration     2270/10000000 | consumed samples:       290560 | consumed tokens:    595066880 | elapsed time per iteration (ms): 4104.6 | learning rate: 3.965E-05 | global batch size:   128 | lm loss: 3.867027E+00 | loss scale: 32768.0 | grad norm: 1.843 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.185 | tokens per gpu per second (tgs): 1995.820 | TFLOPs: 16.06 |
g0345: [2024-08-02 21:06:17,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=0, lr=[3.982841173333334e-05, 3.982841173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2280 loss: 3.7922 iter time (s): 4.098 samples/sec: 31.234
g0364:  iteration     2280/10000000 | consumed samples:       291840 | consumed tokens:    597688320 | elapsed time per iteration (ms): 4132.2 | learning rate: 3.983E-05 | global batch size:   128 | lm loss: 3.871503E+00 | loss scale: 32768.0 | grad norm: 1.960 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.976 | tokens per gpu per second (tgs): 1982.468 | TFLOPs: 15.95 |
g0345: [2024-08-02 21:06:59,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=0, lr=[4.0003174400000006e-05, 4.0003174400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2290 loss: 3.8416 iter time (s): 4.171 samples/sec: 30.692
g0364:  iteration     2290/10000000 | consumed samples:       293120 | consumed tokens:    600309760 | elapsed time per iteration (ms): 4203.1 | learning rate: 4.000E-05 | global batch size:   128 | lm loss: 3.811245E+00 | loss scale: 32768.0 | grad norm: 2.062 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.454 | tokens per gpu per second (tgs): 1949.033 | TFLOPs: 15.68 |
g0345: [2024-08-02 21:07:41,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=0, lr=[4.017793706666667e-05, 4.017793706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2300 loss: 3.7872 iter time (s): 4.146 samples/sec: 30.876
g0364:  iteration     2300/10000000 | consumed samples:       294400 | consumed tokens:    602931200 | elapsed time per iteration (ms): 4178.2 | learning rate: 4.018E-05 | global batch size:   128 | lm loss: 3.885794E+00 | loss scale: 32768.0 | grad norm: 1.875 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.635 | tokens per gpu per second (tgs): 1960.650 | TFLOPs: 15.78 |
g0345: [2024-08-02 21:08:22,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=0, lr=[4.035269973333334e-05, 4.035269973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2310 loss: 3.7964 iter time (s): 4.106 samples/sec: 31.172
g0364:  iteration     2310/10000000 | consumed samples:       295680 | consumed tokens:    605552640 | elapsed time per iteration (ms): 4138.9 | learning rate: 4.035E-05 | global batch size:   128 | lm loss: 3.880715E+00 | loss scale: 32768.0 | grad norm: 2.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.926 | tokens per gpu per second (tgs): 1979.247 | TFLOPs: 15.93 |
g0345: [2024-08-02 21:09:04,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=0, lr=[4.0527462400000004e-05, 4.0527462400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2320 loss: 3.8572 iter time (s): 4.127 samples/sec: 31.016
g0364:  iteration     2320/10000000 | consumed samples:       296960 | consumed tokens:    608174080 | elapsed time per iteration (ms): 4159.9 | learning rate: 4.053E-05 | global batch size:   128 | lm loss: 3.857292E+00 | loss scale: 32768.0 | grad norm: 1.874 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.770 | tokens per gpu per second (tgs): 1969.294 | TFLOPs: 15.85 |
g0345: [2024-08-02 21:09:45,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=0, lr=[4.070222506666667e-05, 4.070222506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2330 loss: 3.8846 iter time (s): 4.107 samples/sec: 31.169
g0364:  iteration     2330/10000000 | consumed samples:       298240 | consumed tokens:    610795520 | elapsed time per iteration (ms): 4139.5 | learning rate: 4.070E-05 | global batch size:   128 | lm loss: 3.802519E+00 | loss scale: 32768.0 | grad norm: 1.702 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.922 | tokens per gpu per second (tgs): 1978.982 | TFLOPs: 15.93 |
g0345: [2024-08-02 21:10:27,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=0, lr=[4.0876987733333336e-05, 4.0876987733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2340 loss: 4.0147 iter time (s): 4.163 samples/sec: 30.744
g0364:  iteration     2340/10000000 | consumed samples:       299520 | consumed tokens:    613416960 | elapsed time per iteration (ms): 4195.6 | learning rate: 4.088E-05 | global batch size:   128 | lm loss: 3.811182E+00 | loss scale: 32768.0 | grad norm: 1.660 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.508 | tokens per gpu per second (tgs): 1952.535 | TFLOPs: 15.71 |
g0345: [2024-08-02 21:11:09,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=0, lr=[4.10517504e-05, 4.10517504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2350 loss: 3.8261 iter time (s): 4.129 samples/sec: 31.003
g0364:  iteration     2350/10000000 | consumed samples:       300800 | consumed tokens:    616038400 | elapsed time per iteration (ms): 4161.0 | learning rate: 4.105E-05 | global batch size:   128 | lm loss: 3.835399E+00 | loss scale: 32768.0 | grad norm: 1.769 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.762 | tokens per gpu per second (tgs): 1968.767 | TFLOPs: 15.84 |
g0345: [2024-08-02 21:11:51,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=0, lr=[4.122651306666667e-05, 4.122651306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2360 loss: 3.9257 iter time (s): 4.175 samples/sec: 30.659
g0364:  iteration     2360/10000000 | consumed samples:       302080 | consumed tokens:    618659840 | elapsed time per iteration (ms): 4207.4 | learning rate: 4.123E-05 | global batch size:   128 | lm loss: 3.870853E+00 | loss scale: 32768.0 | grad norm: 1.676 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1947.040 | TFLOPs: 15.67 |
g0345: [2024-08-02 21:12:32,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=0, lr=[4.1401275733333334e-05, 4.1401275733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2370 loss: 3.7795 iter time (s): 4.081 samples/sec: 31.367
g0364:  iteration     2370/10000000 | consumed samples:       303360 | consumed tokens:    621281280 | elapsed time per iteration (ms): 4113.5 | learning rate: 4.140E-05 | global batch size:   128 | lm loss: 3.768054E+00 | loss scale: 32768.0 | grad norm: 1.788 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.117 | tokens per gpu per second (tgs): 1991.497 | TFLOPs: 16.03 |
g0345: [2024-08-02 21:13:14,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=0, lr=[4.15760384e-05, 4.15760384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2380 loss: 3.9765 iter time (s): 4.181 samples/sec: 30.613
g0364:  iteration     2380/10000000 | consumed samples:       304640 | consumed tokens:    623902720 | elapsed time per iteration (ms): 4213.9 | learning rate: 4.158E-05 | global batch size:   128 | lm loss: 3.789614E+00 | loss scale: 32768.0 | grad norm: 1.592 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.376 | tokens per gpu per second (tgs): 1944.060 | TFLOPs: 15.64 |
g0345: [2024-08-02 21:13:56,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=0, lr=[4.1750801066666666e-05, 4.1750801066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2390 loss: 3.8459 iter time (s): 4.153 samples/sec: 30.819
g0364:  iteration     2390/10000000 | consumed samples:       305920 | consumed tokens:    626524160 | elapsed time per iteration (ms): 4185.7 | learning rate: 4.175E-05 | global batch size:   128 | lm loss: 3.760628E+00 | loss scale: 32768.0 | grad norm: 1.774 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.580 | tokens per gpu per second (tgs): 1957.142 | TFLOPs: 15.75 |
g0345: [2024-08-02 21:14:38,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=0, lr=[4.192556373333333e-05, 4.192556373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2400 loss: 3.7324 iter time (s): 4.126 samples/sec: 31.022
g0364:  iteration     2400/10000000 | consumed samples:       307200 | consumed tokens:    629145600 | elapsed time per iteration (ms): 4158.6 | learning rate: 4.193E-05 | global batch size:   128 | lm loss: 3.765457E+00 | loss scale: 32768.0 | grad norm: 1.890 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.779 | tokens per gpu per second (tgs): 1969.880 | TFLOPs: 15.85 |
g0345: [2024-08-02 21:15:20,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=0, lr=[4.21003264e-05, 4.21003264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2410 loss: 3.8712 iter time (s): 4.168 samples/sec: 30.709
g0364:  iteration     2410/10000000 | consumed samples:       308480 | consumed tokens:    631767040 | elapsed time per iteration (ms): 4201.0 | learning rate: 4.210E-05 | global batch size:   128 | lm loss: 3.784727E+00 | loss scale: 32768.0 | grad norm: 2.038 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.469 | tokens per gpu per second (tgs): 1950.023 | TFLOPs: 15.69 |
g0345: [2024-08-02 21:16:02,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=0, lr=[4.2275089066666664e-05, 4.2275089066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2420 loss: 3.8337 iter time (s): 4.232 samples/sec: 30.247
g0364:  iteration     2420/10000000 | consumed samples:       309760 | consumed tokens:    634388480 | elapsed time per iteration (ms): 4266.0 | learning rate: 4.228E-05 | global batch size:   128 | lm loss: 3.776266E+00 | loss scale: 32768.0 | grad norm: 2.001 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.004 | tokens per gpu per second (tgs): 1920.283 | TFLOPs: 15.45 |
g0345: [2024-08-02 21:16:45,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=0, lr=[4.244985173333334e-05, 4.244985173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2430 loss: 3.9560 iter time (s): 4.231 samples/sec: 30.256
g0364:  iteration     2430/10000000 | consumed samples:       311040 | consumed tokens:    637009920 | elapsed time per iteration (ms): 4263.2 | learning rate: 4.245E-05 | global batch size:   128 | lm loss: 3.799091E+00 | loss scale: 32768.0 | grad norm: 1.691 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.025 | tokens per gpu per second (tgs): 1921.579 | TFLOPs: 15.46 |
g0345: [2024-08-02 21:17:27,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=0, lr=[4.26246144e-05, 4.26246144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2440 loss: 3.6457 iter time (s): 4.192 samples/sec: 30.537
g0364:  iteration     2440/10000000 | consumed samples:       312320 | consumed tokens:    639631360 | elapsed time per iteration (ms): 4224.5 | learning rate: 4.262E-05 | global batch size:   128 | lm loss: 3.696197E+00 | loss scale: 32768.0 | grad norm: 1.762 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.300 | tokens per gpu per second (tgs): 1939.185 | TFLOPs: 15.60 |
g0345: [2024-08-02 21:18:09,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=2450, skipped=0, lr=[4.279937706666667e-05, 4.279937706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2450 loss: 3.6576 iter time (s): 4.121 samples/sec: 31.061
g0364:  iteration     2450/10000000 | consumed samples:       313600 | consumed tokens:    642252800 | elapsed time per iteration (ms): 4154.6 | learning rate: 4.280E-05 | global batch size:   128 | lm loss: 3.744657E+00 | loss scale: 32768.0 | grad norm: 1.757 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.809 | tokens per gpu per second (tgs): 1971.772 | TFLOPs: 15.87 |
g0345: [2024-08-02 21:18:51,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=2460, skipped=0, lr=[4.2974139733333335e-05, 4.2974139733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2460 loss: 3.8321 iter time (s): 4.183 samples/sec: 30.601
g0364:  iteration     2460/10000000 | consumed samples:       314880 | consumed tokens:    644874240 | elapsed time per iteration (ms): 4215.5 | learning rate: 4.297E-05 | global batch size:   128 | lm loss: 3.769535E+00 | loss scale: 32768.0 | grad norm: 1.623 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.364 | tokens per gpu per second (tgs): 1943.319 | TFLOPs: 15.64 |
g0345: [2024-08-02 21:19:32,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=2470, skipped=0, lr=[4.31489024e-05, 4.31489024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2470 loss: 3.7525 iter time (s): 4.037 samples/sec: 31.703
g0364:  iteration     2470/10000000 | consumed samples:       316160 | consumed tokens:    647495680 | elapsed time per iteration (ms): 4071.0 | learning rate: 4.315E-05 | global batch size:   128 | lm loss: 3.725132E+00 | loss scale: 32768.0 | grad norm: 1.768 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.442 | tokens per gpu per second (tgs): 2012.300 | TFLOPs: 16.19 |
g0345: [2024-08-02 21:20:13,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=2480, skipped=0, lr=[4.332366506666667e-05, 4.332366506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2480 loss: 3.6732 iter time (s): 4.119 samples/sec: 31.073
g0364:  iteration     2480/10000000 | consumed samples:       317440 | consumed tokens:    650117120 | elapsed time per iteration (ms): 4152.4 | learning rate: 4.332E-05 | global batch size:   128 | lm loss: 3.765701E+00 | loss scale: 32768.0 | grad norm: 1.692 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.826 | tokens per gpu per second (tgs): 1972.852 | TFLOPs: 15.88 |
g0345: [2024-08-02 21:20:55,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=2490, skipped=0, lr=[4.3498427733333334e-05, 4.3498427733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2490 loss: 3.5701 iter time (s): 4.158 samples/sec: 30.781
g0364:  iteration     2490/10000000 | consumed samples:       318720 | consumed tokens:    652738560 | elapsed time per iteration (ms): 4191.3 | learning rate: 4.350E-05 | global batch size:   128 | lm loss: 3.733726E+00 | loss scale: 32768.0 | grad norm: 1.563 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.539 | tokens per gpu per second (tgs): 1954.505 | TFLOPs: 15.73 |
g0345: [2024-08-02 21:21:37,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=2500, skipped=0, lr=[4.36731904e-05, 4.36731904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2500 loss: 3.6568 iter time (s): 4.133 samples/sec: 30.973
g0364:  iteration     2500/10000000 | consumed samples:       320000 | consumed tokens:    655360000 | elapsed time per iteration (ms): 4165.1 | learning rate: 4.367E-05 | global batch size:   128 | lm loss: 3.754086E+00 | loss scale: 32768.0 | grad norm: 1.931 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.732 | tokens per gpu per second (tgs): 1966.823 | TFLOPs: 15.83 |
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0347: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0347: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0363: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0363: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0363: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0362: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0358: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0362: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0358: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0364: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0364: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0358: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0346: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0364: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0358: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0364: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0352: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0364: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0346: [2024-08-02 21:21:41,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0346: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0346: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0346: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0345: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0364: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 21:21:41,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0345: [2024-08-02 21:22:19,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=2510, skipped=0, lr=[4.3847953066666666e-05, 4.3847953066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2510 loss: 3.6216 iter time (s): 4.179 samples/sec: 30.629
g0364:  iteration     2510/10000000 | consumed samples:       321280 | consumed tokens:    657981440 | elapsed time per iteration (ms): 4211.8 | learning rate: 4.385E-05 | global batch size:   128 | lm loss: 3.718095E+00 | loss scale: 65536.0 | grad norm: 1.817 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.391 | tokens per gpu per second (tgs): 1945.010 | TFLOPs: 15.65 |
g0345: [2024-08-02 21:23:00,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=2520, skipped=0, lr=[4.402271573333334e-05, 4.402271573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2520 loss: 3.8303 iter time (s): 4.069 samples/sec: 31.457
g0364:  iteration     2520/10000000 | consumed samples:       322560 | consumed tokens:    660602880 | elapsed time per iteration (ms): 4101.7 | learning rate: 4.402E-05 | global batch size:   128 | lm loss: 3.704741E+00 | loss scale: 65536.0 | grad norm: 1.687 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.206 | tokens per gpu per second (tgs): 1997.206 | TFLOPs: 16.07 |
g0345: [2024-08-02 21:23:41,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=2530, skipped=0, lr=[4.4197478400000005e-05, 4.4197478400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2530 loss: 3.8049 iter time (s): 4.033 samples/sec: 31.739
g0364:  iteration     2530/10000000 | consumed samples:       323840 | consumed tokens:    663224320 | elapsed time per iteration (ms): 4065.5 | learning rate: 4.420E-05 | global batch size:   128 | lm loss: 3.731816E+00 | loss scale: 65536.0 | grad norm: 1.590 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.485 | tokens per gpu per second (tgs): 2015.025 | TFLOPs: 16.22 |
g0345: [2024-08-02 21:24:21,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=2540, skipped=0, lr=[4.437224106666667e-05, 4.437224106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2540 loss: 3.7695 iter time (s): 4.036 samples/sec: 31.715
g0364:  iteration     2540/10000000 | consumed samples:       325120 | consumed tokens:    665845760 | elapsed time per iteration (ms): 4069.0 | learning rate: 4.437E-05 | global batch size:   128 | lm loss: 3.728246E+00 | loss scale: 65536.0 | grad norm: 1.707 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.457 | tokens per gpu per second (tgs): 2013.270 | TFLOPs: 16.20 |
g0345: [2024-08-02 21:25:03,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=2550, skipped=0, lr=[4.454700373333334e-05, 4.454700373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2550 loss: 3.8431 iter time (s): 4.179 samples/sec: 30.633
g0364:  iteration     2550/10000000 | consumed samples:       326400 | consumed tokens:    668467200 | elapsed time per iteration (ms): 4211.6 | learning rate: 4.455E-05 | global batch size:   128 | lm loss: 3.693754E+00 | loss scale: 65536.0 | grad norm: 1.511 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.392 | tokens per gpu per second (tgs): 1945.092 | TFLOPs: 15.65 |
g0345: [2024-08-02 21:25:44,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=2560, skipped=0, lr=[4.47217664e-05, 4.47217664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2560 loss: 3.8163 iter time (s): 4.075 samples/sec: 31.412
g0364:  iteration     2560/10000000 | consumed samples:       327680 | consumed tokens:    671088640 | elapsed time per iteration (ms): 4107.7 | learning rate: 4.472E-05 | global batch size:   128 | lm loss: 3.732667E+00 | loss scale: 65536.0 | grad norm: 2.055 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.161 | tokens per gpu per second (tgs): 1994.286 | TFLOPs: 16.05 |
g0345: [2024-08-02 21:26:26,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=2570, skipped=0, lr=[4.489652906666667e-05, 4.489652906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2570 loss: 3.5563 iter time (s): 4.142 samples/sec: 30.903
g0364:  iteration     2570/10000000 | consumed samples:       328960 | consumed tokens:    673710080 | elapsed time per iteration (ms): 4174.7 | learning rate: 4.490E-05 | global batch size:   128 | lm loss: 3.728664E+00 | loss scale: 65536.0 | grad norm: 1.861 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.661 | tokens per gpu per second (tgs): 1962.278 | TFLOPs: 15.79 |
g0345: [2024-08-02 21:27:08,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=2580, skipped=0, lr=[4.5071291733333335e-05, 4.5071291733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2580 loss: 3.8328 iter time (s): 4.100 samples/sec: 31.222
g0364:  iteration     2580/10000000 | consumed samples:       330240 | consumed tokens:    676331520 | elapsed time per iteration (ms): 4132.1 | learning rate: 4.507E-05 | global batch size:   128 | lm loss: 3.743899E+00 | loss scale: 65536.0 | grad norm: 1.545 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.977 | tokens per gpu per second (tgs): 1982.523 | TFLOPs: 15.95 |
g0345: [2024-08-02 21:27:49,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=2590, skipped=0, lr=[4.52460544e-05, 4.52460544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2590 loss: 3.7241 iter time (s): 4.101 samples/sec: 31.208
g0364:  iteration     2590/10000000 | consumed samples:       331520 | consumed tokens:    678952960 | elapsed time per iteration (ms): 4134.1 | learning rate: 4.525E-05 | global batch size:   128 | lm loss: 3.663687E+00 | loss scale: 65536.0 | grad norm: 1.786 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.962 | tokens per gpu per second (tgs): 1981.549 | TFLOPs: 15.95 |
g0345: [2024-08-02 21:28:31,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=2600, skipped=0, lr=[4.542081706666667e-05, 4.542081706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2600 loss: 3.7952 iter time (s): 4.158 samples/sec: 30.784
g0364:  iteration     2600/10000000 | consumed samples:       332800 | consumed tokens:    681574400 | elapsed time per iteration (ms): 4190.4 | learning rate: 4.542E-05 | global batch size:   128 | lm loss: 3.734855E+00 | loss scale: 65536.0 | grad norm: 1.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.546 | tokens per gpu per second (tgs): 1954.962 | TFLOPs: 15.73 |
g0345: [2024-08-02 21:29:14,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=2610, skipped=0, lr=[4.559557973333334e-05, 4.559557973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2610 loss: 3.7247 iter time (s): 4.274 samples/sec: 29.947
g0364:  iteration     2610/10000000 | consumed samples:       334080 | consumed tokens:    684195840 | elapsed time per iteration (ms): 4329.7 | learning rate: 4.560E-05 | global batch size:   128 | lm loss: 3.663563E+00 | loss scale: 65536.0 | grad norm: 1.825 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.563 | tokens per gpu per second (tgs): 1892.050 | TFLOPs: 15.23 |
g0345: [2024-08-02 21:29:56,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=2620, skipped=0, lr=[4.5770342400000006e-05, 4.5770342400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2620 loss: 3.8372 iter time (s): 4.167 samples/sec: 30.718
g0364:  iteration     2620/10000000 | consumed samples:       335360 | consumed tokens:    686817280 | elapsed time per iteration (ms): 4199.4 | learning rate: 4.577E-05 | global batch size:   128 | lm loss: 3.692318E+00 | loss scale: 65536.0 | grad norm: 1.656 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.481 | tokens per gpu per second (tgs): 1950.764 | TFLOPs: 15.70 |
g0345: [2024-08-02 21:30:38,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=2630, skipped=0, lr=[4.594510506666667e-05, 4.594510506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2630 loss: 3.6137 iter time (s): 4.121 samples/sec: 31.060
g0364:  iteration     2630/10000000 | consumed samples:       336640 | consumed tokens:    689438720 | elapsed time per iteration (ms): 4157.7 | learning rate: 4.595E-05 | global batch size:   128 | lm loss: 3.686873E+00 | loss scale: 65536.0 | grad norm: 1.634 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.786 | tokens per gpu per second (tgs): 1970.326 | TFLOPs: 15.86 |
g0345: [2024-08-02 21:31:19,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=2640, skipped=0, lr=[4.611986773333334e-05, 4.611986773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2640 loss: 3.7488 iter time (s): 4.135 samples/sec: 30.956
g0364:  iteration     2640/10000000 | consumed samples:       337920 | consumed tokens:    692060160 | elapsed time per iteration (ms): 4167.4 | learning rate: 4.612E-05 | global batch size:   128 | lm loss: 3.670969E+00 | loss scale: 65536.0 | grad norm: 1.671 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.715 | tokens per gpu per second (tgs): 1965.755 | TFLOPs: 15.82 |
g0345: [2024-08-02 21:32:01,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=2650, skipped=0, lr=[4.6294630400000004e-05, 4.6294630400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2650 loss: 3.7865 iter time (s): 4.168 samples/sec: 30.710
g0364:  iteration     2650/10000000 | consumed samples:       339200 | consumed tokens:    694681600 | elapsed time per iteration (ms): 4201.0 | learning rate: 4.629E-05 | global batch size:   128 | lm loss: 3.649887E+00 | loss scale: 65536.0 | grad norm: 1.696 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.469 | tokens per gpu per second (tgs): 1950.010 | TFLOPs: 15.69 |
g0345: [2024-08-02 21:32:43,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=2660, skipped=0, lr=[4.646939306666667e-05, 4.646939306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2660 loss: 3.8261 iter time (s): 4.171 samples/sec: 30.687
g0364:  iteration     2660/10000000 | consumed samples:       340480 | consumed tokens:    697303040 | elapsed time per iteration (ms): 4203.6 | learning rate: 4.647E-05 | global batch size:   128 | lm loss: 3.739521E+00 | loss scale: 65536.0 | grad norm: 1.796 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.450 | tokens per gpu per second (tgs): 1948.822 | TFLOPs: 15.68 |
g0345: [2024-08-02 21:33:25,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=2670, skipped=0, lr=[4.6644155733333336e-05, 4.6644155733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2670 loss: 3.5386 iter time (s): 4.142 samples/sec: 30.901
g0364:  iteration     2670/10000000 | consumed samples:       341760 | consumed tokens:    699924480 | elapsed time per iteration (ms): 4174.9 | learning rate: 4.664E-05 | global batch size:   128 | lm loss: 3.656684E+00 | loss scale: 65536.0 | grad norm: 1.988 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.659 | tokens per gpu per second (tgs): 1962.204 | TFLOPs: 15.79 |
g0345: [2024-08-02 21:34:07,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=2680, skipped=0, lr=[4.68189184e-05, 4.68189184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2680 loss: 3.5689 iter time (s): 4.130 samples/sec: 30.995
g0364:  iteration     2680/10000000 | consumed samples:       343040 | consumed tokens:    702545920 | elapsed time per iteration (ms): 4162.2 | learning rate: 4.682E-05 | global batch size:   128 | lm loss: 3.640647E+00 | loss scale: 65536.0 | grad norm: 1.654 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.753 | tokens per gpu per second (tgs): 1968.183 | TFLOPs: 15.84 |
g0345: [2024-08-02 21:34:49,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=2690, skipped=0, lr=[4.699368106666667e-05, 4.699368106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2690 loss: 3.7874 iter time (s): 4.227 samples/sec: 30.282
g0364:  iteration     2690/10000000 | consumed samples:       344320 | consumed tokens:    705167360 | elapsed time per iteration (ms): 4262.0 | learning rate: 4.699E-05 | global batch size:   128 | lm loss: 3.693328E+00 | loss scale: 65536.0 | grad norm: 1.579 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.033 | tokens per gpu per second (tgs): 1922.121 | TFLOPs: 15.47 |
g0345: [2024-08-02 21:35:31,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=2700, skipped=0, lr=[4.716844373333334e-05, 4.716844373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2700 loss: 3.5703 iter time (s): 4.118 samples/sec: 31.085
g0364:  iteration     2700/10000000 | consumed samples:       345600 | consumed tokens:    707788800 | elapsed time per iteration (ms): 4150.3 | learning rate: 4.717E-05 | global batch size:   128 | lm loss: 3.605691E+00 | loss scale: 65536.0 | grad norm: 1.872 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.841 | tokens per gpu per second (tgs): 1973.849 | TFLOPs: 15.88 |
g0345: [2024-08-02 21:36:13,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=2710, skipped=0, lr=[4.734320640000001e-05, 4.734320640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2710 loss: 3.7227 iter time (s): 4.227 samples/sec: 30.284
g0364:  iteration     2710/10000000 | consumed samples:       346880 | consumed tokens:    710410240 | elapsed time per iteration (ms): 4259.6 | learning rate: 4.734E-05 | global batch size:   128 | lm loss: 3.643904E+00 | loss scale: 65536.0 | grad norm: 1.507 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.050 | tokens per gpu per second (tgs): 1923.180 | TFLOPs: 15.48 |
g0345: [2024-08-02 21:36:55,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=2720, skipped=0, lr=[4.751796906666667e-05, 4.751796906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2720 loss: 3.5660 iter time (s): 4.155 samples/sec: 30.810
g0364:  iteration     2720/10000000 | consumed samples:       348160 | consumed tokens:    713031680 | elapsed time per iteration (ms): 4187.1 | learning rate: 4.752E-05 | global batch size:   128 | lm loss: 3.631005E+00 | loss scale: 65536.0 | grad norm: 1.609 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.570 | tokens per gpu per second (tgs): 1956.470 | TFLOPs: 15.74 |
g0345: [2024-08-02 21:37:37,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=2730, skipped=0, lr=[4.769273173333334e-05, 4.769273173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2730 loss: 3.5544 iter time (s): 4.183 samples/sec: 30.603
g0364:  iteration     2730/10000000 | consumed samples:       349440 | consumed tokens:    715653120 | elapsed time per iteration (ms): 4215.2 | learning rate: 4.769E-05 | global batch size:   128 | lm loss: 3.642952E+00 | loss scale: 65536.0 | grad norm: 1.621 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.366 | tokens per gpu per second (tgs): 1943.428 | TFLOPs: 15.64 |
g0345: [2024-08-02 21:38:19,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=2740, skipped=0, lr=[4.7867494400000005e-05, 4.7867494400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2740 loss: 3.7117 iter time (s): 4.117 samples/sec: 31.092
g0364:  iteration     2740/10000000 | consumed samples:       350720 | consumed tokens:    718274560 | elapsed time per iteration (ms): 4149.8 | learning rate: 4.787E-05 | global batch size:   128 | lm loss: 3.643875E+00 | loss scale: 65536.0 | grad norm: 1.591 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.845 | tokens per gpu per second (tgs): 1974.095 | TFLOPs: 15.89 |
g0345: [2024-08-02 21:39:01,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=2750, skipped=0, lr=[4.804225706666667e-05, 4.804225706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2750 loss: 3.7019 iter time (s): 4.187 samples/sec: 30.573
g0364:  iteration     2750/10000000 | consumed samples:       352000 | consumed tokens:    720896000 | elapsed time per iteration (ms): 4219.1 | learning rate: 4.804E-05 | global batch size:   128 | lm loss: 3.613956E+00 | loss scale: 65536.0 | grad norm: 1.593 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.338 | tokens per gpu per second (tgs): 1941.633 | TFLOPs: 15.62 |
g0345: [2024-08-02 21:39:43,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=2760, skipped=0, lr=[4.821701973333334e-05, 4.821701973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2760 loss: 3.6828 iter time (s): 4.128 samples/sec: 31.011
g0364:  iteration     2760/10000000 | consumed samples:       353280 | consumed tokens:    723517440 | elapsed time per iteration (ms): 4160.2 | learning rate: 4.822E-05 | global batch size:   128 | lm loss: 3.624620E+00 | loss scale: 65536.0 | grad norm: 1.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.768 | tokens per gpu per second (tgs): 1969.144 | TFLOPs: 15.85 |
g0345: [2024-08-02 21:40:25,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=2770, skipped=0, lr=[4.8391782400000004e-05, 4.8391782400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2770 loss: 3.5741 iter time (s): 4.147 samples/sec: 30.867
g0364:  iteration     2770/10000000 | consumed samples:       354560 | consumed tokens:    726138880 | elapsed time per iteration (ms): 4179.6 | learning rate: 4.839E-05 | global batch size:   128 | lm loss: 3.643610E+00 | loss scale: 65536.0 | grad norm: 1.697 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.625 | tokens per gpu per second (tgs): 1959.993 | TFLOPs: 15.77 |
g0345: [2024-08-02 21:41:07,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=2780, skipped=0, lr=[4.856654506666667e-05, 4.856654506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2780 loss: 3.8006 iter time (s): 4.228 samples/sec: 30.275
g0364:  iteration     2780/10000000 | consumed samples:       355840 | consumed tokens:    728760320 | elapsed time per iteration (ms): 4261.0 | learning rate: 4.857E-05 | global batch size:   128 | lm loss: 3.628821E+00 | loss scale: 65536.0 | grad norm: 1.645 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.040 | tokens per gpu per second (tgs): 1922.548 | TFLOPs: 15.47 |
g0345: [2024-08-02 21:41:49,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=2790, skipped=0, lr=[4.874130773333334e-05, 4.874130773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2790 loss: 3.5025 iter time (s): 4.126 samples/sec: 31.022
g0364:  iteration     2790/10000000 | consumed samples:       357120 | consumed tokens:    731381760 | elapsed time per iteration (ms): 4159.3 | learning rate: 4.874E-05 | global batch size:   128 | lm loss: 3.539012E+00 | loss scale: 65536.0 | grad norm: 1.478 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.774 | tokens per gpu per second (tgs): 1969.559 | TFLOPs: 15.85 |
g0345: [2024-08-02 21:42:31,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=2800, skipped=0, lr=[4.891607040000001e-05, 4.891607040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2800 loss: 3.6816 iter time (s): 4.169 samples/sec: 30.701
g0364:  iteration     2800/10000000 | consumed samples:       358400 | consumed tokens:    734003200 | elapsed time per iteration (ms): 4203.2 | learning rate: 4.892E-05 | global batch size:   128 | lm loss: 3.601770E+00 | loss scale: 65536.0 | grad norm: 1.555 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.453 | tokens per gpu per second (tgs): 1949.007 | TFLOPs: 15.68 |
g0345: [2024-08-02 21:43:13,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=2810, skipped=0, lr=[4.9090833066666675e-05, 4.9090833066666675e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2810 loss: 3.6868 iter time (s): 4.159 samples/sec: 30.773
g0364:  iteration     2810/10000000 | consumed samples:       359680 | consumed tokens:    736624640 | elapsed time per iteration (ms): 4191.9 | learning rate: 4.909E-05 | global batch size:   128 | lm loss: 3.718130E+00 | loss scale: 65536.0 | grad norm: 1.548 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.535 | tokens per gpu per second (tgs): 1954.243 | TFLOPs: 15.73 |
g0345: [2024-08-02 21:43:54,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=2820, skipped=0, lr=[4.926559573333334e-05, 4.926559573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2820 loss: 3.5692 iter time (s): 4.101 samples/sec: 31.211
g0364:  iteration     2820/10000000 | consumed samples:       360960 | consumed tokens:    739246080 | elapsed time per iteration (ms): 4133.4 | learning rate: 4.927E-05 | global batch size:   128 | lm loss: 3.602370E+00 | loss scale: 65536.0 | grad norm: 1.574 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.967 | tokens per gpu per second (tgs): 1981.915 | TFLOPs: 15.95 |
g0345: [2024-08-02 21:44:35,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=2830, skipped=0, lr=[4.944035840000001e-05, 4.944035840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2830 loss: 3.5198 iter time (s): 4.090 samples/sec: 31.294
g0364:  iteration     2830/10000000 | consumed samples:       362240 | consumed tokens:    741867520 | elapsed time per iteration (ms): 4122.4 | learning rate: 4.944E-05 | global batch size:   128 | lm loss: 3.632217E+00 | loss scale: 65536.0 | grad norm: 1.544 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.050 | tokens per gpu per second (tgs): 1987.184 | TFLOPs: 15.99 |
g0345: [2024-08-02 21:45:16,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=2840, skipped=0, lr=[4.961512106666667e-05, 4.961512106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2840 loss: 3.7708 iter time (s): 4.079 samples/sec: 31.380
g0364:  iteration     2840/10000000 | consumed samples:       363520 | consumed tokens:    744488960 | elapsed time per iteration (ms): 4111.2 | learning rate: 4.962E-05 | global batch size:   128 | lm loss: 3.613271E+00 | loss scale: 65536.0 | grad norm: 1.571 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.134 | tokens per gpu per second (tgs): 1992.592 | TFLOPs: 16.03 |
g0345: [2024-08-02 21:45:59,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=2850, skipped=0, lr=[4.978988373333333e-05, 4.978988373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2850 loss: 3.5053 iter time (s): 4.210 samples/sec: 30.401
g0364:  iteration     2850/10000000 | consumed samples:       364800 | consumed tokens:    747110400 | elapsed time per iteration (ms): 4243.8 | learning rate: 4.979E-05 | global batch size:   128 | lm loss: 3.600286E+00 | loss scale: 65536.0 | grad norm: 1.419 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.162 | tokens per gpu per second (tgs): 1930.361 | TFLOPs: 15.53 |
g0345: [2024-08-02 21:46:41,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=2860, skipped=0, lr=[4.99646464e-05, 4.99646464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2860 loss: 3.3889 iter time (s): 4.209 samples/sec: 30.414
g0364:  iteration     2860/10000000 | consumed samples:       366080 | consumed tokens:    749731840 | elapsed time per iteration (ms): 4241.0 | learning rate: 4.996E-05 | global batch size:   128 | lm loss: 3.563619E+00 | loss scale: 65536.0 | grad norm: 1.717 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.182 | tokens per gpu per second (tgs): 1931.621 | TFLOPs: 15.54 |
g0345: [2024-08-02 21:47:24,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=2870, skipped=0, lr=[5.0139409066666664e-05, 5.0139409066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2870 loss: 3.5973 iter time (s): 4.207 samples/sec: 30.426
g0364:  iteration     2870/10000000 | consumed samples:       367360 | consumed tokens:    752353280 | elapsed time per iteration (ms): 4240.0 | learning rate: 5.014E-05 | global batch size:   128 | lm loss: 3.618814E+00 | loss scale: 65536.0 | grad norm: 1.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.188 | tokens per gpu per second (tgs): 1932.063 | TFLOPs: 15.55 |
g0345: [2024-08-02 21:48:05,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=0, lr=[5.031417173333333e-05, 5.031417173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2880 loss: 3.5492 iter time (s): 4.112 samples/sec: 31.130
g0364:  iteration     2880/10000000 | consumed samples:       368640 | consumed tokens:    754974720 | elapsed time per iteration (ms): 4144.4 | learning rate: 5.031E-05 | global batch size:   128 | lm loss: 3.634451E+00 | loss scale: 65536.0 | grad norm: 1.627 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.885 | tokens per gpu per second (tgs): 1976.658 | TFLOPs: 15.91 |
g0345: [2024-08-02 21:48:47,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=0, lr=[5.0488934399999996e-05, 5.0488934399999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2890 loss: 3.6705 iter time (s): 4.127 samples/sec: 31.012
g0364:  iteration     2890/10000000 | consumed samples:       369920 | consumed tokens:    757596160 | elapsed time per iteration (ms): 4160.0 | learning rate: 5.049E-05 | global batch size:   128 | lm loss: 3.585100E+00 | loss scale: 65536.0 | grad norm: 1.430 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.770 | tokens per gpu per second (tgs): 1969.250 | TFLOPs: 15.85 |
g0345: [2024-08-02 21:49:28,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=0, lr=[5.066369706666666e-05, 5.066369706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2900 loss: 3.4949 iter time (s): 4.135 samples/sec: 30.956
g0364:  iteration     2900/10000000 | consumed samples:       371200 | consumed tokens:    760217600 | elapsed time per iteration (ms): 4167.1 | learning rate: 5.066E-05 | global batch size:   128 | lm loss: 3.569006E+00 | loss scale: 65536.0 | grad norm: 1.472 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.717 | tokens per gpu per second (tgs): 1965.865 | TFLOPs: 15.82 |
g0345: [2024-08-02 21:50:10,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=0, lr=[5.083845973333333e-05, 5.083845973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2910 loss: 3.6392 iter time (s): 4.096 samples/sec: 31.253
g0364:  iteration     2910/10000000 | consumed samples:       372480 | consumed tokens:    762839040 | elapsed time per iteration (ms): 4128.2 | learning rate: 5.084E-05 | global batch size:   128 | lm loss: 3.630338E+00 | loss scale: 65536.0 | grad norm: 1.636 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.006 | tokens per gpu per second (tgs): 1984.408 | TFLOPs: 15.97 |
g0345: [2024-08-02 21:50:52,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=0, lr=[5.10132224e-05, 5.10132224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2920 loss: 3.5427 iter time (s): 4.177 samples/sec: 30.643
g0364:  iteration     2920/10000000 | consumed samples:       373760 | consumed tokens:    765460480 | elapsed time per iteration (ms): 4209.8 | learning rate: 5.101E-05 | global batch size:   128 | lm loss: 3.573585E+00 | loss scale: 65536.0 | grad norm: 1.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.405 | tokens per gpu per second (tgs): 1945.925 | TFLOPs: 15.66 |
g0345: [2024-08-02 21:51:32,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=0, lr=[5.118798506666667e-05, 5.118798506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2930 loss: 3.6906 iter time (s): 3.999 samples/sec: 32.010
g0364:  iteration     2930/10000000 | consumed samples:       375040 | consumed tokens:    768081920 | elapsed time per iteration (ms): 4031.2 | learning rate: 5.119E-05 | global batch size:   128 | lm loss: 3.587964E+00 | loss scale: 65536.0 | grad norm: 1.522 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.752 | tokens per gpu per second (tgs): 2032.141 | TFLOPs: 16.35 |
g0345: [2024-08-02 21:52:15,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=2940, skipped=0, lr=[5.1362747733333334e-05, 5.1362747733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2940 loss: 3.5821 iter time (s): 4.229 samples/sec: 30.267
g0364:  iteration     2940/10000000 | consumed samples:       376320 | consumed tokens:    770703360 | elapsed time per iteration (ms): 4261.4 | learning rate: 5.136E-05 | global batch size:   128 | lm loss: 3.549769E+00 | loss scale: 65536.0 | grad norm: 1.569 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.037 | tokens per gpu per second (tgs): 1922.352 | TFLOPs: 15.47 |
g0345: [2024-08-02 21:52:57,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=2950, skipped=0, lr=[5.15375104e-05, 5.15375104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2950 loss: 3.5433 iter time (s): 4.244 samples/sec: 30.161
g0364:  iteration     2950/10000000 | consumed samples:       377600 | consumed tokens:    773324800 | elapsed time per iteration (ms): 4276.2 | learning rate: 5.154E-05 | global batch size:   128 | lm loss: 3.552763E+00 | loss scale: 65536.0 | grad norm: 1.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.933 | tokens per gpu per second (tgs): 1915.706 | TFLOPs: 15.42 |
g0345: [2024-08-02 21:53:40,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=2960, skipped=0, lr=[5.1712273066666666e-05, 5.1712273066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2960 loss: 3.5590 iter time (s): 4.216 samples/sec: 30.358
g0364:  iteration     2960/10000000 | consumed samples:       378880 | consumed tokens:    775946240 | elapsed time per iteration (ms): 4248.7 | learning rate: 5.171E-05 | global batch size:   128 | lm loss: 3.558849E+00 | loss scale: 65536.0 | grad norm: 1.584 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.127 | tokens per gpu per second (tgs): 1928.113 | TFLOPs: 15.52 |
g0345: [2024-08-02 21:54:22,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=2970, skipped=0, lr=[5.188703573333333e-05, 5.188703573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2970 loss: 3.5157 iter time (s): 4.222 samples/sec: 30.321
g0364:  iteration     2970/10000000 | consumed samples:       380160 | consumed tokens:    778567680 | elapsed time per iteration (ms): 4255.4 | learning rate: 5.189E-05 | global batch size:   128 | lm loss: 3.586527E+00 | loss scale: 65536.0 | grad norm: 1.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.079 | tokens per gpu per second (tgs): 1925.076 | TFLOPs: 15.49 |
g0345: [2024-08-02 21:55:05,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=2980, skipped=0, lr=[5.20617984e-05, 5.20617984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2980 loss: 3.5361 iter time (s): 4.178 samples/sec: 30.636
g0364:  iteration     2980/10000000 | consumed samples:       381440 | consumed tokens:    781189120 | elapsed time per iteration (ms): 4210.6 | learning rate: 5.206E-05 | global batch size:   128 | lm loss: 3.583978E+00 | loss scale: 65536.0 | grad norm: 1.466 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.400 | tokens per gpu per second (tgs): 1945.572 | TFLOPs: 15.66 |
g0345: [2024-08-02 21:55:47,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=2990, skipped=0, lr=[5.2236561066666664e-05, 5.2236561066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 2990 loss: 3.6416 iter time (s): 4.201 samples/sec: 30.471
g0364:  iteration     2990/10000000 | consumed samples:       382720 | consumed tokens:    783810560 | elapsed time per iteration (ms): 4233.5 | learning rate: 5.224E-05 | global batch size:   128 | lm loss: 3.510416E+00 | loss scale: 65536.0 | grad norm: 1.323 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.235 | tokens per gpu per second (tgs): 1935.063 | TFLOPs: 15.57 |
g0345: [2024-08-02 21:56:29,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=0, lr=[5.241132373333334e-05, 5.241132373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3000 loss: 3.5588 iter time (s): 4.167 samples/sec: 30.714
g0364:  iteration     3000/10000000 | consumed samples:       384000 | consumed tokens:    786432000 | elapsed time per iteration (ms): 4200.1 | learning rate: 5.241E-05 | global batch size:   128 | lm loss: 3.517255E+00 | loss scale: 65536.0 | grad norm: 1.420 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.476 | tokens per gpu per second (tgs): 1950.443 | TFLOPs: 15.70 |
g0364: ------------------------------------------------------------------------------------------------
g0364:  validation loss at iteration 3000 | lm loss value: 3.535136E+00 | lm loss PPL: 3.429968E+01 | 
g0364: ------------------------------------------------------------------------------------------------
g0345: saving checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: [2024-08-02 22:02:47,072] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
g0364: [2024-08-02 22:02:47,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0364: [2024-08-02 22:02:47,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0364: [2024-08-02 22:02:47,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0345: [2024-08-02 22:02:47,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0345: [2024-08-02 22:02:47,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0345: [2024-08-02 22:02:47,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0352: [2024-08-02 22:02:47,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0352: [2024-08-02 22:02:47,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0352: [2024-08-02 22:02:47,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0347: [2024-08-02 22:02:47,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0347: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0347: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0346: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0346: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0346: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0362: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0362: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0362: [2024-08-02 22:02:47,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0363: [2024-08-02 22:02:47,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0363: [2024-08-02 22:02:47,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0363: [2024-08-02 22:02:47,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0358: [2024-08-02 22:02:47,083] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0358: [2024-08-02 22:02:47,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0358: [2024-08-02 22:02:47,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0364: [2024-08-02 22:02:47,102] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt...
g0346: [2024-08-02 22:02:47,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt...
g0362: [2024-08-02 22:02:47,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt...
g0347: [2024-08-02 22:02:47,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt...
g0358: [2024-08-02 22:02:47,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt...
g0352: [2024-08-02 22:02:47,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt...
g0363: [2024-08-02 22:02:47,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt...
g0345: [2024-08-02 22:02:47,127] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt...
g0364: [2024-08-02 22:02:47,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt.
g0364: [2024-08-02 22:02:47,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt...
g0364: [2024-08-02 22:02:47,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt.
g0347: [2024-08-02 22:02:47,222] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt.
g0363: [2024-08-02 22:02:47,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt.
g0358: [2024-08-02 22:02:47,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt.
g0352: [2024-08-02 22:02:47,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt.
g0364: [2024-08-02 22:02:47,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt...
g0347: [2024-08-02 22:02:47,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt...
g0346: [2024-08-02 22:02:47,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt.
g0363: [2024-08-02 22:02:47,268] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt...
g0358: [2024-08-02 22:02:47,272] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt...
g0346: [2024-08-02 22:02:47,295] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt...
g0352: [2024-08-02 22:02:47,295] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt...
g0345: [2024-08-02 22:02:47,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt.
g0345: [2024-08-02 22:02:47,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt...
g0363: [2024-08-02 22:02:47,378] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt.
g0358: [2024-08-02 22:02:47,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt.
g0363: [2024-08-02 22:02:47,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt...
g0364: [2024-08-02 22:02:47,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt.
g0364: [2024-08-02 22:02:47,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt...
g0346: [2024-08-02 22:02:47,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt.
g0352: [2024-08-02 22:02:47,433] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt.
g0358: [2024-08-02 22:02:47,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt...
g0346: [2024-08-02 22:02:47,450] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt...
g0352: [2024-08-02 22:02:47,467] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt...
g0363: [2024-08-02 22:02:47,512] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt.
g0363: [2024-08-02 22:02:47,514] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt...
g0358: [2024-08-02 22:02:47,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt.
g0358: [2024-08-02 22:02:47,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt...
g0346: [2024-08-02 22:02:47,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt.
g0346: [2024-08-02 22:02:47,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt...
g0362: [2024-08-02 22:02:47,588] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt.
g0362: [2024-08-02 22:02:47,623] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt...
g0352: [2024-08-02 22:02:47,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt.
g0352: [2024-08-02 22:02:47,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt...
g0362: [2024-08-02 22:02:47,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt.
g0362: [2024-08-02 22:02:47,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt...
g0362: [2024-08-02 22:02:47,889] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt.
g0362: [2024-08-02 22:02:47,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt...
g0345: [2024-08-02 22:02:48,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt.
g0345: [2024-08-02 22:02:48,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt...
g0347: [2024-08-02 22:02:48,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt.
g0347: [2024-08-02 22:02:48,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt...
g0345: [2024-08-02 22:02:48,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt.
g0345: [2024-08-02 22:02:48,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt...
g0347: [2024-08-02 22:02:48,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt.
g0347: [2024-08-02 22:02:48,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt...
g0345: [2024-08-02 22:02:48,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt.
g0345: [2024-08-02 22:02:48,580] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt
g0345: [2024-08-02 22:02:48,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt...
g0364: [2024-08-02 22:02:49,369] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt.
g0364: [2024-08-02 22:02:49,370] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0363: [2024-08-02 22:02:49,796] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt.
g0363: [2024-08-02 22:02:49,797] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0352: [2024-08-02 22:02:50,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt.
g0352: [2024-08-02 22:02:50,393] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0347: [2024-08-02 22:02:50,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt.
g0347: [2024-08-02 22:02:50,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0346: [2024-08-02 22:02:50,664] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt.
g0346: [2024-08-02 22:02:50,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0362: [2024-08-02 22:02:51,311] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt.
g0362: [2024-08-02 22:02:51,312] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0345: [2024-08-02 22:02:51,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt.
g0345: [2024-08-02 22:02:51,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0358: [2024-08-02 22:02:52,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt.
g0358: [2024-08-02 22:02:52,227] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0345:   successfully saved checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: Checkpoint Save GB: 22.521, GB/Sec: 4.36, Latency(second): 5.168
g0364: (min, max) time across ranks (ms):
g0364:     save-checkpoint ................................: (5167.83, 5168.09)
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0364: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0364: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0345: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0347: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0362: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0364: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0364: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0346: [2024-08-02 22:02:56,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0345: [2024-08-02 22:02:56,581] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0364: [2024-08-02 22:02:56,581] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0345: [2024-08-02 22:03:33,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=3010, skipped=0, lr=[5.25860864e-05, 5.25860864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3010 loss: 3.5868 iter time (s): 4.085 samples/sec: 31.336
g0364:  iteration     3010/10000000 | consumed samples:       385280 | consumed tokens:    789053440 | elapsed time per iteration (ms): 42397.1 | learning rate: 5.259E-05 | global batch size:   128 | lm loss: 3.582771E+00 | loss scale: 131072.0 | grad norm: 1.602 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.019 | tokens per gpu per second (tgs): 193.221 | TFLOPs: 1.55 |
g0345: [2024-08-02 22:04:13,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=3020, skipped=0, lr=[5.276084906666667e-05, 5.276084906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3020 loss: 3.4970 iter time (s): 3.978 samples/sec: 32.176
g0364:  iteration     3020/10000000 | consumed samples:       386560 | consumed tokens:    791674880 | elapsed time per iteration (ms): 4011.3 | learning rate: 5.276E-05 | global batch size:   128 | lm loss: 3.521034E+00 | loss scale: 131072.0 | grad norm: 1.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.910 | tokens per gpu per second (tgs): 2042.250 | TFLOPs: 16.43 |
g0345: [2024-08-02 22:04:54,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=3030, skipped=0, lr=[5.2935611733333335e-05, 5.2935611733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3030 loss: 3.5380 iter time (s): 4.088 samples/sec: 31.313
g0364:  iteration     3030/10000000 | consumed samples:       387840 | consumed tokens:    794296320 | elapsed time per iteration (ms): 4120.5 | learning rate: 5.294E-05 | global batch size:   128 | lm loss: 3.588419E+00 | loss scale: 131072.0 | grad norm: 1.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.065 | tokens per gpu per second (tgs): 1988.130 | TFLOPs: 16.00 |
g0345: [2024-08-02 22:05:34,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=3040, skipped=0, lr=[5.31103744e-05, 5.31103744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3040 loss: 3.4697 iter time (s): 3.961 samples/sec: 32.316
g0364:  iteration     3040/10000000 | consumed samples:       389120 | consumed tokens:    796917760 | elapsed time per iteration (ms): 3994.2 | learning rate: 5.311E-05 | global batch size:   128 | lm loss: 3.502162E+00 | loss scale: 131072.0 | grad norm: 1.501 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.047 | tokens per gpu per second (tgs): 2050.984 | TFLOPs: 16.50 |
g0345: [2024-08-02 22:06:15,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=3050, skipped=0, lr=[5.328513706666667e-05, 5.328513706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3050 loss: 3.6327 iter time (s): 4.089 samples/sec: 31.306
g0364:  iteration     3050/10000000 | consumed samples:       390400 | consumed tokens:    799539200 | elapsed time per iteration (ms): 4121.3 | learning rate: 5.329E-05 | global batch size:   128 | lm loss: 3.535015E+00 | loss scale: 131072.0 | grad norm: 1.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.058 | tokens per gpu per second (tgs): 1987.699 | TFLOPs: 16.00 |
g0345: [2024-08-02 22:06:56,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=3060, skipped=0, lr=[5.345989973333333e-05, 5.345989973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3060 loss: 3.3949 iter time (s): 4.069 samples/sec: 31.458
g0364:  iteration     3060/10000000 | consumed samples:       391680 | consumed tokens:    802160640 | elapsed time per iteration (ms): 4101.3 | learning rate: 5.346E-05 | global batch size:   128 | lm loss: 3.505748E+00 | loss scale: 131072.0 | grad norm: 1.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.209 | tokens per gpu per second (tgs): 1997.400 | TFLOPs: 16.07 |
g0345: [2024-08-02 22:07:39,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=3070, skipped=0, lr=[5.36346624e-05, 5.36346624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3070 loss: 3.6081 iter time (s): 4.261 samples/sec: 30.041
g0364:  iteration     3070/10000000 | consumed samples:       392960 | consumed tokens:    804782080 | elapsed time per iteration (ms): 4293.6 | learning rate: 5.363E-05 | global batch size:   128 | lm loss: 3.557370E+00 | loss scale: 131072.0 | grad norm: 1.357 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.812 | tokens per gpu per second (tgs): 1907.954 | TFLOPs: 15.35 |
g0345: [2024-08-02 22:08:20,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=3080, skipped=0, lr=[5.3809425066666665e-05, 5.3809425066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3080 loss: 3.6514 iter time (s): 4.033 samples/sec: 31.742
g0364:  iteration     3080/10000000 | consumed samples:       394240 | consumed tokens:    807403520 | elapsed time per iteration (ms): 4065.4 | learning rate: 5.381E-05 | global batch size:   128 | lm loss: 3.495771E+00 | loss scale: 131072.0 | grad norm: 1.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.485 | tokens per gpu per second (tgs): 2015.055 | TFLOPs: 16.22 |
g0345: [2024-08-02 22:09:01,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=3090, skipped=0, lr=[5.398418773333334e-05, 5.398418773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3090 loss: 3.3936 iter time (s): 4.026 samples/sec: 31.795
g0364:  iteration     3090/10000000 | consumed samples:       395520 | consumed tokens:    810024960 | elapsed time per iteration (ms): 4058.2 | learning rate: 5.398E-05 | global batch size:   128 | lm loss: 3.491178E+00 | loss scale: 131072.0 | grad norm: 1.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.541 | tokens per gpu per second (tgs): 2018.615 | TFLOPs: 16.24 |
g0345: [2024-08-02 22:09:42,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=3100, skipped=0, lr=[5.4158950400000004e-05, 5.4158950400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3100 loss: 3.4734 iter time (s): 4.148 samples/sec: 30.860
g0364:  iteration     3100/10000000 | consumed samples:       396800 | consumed tokens:    812646400 | elapsed time per iteration (ms): 4180.4 | learning rate: 5.416E-05 | global batch size:   128 | lm loss: 3.525768E+00 | loss scale: 131072.0 | grad norm: 1.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.619 | tokens per gpu per second (tgs): 1959.643 | TFLOPs: 15.77 |
g0345: [2024-08-02 22:10:23,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=3110, skipped=0, lr=[5.433371306666667e-05, 5.433371306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3110 loss: 3.5428 iter time (s): 4.068 samples/sec: 31.466
g0364:  iteration     3110/10000000 | consumed samples:       398080 | consumed tokens:    815267840 | elapsed time per iteration (ms): 4100.4 | learning rate: 5.433E-05 | global batch size:   128 | lm loss: 3.547397E+00 | loss scale: 131072.0 | grad norm: 1.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.217 | tokens per gpu per second (tgs): 1997.864 | TFLOPs: 16.08 |
g0345: [2024-08-02 22:11:05,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=3120, skipped=0, lr=[5.4508475733333336e-05, 5.4508475733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3120 loss: 3.5669 iter time (s): 4.117 samples/sec: 31.088
g0364:  iteration     3120/10000000 | consumed samples:       399360 | consumed tokens:    817889280 | elapsed time per iteration (ms): 4149.9 | learning rate: 5.451E-05 | global batch size:   128 | lm loss: 3.492757E+00 | loss scale: 131072.0 | grad norm: 1.402 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.844 | tokens per gpu per second (tgs): 1974.040 | TFLOPs: 15.89 |
g0345: [2024-08-02 22:11:47,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=3130, skipped=0, lr=[5.46832384e-05, 5.46832384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3130 loss: 3.4144 iter time (s): 4.148 samples/sec: 30.862
g0364:  iteration     3130/10000000 | consumed samples:       400640 | consumed tokens:    820510720 | elapsed time per iteration (ms): 4180.1 | learning rate: 5.468E-05 | global batch size:   128 | lm loss: 3.484568E+00 | loss scale: 131072.0 | grad norm: 1.507 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.621 | tokens per gpu per second (tgs): 1959.751 | TFLOPs: 15.77 |
g0345: [2024-08-02 22:12:28,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=3140, skipped=0, lr=[5.485800106666667e-05, 5.485800106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3140 loss: 3.5921 iter time (s): 4.109 samples/sec: 31.151
g0364:  iteration     3140/10000000 | consumed samples:       401920 | consumed tokens:    823132160 | elapsed time per iteration (ms): 4141.9 | learning rate: 5.486E-05 | global batch size:   128 | lm loss: 3.506689E+00 | loss scale: 131072.0 | grad norm: 1.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.904 | tokens per gpu per second (tgs): 1977.859 | TFLOPs: 15.92 |
g0345: [2024-08-02 22:13:10,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=3150, skipped=0, lr=[5.5032763733333334e-05, 5.5032763733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3150 loss: 3.5932 iter time (s): 4.144 samples/sec: 30.892
g0364:  iteration     3150/10000000 | consumed samples:       403200 | consumed tokens:    825753600 | elapsed time per iteration (ms): 4176.2 | learning rate: 5.503E-05 | global batch size:   128 | lm loss: 3.515565E+00 | loss scale: 131072.0 | grad norm: 1.685 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.650 | tokens per gpu per second (tgs): 1961.572 | TFLOPs: 15.79 |
g0345: [2024-08-02 22:13:52,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=3160, skipped=0, lr=[5.52075264e-05, 5.52075264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3160 loss: 3.4996 iter time (s): 4.180 samples/sec: 30.620
g0364:  iteration     3160/10000000 | consumed samples:       404480 | consumed tokens:    828375040 | elapsed time per iteration (ms): 4212.8 | learning rate: 5.521E-05 | global batch size:   128 | lm loss: 3.512037E+00 | loss scale: 131072.0 | grad norm: 1.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.383 | tokens per gpu per second (tgs): 1944.528 | TFLOPs: 15.65 |
g0345: [2024-08-02 22:14:34,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=3170, skipped=0, lr=[5.5382289066666667e-05, 5.5382289066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3170 loss: 3.4050 iter time (s): 4.198 samples/sec: 30.487
g0364:  iteration     3170/10000000 | consumed samples:       405760 | consumed tokens:    830996480 | elapsed time per iteration (ms): 4231.1 | learning rate: 5.538E-05 | global batch size:   128 | lm loss: 3.493720E+00 | loss scale: 131072.0 | grad norm: 1.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.252 | tokens per gpu per second (tgs): 1936.155 | TFLOPs: 15.58 |
g0345: [2024-08-02 22:15:16,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=3180, skipped=0, lr=[5.555705173333334e-05, 5.555705173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3180 loss: 3.4872 iter time (s): 4.106 samples/sec: 31.176
g0364:  iteration     3180/10000000 | consumed samples:       407040 | consumed tokens:    833617920 | elapsed time per iteration (ms): 4139.1 | learning rate: 5.556E-05 | global batch size:   128 | lm loss: 3.492458E+00 | loss scale: 131072.0 | grad norm: 1.334 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.924 | tokens per gpu per second (tgs): 1979.168 | TFLOPs: 15.93 |
g0345: [2024-08-02 22:15:58,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=3190, skipped=0, lr=[5.5731814400000005e-05, 5.5731814400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3190 loss: 3.5383 iter time (s): 4.192 samples/sec: 30.534
g0364:  iteration     3190/10000000 | consumed samples:       408320 | consumed tokens:    836239360 | elapsed time per iteration (ms): 4224.5 | learning rate: 5.573E-05 | global batch size:   128 | lm loss: 3.485790E+00 | loss scale: 131072.0 | grad norm: 1.451 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.299 | tokens per gpu per second (tgs): 1939.156 | TFLOPs: 15.60 |
g0345: [2024-08-02 22:16:40,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=3200, skipped=0, lr=[5.590657706666667e-05, 5.590657706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3200 loss: 3.5275 iter time (s): 4.157 samples/sec: 30.794
g0364:  iteration     3200/10000000 | consumed samples:       409600 | consumed tokens:    838860800 | elapsed time per iteration (ms): 4188.8 | learning rate: 5.591E-05 | global batch size:   128 | lm loss: 3.500531E+00 | loss scale: 131072.0 | grad norm: 1.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.557 | tokens per gpu per second (tgs): 1955.677 | TFLOPs: 15.74 |
g0345: [2024-08-02 22:17:22,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=3210, skipped=0, lr=[5.608133973333334e-05, 5.608133973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3210 loss: 3.4562 iter time (s): 4.172 samples/sec: 30.678
g0364:  iteration     3210/10000000 | consumed samples:       410880 | consumed tokens:    841482240 | elapsed time per iteration (ms): 4204.7 | learning rate: 5.608E-05 | global batch size:   128 | lm loss: 3.484305E+00 | loss scale: 131072.0 | grad norm: 1.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.442 | tokens per gpu per second (tgs): 1948.314 | TFLOPs: 15.68 |
g0345: [2024-08-02 22:18:03,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=3220, skipped=0, lr=[5.6256102400000004e-05, 5.6256102400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3220 loss: 3.4792 iter time (s): 4.128 samples/sec: 31.008
g0364:  iteration     3220/10000000 | consumed samples:       412160 | consumed tokens:    844103680 | elapsed time per iteration (ms): 4160.4 | learning rate: 5.626E-05 | global batch size:   128 | lm loss: 3.448027E+00 | loss scale: 131072.0 | grad norm: 1.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.766 | tokens per gpu per second (tgs): 1969.027 | TFLOPs: 15.85 |
g0345: [2024-08-02 22:18:45,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=3230, skipped=0, lr=[5.643086506666667e-05, 5.643086506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3230 loss: 3.6439 iter time (s): 4.166 samples/sec: 30.722
g0364:  iteration     3230/10000000 | consumed samples:       413440 | consumed tokens:    846725120 | elapsed time per iteration (ms): 4198.9 | learning rate: 5.643E-05 | global batch size:   128 | lm loss: 3.494119E+00 | loss scale: 131072.0 | grad norm: 1.505 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1951.003 | TFLOPs: 15.70 |
g0345: [2024-08-02 22:19:27,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=3240, skipped=0, lr=[5.6605627733333336e-05, 5.6605627733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3240 loss: 3.4149 iter time (s): 4.125 samples/sec: 31.030
g0364:  iteration     3240/10000000 | consumed samples:       414720 | consumed tokens:    849346560 | elapsed time per iteration (ms): 4157.8 | learning rate: 5.661E-05 | global batch size:   128 | lm loss: 3.454343E+00 | loss scale: 131072.0 | grad norm: 1.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.786 | tokens per gpu per second (tgs): 1970.282 | TFLOPs: 15.86 |
g0345: [2024-08-02 22:20:10,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=3250, skipped=0, lr=[5.67803904e-05, 5.67803904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3250 loss: 3.4497 iter time (s): 4.225 samples/sec: 30.295
g0364:  iteration     3250/10000000 | consumed samples:       416000 | consumed tokens:    851968000 | elapsed time per iteration (ms): 4257.5 | learning rate: 5.678E-05 | global batch size:   128 | lm loss: 3.494047E+00 | loss scale: 131072.0 | grad norm: 1.294 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.065 | tokens per gpu per second (tgs): 1924.156 | TFLOPs: 15.48 |
g0345: [2024-08-02 22:20:51,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=3260, skipped=0, lr=[5.695515306666667e-05, 5.695515306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3260 loss: 3.4133 iter time (s): 4.096 samples/sec: 31.250
g0364:  iteration     3260/10000000 | consumed samples:       417280 | consumed tokens:    854589440 | elapsed time per iteration (ms): 4128.3 | learning rate: 5.696E-05 | global batch size:   128 | lm loss: 3.492705E+00 | loss scale: 131072.0 | grad norm: 1.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.005 | tokens per gpu per second (tgs): 1984.331 | TFLOPs: 15.97 |
g0345: [2024-08-02 22:21:33,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=3270, skipped=0, lr=[5.712991573333334e-05, 5.712991573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3270 loss: 3.3423 iter time (s): 4.167 samples/sec: 30.721
g0364:  iteration     3270/10000000 | consumed samples:       418560 | consumed tokens:    857210880 | elapsed time per iteration (ms): 4198.9 | learning rate: 5.713E-05 | global batch size:   128 | lm loss: 3.504458E+00 | loss scale: 131072.0 | grad norm: 1.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1950.988 | TFLOPs: 15.70 |
g0345: [2024-08-02 22:22:15,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=3280, skipped=0, lr=[5.730467840000001e-05, 5.730467840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3280 loss: 3.2651 iter time (s): 4.205 samples/sec: 30.437
g0364:  iteration     3280/10000000 | consumed samples:       419840 | consumed tokens:    859832320 | elapsed time per iteration (ms): 4237.9 | learning rate: 5.730E-05 | global batch size:   128 | lm loss: 3.484105E+00 | loss scale: 131072.0 | grad norm: 1.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.204 | tokens per gpu per second (tgs): 1933.031 | TFLOPs: 15.56 |
g0345: [2024-08-02 22:22:57,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=3290, skipped=0, lr=[5.747944106666667e-05, 5.747944106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3290 loss: 3.4491 iter time (s): 4.114 samples/sec: 31.112
g0364:  iteration     3290/10000000 | consumed samples:       421120 | consumed tokens:    862453760 | elapsed time per iteration (ms): 4146.6 | learning rate: 5.748E-05 | global batch size:   128 | lm loss: 3.474775E+00 | loss scale: 131072.0 | grad norm: 1.284 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.869 | tokens per gpu per second (tgs): 1975.586 | TFLOPs: 15.90 |
g0345: [2024-08-02 22:23:39,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=3300, skipped=0, lr=[5.765420373333334e-05, 5.765420373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3300 loss: 3.4045 iter time (s): 4.160 samples/sec: 30.770
g0364:  iteration     3300/10000000 | consumed samples:       422400 | consumed tokens:    865075200 | elapsed time per iteration (ms): 4192.5 | learning rate: 5.765E-05 | global batch size:   128 | lm loss: 3.447620E+00 | loss scale: 131072.0 | grad norm: 1.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.531 | tokens per gpu per second (tgs): 1953.953 | TFLOPs: 15.72 |
g0345: [2024-08-02 22:24:21,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=3310, skipped=0, lr=[5.7828966400000005e-05, 5.7828966400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3310 loss: 3.2664 iter time (s): 4.191 samples/sec: 30.542
g0364:  iteration     3310/10000000 | consumed samples:       423680 | consumed tokens:    867696640 | elapsed time per iteration (ms): 4223.4 | learning rate: 5.783E-05 | global batch size:   128 | lm loss: 3.455829E+00 | loss scale: 131072.0 | grad norm: 1.354 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.307 | tokens per gpu per second (tgs): 1939.675 | TFLOPs: 15.61 |
g0345: [2024-08-02 22:25:03,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=3320, skipped=0, lr=[5.800372906666667e-05, 5.800372906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3320 loss: 3.4593 iter time (s): 4.199 samples/sec: 30.481
g0364:  iteration     3320/10000000 | consumed samples:       424960 | consumed tokens:    870318080 | elapsed time per iteration (ms): 4231.7 | learning rate: 5.800E-05 | global batch size:   128 | lm loss: 3.484787E+00 | loss scale: 131072.0 | grad norm: 1.452 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.248 | tokens per gpu per second (tgs): 1935.866 | TFLOPs: 15.58 |
g0345: [2024-08-02 22:25:45,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=3330, skipped=0, lr=[5.817849173333334e-05, 5.817849173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3330 loss: 3.3638 iter time (s): 4.169 samples/sec: 30.704
g0364:  iteration     3330/10000000 | consumed samples:       426240 | consumed tokens:    872939520 | elapsed time per iteration (ms): 4202.4 | learning rate: 5.818E-05 | global batch size:   128 | lm loss: 3.392343E+00 | loss scale: 131072.0 | grad norm: 1.416 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.459 | tokens per gpu per second (tgs): 1949.380 | TFLOPs: 15.69 |
g0345: [2024-08-02 22:26:27,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=3340, skipped=0, lr=[5.83532544e-05, 5.83532544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3340 loss: 3.4501 iter time (s): 4.158 samples/sec: 30.785
g0364:  iteration     3340/10000000 | consumed samples:       427520 | consumed tokens:    875560960 | elapsed time per iteration (ms): 4191.0 | learning rate: 5.835E-05 | global batch size:   128 | lm loss: 3.427972E+00 | loss scale: 131072.0 | grad norm: 1.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.542 | tokens per gpu per second (tgs): 1954.674 | TFLOPs: 15.73 |
g0345: [2024-08-02 22:27:09,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=3350, skipped=0, lr=[5.852801706666667e-05, 5.852801706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3350 loss: 3.4980 iter time (s): 4.143 samples/sec: 30.896
g0364:  iteration     3350/10000000 | consumed samples:       428800 | consumed tokens:    878182400 | elapsed time per iteration (ms): 4175.6 | learning rate: 5.853E-05 | global batch size:   128 | lm loss: 3.413244E+00 | loss scale: 131072.0 | grad norm: 1.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.655 | tokens per gpu per second (tgs): 1961.897 | TFLOPs: 15.79 |
g0345: [2024-08-02 22:27:51,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=3360, skipped=0, lr=[5.870277973333334e-05, 5.870277973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3360 loss: 3.3730 iter time (s): 4.196 samples/sec: 30.507
g0364:  iteration     3360/10000000 | consumed samples:       430080 | consumed tokens:    880803840 | elapsed time per iteration (ms): 4228.3 | learning rate: 5.870E-05 | global batch size:   128 | lm loss: 3.403698E+00 | loss scale: 131072.0 | grad norm: 1.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.272 | tokens per gpu per second (tgs): 1937.432 | TFLOPs: 15.59 |
g0345: [2024-08-02 22:28:33,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=3370, skipped=0, lr=[5.887754240000001e-05, 5.887754240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3370 loss: 3.4629 iter time (s): 4.189 samples/sec: 30.555
g0364:  iteration     3370/10000000 | consumed samples:       431360 | consumed tokens:    883425280 | elapsed time per iteration (ms): 4221.8 | learning rate: 5.888E-05 | global batch size:   128 | lm loss: 3.488645E+00 | loss scale: 131072.0 | grad norm: 1.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.319 | tokens per gpu per second (tgs): 1940.407 | TFLOPs: 15.61 |
g0345: [2024-08-02 22:29:16,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=3380, skipped=0, lr=[5.9052305066666674e-05, 5.9052305066666674e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3380 loss: 3.3025 iter time (s): 4.191 samples/sec: 30.543
g0364:  iteration     3380/10000000 | consumed samples:       432640 | consumed tokens:    886046720 | elapsed time per iteration (ms): 4224.0 | learning rate: 5.905E-05 | global batch size:   128 | lm loss: 3.442720E+00 | loss scale: 131072.0 | grad norm: 1.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.303 | tokens per gpu per second (tgs): 1939.402 | TFLOPs: 15.61 |
g0345: [2024-08-02 22:29:57,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=3390, skipped=0, lr=[5.922706773333334e-05, 5.922706773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3390 loss: 3.4217 iter time (s): 4.153 samples/sec: 30.825
g0364:  iteration     3390/10000000 | consumed samples:       433920 | consumed tokens:    888668160 | elapsed time per iteration (ms): 4185.6 | learning rate: 5.923E-05 | global batch size:   128 | lm loss: 3.478316E+00 | loss scale: 131072.0 | grad norm: 1.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.581 | tokens per gpu per second (tgs): 1957.189 | TFLOPs: 15.75 |
g0345: [2024-08-02 22:30:39,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=3400, skipped=0, lr=[5.9401830400000006e-05, 5.9401830400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3400 loss: 3.3887 iter time (s): 4.086 samples/sec: 31.328
g0364:  iteration     3400/10000000 | consumed samples:       435200 | consumed tokens:    891289600 | elapsed time per iteration (ms): 4118.2 | learning rate: 5.940E-05 | global batch size:   128 | lm loss: 3.406161E+00 | loss scale: 131072.0 | grad norm: 1.355 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.082 | tokens per gpu per second (tgs): 1989.237 | TFLOPs: 16.01 |
g0345: [2024-08-02 22:31:20,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=3410, skipped=0, lr=[5.957659306666667e-05, 5.957659306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3410 loss: 3.4188 iter time (s): 4.149 samples/sec: 30.852
g0364:  iteration     3410/10000000 | consumed samples:       436480 | consumed tokens:    893911040 | elapsed time per iteration (ms): 4181.2 | learning rate: 5.958E-05 | global batch size:   128 | lm loss: 3.413268E+00 | loss scale: 131072.0 | grad norm: 1.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.613 | tokens per gpu per second (tgs): 1959.262 | TFLOPs: 15.77 |
g0345: [2024-08-02 22:32:03,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=0, lr=[5.975135573333334e-05, 5.975135573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3420 loss: 3.5563 iter time (s): 4.212 samples/sec: 30.392
g0364:  iteration     3420/10000000 | consumed samples:       437760 | consumed tokens:    896532480 | elapsed time per iteration (ms): 4243.9 | learning rate: 5.975E-05 | global batch size:   128 | lm loss: 3.419010E+00 | loss scale: 131072.0 | grad norm: 1.280 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.161 | tokens per gpu per second (tgs): 1930.318 | TFLOPs: 15.53 |
g0345: [2024-08-02 22:32:45,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=0, lr=[5.9926118400000004e-05, 5.9926118400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3430 loss: 3.5273 iter time (s): 4.183 samples/sec: 30.597
g0364:  iteration     3430/10000000 | consumed samples:       439040 | consumed tokens:    899153920 | elapsed time per iteration (ms): 4216.0 | learning rate: 5.993E-05 | global batch size:   128 | lm loss: 3.511401E+00 | loss scale: 131072.0 | grad norm: 1.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.360 | tokens per gpu per second (tgs): 1943.072 | TFLOPs: 15.64 |
g0345: [2024-08-02 22:33:27,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=0, lr=[6.010088106666667e-05, 6.010088106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3440 loss: 3.4654 iter time (s): 4.162 samples/sec: 30.757
g0364:  iteration     3440/10000000 | consumed samples:       440320 | consumed tokens:    901775360 | elapsed time per iteration (ms): 4194.5 | learning rate: 6.010E-05 | global batch size:   128 | lm loss: 3.394936E+00 | loss scale: 131072.0 | grad norm: 1.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.516 | tokens per gpu per second (tgs): 1953.026 | TFLOPs: 15.72 |
g0345: [2024-08-02 22:34:10,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=0, lr=[6.027564373333334e-05, 6.027564373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3450 loss: 3.5129 iter time (s): 4.224 samples/sec: 30.303
g0364:  iteration     3450/10000000 | consumed samples:       441600 | consumed tokens:    904396800 | elapsed time per iteration (ms): 4270.8 | learning rate: 6.028E-05 | global batch size:   128 | lm loss: 3.381532E+00 | loss scale: 131072.0 | grad norm: 1.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.971 | tokens per gpu per second (tgs): 1918.131 | TFLOPs: 15.44 |
g0345: [2024-08-02 22:34:52,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=0, lr=[6.045040640000001e-05, 6.045040640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3460 loss: 3.4605 iter time (s): 4.193 samples/sec: 30.528
g0364:  iteration     3460/10000000 | consumed samples:       442880 | consumed tokens:    907018240 | elapsed time per iteration (ms): 4225.6 | learning rate: 6.045E-05 | global batch size:   128 | lm loss: 3.421690E+00 | loss scale: 131072.0 | grad norm: 1.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.291 | tokens per gpu per second (tgs): 1938.639 | TFLOPs: 15.60 |
g0345: [2024-08-02 22:35:33,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=0, lr=[6.0625169066666676e-05, 6.0625169066666676e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3470 loss: 3.3347 iter time (s): 4.067 samples/sec: 31.473
g0364:  iteration     3470/10000000 | consumed samples:       444160 | consumed tokens:    909639680 | elapsed time per iteration (ms): 4104.2 | learning rate: 6.063E-05 | global batch size:   128 | lm loss: 3.398736E+00 | loss scale: 131072.0 | grad norm: 1.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.188 | tokens per gpu per second (tgs): 1996.022 | TFLOPs: 16.06 |
g0345: [2024-08-02 22:36:13,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=0, lr=[6.079993173333334e-05, 6.079993173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3480 loss: 3.3665 iter time (s): 3.970 samples/sec: 32.242
g0364:  iteration     3480/10000000 | consumed samples:       445440 | consumed tokens:    912261120 | elapsed time per iteration (ms): 4002.4 | learning rate: 6.080E-05 | global batch size:   128 | lm loss: 3.392014E+00 | loss scale: 131072.0 | grad norm: 1.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.981 | tokens per gpu per second (tgs): 2046.780 | TFLOPs: 16.47 |
g0345: [2024-08-02 22:36:54,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=0, lr=[6.097469440000001e-05, 6.097469440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3490 loss: 3.3796 iter time (s): 4.108 samples/sec: 31.162
g0364:  iteration     3490/10000000 | consumed samples:       446720 | consumed tokens:    914882560 | elapsed time per iteration (ms): 4139.9 | learning rate: 6.097E-05 | global batch size:   128 | lm loss: 3.454268E+00 | loss scale: 131072.0 | grad norm: 1.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.919 | tokens per gpu per second (tgs): 1978.815 | TFLOPs: 15.92 |
g0345: [2024-08-02 22:37:35,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=0, lr=[6.114945706666668e-05, 6.114945706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3500 loss: 3.4198 iter time (s): 4.007 samples/sec: 31.944
g0364:  iteration     3500/10000000 | consumed samples:       448000 | consumed tokens:    917504000 | elapsed time per iteration (ms): 4039.3 | learning rate: 6.115E-05 | global batch size:   128 | lm loss: 3.374532E+00 | loss scale: 131072.0 | grad norm: 1.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.689 | tokens per gpu per second (tgs): 2028.068 | TFLOPs: 16.32 |
g0345: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0362: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0362: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0362: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0363: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0362: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0363: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0363: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0362: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0358: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0346: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0362: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0345: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0345: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0363: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0364: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0345: [2024-08-02 22:37:39,560] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 22:37:39,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0362: [2024-08-02 22:37:39,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0363: [2024-08-02 22:37:39,561] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0345: [2024-08-02 22:38:16,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=3510, skipped=0, lr=[6.132421973333333e-05, 6.132421973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3510 loss: 3.2592 iter time (s): 4.063 samples/sec: 31.500
g0364:  iteration     3510/10000000 | consumed samples:       449280 | consumed tokens:    920125440 | elapsed time per iteration (ms): 4096.2 | learning rate: 6.132E-05 | global batch size:   128 | lm loss: 3.358387E+00 | loss scale: 262144.0 | grad norm: 1.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.249 | tokens per gpu per second (tgs): 1999.917 | TFLOPs: 16.09 |
g0345: [2024-08-02 22:38:57,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=3520, skipped=0, lr=[6.14989824e-05, 6.14989824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3520 loss: 3.4930 iter time (s): 4.066 samples/sec: 31.480
g0364:  iteration     3520/10000000 | consumed samples:       450560 | consumed tokens:    922746880 | elapsed time per iteration (ms): 4098.6 | learning rate: 6.150E-05 | global batch size:   128 | lm loss: 3.432281E+00 | loss scale: 262144.0 | grad norm: 1.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.230 | tokens per gpu per second (tgs): 1998.719 | TFLOPs: 16.08 |
g0345: [2024-08-02 22:39:38,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=3530, skipped=0, lr=[6.167374506666667e-05, 6.167374506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3530 loss: 3.3511 iter time (s): 4.090 samples/sec: 31.294
g0364:  iteration     3530/10000000 | consumed samples:       451840 | consumed tokens:    925368320 | elapsed time per iteration (ms): 4124.4 | learning rate: 6.167E-05 | global batch size:   128 | lm loss: 3.375595E+00 | loss scale: 262144.0 | grad norm: 1.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.035 | tokens per gpu per second (tgs): 1986.214 | TFLOPs: 15.98 |
g0345: [2024-08-02 22:40:19,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=3540, skipped=0, lr=[6.184850773333333e-05, 6.184850773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3540 loss: 3.3759 iter time (s): 4.090 samples/sec: 31.294
g0364:  iteration     3540/10000000 | consumed samples:       453120 | consumed tokens:    927989760 | elapsed time per iteration (ms): 4122.4 | learning rate: 6.185E-05 | global batch size:   128 | lm loss: 3.328508E+00 | loss scale: 262144.0 | grad norm: 1.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.050 | tokens per gpu per second (tgs): 1987.180 | TFLOPs: 15.99 |
g0345: [2024-08-02 22:41:00,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=3550, skipped=0, lr=[6.20232704e-05, 6.20232704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3550 loss: 3.4421 iter time (s): 4.068 samples/sec: 31.463
g0364:  iteration     3550/10000000 | consumed samples:       454400 | consumed tokens:    930611200 | elapsed time per iteration (ms): 4100.6 | learning rate: 6.202E-05 | global batch size:   128 | lm loss: 3.331609E+00 | loss scale: 262144.0 | grad norm: 1.357 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.215 | tokens per gpu per second (tgs): 1997.764 | TFLOPs: 16.08 |
g0345: [2024-08-02 22:41:41,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=3560, skipped=0, lr=[6.219803306666666e-05, 6.219803306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3560 loss: 3.5630 iter time (s): 4.031 samples/sec: 31.751
g0364:  iteration     3560/10000000 | consumed samples:       455680 | consumed tokens:    933232640 | elapsed time per iteration (ms): 4063.8 | learning rate: 6.220E-05 | global batch size:   128 | lm loss: 3.429024E+00 | loss scale: 262144.0 | grad norm: 1.117 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.498 | tokens per gpu per second (tgs): 2015.870 | TFLOPs: 16.22 |
g0345: [2024-08-02 22:42:21,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=3570, skipped=0, lr=[6.237279573333333e-05, 6.237279573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3570 loss: 3.5380 iter time (s): 4.016 samples/sec: 31.874
g0364:  iteration     3570/10000000 | consumed samples:       456960 | consumed tokens:    935854080 | elapsed time per iteration (ms): 4048.3 | learning rate: 6.237E-05 | global batch size:   128 | lm loss: 3.366521E+00 | loss scale: 262144.0 | grad norm: 1.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.618 | tokens per gpu per second (tgs): 2023.578 | TFLOPs: 16.28 |
g0345: [2024-08-02 22:43:02,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=3580, skipped=0, lr=[6.25475584e-05, 6.25475584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3580 loss: 3.3881 iter time (s): 4.037 samples/sec: 31.708
g0364:  iteration     3580/10000000 | consumed samples:       458240 | consumed tokens:    938475520 | elapsed time per iteration (ms): 4069.3 | learning rate: 6.255E-05 | global batch size:   128 | lm loss: 3.354356E+00 | loss scale: 262144.0 | grad norm: 1.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.455 | tokens per gpu per second (tgs): 2013.115 | TFLOPs: 16.20 |
g0345: [2024-08-02 22:43:44,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=3590, skipped=0, lr=[6.272232106666666e-05, 6.272232106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3590 loss: 3.3930 iter time (s): 4.157 samples/sec: 30.793
g0364:  iteration     3590/10000000 | consumed samples:       459520 | consumed tokens:    941096960 | elapsed time per iteration (ms): 4189.6 | learning rate: 6.272E-05 | global batch size:   128 | lm loss: 3.376908E+00 | loss scale: 262144.0 | grad norm: 1.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.551 | tokens per gpu per second (tgs): 1955.296 | TFLOPs: 15.73 |
g0345: [2024-08-02 22:44:26,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=3600, skipped=0, lr=[6.289708373333333e-05, 6.289708373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3600 loss: 3.3293 iter time (s): 4.124 samples/sec: 31.037
g0364:  iteration     3600/10000000 | consumed samples:       460800 | consumed tokens:    943718400 | elapsed time per iteration (ms): 4156.8 | learning rate: 6.290E-05 | global batch size:   128 | lm loss: 3.323283E+00 | loss scale: 262144.0 | grad norm: 1.100 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.793 | tokens per gpu per second (tgs): 1970.726 | TFLOPs: 15.86 |
g0345: [2024-08-02 22:45:07,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=3610, skipped=0, lr=[6.30718464e-05, 6.30718464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3610 loss: 3.4191 iter time (s): 4.080 samples/sec: 31.371
g0364:  iteration     3610/10000000 | consumed samples:       462080 | consumed tokens:    946339840 | elapsed time per iteration (ms): 4113.1 | learning rate: 6.307E-05 | global batch size:   128 | lm loss: 3.359272E+00 | loss scale: 262144.0 | grad norm: 1.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.120 | tokens per gpu per second (tgs): 1991.692 | TFLOPs: 16.03 |
g0345: [2024-08-02 22:45:48,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=3620, skipped=0, lr=[6.324660906666667e-05, 6.324660906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3620 loss: 3.2257 iter time (s): 4.104 samples/sec: 31.186
g0364:  iteration     3620/10000000 | consumed samples:       463360 | consumed tokens:    948961280 | elapsed time per iteration (ms): 4137.1 | learning rate: 6.325E-05 | global batch size:   128 | lm loss: 3.320333E+00 | loss scale: 262144.0 | grad norm: 1.133 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.940 | tokens per gpu per second (tgs): 1980.149 | TFLOPs: 15.93 |
g0345: [2024-08-02 22:46:29,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=3630, skipped=0, lr=[6.342137173333334e-05, 6.342137173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3630 loss: 3.7109 iter time (s): 4.096 samples/sec: 31.252
g0364:  iteration     3630/10000000 | consumed samples:       464640 | consumed tokens:    951582720 | elapsed time per iteration (ms): 4128.4 | learning rate: 6.342E-05 | global batch size:   128 | lm loss: 3.412852E+00 | loss scale: 262144.0 | grad norm: 1.053 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.004 | tokens per gpu per second (tgs): 1984.286 | TFLOPs: 15.97 |
g0345: [2024-08-02 22:47:10,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=3640, skipped=0, lr=[6.35961344e-05, 6.35961344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3640 loss: 3.5327 iter time (s): 4.061 samples/sec: 31.521
g0364:  iteration     3640/10000000 | consumed samples:       465920 | consumed tokens:    954204160 | elapsed time per iteration (ms): 4093.5 | learning rate: 6.360E-05 | global batch size:   128 | lm loss: 3.283243E+00 | loss scale: 262144.0 | grad norm: 1.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.269 | tokens per gpu per second (tgs): 2001.204 | TFLOPs: 16.10 |
g0345: [2024-08-02 22:47:52,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=3650, skipped=0, lr=[6.377089706666667e-05, 6.377089706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3650 loss: 3.3326 iter time (s): 4.127 samples/sec: 31.013
g0364:  iteration     3650/10000000 | consumed samples:       467200 | consumed tokens:    956825600 | elapsed time per iteration (ms): 4159.8 | learning rate: 6.377E-05 | global batch size:   128 | lm loss: 3.347774E+00 | loss scale: 262144.0 | grad norm: 1.469 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.770 | tokens per gpu per second (tgs): 1969.305 | TFLOPs: 15.85 |
g0345: [2024-08-02 22:48:33,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=3660, skipped=0, lr=[6.394565973333334e-05, 6.394565973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3660 loss: 3.1682 iter time (s): 4.077 samples/sec: 31.394
g0364:  iteration     3660/10000000 | consumed samples:       468480 | consumed tokens:    959447040 | elapsed time per iteration (ms): 4109.9 | learning rate: 6.395E-05 | global batch size:   128 | lm loss: 3.298220E+00 | loss scale: 262144.0 | grad norm: 1.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.144 | tokens per gpu per second (tgs): 1993.229 | TFLOPs: 16.04 |
g0345: [2024-08-02 22:49:14,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=3670, skipped=0, lr=[6.41204224e-05, 6.41204224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3670 loss: 3.3963 iter time (s): 4.082 samples/sec: 31.358
g0364:  iteration     3670/10000000 | consumed samples:       469760 | consumed tokens:    962068480 | elapsed time per iteration (ms): 4114.5 | learning rate: 6.412E-05 | global batch size:   128 | lm loss: 3.405795E+00 | loss scale: 262144.0 | grad norm: 1.062 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.110 | tokens per gpu per second (tgs): 1991.024 | TFLOPs: 16.02 |
g0345: [2024-08-02 22:49:56,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=3680, skipped=0, lr=[6.429518506666667e-05, 6.429518506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3680 loss: 3.3537 iter time (s): 4.145 samples/sec: 30.884
g0364:  iteration     3680/10000000 | consumed samples:       471040 | consumed tokens:    964689920 | elapsed time per iteration (ms): 4176.9 | learning rate: 6.430E-05 | global batch size:   128 | lm loss: 3.398907E+00 | loss scale: 262144.0 | grad norm: 1.144 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.644 | tokens per gpu per second (tgs): 1961.245 | TFLOPs: 15.78 |
g0345: [2024-08-02 22:50:39,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=3690, skipped=0, lr=[6.446994773333334e-05, 6.446994773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3690 loss: 3.2725 iter time (s): 4.239 samples/sec: 30.198
g0364:  iteration     3690/10000000 | consumed samples:       472320 | consumed tokens:    967311360 | elapsed time per iteration (ms): 4271.1 | learning rate: 6.447E-05 | global batch size:   128 | lm loss: 3.308197E+00 | loss scale: 262144.0 | grad norm: 1.317 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.969 | tokens per gpu per second (tgs): 1917.987 | TFLOPs: 15.43 |
g0345: [2024-08-02 22:51:20,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=3700, skipped=0, lr=[6.46447104e-05, 6.46447104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3700 loss: 3.3012 iter time (s): 4.095 samples/sec: 31.255
g0364:  iteration     3700/10000000 | consumed samples:       473600 | consumed tokens:    969932800 | elapsed time per iteration (ms): 4128.0 | learning rate: 6.464E-05 | global batch size:   128 | lm loss: 3.343968E+00 | loss scale: 262144.0 | grad norm: 1.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.008 | tokens per gpu per second (tgs): 1984.516 | TFLOPs: 15.97 |
g0345: [2024-08-02 22:52:02,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=3710, skipped=0, lr=[6.481947306666667e-05, 6.481947306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3710 loss: 3.5332 iter time (s): 4.140 samples/sec: 30.921
g0364:  iteration     3710/10000000 | consumed samples:       474880 | consumed tokens:    972554240 | elapsed time per iteration (ms): 4173.1 | learning rate: 6.482E-05 | global batch size:   128 | lm loss: 3.408791E+00 | loss scale: 262144.0 | grad norm: 1.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.672 | tokens per gpu per second (tgs): 1963.032 | TFLOPs: 15.80 |
g0345: [2024-08-02 22:52:43,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=3720, skipped=0, lr=[6.499423573333333e-05, 6.499423573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3720 loss: 3.1816 iter time (s): 4.117 samples/sec: 31.093
g0364:  iteration     3720/10000000 | consumed samples:       476160 | consumed tokens:    975175680 | elapsed time per iteration (ms): 4149.7 | learning rate: 6.499E-05 | global batch size:   128 | lm loss: 3.324199E+00 | loss scale: 262144.0 | grad norm: 1.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.846 | tokens per gpu per second (tgs): 1974.122 | TFLOPs: 15.89 |
g0345: [2024-08-02 22:53:25,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=3730, skipped=0, lr=[6.51689984e-05, 6.51689984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3730 loss: 3.2883 iter time (s): 4.144 samples/sec: 30.890
g0364:  iteration     3730/10000000 | consumed samples:       477440 | consumed tokens:    977797120 | elapsed time per iteration (ms): 4176.8 | learning rate: 6.517E-05 | global batch size:   128 | lm loss: 3.274871E+00 | loss scale: 262144.0 | grad norm: 1.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.645 | tokens per gpu per second (tgs): 1961.295 | TFLOPs: 15.78 |
g0345: [2024-08-02 22:54:06,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=3740, skipped=0, lr=[6.534376106666667e-05, 6.534376106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3740 loss: 3.3586 iter time (s): 4.073 samples/sec: 31.424
g0364:  iteration     3740/10000000 | consumed samples:       478720 | consumed tokens:    980418560 | elapsed time per iteration (ms): 4107.0 | learning rate: 6.534E-05 | global batch size:   128 | lm loss: 3.261945E+00 | loss scale: 262144.0 | grad norm: 1.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.167 | tokens per gpu per second (tgs): 1994.666 | TFLOPs: 16.05 |
g0345: [2024-08-02 22:54:48,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=3750, skipped=0, lr=[6.551852373333333e-05, 6.551852373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3750 loss: 3.1377 iter time (s): 4.198 samples/sec: 30.494
g0364:  iteration     3750/10000000 | consumed samples:       480000 | consumed tokens:    983040000 | elapsed time per iteration (ms): 4230.8 | learning rate: 6.552E-05 | global batch size:   128 | lm loss: 3.339975E+00 | loss scale: 262144.0 | grad norm: 1.321 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.254 | tokens per gpu per second (tgs): 1936.264 | TFLOPs: 15.58 |
g0345: [2024-08-02 22:55:30,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=3760, skipped=0, lr=[6.56932864e-05, 6.56932864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3760 loss: 3.3764 iter time (s): 4.114 samples/sec: 31.116
g0364:  iteration     3760/10000000 | consumed samples:       481280 | consumed tokens:    985661440 | elapsed time per iteration (ms): 4145.7 | learning rate: 6.569E-05 | global batch size:   128 | lm loss: 3.303892E+00 | loss scale: 262144.0 | grad norm: 1.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.875 | tokens per gpu per second (tgs): 1976.003 | TFLOPs: 15.90 |
g0345: [2024-08-02 22:56:11,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=3770, skipped=0, lr=[6.586804906666666e-05, 6.586804906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3770 loss: 3.3544 iter time (s): 4.137 samples/sec: 30.937
g0364:  iteration     3770/10000000 | consumed samples:       482560 | consumed tokens:    988282880 | elapsed time per iteration (ms): 4169.8 | learning rate: 6.587E-05 | global batch size:   128 | lm loss: 3.362925E+00 | loss scale: 262144.0 | grad norm: 1.122 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.697 | tokens per gpu per second (tgs): 1964.580 | TFLOPs: 15.81 |
g0345: [2024-08-02 22:56:54,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=3780, skipped=0, lr=[6.604281173333333e-05, 6.604281173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3780 loss: 3.3712 iter time (s): 4.199 samples/sec: 30.487
g0364:  iteration     3780/10000000 | consumed samples:       483840 | consumed tokens:    990904320 | elapsed time per iteration (ms): 4231.4 | learning rate: 6.604E-05 | global batch size:   128 | lm loss: 3.308704E+00 | loss scale: 262144.0 | grad norm: 1.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.250 | tokens per gpu per second (tgs): 1935.999 | TFLOPs: 15.58 |
g0345: [2024-08-02 22:57:36,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=3790, skipped=0, lr=[6.62175744e-05, 6.62175744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3790 loss: 3.2634 iter time (s): 4.171 samples/sec: 30.684
g0364:  iteration     3790/10000000 | consumed samples:       485120 | consumed tokens:    993525760 | elapsed time per iteration (ms): 4204.7 | learning rate: 6.622E-05 | global batch size:   128 | lm loss: 3.242545E+00 | loss scale: 262144.0 | grad norm: 1.092 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.442 | tokens per gpu per second (tgs): 1948.315 | TFLOPs: 15.68 |
g0345: [2024-08-02 22:58:18,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=3800, skipped=0, lr=[6.639233706666668e-05, 6.639233706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3800 loss: 3.4106 iter time (s): 4.200 samples/sec: 30.476
g0364:  iteration     3800/10000000 | consumed samples:       486400 | consumed tokens:    996147200 | elapsed time per iteration (ms): 4232.9 | learning rate: 6.639E-05 | global batch size:   128 | lm loss: 3.371426E+00 | loss scale: 262144.0 | grad norm: 1.109 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.239 | tokens per gpu per second (tgs): 1935.300 | TFLOPs: 15.57 |
g0345: [2024-08-02 22:58:59,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=3810, skipped=0, lr=[6.656709973333334e-05, 6.656709973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3810 loss: 3.2899 iter time (s): 4.098 samples/sec: 31.236
g0364:  iteration     3810/10000000 | consumed samples:       487680 | consumed tokens:    998768640 | elapsed time per iteration (ms): 4130.4 | learning rate: 6.657E-05 | global batch size:   128 | lm loss: 3.271869E+00 | loss scale: 262144.0 | grad norm: 1.070 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.990 | tokens per gpu per second (tgs): 1983.363 | TFLOPs: 15.96 |
g0345: [2024-08-02 22:59:41,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=3820, skipped=0, lr=[6.674186240000001e-05, 6.674186240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3820 loss: 3.3693 iter time (s): 4.162 samples/sec: 30.752
g0364:  iteration     3820/10000000 | consumed samples:       488960 | consumed tokens:   1001390080 | elapsed time per iteration (ms): 4195.3 | learning rate: 6.674E-05 | global batch size:   128 | lm loss: 3.347580E+00 | loss scale: 262144.0 | grad norm: 1.088 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.510 | tokens per gpu per second (tgs): 1952.662 | TFLOPs: 15.71 |
g0345: [2024-08-02 23:00:23,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=3830, skipped=0, lr=[6.691662506666667e-05, 6.691662506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3830 loss: 3.2679 iter time (s): 4.115 samples/sec: 31.102
g0364:  iteration     3830/10000000 | consumed samples:       490240 | consumed tokens:   1004011520 | elapsed time per iteration (ms): 4148.0 | learning rate: 6.692E-05 | global batch size:   128 | lm loss: 3.288164E+00 | loss scale: 262144.0 | grad norm: 1.087 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.858 | tokens per gpu per second (tgs): 1974.918 | TFLOPs: 15.89 |
g0345: [2024-08-02 23:01:04,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=3840, skipped=0, lr=[6.709138773333334e-05, 6.709138773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3840 loss: 3.3173 iter time (s): 4.136 samples/sec: 30.951
g0364:  iteration     3840/10000000 | consumed samples:       491520 | consumed tokens:   1006632960 | elapsed time per iteration (ms): 4168.1 | learning rate: 6.709E-05 | global batch size:   128 | lm loss: 3.228967E+00 | loss scale: 262144.0 | grad norm: 1.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.710 | tokens per gpu per second (tgs): 1965.425 | TFLOPs: 15.82 |
g0345: [2024-08-02 23:01:46,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=3850, skipped=0, lr=[6.72661504e-05, 6.72661504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3850 loss: 3.3625 iter time (s): 4.126 samples/sec: 31.021
g0364:  iteration     3850/10000000 | consumed samples:       492800 | consumed tokens:   1009254400 | elapsed time per iteration (ms): 4159.1 | learning rate: 6.727E-05 | global batch size:   128 | lm loss: 3.201244E+00 | loss scale: 262144.0 | grad norm: 1.043 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.776 | tokens per gpu per second (tgs): 1969.637 | TFLOPs: 15.85 |
g0345: [2024-08-02 23:02:28,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=3860, skipped=0, lr=[6.744091306666667e-05, 6.744091306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3860 loss: 3.1697 iter time (s): 4.144 samples/sec: 30.889
g0364:  iteration     3860/10000000 | consumed samples:       494080 | consumed tokens:   1011875840 | elapsed time per iteration (ms): 4176.6 | learning rate: 6.744E-05 | global batch size:   128 | lm loss: 3.311334E+00 | loss scale: 262144.0 | grad norm: 1.398 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.647 | tokens per gpu per second (tgs): 1961.416 | TFLOPs: 15.78 |
g0345: [2024-08-02 23:03:10,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=3870, skipped=0, lr=[6.761567573333334e-05, 6.761567573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3870 loss: 3.3731 iter time (s): 4.225 samples/sec: 30.298
g0364:  iteration     3870/10000000 | consumed samples:       495360 | consumed tokens:   1014497280 | elapsed time per iteration (ms): 4257.4 | learning rate: 6.762E-05 | global batch size:   128 | lm loss: 3.284151E+00 | loss scale: 262144.0 | grad norm: 1.294 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.065 | tokens per gpu per second (tgs): 1924.184 | TFLOPs: 15.48 |
g0345: [2024-08-02 23:03:51,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=3880, skipped=0, lr=[6.77904384e-05, 6.77904384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3880 loss: 3.3102 iter time (s): 4.074 samples/sec: 31.417
g0364:  iteration     3880/10000000 | consumed samples:       496640 | consumed tokens:   1017118720 | elapsed time per iteration (ms): 4107.0 | learning rate: 6.779E-05 | global batch size:   128 | lm loss: 3.341590E+00 | loss scale: 262144.0 | grad norm: 1.146 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.167 | tokens per gpu per second (tgs): 1994.658 | TFLOPs: 16.05 |
g0345: [2024-08-02 23:04:34,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=3890, skipped=0, lr=[6.796520106666667e-05, 6.796520106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3890 loss: 3.2723 iter time (s): 4.208 samples/sec: 30.420
g0364:  iteration     3890/10000000 | consumed samples:       497920 | consumed tokens:   1019740160 | elapsed time per iteration (ms): 4240.3 | learning rate: 6.797E-05 | global batch size:   128 | lm loss: 3.295653E+00 | loss scale: 262144.0 | grad norm: 1.049 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.187 | tokens per gpu per second (tgs): 1931.944 | TFLOPs: 15.55 |
g0345: [2024-08-02 23:05:16,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=3900, skipped=0, lr=[6.813996373333334e-05, 6.813996373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3900 loss: 3.3043 iter time (s): 4.152 samples/sec: 30.828
g0364:  iteration     3900/10000000 | consumed samples:       499200 | consumed tokens:   1022361600 | elapsed time per iteration (ms): 4185.4 | learning rate: 6.814E-05 | global batch size:   128 | lm loss: 3.328524E+00 | loss scale: 262144.0 | grad norm: 1.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.583 | tokens per gpu per second (tgs): 1957.293 | TFLOPs: 15.75 |
g0345: [2024-08-02 23:05:58,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=3910, skipped=0, lr=[6.83147264e-05, 6.83147264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3910 loss: 3.3709 iter time (s): 4.185 samples/sec: 30.582
g0364:  iteration     3910/10000000 | consumed samples:       500480 | consumed tokens:   1024983040 | elapsed time per iteration (ms): 4218.6 | learning rate: 6.831E-05 | global batch size:   128 | lm loss: 3.280884E+00 | loss scale: 262144.0 | grad norm: 1.145 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.342 | tokens per gpu per second (tgs): 1941.861 | TFLOPs: 15.63 |
g0345: [2024-08-02 23:06:40,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=3920, skipped=0, lr=[6.848948906666667e-05, 6.848948906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3920 loss: 3.0889 iter time (s): 4.183 samples/sec: 30.598
g0364:  iteration     3920/10000000 | consumed samples:       501760 | consumed tokens:   1027604480 | elapsed time per iteration (ms): 4215.8 | learning rate: 6.849E-05 | global batch size:   128 | lm loss: 3.275442E+00 | loss scale: 262144.0 | grad norm: 1.110 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.362 | tokens per gpu per second (tgs): 1943.155 | TFLOPs: 15.64 |
g0345: [2024-08-02 23:07:22,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=3930, skipped=0, lr=[6.866425173333333e-05, 6.866425173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3930 loss: 3.2122 iter time (s): 4.151 samples/sec: 30.835
g0364:  iteration     3930/10000000 | consumed samples:       503040 | consumed tokens:   1030225920 | elapsed time per iteration (ms): 4184.3 | learning rate: 6.866E-05 | global batch size:   128 | lm loss: 3.274924E+00 | loss scale: 262144.0 | grad norm: 1.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.590 | tokens per gpu per second (tgs): 1957.785 | TFLOPs: 15.75 |
g0345: [2024-08-02 23:08:03,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=3940, skipped=0, lr=[6.88390144e-05, 6.88390144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3940 loss: 3.1722 iter time (s): 4.120 samples/sec: 31.070
g0364:  iteration     3940/10000000 | consumed samples:       504320 | consumed tokens:   1032847360 | elapsed time per iteration (ms): 4152.5 | learning rate: 6.884E-05 | global batch size:   128 | lm loss: 3.221719E+00 | loss scale: 262144.0 | grad norm: 1.115 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.825 | tokens per gpu per second (tgs): 1972.810 | TFLOPs: 15.88 |
g0345: [2024-08-02 23:08:43,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=3950, skipped=0, lr=[6.901377706666667e-05, 6.901377706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3950 loss: 3.2264 iter time (s): 3.940 samples/sec: 32.487
g0364:  iteration     3950/10000000 | consumed samples:       505600 | consumed tokens:   1035468800 | elapsed time per iteration (ms): 3973.5 | learning rate: 6.901E-05 | global batch size:   128 | lm loss: 3.173634E+00 | loss scale: 262144.0 | grad norm: 1.065 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.213 | tokens per gpu per second (tgs): 2061.644 | TFLOPs: 16.59 |
g0345: [2024-08-02 23:09:24,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=3960, skipped=0, lr=[6.918853973333333e-05, 6.918853973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3960 loss: 3.3737 iter time (s): 4.086 samples/sec: 31.330
g0364:  iteration     3960/10000000 | consumed samples:       506880 | consumed tokens:   1038090240 | elapsed time per iteration (ms): 4118.3 | learning rate: 6.919E-05 | global batch size:   128 | lm loss: 3.202669E+00 | loss scale: 262144.0 | grad norm: 0.949 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.081 | tokens per gpu per second (tgs): 1989.173 | TFLOPs: 16.01 |
g0345: [2024-08-02 23:10:05,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=3970, skipped=0, lr=[6.93633024e-05, 6.93633024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3970 loss: 3.1853 iter time (s): 4.030 samples/sec: 31.759
g0364:  iteration     3970/10000000 | consumed samples:       508160 | consumed tokens:   1040711680 | elapsed time per iteration (ms): 4062.9 | learning rate: 6.936E-05 | global batch size:   128 | lm loss: 3.193981E+00 | loss scale: 262144.0 | grad norm: 1.089 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.505 | tokens per gpu per second (tgs): 2016.292 | TFLOPs: 16.23 |
g0345: [2024-08-02 23:10:45,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=3980, skipped=0, lr=[6.953806506666668e-05, 6.953806506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3980 loss: 3.1334 iter time (s): 3.976 samples/sec: 32.195
g0364:  iteration     3980/10000000 | consumed samples:       509440 | consumed tokens:   1043333120 | elapsed time per iteration (ms): 4008.5 | learning rate: 6.954E-05 | global batch size:   128 | lm loss: 3.223756E+00 | loss scale: 262144.0 | grad norm: 1.048 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.932 | tokens per gpu per second (tgs): 2043.676 | TFLOPs: 16.45 |
g0345: [2024-08-02 23:11:25,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=3990, skipped=0, lr=[6.971282773333334e-05, 6.971282773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 3990 loss: 3.1590 iter time (s): 3.950 samples/sec: 32.408
g0364:  iteration     3990/10000000 | consumed samples:       510720 | consumed tokens:   1045954560 | elapsed time per iteration (ms): 3981.9 | learning rate: 6.971E-05 | global batch size:   128 | lm loss: 3.217489E+00 | loss scale: 262144.0 | grad norm: 1.099 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.145 | tokens per gpu per second (tgs): 2057.298 | TFLOPs: 16.56 |
g0345: [2024-08-02 23:12:08,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=0, lr=[6.988759040000001e-05, 6.988759040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4000 loss: 3.1803 iter time (s): 4.246 samples/sec: 30.143
g0364:  iteration     4000/10000000 | consumed samples:       512000 | consumed tokens:   1048576000 | elapsed time per iteration (ms): 4279.1 | learning rate: 6.989E-05 | global batch size:   128 | lm loss: 3.211689E+00 | loss scale: 262144.0 | grad norm: 1.058 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.913 | tokens per gpu per second (tgs): 1914.403 | TFLOPs: 15.41 |
g0364: ------------------------------------------------------------------------------------------------
g0364:  validation loss at iteration 4000 | lm loss value: 3.231133E+00 | lm loss PPL: 2.530830E+01 | 
g0364: ------------------------------------------------------------------------------------------------
g0345: saving checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: [2024-08-02 23:18:22,556] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
g0364: [2024-08-02 23:18:22,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0364: [2024-08-02 23:18:22,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0364: [2024-08-02 23:18:22,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0345: [2024-08-02 23:18:22,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0345: [2024-08-02 23:18:22,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0347: [2024-08-02 23:18:22,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0347: [2024-08-02 23:18:22,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0347: [2024-08-02 23:18:22,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0363: [2024-08-02 23:18:22,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0363: [2024-08-02 23:18:22,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0358: [2024-08-02 23:18:22,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0358: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0358: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0345: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0363: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0352: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0352: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0352: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0362: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0362: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0362: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0346: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0346: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0346: [2024-08-02 23:18:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0364: [2024-08-02 23:18:22,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt...
g0358: [2024-08-02 23:18:22,598] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt...
g0362: [2024-08-02 23:18:22,599] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt...
g0347: [2024-08-02 23:18:22,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt...
g0346: [2024-08-02 23:18:22,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt...
g0363: [2024-08-02 23:18:22,603] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt...
g0352: [2024-08-02 23:18:22,605] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt...
g0345: [2024-08-02 23:18:22,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt...
g0358: [2024-08-02 23:18:22,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt.
g0346: [2024-08-02 23:18:22,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt.
g0358: [2024-08-02 23:18:22,770] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt...
g0364: [2024-08-02 23:18:22,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt.
g0364: [2024-08-02 23:18:22,779] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt...
g0364: [2024-08-02 23:18:22,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt.
g0363: [2024-08-02 23:18:22,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt.
g0347: [2024-08-02 23:18:22,793] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt.
g0346: [2024-08-02 23:18:22,796] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt...
g0362: [2024-08-02 23:18:22,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt.
g0364: [2024-08-02 23:18:22,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt...
g0363: [2024-08-02 23:18:22,829] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt...
g0347: [2024-08-02 23:18:22,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt...
g0352: [2024-08-02 23:18:22,852] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt.
g0362: [2024-08-02 23:18:22,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt...
g0352: [2024-08-02 23:18:22,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt...
g0364: [2024-08-02 23:18:22,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt.
g0364: [2024-08-02 23:18:22,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt...
g0358: [2024-08-02 23:18:22,980] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt.
g0352: [2024-08-02 23:18:22,991] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt.
g0358: [2024-08-02 23:18:23,006] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt...
g0347: [2024-08-02 23:18:23,021] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt.
g0352: [2024-08-02 23:18:23,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt...
g0347: [2024-08-02 23:18:23,054] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt...
g0362: [2024-08-02 23:18:23,059] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt.
g0362: [2024-08-02 23:18:23,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt...
g0358: [2024-08-02 23:18:23,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt.
g0358: [2024-08-02 23:18:23,107] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt...
g0352: [2024-08-02 23:18:23,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt.
g0352: [2024-08-02 23:18:23,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt...
g0347: [2024-08-02 23:18:23,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt.
g0347: [2024-08-02 23:18:23,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt...
g0362: [2024-08-02 23:18:23,217] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt.
g0362: [2024-08-02 23:18:23,219] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt...
g0345: [2024-08-02 23:18:23,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt.
g0345: [2024-08-02 23:18:23,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt...
g0363: [2024-08-02 23:18:23,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt.
g0363: [2024-08-02 23:18:23,416] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt...
g0346: [2024-08-02 23:18:23,491] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt.
g0345: [2024-08-02 23:18:23,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt.
g0363: [2024-08-02 23:18:23,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt.
g0363: [2024-08-02 23:18:23,516] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt...
g0346: [2024-08-02 23:18:23,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt...
g0345: [2024-08-02 23:18:23,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt...
g0346: [2024-08-02 23:18:23,616] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt.
g0346: [2024-08-02 23:18:23,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt...
g0345: [2024-08-02 23:18:23,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt.
g0345: [2024-08-02 23:18:23,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt...
g0345: [2024-08-02 23:18:23,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt.
g0345: [2024-08-02 23:18:23,974] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt
g0345: [2024-08-02 23:18:23,974] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt...
g0364: [2024-08-02 23:18:24,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt.
g0364: [2024-08-02 23:18:24,820] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0358: [2024-08-02 23:18:25,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt.
g0358: [2024-08-02 23:18:25,433] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0347: [2024-08-02 23:18:25,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt.
g0347: [2024-08-02 23:18:25,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0352: [2024-08-02 23:18:25,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt.
g0352: [2024-08-02 23:18:25,666] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0362: [2024-08-02 23:18:25,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt.
g0362: [2024-08-02 23:18:25,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0363: [2024-08-02 23:18:25,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt.
g0363: [2024-08-02 23:18:25,867] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0346: [2024-08-02 23:18:25,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt.
g0346: [2024-08-02 23:18:25,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0345: [2024-08-02 23:18:27,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt.
g0345: [2024-08-02 23:18:27,505] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0345:   successfully saved checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: Checkpoint Save GB: 22.521, GB/Sec: 3.9, Latency(second): 5.778
g0364: (min, max) time across ranks (ms):
g0364:     save-checkpoint ................................: (5777.57, 5777.80)
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0364: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0363: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0352: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0347: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0362: [2024-08-02 23:18:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0345: [2024-08-02 23:19:13,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=4010, skipped=0, lr=[7.006235306666668e-05, 7.006235306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4010 loss: 3.3358 iter time (s): 4.459 samples/sec: 28.703
g0364:  iteration     4010/10000000 | consumed samples:       513280 | consumed tokens:   1051197440 | elapsed time per iteration (ms): 42500.8 | learning rate: 7.006E-05 | global batch size:   128 | lm loss: 3.222944E+00 | loss scale: 524288.0 | grad norm: 1.024 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.012 | tokens per gpu per second (tgs): 192.749 | TFLOPs: 1.55 |
g0345: [2024-08-02 23:20:01,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=4020, skipped=0, lr=[7.023711573333334e-05, 7.023711573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4020 loss: 3.1719 iter time (s): 4.784 samples/sec: 26.758
g0364:  iteration     4020/10000000 | consumed samples:       514560 | consumed tokens:   1053818880 | elapsed time per iteration (ms): 4816.2 | learning rate: 7.024E-05 | global batch size:   128 | lm loss: 3.219912E+00 | loss scale: 524288.0 | grad norm: 1.329 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.577 | tokens per gpu per second (tgs): 1700.925 | TFLOPs: 13.69 |
g0345: [2024-08-02 23:20:42,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=4030, skipped=0, lr=[7.041187840000001e-05, 7.041187840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4030 loss: 3.4748 iter time (s): 4.052 samples/sec: 31.589
g0364:  iteration     4030/10000000 | consumed samples:       515840 | consumed tokens:   1056440320 | elapsed time per iteration (ms): 4084.9 | learning rate: 7.041E-05 | global batch size:   128 | lm loss: 3.302928E+00 | loss scale: 524288.0 | grad norm: 1.051 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.335 | tokens per gpu per second (tgs): 2005.436 | TFLOPs: 16.14 |
g0345: [2024-08-02 23:21:23,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=4040, skipped=0, lr=[7.058664106666667e-05, 7.058664106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4040 loss: 3.2243 iter time (s): 4.109 samples/sec: 31.152
g0364:  iteration     4040/10000000 | consumed samples:       517120 | consumed tokens:   1059061760 | elapsed time per iteration (ms): 4142.7 | learning rate: 7.059E-05 | global batch size:   128 | lm loss: 3.221472E+00 | loss scale: 524288.0 | grad norm: 0.964 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.898 | tokens per gpu per second (tgs): 1977.450 | TFLOPs: 15.91 |
g0345: [2024-08-02 23:22:05,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=4050, skipped=0, lr=[7.076140373333334e-05, 7.076140373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4050 loss: 3.2091 iter time (s): 4.112 samples/sec: 31.126
g0364:  iteration     4050/10000000 | consumed samples:       518400 | consumed tokens:   1061683200 | elapsed time per iteration (ms): 4144.9 | learning rate: 7.076E-05 | global batch size:   128 | lm loss: 3.224623E+00 | loss scale: 524288.0 | grad norm: 1.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.881 | tokens per gpu per second (tgs): 1976.384 | TFLOPs: 15.90 |
g0345: [2024-08-02 23:22:45,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=4060, skipped=0, lr=[7.093616640000001e-05, 7.093616640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4060 loss: 3.1705 iter time (s): 4.023 samples/sec: 31.818
g0364:  iteration     4060/10000000 | consumed samples:       519680 | consumed tokens:   1064304640 | elapsed time per iteration (ms): 4055.5 | learning rate: 7.094E-05 | global batch size:   128 | lm loss: 3.225492E+00 | loss scale: 524288.0 | grad norm: 1.045 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.562 | tokens per gpu per second (tgs): 2019.987 | TFLOPs: 16.26 |
g0345: [2024-08-02 23:23:26,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=4070, skipped=0, lr=[7.111092906666667e-05, 7.111092906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4070 loss: 3.1839 iter time (s): 4.087 samples/sec: 31.321
g0364:  iteration     4070/10000000 | consumed samples:       520960 | consumed tokens:   1066926080 | elapsed time per iteration (ms): 4119.4 | learning rate: 7.111E-05 | global batch size:   128 | lm loss: 3.248416E+00 | loss scale: 524288.0 | grad norm: 1.105 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.073 | tokens per gpu per second (tgs): 1988.644 | TFLOPs: 16.00 |
g0345: [2024-08-02 23:24:08,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=4080, skipped=0, lr=[7.128569173333334e-05, 7.128569173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4080 loss: 3.2337 iter time (s): 4.141 samples/sec: 30.910
g0364:  iteration     4080/10000000 | consumed samples:       522240 | consumed tokens:   1069547520 | elapsed time per iteration (ms): 4173.5 | learning rate: 7.129E-05 | global batch size:   128 | lm loss: 3.263595E+00 | loss scale: 524288.0 | grad norm: 1.037 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.670 | tokens per gpu per second (tgs): 1962.862 | TFLOPs: 15.80 |
g0345: [2024-08-02 23:24:50,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=4090, skipped=0, lr=[7.14604544e-05, 7.14604544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4090 loss: 3.4611 iter time (s): 4.165 samples/sec: 30.734
g0364:  iteration     4090/10000000 | consumed samples:       523520 | consumed tokens:   1072168960 | elapsed time per iteration (ms): 4197.8 | learning rate: 7.146E-05 | global batch size:   128 | lm loss: 3.295974E+00 | loss scale: 524288.0 | grad norm: 1.008 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.492 | tokens per gpu per second (tgs): 1951.498 | TFLOPs: 15.70 |
g0345: [2024-08-02 23:25:31,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=4100, skipped=0, lr=[7.163521706666667e-05, 7.163521706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4100 loss: 3.1879 iter time (s): 4.082 samples/sec: 31.359
g0364:  iteration     4100/10000000 | consumed samples:       524800 | consumed tokens:   1074790400 | elapsed time per iteration (ms): 4115.7 | learning rate: 7.164E-05 | global batch size:   128 | lm loss: 3.204573E+00 | loss scale: 524288.0 | grad norm: 1.134 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.100 | tokens per gpu per second (tgs): 1990.425 | TFLOPs: 16.02 |
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 4100
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 4100
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 4100
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0345: Grad overflow on iteration 4100
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 4100
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0364: Grad overflow on iteration 4100
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 4100
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 4100
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0363: Grad overflow on iteration 4100
g0346: Grad overflow on iteration 4100
g0362: Grad overflow on iteration 4100
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 4100
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0358: Grad overflow on iteration 4100
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 4100
g0364: Grad overflow on iteration 4100
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0362: Grad overflow on iteration 4100
g0347: Grad overflow on iteration 4100
g0362: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: Grad overflow on iteration 4100
g0362: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0362: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 4100
g0358: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 4100
g0347: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0358: Grad overflow on iteration 4100
g0362: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 4100
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 4100
g0345: Grad overflow on iteration 4100
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0364: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0358: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 4100
g0358: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 4100
g0358: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 4100
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0362: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0346: Grad overflow on iteration 4100
g0363: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: Grad overflow on iteration 4100
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 4100
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0346: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 4100
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0352: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 4100
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0345: [2024-08-02 23:25:36,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0345: [2024-08-02 23:25:36,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288, reducing to 262144.0
g0358: [2024-08-02 23:25:36,054] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0362: [2024-08-02 23:25:36,054] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0345: [2024-08-02 23:26:13,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=4110, skipped=1, lr=[7.180997973333334e-05, 7.180997973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4110 loss: 3.1729 iter time (s): 4.161 samples/sec: 30.763
g0364:  iteration     4110/10000000 | consumed samples:       526080 | consumed tokens:   1077411840 | elapsed time per iteration (ms): 4193.8 | learning rate: 7.181E-05 | global batch size:   128 | lm loss: 3.236515E+00 | loss scale: 262144.0 | grad norm: 1.052 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.521 | tokens per gpu per second (tgs): 1953.375 | TFLOPs: 15.72 |
g0345: [2024-08-02 23:26:54,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=4120, skipped=1, lr=[7.19847424e-05, 7.19847424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4120 loss: 3.3713 iter time (s): 4.095 samples/sec: 31.256
g0364:  iteration     4120/10000000 | consumed samples:       527360 | consumed tokens:   1080033280 | elapsed time per iteration (ms): 4127.9 | learning rate: 7.198E-05 | global batch size:   128 | lm loss: 3.205910E+00 | loss scale: 262144.0 | grad norm: 1.029 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.008 | tokens per gpu per second (tgs): 1984.543 | TFLOPs: 15.97 |
g0345: [2024-08-02 23:27:35,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=4130, skipped=1, lr=[7.215950506666667e-05, 7.215950506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4130 loss: 3.1401 iter time (s): 4.067 samples/sec: 31.475
g0364:  iteration     4130/10000000 | consumed samples:       528640 | consumed tokens:   1082654720 | elapsed time per iteration (ms): 4099.5 | learning rate: 7.216E-05 | global batch size:   128 | lm loss: 3.183500E+00 | loss scale: 262144.0 | grad norm: 0.960 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.223 | tokens per gpu per second (tgs): 1998.274 | TFLOPs: 16.08 |
g0345: [2024-08-02 23:28:18,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=4140, skipped=1, lr=[7.233426773333334e-05, 7.233426773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4140 loss: 3.2490 iter time (s): 4.193 samples/sec: 30.525
g0364:  iteration     4140/10000000 | consumed samples:       529920 | consumed tokens:   1085276160 | elapsed time per iteration (ms): 4226.2 | learning rate: 7.233E-05 | global batch size:   128 | lm loss: 3.171940E+00 | loss scale: 262144.0 | grad norm: 0.980 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.287 | tokens per gpu per second (tgs): 1938.382 | TFLOPs: 15.60 |
g0345: [2024-08-02 23:28:59,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=4150, skipped=1, lr=[7.25090304e-05, 7.25090304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4150 loss: 3.0598 iter time (s): 4.104 samples/sec: 31.189
g0364:  iteration     4150/10000000 | consumed samples:       531200 | consumed tokens:   1087897600 | elapsed time per iteration (ms): 4137.2 | learning rate: 7.251E-05 | global batch size:   128 | lm loss: 3.220594E+00 | loss scale: 262144.0 | grad norm: 0.985 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.939 | tokens per gpu per second (tgs): 1980.068 | TFLOPs: 15.93 |
g0345: [2024-08-02 23:29:42,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=4160, skipped=1, lr=[7.268379306666668e-05, 7.268379306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4160 loss: 3.1688 iter time (s): 4.251 samples/sec: 30.109
g0364:  iteration     4160/10000000 | consumed samples:       532480 | consumed tokens:   1090519040 | elapsed time per iteration (ms): 4284.0 | learning rate: 7.268E-05 | global batch size:   128 | lm loss: 3.217602E+00 | loss scale: 262144.0 | grad norm: 1.010 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.879 | tokens per gpu per second (tgs): 1912.247 | TFLOPs: 15.39 |
g0345: [2024-08-02 23:30:25,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=4170, skipped=1, lr=[7.285855573333333e-05, 7.285855573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4170 loss: 3.2470 iter time (s): 4.255 samples/sec: 30.083
g0364:  iteration     4170/10000000 | consumed samples:       533760 | consumed tokens:   1093140480 | elapsed time per iteration (ms): 4288.7 | learning rate: 7.286E-05 | global batch size:   128 | lm loss: 3.258568E+00 | loss scale: 262144.0 | grad norm: 0.991 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.846 | tokens per gpu per second (tgs): 1910.150 | TFLOPs: 15.37 |
g0345: [2024-08-02 23:31:06,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=4180, skipped=1, lr=[7.30333184e-05, 7.30333184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4180 loss: 3.0791 iter time (s): 4.133 samples/sec: 30.971
g0364:  iteration     4180/10000000 | consumed samples:       535040 | consumed tokens:   1095761920 | elapsed time per iteration (ms): 4165.8 | learning rate: 7.303E-05 | global batch size:   128 | lm loss: 3.187129E+00 | loss scale: 262144.0 | grad norm: 0.949 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.727 | tokens per gpu per second (tgs): 1966.498 | TFLOPs: 15.82 |
g0345: [2024-08-02 23:31:49,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=4190, skipped=1, lr=[7.320808106666667e-05, 7.320808106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4190 loss: 3.1304 iter time (s): 4.178 samples/sec: 30.637
g0364:  iteration     4190/10000000 | consumed samples:       536320 | consumed tokens:   1098383360 | elapsed time per iteration (ms): 4210.8 | learning rate: 7.321E-05 | global batch size:   128 | lm loss: 3.152594E+00 | loss scale: 262144.0 | grad norm: 0.969 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.398 | tokens per gpu per second (tgs): 1945.489 | TFLOPs: 15.66 |
g0345: [2024-08-02 23:32:30,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=4200, skipped=1, lr=[7.338284373333333e-05, 7.338284373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4200 loss: 3.2202 iter time (s): 4.070 samples/sec: 31.452
g0364:  iteration     4200/10000000 | consumed samples:       537600 | consumed tokens:   1101004800 | elapsed time per iteration (ms): 4102.4 | learning rate: 7.338E-05 | global batch size:   128 | lm loss: 3.141377E+00 | loss scale: 262144.0 | grad norm: 1.134 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.201 | tokens per gpu per second (tgs): 1996.880 | TFLOPs: 16.07 |
g0345: [2024-08-02 23:33:11,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=4210, skipped=1, lr=[7.35576064e-05, 7.35576064e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4210 loss: 3.1266 iter time (s): 4.076 samples/sec: 31.405
g0364:  iteration     4210/10000000 | consumed samples:       538880 | consumed tokens:   1103626240 | elapsed time per iteration (ms): 4108.5 | learning rate: 7.356E-05 | global batch size:   128 | lm loss: 3.155426E+00 | loss scale: 262144.0 | grad norm: 0.952 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.155 | tokens per gpu per second (tgs): 1993.924 | TFLOPs: 16.05 |
g0345: [2024-08-02 23:33:52,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=4220, skipped=1, lr=[7.373236906666666e-05, 7.373236906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4220 loss: 3.1377 iter time (s): 4.075 samples/sec: 31.411
g0364:  iteration     4220/10000000 | consumed samples:       540160 | consumed tokens:   1106247680 | elapsed time per iteration (ms): 4107.6 | learning rate: 7.373E-05 | global batch size:   128 | lm loss: 3.183915E+00 | loss scale: 262144.0 | grad norm: 0.955 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.161 | tokens per gpu per second (tgs): 1994.330 | TFLOPs: 16.05 |
g0345: [2024-08-02 23:34:33,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=4230, skipped=1, lr=[7.390713173333333e-05, 7.390713173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4230 loss: 3.1576 iter time (s): 4.118 samples/sec: 31.081
g0364:  iteration     4230/10000000 | consumed samples:       541440 | consumed tokens:   1108869120 | elapsed time per iteration (ms): 4152.0 | learning rate: 7.391E-05 | global batch size:   128 | lm loss: 3.161299E+00 | loss scale: 262144.0 | grad norm: 0.997 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.828 | tokens per gpu per second (tgs): 1973.022 | TFLOPs: 15.88 |
g0345: [2024-08-02 23:35:15,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=4240, skipped=1, lr=[7.40818944e-05, 7.40818944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4240 loss: 3.0759 iter time (s): 4.138 samples/sec: 30.931
g0364:  iteration     4240/10000000 | consumed samples:       542720 | consumed tokens:   1111490560 | elapsed time per iteration (ms): 4171.1 | learning rate: 7.408E-05 | global batch size:   128 | lm loss: 3.211763E+00 | loss scale: 262144.0 | grad norm: 1.029 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.688 | tokens per gpu per second (tgs): 1964.010 | TFLOPs: 15.80 |
g0345: [2024-08-02 23:35:57,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=4250, skipped=1, lr=[7.425665706666666e-05, 7.425665706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4250 loss: 3.1232 iter time (s): 4.176 samples/sec: 30.648
g0364:  iteration     4250/10000000 | consumed samples:       544000 | consumed tokens:   1114112000 | elapsed time per iteration (ms): 4208.9 | learning rate: 7.426E-05 | global batch size:   128 | lm loss: 3.092277E+00 | loss scale: 262144.0 | grad norm: 0.925 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.412 | tokens per gpu per second (tgs): 1946.356 | TFLOPs: 15.66 |
g0345: [2024-08-02 23:36:39,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=4260, skipped=1, lr=[7.443141973333333e-05, 7.443141973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4260 loss: 3.0355 iter time (s): 4.132 samples/sec: 30.979
g0364:  iteration     4260/10000000 | consumed samples:       545280 | consumed tokens:   1116733440 | elapsed time per iteration (ms): 4164.5 | learning rate: 7.443E-05 | global batch size:   128 | lm loss: 3.148370E+00 | loss scale: 262144.0 | grad norm: 0.986 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.736 | tokens per gpu per second (tgs): 1967.106 | TFLOPs: 15.83 |
g0345: [2024-08-02 23:37:20,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=4270, skipped=1, lr=[7.46061824e-05, 7.46061824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4270 loss: 3.1774 iter time (s): 4.077 samples/sec: 31.392
g0364:  iteration     4270/10000000 | consumed samples:       546560 | consumed tokens:   1119354880 | elapsed time per iteration (ms): 4109.9 | learning rate: 7.461E-05 | global batch size:   128 | lm loss: 3.126020E+00 | loss scale: 262144.0 | grad norm: 0.935 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.144 | tokens per gpu per second (tgs): 1993.236 | TFLOPs: 16.04 |
g0345: [2024-08-02 23:38:01,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=4280, skipped=1, lr=[7.478094506666666e-05, 7.478094506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4280 loss: 2.9330 iter time (s): 4.096 samples/sec: 31.253
g0364:  iteration     4280/10000000 | consumed samples:       547840 | consumed tokens:   1121976320 | elapsed time per iteration (ms): 4129.3 | learning rate: 7.478E-05 | global batch size:   128 | lm loss: 3.156539E+00 | loss scale: 262144.0 | grad norm: 1.047 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.998 | tokens per gpu per second (tgs): 1983.854 | TFLOPs: 15.96 |
g0345: [2024-08-02 23:38:43,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=4290, skipped=1, lr=[7.495570773333334e-05, 7.495570773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4290 loss: 3.1103 iter time (s): 4.159 samples/sec: 30.774
g0364:  iteration     4290/10000000 | consumed samples:       549120 | consumed tokens:   1124597760 | elapsed time per iteration (ms): 4194.8 | learning rate: 7.496E-05 | global batch size:   128 | lm loss: 3.109337E+00 | loss scale: 262144.0 | grad norm: 0.948 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.514 | tokens per gpu per second (tgs): 1952.913 | TFLOPs: 15.72 |
g0345: [2024-08-02 23:39:26,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=4300, skipped=1, lr=[7.51304704e-05, 7.51304704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4300 loss: 3.1499 iter time (s): 4.242 samples/sec: 30.171
g0364:  iteration     4300/10000000 | consumed samples:       550400 | consumed tokens:   1127219200 | elapsed time per iteration (ms): 4293.3 | learning rate: 7.513E-05 | global batch size:   128 | lm loss: 3.147036E+00 | loss scale: 262144.0 | grad norm: 1.061 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.814 | tokens per gpu per second (tgs): 1908.073 | TFLOPs: 15.35 |
g0345: [2024-08-02 23:40:08,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=4310, skipped=1, lr=[7.530523306666667e-05, 7.530523306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4310 loss: 3.0409 iter time (s): 4.129 samples/sec: 31.001
g0364:  iteration     4310/10000000 | consumed samples:       551680 | consumed tokens:   1129840640 | elapsed time per iteration (ms): 4161.4 | learning rate: 7.531E-05 | global batch size:   128 | lm loss: 3.115820E+00 | loss scale: 262144.0 | grad norm: 1.149 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.759 | tokens per gpu per second (tgs): 1968.559 | TFLOPs: 15.84 |
g0345: [2024-08-02 23:40:49,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=4320, skipped=1, lr=[7.547999573333334e-05, 7.547999573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4320 loss: 3.1334 iter time (s): 4.096 samples/sec: 31.249
g0364:  iteration     4320/10000000 | consumed samples:       552960 | consumed tokens:   1132462080 | elapsed time per iteration (ms): 4133.3 | learning rate: 7.548E-05 | global batch size:   128 | lm loss: 3.109691E+00 | loss scale: 262144.0 | grad norm: 1.017 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.968 | tokens per gpu per second (tgs): 1981.951 | TFLOPs: 15.95 |
g0345: [2024-08-02 23:41:30,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=4330, skipped=1, lr=[7.56547584e-05, 7.56547584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4330 loss: 2.9952 iter time (s): 4.114 samples/sec: 31.111
g0364:  iteration     4330/10000000 | consumed samples:       554240 | consumed tokens:   1135083520 | elapsed time per iteration (ms): 4147.2 | learning rate: 7.565E-05 | global batch size:   128 | lm loss: 3.127979E+00 | loss scale: 262144.0 | grad norm: 1.062 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.864 | tokens per gpu per second (tgs): 1975.300 | TFLOPs: 15.90 |
g0345: [2024-08-02 23:42:12,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=4340, skipped=1, lr=[7.582952106666667e-05, 7.582952106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4340 loss: 3.1803 iter time (s): 4.149 samples/sec: 30.854
g0364:  iteration     4340/10000000 | consumed samples:       555520 | consumed tokens:   1137704960 | elapsed time per iteration (ms): 4181.1 | learning rate: 7.583E-05 | global batch size:   128 | lm loss: 3.184748E+00 | loss scale: 262144.0 | grad norm: 0.970 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.614 | tokens per gpu per second (tgs): 1959.301 | TFLOPs: 15.77 |
g0345: [2024-08-02 23:42:55,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=4350, skipped=1, lr=[7.600428373333334e-05, 7.600428373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4350 loss: 3.0098 iter time (s): 4.216 samples/sec: 30.358
g0364:  iteration     4350/10000000 | consumed samples:       556800 | consumed tokens:   1140326400 | elapsed time per iteration (ms): 4249.0 | learning rate: 7.600E-05 | global batch size:   128 | lm loss: 3.117568E+00 | loss scale: 262144.0 | grad norm: 1.064 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.125 | tokens per gpu per second (tgs): 1927.988 | TFLOPs: 15.51 |
g0345: [2024-08-02 23:43:36,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=4360, skipped=1, lr=[7.61790464e-05, 7.61790464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4360 loss: 3.0419 iter time (s): 4.071 samples/sec: 31.439
g0364:  iteration     4360/10000000 | consumed samples:       558080 | consumed tokens:   1142947840 | elapsed time per iteration (ms): 4104.4 | learning rate: 7.618E-05 | global batch size:   128 | lm loss: 3.069483E+00 | loss scale: 262144.0 | grad norm: 0.963 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.186 | tokens per gpu per second (tgs): 1995.927 | TFLOPs: 16.06 |
g0345: [2024-08-02 23:44:18,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=4370, skipped=1, lr=[7.635380906666667e-05, 7.635380906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4370 loss: 3.1149 iter time (s): 4.162 samples/sec: 30.754
g0364:  iteration     4370/10000000 | consumed samples:       559360 | consumed tokens:   1145569280 | elapsed time per iteration (ms): 4197.7 | learning rate: 7.635E-05 | global batch size:   128 | lm loss: 3.140159E+00 | loss scale: 262144.0 | grad norm: 0.982 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.493 | tokens per gpu per second (tgs): 1951.553 | TFLOPs: 15.70 |
g0345: [2024-08-02 23:45:00,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=4380, skipped=1, lr=[7.652857173333333e-05, 7.652857173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4380 loss: 2.9991 iter time (s): 4.177 samples/sec: 30.641
g0364:  iteration     4380/10000000 | consumed samples:       560640 | consumed tokens:   1148190720 | elapsed time per iteration (ms): 4210.7 | learning rate: 7.653E-05 | global batch size:   128 | lm loss: 3.107610E+00 | loss scale: 262144.0 | grad norm: 0.983 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.398 | tokens per gpu per second (tgs): 1945.497 | TFLOPs: 15.66 |
g0345: [2024-08-02 23:45:42,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=4390, skipped=1, lr=[7.67033344e-05, 7.67033344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4390 loss: 3.1403 iter time (s): 4.191 samples/sec: 30.543
g0364:  iteration     4390/10000000 | consumed samples:       561920 | consumed tokens:   1150812160 | elapsed time per iteration (ms): 4223.4 | learning rate: 7.670E-05 | global batch size:   128 | lm loss: 3.075644E+00 | loss scale: 262144.0 | grad norm: 0.847 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.308 | tokens per gpu per second (tgs): 1939.691 | TFLOPs: 15.61 |
g0345: [2024-08-02 23:46:25,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=4400, skipped=1, lr=[7.687809706666667e-05, 7.687809706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4400 loss: 3.1163 iter time (s): 4.235 samples/sec: 30.228
g0364:  iteration     4400/10000000 | consumed samples:       563200 | consumed tokens:   1153433600 | elapsed time per iteration (ms): 4267.2 | learning rate: 7.688E-05 | global batch size:   128 | lm loss: 3.074342E+00 | loss scale: 262144.0 | grad norm: 0.887 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.996 | tokens per gpu per second (tgs): 1919.753 | TFLOPs: 15.45 |
g0345: [2024-08-02 23:47:07,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=4410, skipped=1, lr=[7.705285973333333e-05, 7.705285973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4410 loss: 3.0254 iter time (s): 4.220 samples/sec: 30.335
g0364:  iteration     4410/10000000 | consumed samples:       564480 | consumed tokens:   1156055040 | elapsed time per iteration (ms): 4251.9 | learning rate: 7.705E-05 | global batch size:   128 | lm loss: 3.090421E+00 | loss scale: 262144.0 | grad norm: 1.043 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.104 | tokens per gpu per second (tgs): 1926.667 | TFLOPs: 15.50 |
g0345: [2024-08-02 23:47:49,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=4420, skipped=1, lr=[7.72276224e-05, 7.72276224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4420 loss: 3.2236 iter time (s): 4.158 samples/sec: 30.782
g0364:  iteration     4420/10000000 | consumed samples:       565760 | consumed tokens:   1158676480 | elapsed time per iteration (ms): 4191.0 | learning rate: 7.723E-05 | global batch size:   128 | lm loss: 3.133593E+00 | loss scale: 262144.0 | grad norm: 0.931 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.542 | tokens per gpu per second (tgs): 1954.676 | TFLOPs: 15.73 |
g0345: [2024-08-02 23:48:32,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=4430, skipped=1, lr=[7.740238506666667e-05, 7.740238506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4430 loss: 3.1719 iter time (s): 4.206 samples/sec: 30.431
g0364:  iteration     4430/10000000 | consumed samples:       567040 | consumed tokens:   1161297920 | elapsed time per iteration (ms): 4238.8 | learning rate: 7.740E-05 | global batch size:   128 | lm loss: 3.028178E+00 | loss scale: 262144.0 | grad norm: 0.970 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.197 | tokens per gpu per second (tgs): 1932.635 | TFLOPs: 15.55 |
g0345: [2024-08-02 23:49:13,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=4440, skipped=1, lr=[7.757714773333333e-05, 7.757714773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4440 loss: 3.2868 iter time (s): 4.150 samples/sec: 30.846
g0364:  iteration     4440/10000000 | consumed samples:       568320 | consumed tokens:   1163919360 | elapsed time per iteration (ms): 4182.5 | learning rate: 7.758E-05 | global batch size:   128 | lm loss: 3.155163E+00 | loss scale: 262144.0 | grad norm: 1.021 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.604 | tokens per gpu per second (tgs): 1958.625 | TFLOPs: 15.76 |
g0345: [2024-08-02 23:49:56,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=4450, skipped=1, lr=[7.77519104e-05, 7.77519104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4450 loss: 2.9820 iter time (s): 4.182 samples/sec: 30.605
g0364:  iteration     4450/10000000 | consumed samples:       569600 | consumed tokens:   1166540800 | elapsed time per iteration (ms): 4215.3 | learning rate: 7.775E-05 | global batch size:   128 | lm loss: 3.094462E+00 | loss scale: 262144.0 | grad norm: 0.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.366 | tokens per gpu per second (tgs): 1943.407 | TFLOPs: 15.64 |
g0345: [2024-08-02 23:50:38,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=4460, skipped=1, lr=[7.792667306666666e-05, 7.792667306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4460 loss: 2.8857 iter time (s): 4.166 samples/sec: 30.723
g0364:  iteration     4460/10000000 | consumed samples:       570880 | consumed tokens:   1169162240 | elapsed time per iteration (ms): 4198.6 | learning rate: 7.793E-05 | global batch size:   128 | lm loss: 3.128326E+00 | loss scale: 262144.0 | grad norm: 0.928 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.486 | tokens per gpu per second (tgs): 1951.117 | TFLOPs: 15.70 |
g0345: [2024-08-02 23:51:19,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=4470, skipped=1, lr=[7.810143573333334e-05, 7.810143573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4470 loss: 3.0221 iter time (s): 4.154 samples/sec: 30.816
g0364:  iteration     4470/10000000 | consumed samples:       572160 | consumed tokens:   1171783680 | elapsed time per iteration (ms): 4186.3 | learning rate: 7.810E-05 | global batch size:   128 | lm loss: 3.123557E+00 | loss scale: 262144.0 | grad norm: 0.958 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.576 | tokens per gpu per second (tgs): 1956.879 | TFLOPs: 15.75 |
g0345: [2024-08-02 23:52:01,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=4480, skipped=1, lr=[7.827619840000001e-05, 7.827619840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4480 loss: 3.1312 iter time (s): 4.155 samples/sec: 30.806
g0364:  iteration     4480/10000000 | consumed samples:       573440 | consumed tokens:   1174405120 | elapsed time per iteration (ms): 4187.3 | learning rate: 7.828E-05 | global batch size:   128 | lm loss: 3.047705E+00 | loss scale: 262144.0 | grad norm: 0.858 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.569 | tokens per gpu per second (tgs): 1956.414 | TFLOPs: 15.74 |
g0345: [2024-08-02 23:52:43,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=4490, skipped=1, lr=[7.845096106666667e-05, 7.845096106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4490 loss: 2.9205 iter time (s): 4.174 samples/sec: 30.667
g0364:  iteration     4490/10000000 | consumed samples:       574720 | consumed tokens:   1177026560 | elapsed time per iteration (ms): 4207.2 | learning rate: 7.845E-05 | global batch size:   128 | lm loss: 3.048979E+00 | loss scale: 262144.0 | grad norm: 1.117 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.424 | tokens per gpu per second (tgs): 1947.161 | TFLOPs: 15.67 |
g0345: [2024-08-02 23:53:25,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=4500, skipped=1, lr=[7.862572373333334e-05, 7.862572373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4500 loss: 3.0282 iter time (s): 4.164 samples/sec: 30.741
g0364:  iteration     4500/10000000 | consumed samples:       576000 | consumed tokens:   1179648000 | elapsed time per iteration (ms): 4196.9 | learning rate: 7.863E-05 | global batch size:   128 | lm loss: 3.031749E+00 | loss scale: 262144.0 | grad norm: 0.929 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.498 | tokens per gpu per second (tgs): 1951.896 | TFLOPs: 15.71 |
g0345: [2024-08-02 23:54:07,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=4510, skipped=1, lr=[7.880048640000001e-05, 7.880048640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4510 loss: 3.0994 iter time (s): 4.153 samples/sec: 30.820
g0364:  iteration     4510/10000000 | consumed samples:       577280 | consumed tokens:   1182269440 | elapsed time per iteration (ms): 4185.8 | learning rate: 7.880E-05 | global batch size:   128 | lm loss: 3.074940E+00 | loss scale: 262144.0 | grad norm: 0.960 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.580 | tokens per gpu per second (tgs): 1957.101 | TFLOPs: 15.75 |
g0345: [2024-08-02 23:54:49,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=4520, skipped=1, lr=[7.897524906666667e-05, 7.897524906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4520 loss: 2.9290 iter time (s): 4.129 samples/sec: 30.998
g0364:  iteration     4520/10000000 | consumed samples:       578560 | consumed tokens:   1184890880 | elapsed time per iteration (ms): 4162.0 | learning rate: 7.898E-05 | global batch size:   128 | lm loss: 3.030383E+00 | loss scale: 262144.0 | grad norm: 0.970 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.754 | tokens per gpu per second (tgs): 1968.277 | TFLOPs: 15.84 |
g0345: [2024-08-02 23:55:31,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=4530, skipped=1, lr=[7.915001173333334e-05, 7.915001173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4530 loss: 2.9892 iter time (s): 4.150 samples/sec: 30.842
g0364:  iteration     4530/10000000 | consumed samples:       579840 | consumed tokens:   1187512320 | elapsed time per iteration (ms): 4183.3 | learning rate: 7.915E-05 | global batch size:   128 | lm loss: 3.026461E+00 | loss scale: 262144.0 | grad norm: 1.002 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.598 | tokens per gpu per second (tgs): 1958.254 | TFLOPs: 15.76 |
g0345: [2024-08-02 23:56:12,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=4540, skipped=1, lr=[7.93247744e-05, 7.93247744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4540 loss: 3.0664 iter time (s): 4.158 samples/sec: 30.787
g0364:  iteration     4540/10000000 | consumed samples:       581120 | consumed tokens:   1190133760 | elapsed time per iteration (ms): 4190.1 | learning rate: 7.932E-05 | global batch size:   128 | lm loss: 3.021877E+00 | loss scale: 262144.0 | grad norm: 1.028 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.548 | tokens per gpu per second (tgs): 1955.072 | TFLOPs: 15.73 |
g0345: [2024-08-02 23:56:54,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=4550, skipped=1, lr=[7.949953706666667e-05, 7.949953706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4550 loss: 2.9253 iter time (s): 4.158 samples/sec: 30.788
g0364:  iteration     4550/10000000 | consumed samples:       582400 | consumed tokens:   1192755200 | elapsed time per iteration (ms): 4189.9 | learning rate: 7.950E-05 | global batch size:   128 | lm loss: 3.015762E+00 | loss scale: 262144.0 | grad norm: 0.895 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.550 | tokens per gpu per second (tgs): 1955.169 | TFLOPs: 15.73 |
g0345: [2024-08-02 23:57:37,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=4560, skipped=1, lr=[7.967429973333334e-05, 7.967429973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4560 loss: 2.8486 iter time (s): 4.193 samples/sec: 30.531
g0364:  iteration     4560/10000000 | consumed samples:       583680 | consumed tokens:   1195376640 | elapsed time per iteration (ms): 4225.1 | learning rate: 7.967E-05 | global batch size:   128 | lm loss: 3.011110E+00 | loss scale: 262144.0 | grad norm: 0.968 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.295 | tokens per gpu per second (tgs): 1938.870 | TFLOPs: 15.60 |
g0345: [2024-08-02 23:58:19,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=4570, skipped=1, lr=[7.98490624e-05, 7.98490624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4570 loss: 3.2276 iter time (s): 4.175 samples/sec: 30.657
g0364:  iteration     4570/10000000 | consumed samples:       584960 | consumed tokens:   1197998080 | elapsed time per iteration (ms): 4207.6 | learning rate: 7.985E-05 | global batch size:   128 | lm loss: 3.038315E+00 | loss scale: 262144.0 | grad norm: 0.890 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.421 | tokens per gpu per second (tgs): 1946.934 | TFLOPs: 15.67 |
g0345: [2024-08-02 23:59:01,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=4580, skipped=1, lr=[8.002382506666667e-05, 8.002382506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4580 loss: 2.9654 iter time (s): 4.186 samples/sec: 30.576
g0364:  iteration     4580/10000000 | consumed samples:       586240 | consumed tokens:   1200619520 | elapsed time per iteration (ms): 4219.2 | learning rate: 8.002E-05 | global batch size:   128 | lm loss: 3.001469E+00 | loss scale: 262144.0 | grad norm: 0.955 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.338 | tokens per gpu per second (tgs): 1941.622 | TFLOPs: 15.62 |
g0345: [2024-08-02 23:59:43,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=4590, skipped=1, lr=[8.019858773333334e-05, 8.019858773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4590 loss: 2.8478 iter time (s): 4.143 samples/sec: 30.897
g0364:  iteration     4590/10000000 | consumed samples:       587520 | consumed tokens:   1203240960 | elapsed time per iteration (ms): 4175.5 | learning rate: 8.020E-05 | global batch size:   128 | lm loss: 2.977402E+00 | loss scale: 262144.0 | grad norm: 0.959 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.655 | tokens per gpu per second (tgs): 1961.937 | TFLOPs: 15.79 |
g0345: [2024-08-03 00:00:25,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=4600, skipped=1, lr=[8.03733504e-05, 8.03733504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4600 loss: 2.9995 iter time (s): 4.199 samples/sec: 30.484
g0364:  iteration     4600/10000000 | consumed samples:       588800 | consumed tokens:   1205862400 | elapsed time per iteration (ms): 4231.4 | learning rate: 8.037E-05 | global batch size:   128 | lm loss: 3.028291E+00 | loss scale: 262144.0 | grad norm: 0.918 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.250 | tokens per gpu per second (tgs): 1936.019 | TFLOPs: 15.58 |
g0345: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:00:33,749] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 00:00:33,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 00:01:07,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=4610, skipped=1, lr=[8.054811306666667e-05, 8.054811306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4610 loss: 3.1169 iter time (s): 4.141 samples/sec: 30.911
g0364:  iteration     4610/10000000 | consumed samples:       590080 | consumed tokens:   1208483840 | elapsed time per iteration (ms): 4174.2 | learning rate: 8.055E-05 | global batch size:   128 | lm loss: 3.058108E+00 | loss scale: 524288.0 | grad norm: 0.947 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.665 | tokens per gpu per second (tgs): 1962.548 | TFLOPs: 15.79 |
g0345: [2024-08-03 00:01:49,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=4620, skipped=1, lr=[8.072287573333333e-05, 8.072287573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4620 loss: 3.0613 iter time (s): 4.169 samples/sec: 30.704
g0364:  iteration     4620/10000000 | consumed samples:       591360 | consumed tokens:   1211105280 | elapsed time per iteration (ms): 4201.8 | learning rate: 8.072E-05 | global batch size:   128 | lm loss: 2.969041E+00 | loss scale: 524288.0 | grad norm: 0.867 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.463 | tokens per gpu per second (tgs): 1949.644 | TFLOPs: 15.69 |
g0345: [2024-08-03 00:02:30,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=4630, skipped=1, lr=[8.08976384e-05, 8.08976384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4630 loss: 3.0492 iter time (s): 4.139 samples/sec: 30.928
g0364:  iteration     4630/10000000 | consumed samples:       592640 | consumed tokens:   1213726720 | elapsed time per iteration (ms): 4171.0 | learning rate: 8.090E-05 | global batch size:   128 | lm loss: 2.996303E+00 | loss scale: 524288.0 | grad norm: 0.856 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.688 | tokens per gpu per second (tgs): 1964.027 | TFLOPs: 15.80 |
g0345: [2024-08-03 00:03:13,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=4640, skipped=1, lr=[8.107240106666668e-05, 8.107240106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4640 loss: 2.8330 iter time (s): 4.181 samples/sec: 30.616
g0364:  iteration     4640/10000000 | consumed samples:       593920 | consumed tokens:   1216348160 | elapsed time per iteration (ms): 4214.3 | learning rate: 8.107E-05 | global batch size:   128 | lm loss: 2.934849E+00 | loss scale: 524288.0 | grad norm: 0.960 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.372 | tokens per gpu per second (tgs): 1943.840 | TFLOPs: 15.64 |
g0345: [2024-08-03 00:03:55,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=4650, skipped=1, lr=[8.124716373333335e-05, 8.124716373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4650 loss: 3.0294 iter time (s): 4.163 samples/sec: 30.747
g0364:  iteration     4650/10000000 | consumed samples:       595200 | consumed tokens:   1218969600 | elapsed time per iteration (ms): 4195.8 | learning rate: 8.125E-05 | global batch size:   128 | lm loss: 2.989596E+00 | loss scale: 524288.0 | grad norm: 0.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.507 | tokens per gpu per second (tgs): 1952.433 | TFLOPs: 15.71 |
g0345: [2024-08-03 00:04:37,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=4660, skipped=1, lr=[8.142192640000001e-05, 8.142192640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4660 loss: 3.1588 iter time (s): 4.222 samples/sec: 30.314
g0364:  iteration     4660/10000000 | consumed samples:       596480 | consumed tokens:   1221591040 | elapsed time per iteration (ms): 4255.2 | learning rate: 8.142E-05 | global batch size:   128 | lm loss: 2.982722E+00 | loss scale: 524288.0 | grad norm: 0.806 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.081 | tokens per gpu per second (tgs): 1925.153 | TFLOPs: 15.49 |
g0345: [2024-08-03 00:05:20,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=4670, skipped=1, lr=[8.159668906666668e-05, 8.159668906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4670 loss: 3.0553 iter time (s): 4.218 samples/sec: 30.345
g0364:  iteration     4670/10000000 | consumed samples:       597760 | consumed tokens:   1224212480 | elapsed time per iteration (ms): 4250.8 | learning rate: 8.160E-05 | global batch size:   128 | lm loss: 2.948141E+00 | loss scale: 524288.0 | grad norm: 1.009 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.112 | tokens per gpu per second (tgs): 1927.153 | TFLOPs: 15.51 |
g0345: [2024-08-03 00:06:02,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=4680, skipped=1, lr=[8.177145173333334e-05, 8.177145173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4680 loss: 3.0034 iter time (s): 4.183 samples/sec: 30.599
g0364:  iteration     4680/10000000 | consumed samples:       599040 | consumed tokens:   1226833920 | elapsed time per iteration (ms): 4216.4 | learning rate: 8.177E-05 | global batch size:   128 | lm loss: 2.956414E+00 | loss scale: 524288.0 | grad norm: 0.923 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.358 | tokens per gpu per second (tgs): 1942.897 | TFLOPs: 15.63 |
g0345: [2024-08-03 00:06:44,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=4690, skipped=1, lr=[8.194621440000001e-05, 8.194621440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4690 loss: 2.9779 iter time (s): 4.142 samples/sec: 30.901
g0364:  iteration     4690/10000000 | consumed samples:       600320 | consumed tokens:   1229455360 | elapsed time per iteration (ms): 4176.1 | learning rate: 8.195E-05 | global batch size:   128 | lm loss: 3.020472E+00 | loss scale: 524288.0 | grad norm: 0.952 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.651 | tokens per gpu per second (tgs): 1961.656 | TFLOPs: 15.79 |
g0345: [2024-08-03 00:07:26,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=4700, skipped=1, lr=[8.212097706666668e-05, 8.212097706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4700 loss: 2.8982 iter time (s): 4.248 samples/sec: 30.132
g0364:  iteration     4700/10000000 | consumed samples:       601600 | consumed tokens:   1232076800 | elapsed time per iteration (ms): 4281.1 | learning rate: 8.212E-05 | global batch size:   128 | lm loss: 2.875845E+00 | loss scale: 524288.0 | grad norm: 0.918 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.899 | tokens per gpu per second (tgs): 1913.505 | TFLOPs: 15.40 |
g0345: [2024-08-03 00:08:09,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=4710, skipped=1, lr=[8.229573973333334e-05, 8.229573973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4710 loss: 2.9850 iter time (s): 4.251 samples/sec: 30.109
g0364:  iteration     4710/10000000 | consumed samples:       602880 | consumed tokens:   1234698240 | elapsed time per iteration (ms): 4283.9 | learning rate: 8.230E-05 | global batch size:   128 | lm loss: 2.961080E+00 | loss scale: 524288.0 | grad norm: 0.974 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.879 | tokens per gpu per second (tgs): 1912.280 | TFLOPs: 15.39 |
g0345: [2024-08-03 00:08:52,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=4720, skipped=1, lr=[8.247050240000001e-05, 8.247050240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4720 loss: 2.9033 iter time (s): 4.251 samples/sec: 30.112
g0364:  iteration     4720/10000000 | consumed samples:       604160 | consumed tokens:   1237319680 | elapsed time per iteration (ms): 4283.6 | learning rate: 8.247E-05 | global batch size:   128 | lm loss: 2.939906E+00 | loss scale: 524288.0 | grad norm: 0.840 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.881 | tokens per gpu per second (tgs): 1912.398 | TFLOPs: 15.39 |
g0345: [2024-08-03 00:09:34,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=4730, skipped=1, lr=[8.264526506666667e-05, 8.264526506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4730 loss: 2.8864 iter time (s): 4.207 samples/sec: 30.425
g0364:  iteration     4730/10000000 | consumed samples:       605440 | consumed tokens:   1239941120 | elapsed time per iteration (ms): 4239.9 | learning rate: 8.265E-05 | global batch size:   128 | lm loss: 2.926035E+00 | loss scale: 524288.0 | grad norm: 1.023 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.189 | tokens per gpu per second (tgs): 1932.127 | TFLOPs: 15.55 |
g0345: [2024-08-03 00:10:16,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=4740, skipped=1, lr=[8.282002773333334e-05, 8.282002773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4740 loss: 2.9614 iter time (s): 4.157 samples/sec: 30.794
g0364:  iteration     4740/10000000 | consumed samples:       606720 | consumed tokens:   1242562560 | elapsed time per iteration (ms): 4189.2 | learning rate: 8.282E-05 | global batch size:   128 | lm loss: 2.922172E+00 | loss scale: 524288.0 | grad norm: 0.836 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.554 | tokens per gpu per second (tgs): 1955.486 | TFLOPs: 15.74 |
g0345: [2024-08-03 00:10:59,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=4750, skipped=1, lr=[8.29947904e-05, 8.29947904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4750 loss: 2.7398 iter time (s): 4.239 samples/sec: 30.197
g0364:  iteration     4750/10000000 | consumed samples:       608000 | consumed tokens:   1245184000 | elapsed time per iteration (ms): 4271.1 | learning rate: 8.299E-05 | global batch size:   128 | lm loss: 2.927595E+00 | loss scale: 524288.0 | grad norm: 0.977 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.969 | tokens per gpu per second (tgs): 1918.008 | TFLOPs: 15.43 |
g0345: [2024-08-03 00:11:41,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=4760, skipped=1, lr=[8.316955306666667e-05, 8.316955306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4760 loss: 2.8782 iter time (s): 4.132 samples/sec: 30.979
g0364:  iteration     4760/10000000 | consumed samples:       609280 | consumed tokens:   1247805440 | elapsed time per iteration (ms): 4164.5 | learning rate: 8.317E-05 | global batch size:   128 | lm loss: 2.879769E+00 | loss scale: 524288.0 | grad norm: 1.003 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.736 | tokens per gpu per second (tgs): 1967.115 | TFLOPs: 15.83 |
g0345: [2024-08-03 00:12:23,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=4770, skipped=1, lr=[8.334431573333334e-05, 8.334431573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4770 loss: 2.9331 iter time (s): 4.150 samples/sec: 30.840
g0364:  iteration     4770/10000000 | consumed samples:       610560 | consumed tokens:   1250426880 | elapsed time per iteration (ms): 4182.8 | learning rate: 8.334E-05 | global batch size:   128 | lm loss: 2.906307E+00 | loss scale: 524288.0 | grad norm: 1.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.601 | tokens per gpu per second (tgs): 1958.494 | TFLOPs: 15.76 |
g0345: [2024-08-03 00:13:05,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=4780, skipped=1, lr=[8.35190784e-05, 8.35190784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4780 loss: 2.9567 iter time (s): 4.169 samples/sec: 30.705
g0364:  iteration     4780/10000000 | consumed samples:       611840 | consumed tokens:   1253048320 | elapsed time per iteration (ms): 4201.6 | learning rate: 8.352E-05 | global batch size:   128 | lm loss: 2.897421E+00 | loss scale: 524288.0 | grad norm: 0.937 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.465 | tokens per gpu per second (tgs): 1949.734 | TFLOPs: 15.69 |
g0345: [2024-08-03 00:13:47,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=4790, skipped=1, lr=[8.369384106666667e-05, 8.369384106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4790 loss: 2.7247 iter time (s): 4.177 samples/sec: 30.644
g0364:  iteration     4790/10000000 | consumed samples:       613120 | consumed tokens:   1255669760 | elapsed time per iteration (ms): 4209.7 | learning rate: 8.369E-05 | global batch size:   128 | lm loss: 2.858273E+00 | loss scale: 524288.0 | grad norm: 0.971 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.406 | tokens per gpu per second (tgs): 1945.983 | TFLOPs: 15.66 |
g0345: [2024-08-03 00:14:29,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=4800, skipped=1, lr=[8.386860373333334e-05, 8.386860373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4800 loss: 2.7727 iter time (s): 4.156 samples/sec: 30.800
g0364:  iteration     4800/10000000 | consumed samples:       614400 | consumed tokens:   1258291200 | elapsed time per iteration (ms): 4188.9 | learning rate: 8.387E-05 | global batch size:   128 | lm loss: 2.873226E+00 | loss scale: 524288.0 | grad norm: 0.931 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.557 | tokens per gpu per second (tgs): 1955.642 | TFLOPs: 15.74 |
g0345: [2024-08-03 00:15:11,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=4810, skipped=1, lr=[8.40433664e-05, 8.40433664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4810 loss: 2.7945 iter time (s): 4.220 samples/sec: 30.335
g0364:  iteration     4810/10000000 | consumed samples:       615680 | consumed tokens:   1260912640 | elapsed time per iteration (ms): 4253.4 | learning rate: 8.404E-05 | global batch size:   128 | lm loss: 2.931194E+00 | loss scale: 524288.0 | grad norm: 1.154 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.093 | tokens per gpu per second (tgs): 1925.979 | TFLOPs: 15.50 |
g0345: [2024-08-03 00:15:54,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=4820, skipped=1, lr=[8.421812906666668e-05, 8.421812906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4820 loss: 3.1015 iter time (s): 4.251 samples/sec: 30.107
g0364:  iteration     4820/10000000 | consumed samples:       616960 | consumed tokens:   1263534080 | elapsed time per iteration (ms): 4284.0 | learning rate: 8.422E-05 | global batch size:   128 | lm loss: 2.868952E+00 | loss scale: 524288.0 | grad norm: 1.113 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.879 | tokens per gpu per second (tgs): 1912.251 | TFLOPs: 15.39 |
g0345: [2024-08-03 00:16:36,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=4830, skipped=1, lr=[8.439289173333333e-05, 8.439289173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4830 loss: 2.9335 iter time (s): 4.204 samples/sec: 30.450
g0364:  iteration     4830/10000000 | consumed samples:       618240 | consumed tokens:   1266155520 | elapsed time per iteration (ms): 4236.0 | learning rate: 8.439E-05 | global batch size:   128 | lm loss: 2.815992E+00 | loss scale: 524288.0 | grad norm: 0.903 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.217 | tokens per gpu per second (tgs): 1933.878 | TFLOPs: 15.56 |
g0345: [2024-08-03 00:17:19,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=4840, skipped=1, lr=[8.45676544e-05, 8.45676544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4840 loss: 2.9596 iter time (s): 4.195 samples/sec: 30.516
g0364:  iteration     4840/10000000 | consumed samples:       619520 | consumed tokens:   1268776960 | elapsed time per iteration (ms): 4227.2 | learning rate: 8.457E-05 | global batch size:   128 | lm loss: 2.797404E+00 | loss scale: 524288.0 | grad norm: 0.902 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.280 | tokens per gpu per second (tgs): 1937.918 | TFLOPs: 15.59 |
g0345: [2024-08-03 00:18:01,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=4850, skipped=1, lr=[8.474241706666667e-05, 8.474241706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4850 loss: 2.6855 iter time (s): 4.212 samples/sec: 30.390
g0364:  iteration     4850/10000000 | consumed samples:       620800 | consumed tokens:   1271398400 | elapsed time per iteration (ms): 4244.9 | learning rate: 8.474E-05 | global batch size:   128 | lm loss: 2.875051E+00 | loss scale: 524288.0 | grad norm: 1.002 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.154 | tokens per gpu per second (tgs): 1929.840 | TFLOPs: 15.53 |
g0345: [2024-08-03 00:18:43,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=4860, skipped=1, lr=[8.491717973333333e-05, 8.491717973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4860 loss: 2.7679 iter time (s): 4.159 samples/sec: 30.774
g0364:  iteration     4860/10000000 | consumed samples:       622080 | consumed tokens:   1274019840 | elapsed time per iteration (ms): 4192.1 | learning rate: 8.492E-05 | global batch size:   128 | lm loss: 2.800613E+00 | loss scale: 524288.0 | grad norm: 1.022 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.534 | tokens per gpu per second (tgs): 1954.172 | TFLOPs: 15.73 |
g0345: [2024-08-03 00:19:26,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=4870, skipped=1, lr=[8.50919424e-05, 8.50919424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4870 loss: 2.6972 iter time (s): 4.234 samples/sec: 30.233
g0364:  iteration     4870/10000000 | consumed samples:       623360 | consumed tokens:   1276641280 | elapsed time per iteration (ms): 4266.6 | learning rate: 8.509E-05 | global batch size:   128 | lm loss: 2.825484E+00 | loss scale: 524288.0 | grad norm: 1.060 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.001 | tokens per gpu per second (tgs): 1920.043 | TFLOPs: 15.45 |
g0345: [2024-08-03 00:20:08,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=4880, skipped=1, lr=[8.526670506666666e-05, 8.526670506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4880 loss: 2.7398 iter time (s): 4.229 samples/sec: 30.266
g0364:  iteration     4880/10000000 | consumed samples:       624640 | consumed tokens:   1279262720 | elapsed time per iteration (ms): 4261.8 | learning rate: 8.527E-05 | global batch size:   128 | lm loss: 2.780928E+00 | loss scale: 524288.0 | grad norm: 0.942 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.035 | tokens per gpu per second (tgs): 1922.208 | TFLOPs: 15.47 |
g0345: [2024-08-03 00:20:50,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=4890, skipped=1, lr=[8.544146773333333e-05, 8.544146773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4890 loss: 3.0141 iter time (s): 4.180 samples/sec: 30.619
g0364:  iteration     4890/10000000 | consumed samples:       625920 | consumed tokens:   1281884160 | elapsed time per iteration (ms): 4212.7 | learning rate: 8.544E-05 | global batch size:   128 | lm loss: 2.910496E+00 | loss scale: 524288.0 | grad norm: 0.799 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.384 | tokens per gpu per second (tgs): 1944.577 | TFLOPs: 15.65 |
g0345: [2024-08-03 00:21:33,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=4900, skipped=1, lr=[8.56162304e-05, 8.56162304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4900 loss: 2.7622 iter time (s): 4.225 samples/sec: 30.299
g0364:  iteration     4900/10000000 | consumed samples:       627200 | consumed tokens:   1284505600 | elapsed time per iteration (ms): 4256.8 | learning rate: 8.562E-05 | global batch size:   128 | lm loss: 2.783491E+00 | loss scale: 524288.0 | grad norm: 1.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.070 | tokens per gpu per second (tgs): 1924.464 | TFLOPs: 15.49 |
g0345: [2024-08-03 00:22:15,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=4910, skipped=1, lr=[8.579099306666666e-05, 8.579099306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4910 loss: 2.8703 iter time (s): 4.163 samples/sec: 30.751
g0364:  iteration     4910/10000000 | consumed samples:       628480 | consumed tokens:   1287127040 | elapsed time per iteration (ms): 4195.3 | learning rate: 8.579E-05 | global batch size:   128 | lm loss: 2.804750E+00 | loss scale: 524288.0 | grad norm: 0.836 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.510 | tokens per gpu per second (tgs): 1952.639 | TFLOPs: 15.71 |
g0345: [2024-08-03 00:22:55,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=4920, skipped=1, lr=[8.596575573333333e-05, 8.596575573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4920 loss: 2.7733 iter time (s): 3.982 samples/sec: 32.143
g0364:  iteration     4920/10000000 | consumed samples:       629760 | consumed tokens:   1289748480 | elapsed time per iteration (ms): 4015.2 | learning rate: 8.597E-05 | global batch size:   128 | lm loss: 2.780168E+00 | loss scale: 524288.0 | grad norm: 0.879 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.879 | tokens per gpu per second (tgs): 2040.255 | TFLOPs: 16.42 |
g0345: [2024-08-03 00:23:35,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=4930, skipped=1, lr=[8.61405184e-05, 8.61405184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4930 loss: 2.7556 iter time (s): 3.992 samples/sec: 32.061
g0364:  iteration     4930/10000000 | consumed samples:       631040 | consumed tokens:   1292369920 | elapsed time per iteration (ms): 4026.2 | learning rate: 8.614E-05 | global batch size:   128 | lm loss: 2.786391E+00 | loss scale: 524288.0 | grad norm: 1.071 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.792 | tokens per gpu per second (tgs): 2034.688 | TFLOPs: 16.37 |
g0345: [2024-08-03 00:24:16,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=4940, skipped=1, lr=[8.631528106666666e-05, 8.631528106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4940 loss: 2.8632 iter time (s): 4.079 samples/sec: 31.377
g0364:  iteration     4940/10000000 | consumed samples:       632320 | consumed tokens:   1294991360 | elapsed time per iteration (ms): 4111.8 | learning rate: 8.632E-05 | global batch size:   128 | lm loss: 2.742552E+00 | loss scale: 524288.0 | grad norm: 0.957 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.130 | tokens per gpu per second (tgs): 1992.314 | TFLOPs: 16.03 |
g0345: [2024-08-03 00:24:57,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=4950, skipped=1, lr=[8.649004373333334e-05, 8.649004373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4950 loss: 3.0263 iter time (s): 4.047 samples/sec: 31.631
g0364:  iteration     4950/10000000 | consumed samples:       633600 | consumed tokens:   1297612800 | elapsed time per iteration (ms): 4079.5 | learning rate: 8.649E-05 | global batch size:   128 | lm loss: 2.786755E+00 | loss scale: 524288.0 | grad norm: 1.015 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.376 | tokens per gpu per second (tgs): 2008.090 | TFLOPs: 16.16 |
g0345: [2024-08-03 00:25:37,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=4960, skipped=1, lr=[8.666480640000001e-05, 8.666480640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4960 loss: 2.7675 iter time (s): 3.948 samples/sec: 32.424
g0364:  iteration     4960/10000000 | consumed samples:       634880 | consumed tokens:   1300234240 | elapsed time per iteration (ms): 3980.1 | learning rate: 8.666E-05 | global batch size:   128 | lm loss: 2.727818E+00 | loss scale: 524288.0 | grad norm: 0.945 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.160 | tokens per gpu per second (tgs): 2058.256 | TFLOPs: 16.56 |
g0345: [2024-08-03 00:26:18,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=4970, skipped=1, lr=[8.683956906666667e-05, 8.683956906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4970 loss: 2.7017 iter time (s): 4.067 samples/sec: 31.471
g0364:  iteration     4970/10000000 | consumed samples:       636160 | consumed tokens:   1302855680 | elapsed time per iteration (ms): 4099.5 | learning rate: 8.684E-05 | global batch size:   128 | lm loss: 2.695338E+00 | loss scale: 524288.0 | grad norm: 1.117 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.223 | tokens per gpu per second (tgs): 1998.282 | TFLOPs: 16.08 |
g0345: [2024-08-03 00:26:59,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=4980, skipped=1, lr=[8.701433173333334e-05, 8.701433173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4980 loss: 2.7814 iter time (s): 4.105 samples/sec: 31.179
g0364:  iteration     4980/10000000 | consumed samples:       637440 | consumed tokens:   1305477120 | elapsed time per iteration (ms): 4137.9 | learning rate: 8.701E-05 | global batch size:   128 | lm loss: 2.695761E+00 | loss scale: 524288.0 | grad norm: 0.880 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.934 | tokens per gpu per second (tgs): 1979.760 | TFLOPs: 15.93 |
g0345: [2024-08-03 00:27:41,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=4990, skipped=1, lr=[8.71890944e-05, 8.71890944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 4990 loss: 2.7073 iter time (s): 4.145 samples/sec: 30.882
g0364:  iteration     4990/10000000 | consumed samples:       638720 | consumed tokens:   1308098560 | elapsed time per iteration (ms): 4177.6 | learning rate: 8.719E-05 | global batch size:   128 | lm loss: 2.621048E+00 | loss scale: 524288.0 | grad norm: 1.005 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.640 | tokens per gpu per second (tgs): 1960.947 | TFLOPs: 15.78 |
g0345: [2024-08-03 00:28:22,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=1, lr=[8.736385706666667e-05, 8.736385706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5000 loss: 2.4510 iter time (s): 4.078 samples/sec: 31.384
g0364:  iteration     5000/10000000 | consumed samples:       640000 | consumed tokens:   1310720000 | elapsed time per iteration (ms): 4111.1 | learning rate: 8.736E-05 | global batch size:   128 | lm loss: 2.748311E+00 | loss scale: 524288.0 | grad norm: 0.956 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.136 | tokens per gpu per second (tgs): 1992.674 | TFLOPs: 16.04 |
g0364: ------------------------------------------------------------------------------------------------
g0364:  validation loss at iteration 5000 | lm loss value: 2.666846E+00 | lm loss PPL: 1.439449E+01 | 
g0364: ------------------------------------------------------------------------------------------------
g0345: saving checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: [2024-08-03 00:34:37,932] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!
g0345: [2024-08-03 00:34:37,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0345: [2024-08-03 00:34:37,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0345: [2024-08-03 00:34:37,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0364: [2024-08-03 00:34:37,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0364: [2024-08-03 00:34:37,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0364: [2024-08-03 00:34:37,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0346: [2024-08-03 00:34:37,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0362: [2024-08-03 00:34:37,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0362: [2024-08-03 00:34:37,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0362: [2024-08-03 00:34:37,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0346: [2024-08-03 00:34:37,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0346: [2024-08-03 00:34:37,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0352: [2024-08-03 00:34:37,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0352: [2024-08-03 00:34:37,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0352: [2024-08-03 00:34:37,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0363: [2024-08-03 00:34:37,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0363: [2024-08-03 00:34:37,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0363: [2024-08-03 00:34:37,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0358: [2024-08-03 00:34:37,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0358: [2024-08-03 00:34:37,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0358: [2024-08-03 00:34:37,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0347: [2024-08-03 00:34:37,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0347: [2024-08-03 00:34:37,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0347: [2024-08-03 00:34:37,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0364: [2024-08-03 00:34:37,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt...
g0362: [2024-08-03 00:34:37,972] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt...
g0346: [2024-08-03 00:34:37,972] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt...
g0358: [2024-08-03 00:34:37,975] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt...
g0352: [2024-08-03 00:34:37,975] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt...
g0363: [2024-08-03 00:34:37,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt...
g0347: [2024-08-03 00:34:37,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt...
g0345: [2024-08-03 00:34:37,983] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt...
g0346: [2024-08-03 00:34:38,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt.
g0364: [2024-08-03 00:34:38,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt.
g0364: [2024-08-03 00:34:38,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt...
g0364: [2024-08-03 00:34:38,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt.
g0346: [2024-08-03 00:34:38,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt...
g0345: [2024-08-03 00:34:38,174] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt.
g0364: [2024-08-03 00:34:38,185] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt...
g0345: [2024-08-03 00:34:38,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt...
g0347: [2024-08-03 00:34:38,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt.
g0362: [2024-08-03 00:34:38,192] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt.
g0352: [2024-08-03 00:34:38,199] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt.
g0358: [2024-08-03 00:34:38,211] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt.
g0362: [2024-08-03 00:34:38,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt...
g0347: [2024-08-03 00:34:38,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt...
g0363: [2024-08-03 00:34:38,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt.
g0352: [2024-08-03 00:34:38,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt...
g0358: [2024-08-03 00:34:38,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt...
g0363: [2024-08-03 00:34:38,272] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt...
g0346: [2024-08-03 00:34:38,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt.
g0345: [2024-08-03 00:34:38,326] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt.
g0346: [2024-08-03 00:34:38,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt...
g0345: [2024-08-03 00:34:38,345] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt...
g0364: [2024-08-03 00:34:38,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt.
g0364: [2024-08-03 00:34:38,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt...
g0358: [2024-08-03 00:34:38,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt.
g0352: [2024-08-03 00:34:38,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt.
g0347: [2024-08-03 00:34:38,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt.
g0358: [2024-08-03 00:34:38,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt...
g0362: [2024-08-03 00:34:38,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt.
g0352: [2024-08-03 00:34:38,428] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt...
g0347: [2024-08-03 00:34:38,434] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt...
g0362: [2024-08-03 00:34:38,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt...
g0345: [2024-08-03 00:34:38,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt.
g0345: [2024-08-03 00:34:38,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt...
g0362: [2024-08-03 00:34:38,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt.
g0362: [2024-08-03 00:34:38,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt...
g0352: [2024-08-03 00:34:38,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt.
g0352: [2024-08-03 00:34:38,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt...
g0346: [2024-08-03 00:34:38,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt.
g0346: [2024-08-03 00:34:38,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt...
g0358: [2024-08-03 00:34:38,602] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt.
g0358: [2024-08-03 00:34:38,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt...
g0345: [2024-08-03 00:34:38,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt.
g0345: [2024-08-03 00:34:38,619] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt
g0345: [2024-08-03 00:34:38,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt...
g0363: [2024-08-03 00:34:38,627] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt.
g0363: [2024-08-03 00:34:38,662] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt...
g0363: [2024-08-03 00:34:38,826] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt.
g0363: [2024-08-03 00:34:38,828] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt...
g0347: [2024-08-03 00:34:38,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt.
g0347: [2024-08-03 00:34:38,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt...
g0364: [2024-08-03 00:34:40,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt.
g0364: [2024-08-03 00:34:40,301] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0358: [2024-08-03 00:34:41,008] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt.
g0358: [2024-08-03 00:34:41,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0346: [2024-08-03 00:34:41,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt.
g0346: [2024-08-03 00:34:41,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0352: [2024-08-03 00:34:41,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt.
g0352: [2024-08-03 00:34:41,085] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0362: [2024-08-03 00:34:41,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt.
g0362: [2024-08-03 00:34:41,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0363: [2024-08-03 00:34:41,143] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt.
g0363: [2024-08-03 00:34:41,144] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0347: [2024-08-03 00:34:41,173] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt.
g0347: [2024-08-03 00:34:41,173] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0345: [2024-08-03 00:34:41,937] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt.
g0345: [2024-08-03 00:34:41,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0345:   successfully saved checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: Checkpoint Save GB: 22.521, GB/Sec: 5.6, Latency(second): 4.019
g0364: (min, max) time across ranks (ms):
g0364:     save-checkpoint ................................: (4018.67, 4018.80)
g0345: [2024-08-03 00:35:25,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=5010, skipped=1, lr=[8.753861973333334e-05, 8.753861973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5010 loss: 2.6550 iter time (s): 4.366 samples/sec: 29.320
g0364:  iteration     5010/10000000 | consumed samples:       641280 | consumed tokens:   1313341440 | elapsed time per iteration (ms): 42317.4 | learning rate: 8.754E-05 | global batch size:   128 | lm loss: 2.772716E+00 | loss scale: 524288.0 | grad norm: 0.925 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.025 | tokens per gpu per second (tgs): 193.585 | TFLOPs: 1.56 |
g0345: [2024-08-03 00:36:06,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=5020, skipped=1, lr=[8.77133824e-05, 8.77133824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5020 loss: 2.7611 iter time (s): 4.053 samples/sec: 31.583
g0364:  iteration     5020/10000000 | consumed samples:       642560 | consumed tokens:   1315962880 | elapsed time per iteration (ms): 4085.5 | learning rate: 8.771E-05 | global batch size:   128 | lm loss: 2.721512E+00 | loss scale: 524288.0 | grad norm: 0.932 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.330 | tokens per gpu per second (tgs): 2005.150 | TFLOPs: 16.14 |
g0345: [2024-08-03 00:36:47,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=5030, skipped=1, lr=[8.788814506666667e-05, 8.788814506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5030 loss: 2.5315 iter time (s): 4.031 samples/sec: 31.754
g0364:  iteration     5030/10000000 | consumed samples:       643840 | consumed tokens:   1318584320 | elapsed time per iteration (ms): 4064.0 | learning rate: 8.789E-05 | global batch size:   128 | lm loss: 2.658837E+00 | loss scale: 524288.0 | grad norm: 0.929 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.496 | tokens per gpu per second (tgs): 2015.769 | TFLOPs: 16.22 |
g0345: [2024-08-03 00:37:27,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=5040, skipped=1, lr=[8.806290773333334e-05, 8.806290773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5040 loss: 2.7775 iter time (s): 3.999 samples/sec: 32.009
g0364:  iteration     5040/10000000 | consumed samples:       645120 | consumed tokens:   1321205760 | elapsed time per iteration (ms): 4032.0 | learning rate: 8.806E-05 | global batch size:   128 | lm loss: 2.649395E+00 | loss scale: 524288.0 | grad norm: 0.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.746 | tokens per gpu per second (tgs): 2031.737 | TFLOPs: 16.35 |
g0345: [2024-08-03 00:38:09,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=5050, skipped=1, lr=[8.82376704e-05, 8.82376704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5050 loss: 2.6795 iter time (s): 4.117 samples/sec: 31.089
g0364:  iteration     5050/10000000 | consumed samples:       646400 | consumed tokens:   1323827200 | elapsed time per iteration (ms): 4150.1 | learning rate: 8.824E-05 | global batch size:   128 | lm loss: 2.684963E+00 | loss scale: 524288.0 | grad norm: 0.942 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.842 | tokens per gpu per second (tgs): 1973.918 | TFLOPs: 15.88 |
g0345: [2024-08-03 00:38:50,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=5060, skipped=1, lr=[8.841243306666667e-05, 8.841243306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5060 loss: 2.6498 iter time (s): 4.091 samples/sec: 31.287
g0364:  iteration     5060/10000000 | consumed samples:       647680 | consumed tokens:   1326448640 | elapsed time per iteration (ms): 4123.6 | learning rate: 8.841E-05 | global batch size:   128 | lm loss: 2.667100E+00 | loss scale: 524288.0 | grad norm: 0.955 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.041 | tokens per gpu per second (tgs): 1986.599 | TFLOPs: 15.99 |
g0345: [2024-08-03 00:39:31,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=5070, skipped=1, lr=[8.858719573333333e-05, 8.858719573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5070 loss: 2.6638 iter time (s): 4.087 samples/sec: 31.319
g0364:  iteration     5070/10000000 | consumed samples:       648960 | consumed tokens:   1329070080 | elapsed time per iteration (ms): 4120.2 | learning rate: 8.859E-05 | global batch size:   128 | lm loss: 2.683749E+00 | loss scale: 524288.0 | grad norm: 0.867 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.066 | tokens per gpu per second (tgs): 1988.253 | TFLOPs: 16.00 |
g0345: [2024-08-03 00:40:13,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=5080, skipped=1, lr=[8.87619584e-05, 8.87619584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5080 loss: 2.4708 iter time (s): 4.136 samples/sec: 30.946
g0364:  iteration     5080/10000000 | consumed samples:       650240 | consumed tokens:   1331691520 | elapsed time per iteration (ms): 4168.8 | learning rate: 8.876E-05 | global batch size:   128 | lm loss: 2.628695E+00 | loss scale: 524288.0 | grad norm: 0.902 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.704 | tokens per gpu per second (tgs): 1965.088 | TFLOPs: 15.81 |
g0345: [2024-08-03 00:40:54,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=5090, skipped=1, lr=[8.893672106666667e-05, 8.893672106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5090 loss: 2.4843 iter time (s): 4.079 samples/sec: 31.383
g0364:  iteration     5090/10000000 | consumed samples:       651520 | consumed tokens:   1334312960 | elapsed time per iteration (ms): 4111.2 | learning rate: 8.894E-05 | global batch size:   128 | lm loss: 2.647031E+00 | loss scale: 524288.0 | grad norm: 0.840 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.135 | tokens per gpu per second (tgs): 1992.613 | TFLOPs: 16.03 |
g0345: [2024-08-03 00:41:35,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=5100, skipped=1, lr=[8.911148373333333e-05, 8.911148373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5100 loss: 2.7241 iter time (s): 4.121 samples/sec: 31.058
g0364:  iteration     5100/10000000 | consumed samples:       652800 | consumed tokens:   1336934400 | elapsed time per iteration (ms): 4154.2 | learning rate: 8.911E-05 | global batch size:   128 | lm loss: 2.635527E+00 | loss scale: 524288.0 | grad norm: 0.942 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.812 | tokens per gpu per second (tgs): 1971.965 | TFLOPs: 15.87 |
g0363: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0363: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0347: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0364: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0352: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0345: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0358: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0352: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0363: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0358: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0347: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0347: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0346: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0362: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0346: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0362: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0346: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0363: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0362: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0364: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0363: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0352: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0346: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0362: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0358: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0345: [2024-08-03 00:41:44,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0352: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0347: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0345: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0347: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0364: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0345: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 00:41:44,407] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0345: [2024-08-03 00:42:17,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=5110, skipped=1, lr=[8.92862464e-05, 8.92862464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5110 loss: 2.6432 iter time (s): 4.107 samples/sec: 31.170
g0364:  iteration     5110/10000000 | consumed samples:       654080 | consumed tokens:   1339555840 | elapsed time per iteration (ms): 4140.2 | learning rate: 8.929E-05 | global batch size:   128 | lm loss: 2.567280E+00 | loss scale: 1048576.0 | grad norm: 0.974 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.916 | tokens per gpu per second (tgs): 1978.637 | TFLOPs: 15.92 |
g0363: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 5118
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 5118
g0363: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 5118
g0363: Grad overflow on iteration 5118
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0352: Grad overflow on iteration 5118
g0363: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 5118
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 5118
g0363: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0347: Grad overflow on iteration 5118
g0363: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 5118
g0363: Grad overflow on iteration 5118
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 5118
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 5118
g0347: Grad overflow on iteration 5118
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 5118
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 5118
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0345: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 5118
g0345: Grad overflow on iteration 5118
g0347: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0346: Grad overflow on iteration 5118
g0358: Grad overflow on iteration 5118
g0352: Grad overflow on iteration 5118
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 5118
g0345: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 5118
g0364: Grad overflow on iteration 5118
g0345: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 5118
g0345: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0358: Grad overflow on iteration 5118
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0352: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0364: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 5118
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0364: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 5118
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 5118
g0362: Grad overflow on iteration 5118
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0345: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0345: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 5118
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0345: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0358: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0362: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0346: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0364: Grad overflow on iteration 5118
g0364: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0364: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0364: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-03 00:42:55,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0364: Grad overflow on iteration 5118
g0364: [2024-08-03 00:42:55,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0364: [2024-08-03 00:42:55,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0363: [2024-08-03 00:42:55,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0345: [2024-08-03 00:42:55,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
g0345: [2024-08-03 00:42:59,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=5120, skipped=2, lr=[8.946100906666666e-05, 8.946100906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5120 loss: 2.5856 iter time (s): 4.191 samples/sec: 30.544
g0364:  iteration     5120/10000000 | consumed samples:       655360 | consumed tokens:   1342177280 | elapsed time per iteration (ms): 4223.6 | learning rate: 8.946E-05 | global batch size:   128 | lm loss: 2.568982E+00 | loss scale: 524288.0 | grad norm: 0.991 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.557 | TFLOPs: 15.61 |
g0345: [2024-08-03 00:43:40,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=5130, skipped=2, lr=[8.963577173333334e-05, 8.963577173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5130 loss: 2.2038 iter time (s): 4.045 samples/sec: 31.645
g0364:  iteration     5130/10000000 | consumed samples:       656640 | consumed tokens:   1344798720 | elapsed time per iteration (ms): 4077.7 | learning rate: 8.964E-05 | global batch size:   128 | lm loss: 2.552725E+00 | loss scale: 524288.0 | grad norm: 1.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.390 | tokens per gpu per second (tgs): 2008.976 | TFLOPs: 16.17 |
g0345: [2024-08-03 00:44:23,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=5140, skipped=2, lr=[8.981053440000001e-05, 8.981053440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5140 loss: 2.7566 iter time (s): 4.219 samples/sec: 30.341
g0364:  iteration     5140/10000000 | consumed samples:       657920 | consumed tokens:   1347420160 | elapsed time per iteration (ms): 4268.4 | learning rate: 8.981E-05 | global batch size:   128 | lm loss: 2.595430E+00 | loss scale: 524288.0 | grad norm: 0.844 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.988 | tokens per gpu per second (tgs): 1919.224 | TFLOPs: 15.44 |
g0345: [2024-08-03 00:45:04,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=5150, skipped=2, lr=[8.998529706666668e-05, 8.998529706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5150 loss: 2.7080 iter time (s): 4.112 samples/sec: 31.132
g0364:  iteration     5150/10000000 | consumed samples:       659200 | consumed tokens:   1350041600 | elapsed time per iteration (ms): 4144.0 | learning rate: 8.999E-05 | global batch size:   128 | lm loss: 2.599447E+00 | loss scale: 524288.0 | grad norm: 0.803 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.888 | tokens per gpu per second (tgs): 1976.815 | TFLOPs: 15.91 |
g0345: [2024-08-03 00:45:46,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=5160, skipped=2, lr=[9.016005973333334e-05, 9.016005973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5160 loss: 2.4721 iter time (s): 4.118 samples/sec: 31.085
g0364:  iteration     5160/10000000 | consumed samples:       660480 | consumed tokens:   1352663040 | elapsed time per iteration (ms): 4154.0 | learning rate: 9.016E-05 | global batch size:   128 | lm loss: 2.590401E+00 | loss scale: 524288.0 | grad norm: 1.076 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.814 | tokens per gpu per second (tgs): 1972.080 | TFLOPs: 15.87 |
g0345: [2024-08-03 00:46:28,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=5170, skipped=2, lr=[9.033482240000001e-05, 9.033482240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5170 loss: 2.6001 iter time (s): 4.164 samples/sec: 30.742
g0364:  iteration     5170/10000000 | consumed samples:       661760 | consumed tokens:   1355284480 | elapsed time per iteration (ms): 4196.1 | learning rate: 9.033E-05 | global batch size:   128 | lm loss: 2.555488E+00 | loss scale: 524288.0 | grad norm: 0.937 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.505 | tokens per gpu per second (tgs): 1952.292 | TFLOPs: 15.71 |
g0345: [2024-08-03 00:47:09,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=5180, skipped=2, lr=[9.050958506666667e-05, 9.050958506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5180 loss: 2.4836 iter time (s): 4.126 samples/sec: 31.019
g0364:  iteration     5180/10000000 | consumed samples:       663040 | consumed tokens:   1357905920 | elapsed time per iteration (ms): 4159.2 | learning rate: 9.051E-05 | global batch size:   128 | lm loss: 2.519809E+00 | loss scale: 524288.0 | grad norm: 0.959 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.775 | tokens per gpu per second (tgs): 1969.620 | TFLOPs: 15.85 |
g0345: [2024-08-03 00:47:51,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=5190, skipped=2, lr=[9.068434773333334e-05, 9.068434773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5190 loss: 2.2574 iter time (s): 4.127 samples/sec: 31.016
g0364:  iteration     5190/10000000 | consumed samples:       664320 | consumed tokens:   1360527360 | elapsed time per iteration (ms): 4159.3 | learning rate: 9.068E-05 | global batch size:   128 | lm loss: 2.402435E+00 | loss scale: 524288.0 | grad norm: 0.960 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.775 | tokens per gpu per second (tgs): 1969.571 | TFLOPs: 15.85 |
g0345: [2024-08-03 00:48:32,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=5200, skipped=2, lr=[9.08591104e-05, 9.08591104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5200 loss: 2.5655 iter time (s): 4.103 samples/sec: 31.199
g0364:  iteration     5200/10000000 | consumed samples:       665600 | consumed tokens:   1363148800 | elapsed time per iteration (ms): 4135.3 | learning rate: 9.086E-05 | global batch size:   128 | lm loss: 2.567512E+00 | loss scale: 524288.0 | grad norm: 1.096 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.953 | tokens per gpu per second (tgs): 1980.991 | TFLOPs: 15.94 |
g0345: [2024-08-03 00:49:13,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=5210, skipped=2, lr=[9.103387306666667e-05, 9.103387306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5210 loss: 2.2908 iter time (s): 4.014 samples/sec: 31.885
g0364:  iteration     5210/10000000 | consumed samples:       666880 | consumed tokens:   1365770240 | elapsed time per iteration (ms): 4047.0 | learning rate: 9.103E-05 | global batch size:   128 | lm loss: 2.453000E+00 | loss scale: 524288.0 | grad norm: 1.136 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.628 | tokens per gpu per second (tgs): 2024.208 | TFLOPs: 16.29 |
g0345: [2024-08-03 00:49:54,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=5220, skipped=2, lr=[9.120863573333334e-05, 9.120863573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5220 loss: 2.5487 iter time (s): 4.114 samples/sec: 31.111
g0364:  iteration     5220/10000000 | consumed samples:       668160 | consumed tokens:   1368391680 | elapsed time per iteration (ms): 4149.0 | learning rate: 9.121E-05 | global batch size:   128 | lm loss: 2.424438E+00 | loss scale: 524288.0 | grad norm: 0.878 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.851 | tokens per gpu per second (tgs): 1974.440 | TFLOPs: 15.89 |
g0345: [2024-08-03 00:50:36,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=5230, skipped=2, lr=[9.13833984e-05, 9.13833984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5230 loss: 2.3191 iter time (s): 4.115 samples/sec: 31.102
g0364:  iteration     5230/10000000 | consumed samples:       669440 | consumed tokens:   1371013120 | elapsed time per iteration (ms): 4148.2 | learning rate: 9.138E-05 | global batch size:   128 | lm loss: 2.460894E+00 | loss scale: 524288.0 | grad norm: 0.984 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.814 | TFLOPs: 15.89 |
g0345: [2024-08-03 00:51:17,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=5240, skipped=2, lr=[9.155816106666667e-05, 9.155816106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5240 loss: 2.3125 iter time (s): 4.116 samples/sec: 31.099
g0364:  iteration     5240/10000000 | consumed samples:       670720 | consumed tokens:   1373634560 | elapsed time per iteration (ms): 4148.3 | learning rate: 9.156E-05 | global batch size:   128 | lm loss: 2.412282E+00 | loss scale: 524288.0 | grad norm: 1.015 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.795 | TFLOPs: 15.89 |
g0345: [2024-08-03 00:51:59,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=5250, skipped=2, lr=[9.173292373333334e-05, 9.173292373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5250 loss: 2.1290 iter time (s): 4.137 samples/sec: 30.941
g0364:  iteration     5250/10000000 | consumed samples:       672000 | consumed tokens:   1376256000 | elapsed time per iteration (ms): 4169.7 | learning rate: 9.173E-05 | global batch size:   128 | lm loss: 2.330689E+00 | loss scale: 524288.0 | grad norm: 1.458 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.697 | tokens per gpu per second (tgs): 1964.639 | TFLOPs: 15.81 |
g0345: [2024-08-03 00:52:41,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=5260, skipped=2, lr=[9.19076864e-05, 9.19076864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5260 loss: 2.3424 iter time (s): 4.156 samples/sec: 30.801
g0364:  iteration     5260/10000000 | consumed samples:       673280 | consumed tokens:   1378877440 | elapsed time per iteration (ms): 4189.4 | learning rate: 9.191E-05 | global batch size:   128 | lm loss: 2.413162E+00 | loss scale: 524288.0 | grad norm: 1.144 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.553 | tokens per gpu per second (tgs): 1955.415 | TFLOPs: 15.74 |
g0345: [2024-08-03 00:53:22,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=5270, skipped=2, lr=[9.208244906666667e-05, 9.208244906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5270 loss: 2.6676 iter time (s): 4.153 samples/sec: 30.818
g0364:  iteration     5270/10000000 | consumed samples:       674560 | consumed tokens:   1381498880 | elapsed time per iteration (ms): 4185.9 | learning rate: 9.208E-05 | global batch size:   128 | lm loss: 2.377402E+00 | loss scale: 524288.0 | grad norm: 1.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.579 | tokens per gpu per second (tgs): 1957.047 | TFLOPs: 15.75 |
g0345: [2024-08-03 00:54:04,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=5280, skipped=2, lr=[9.225721173333333e-05, 9.225721173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5280 loss: 2.3483 iter time (s): 4.145 samples/sec: 30.881
g0364:  iteration     5280/10000000 | consumed samples:       675840 | consumed tokens:   1384120320 | elapsed time per iteration (ms): 4177.4 | learning rate: 9.226E-05 | global batch size:   128 | lm loss: 2.372610E+00 | loss scale: 524288.0 | grad norm: 1.076 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.641 | tokens per gpu per second (tgs): 1961.038 | TFLOPs: 15.78 |
g0345: [2024-08-03 00:54:46,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=5290, skipped=2, lr=[9.24319744e-05, 9.24319744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5290 loss: 2.4148 iter time (s): 4.135 samples/sec: 30.953
g0364:  iteration     5290/10000000 | consumed samples:       677120 | consumed tokens:   1386741760 | elapsed time per iteration (ms): 4167.6 | learning rate: 9.243E-05 | global batch size:   128 | lm loss: 2.430160E+00 | loss scale: 524288.0 | grad norm: 1.118 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.713 | tokens per gpu per second (tgs): 1965.639 | TFLOPs: 15.82 |
g0345: [2024-08-03 00:55:27,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=5300, skipped=2, lr=[9.260673706666667e-05, 9.260673706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5300 loss: 2.1739 iter time (s): 4.065 samples/sec: 31.490
g0364:  iteration     5300/10000000 | consumed samples:       678400 | consumed tokens:   1389363200 | elapsed time per iteration (ms): 4097.7 | learning rate: 9.261E-05 | global batch size:   128 | lm loss: 2.371850E+00 | loss scale: 524288.0 | grad norm: 0.885 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.237 | tokens per gpu per second (tgs): 1999.172 | TFLOPs: 16.09 |
g0345: [2024-08-03 00:56:09,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=5310, skipped=2, lr=[9.278149973333335e-05, 9.278149973333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5310 loss: 2.3218 iter time (s): 4.220 samples/sec: 30.333
g0364:  iteration     5310/10000000 | consumed samples:       679680 | consumed tokens:   1391984640 | elapsed time per iteration (ms): 4252.6 | learning rate: 9.278E-05 | global batch size:   128 | lm loss: 2.296646E+00 | loss scale: 524288.0 | grad norm: 1.341 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.099 | tokens per gpu per second (tgs): 1926.351 | TFLOPs: 15.50 |
g0345: [2024-08-03 00:56:51,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=5320, skipped=2, lr=[9.295626240000001e-05, 9.295626240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5320 loss: 2.3053 iter time (s): 4.169 samples/sec: 30.701
g0364:  iteration     5320/10000000 | consumed samples:       680960 | consumed tokens:   1394606080 | elapsed time per iteration (ms): 4201.9 | learning rate: 9.296E-05 | global batch size:   128 | lm loss: 2.317220E+00 | loss scale: 524288.0 | grad norm: 0.906 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.462 | tokens per gpu per second (tgs): 1949.573 | TFLOPs: 15.69 |
g0345: [2024-08-03 00:57:33,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=5330, skipped=2, lr=[9.313102506666668e-05, 9.313102506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5330 loss: 2.5584 iter time (s): 4.098 samples/sec: 31.234
g0364:  iteration     5330/10000000 | consumed samples:       682240 | consumed tokens:   1397227520 | elapsed time per iteration (ms): 4131.0 | learning rate: 9.313E-05 | global batch size:   128 | lm loss: 2.262698E+00 | loss scale: 524288.0 | grad norm: 0.880 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.985 | tokens per gpu per second (tgs): 1983.068 | TFLOPs: 15.96 |
g0345: [2024-08-03 00:58:15,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=5340, skipped=2, lr=[9.330578773333334e-05, 9.330578773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5340 loss: 2.5584 iter time (s): 4.147 samples/sec: 30.867
g0364:  iteration     5340/10000000 | consumed samples:       683520 | consumed tokens:   1399848960 | elapsed time per iteration (ms): 4179.8 | learning rate: 9.331E-05 | global batch size:   128 | lm loss: 2.254549E+00 | loss scale: 524288.0 | grad norm: 0.848 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.624 | tokens per gpu per second (tgs): 1959.921 | TFLOPs: 15.77 |
g0345: [2024-08-03 00:58:56,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=5350, skipped=2, lr=[9.348055040000001e-05, 9.348055040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5350 loss: 2.2124 iter time (s): 4.146 samples/sec: 30.873
g0364:  iteration     5350/10000000 | consumed samples:       684800 | consumed tokens:   1402470400 | elapsed time per iteration (ms): 4178.9 | learning rate: 9.348E-05 | global batch size:   128 | lm loss: 2.250788E+00 | loss scale: 524288.0 | grad norm: 0.945 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.630 | tokens per gpu per second (tgs): 1960.315 | TFLOPs: 15.77 |
g0345: [2024-08-03 00:59:38,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=5360, skipped=2, lr=[9.365531306666668e-05, 9.365531306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5360 loss: 2.3136 iter time (s): 4.182 samples/sec: 30.609
g0364:  iteration     5360/10000000 | consumed samples:       686080 | consumed tokens:   1405091840 | elapsed time per iteration (ms): 4214.4 | learning rate: 9.366E-05 | global batch size:   128 | lm loss: 2.297182E+00 | loss scale: 524288.0 | grad norm: 1.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.372 | tokens per gpu per second (tgs): 1943.816 | TFLOPs: 15.64 |
g0345: [2024-08-03 01:00:20,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=5370, skipped=2, lr=[9.383007573333334e-05, 9.383007573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5370 loss: 2.4463 iter time (s): 4.084 samples/sec: 31.343
g0364:  iteration     5370/10000000 | consumed samples:       687360 | consumed tokens:   1407713280 | elapsed time per iteration (ms): 4116.1 | learning rate: 9.383E-05 | global batch size:   128 | lm loss: 2.288292E+00 | loss scale: 524288.0 | grad norm: 0.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.097 | tokens per gpu per second (tgs): 1990.240 | TFLOPs: 16.02 |
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 5371
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 5371
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 5371
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 5371
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 5371
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 5371
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: Grad overflow on iteration 5371
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 5371
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 5371
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: Grad overflow on iteration 5371
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 5371
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 5371
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 5371
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 5371
g0364: Grad overflow on iteration 5371
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 5371
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 5371
g0345: Grad overflow on iteration 5371
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 5371
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 5371
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 5371
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: Grad overflow on iteration 5371
g0352: Grad overflow on iteration 5371
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 5371
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 5371
g0358: Grad overflow on iteration 5371
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: Grad overflow on iteration 5371
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: Grad overflow on iteration 5371
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 5371
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 5371
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 5371
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: Grad overflow on iteration 5371
g0362: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 01:00:28,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 01:00:28,383] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0345: [2024-08-03 01:01:01,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=5380, skipped=3, lr=[9.400483840000001e-05, 9.400483840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5380 loss: 2.1578 iter time (s): 4.137 samples/sec: 30.941
g0364:  iteration     5380/10000000 | consumed samples:       688640 | consumed tokens:   1410334720 | elapsed time per iteration (ms): 4169.0 | learning rate: 9.400E-05 | global batch size:   128 | lm loss: 2.241154E+00 | loss scale: 262144.0 | grad norm: 1.049 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.702 | tokens per gpu per second (tgs): 1964.957 | TFLOPs: 15.81 |
g0345: [2024-08-03 01:01:44,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=5390, skipped=3, lr=[9.417960106666667e-05, 9.417960106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5390 loss: 2.2908 iter time (s): 4.217 samples/sec: 30.355
g0364:  iteration     5390/10000000 | consumed samples:       689920 | consumed tokens:   1412956160 | elapsed time per iteration (ms): 4249.3 | learning rate: 9.418E-05 | global batch size:   128 | lm loss: 2.259276E+00 | loss scale: 262144.0 | grad norm: 0.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.122 | tokens per gpu per second (tgs): 1927.832 | TFLOPs: 15.51 |
g0345: [2024-08-03 01:02:26,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=5400, skipped=3, lr=[9.435436373333334e-05, 9.435436373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5400 loss: 2.1028 iter time (s): 4.208 samples/sec: 30.416
g0364:  iteration     5400/10000000 | consumed samples:       691200 | consumed tokens:   1415577600 | elapsed time per iteration (ms): 4241.6 | learning rate: 9.435E-05 | global batch size:   128 | lm loss: 2.312060E+00 | loss scale: 262144.0 | grad norm: 1.003 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.177 | tokens per gpu per second (tgs): 1931.337 | TFLOPs: 15.54 |
g0345: [2024-08-03 01:03:07,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=5410, skipped=3, lr=[9.452912640000001e-05, 9.452912640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5410 loss: 2.2424 iter time (s): 4.092 samples/sec: 31.279
g0364:  iteration     5410/10000000 | consumed samples:       692480 | consumed tokens:   1418199040 | elapsed time per iteration (ms): 4125.1 | learning rate: 9.453E-05 | global batch size:   128 | lm loss: 2.333860E+00 | loss scale: 262144.0 | grad norm: 0.905 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.030 | tokens per gpu per second (tgs): 1985.911 | TFLOPs: 15.98 |
g0345: [2024-08-03 01:03:49,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=5420, skipped=3, lr=[9.470388906666667e-05, 9.470388906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5420 loss: 2.3472 iter time (s): 4.115 samples/sec: 31.106
g0364:  iteration     5420/10000000 | consumed samples:       693760 | consumed tokens:   1420820480 | elapsed time per iteration (ms): 4148.3 | learning rate: 9.470E-05 | global batch size:   128 | lm loss: 2.305808E+00 | loss scale: 262144.0 | grad norm: 0.908 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.777 | TFLOPs: 15.89 |
g0345: [2024-08-03 01:04:31,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=5430, skipped=3, lr=[9.487865173333334e-05, 9.487865173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5430 loss: 2.1572 iter time (s): 4.149 samples/sec: 30.854
g0364:  iteration     5430/10000000 | consumed samples:       695040 | consumed tokens:   1423441920 | elapsed time per iteration (ms): 4181.9 | learning rate: 9.488E-05 | global batch size:   128 | lm loss: 2.230258E+00 | loss scale: 262144.0 | grad norm: 0.989 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.608 | tokens per gpu per second (tgs): 1958.923 | TFLOPs: 15.76 |
g0345: [2024-08-03 01:05:13,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=5440, skipped=3, lr=[9.50534144e-05, 9.50534144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5440 loss: 2.3696 iter time (s): 4.187 samples/sec: 30.567
g0364:  iteration     5440/10000000 | consumed samples:       696320 | consumed tokens:   1426063360 | elapsed time per iteration (ms): 4220.5 | learning rate: 9.505E-05 | global batch size:   128 | lm loss: 2.261367E+00 | loss scale: 262144.0 | grad norm: 1.044 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.328 | tokens per gpu per second (tgs): 1941.020 | TFLOPs: 15.62 |
g0345: [2024-08-03 01:05:54,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=5450, skipped=3, lr=[9.522817706666667e-05, 9.522817706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5450 loss: 2.1504 iter time (s): 4.120 samples/sec: 31.069
g0364:  iteration     5450/10000000 | consumed samples:       697600 | consumed tokens:   1428684800 | elapsed time per iteration (ms): 4152.1 | learning rate: 9.523E-05 | global batch size:   128 | lm loss: 2.279409E+00 | loss scale: 262144.0 | grad norm: 0.847 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.828 | tokens per gpu per second (tgs): 1972.989 | TFLOPs: 15.88 |
g0345: [2024-08-03 01:06:36,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=5460, skipped=3, lr=[9.540293973333334e-05, 9.540293973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5460 loss: 2.0729 iter time (s): 4.151 samples/sec: 30.836
g0364:  iteration     5460/10000000 | consumed samples:       698880 | consumed tokens:   1431306240 | elapsed time per iteration (ms): 4183.2 | learning rate: 9.540E-05 | global batch size:   128 | lm loss: 2.209600E+00 | loss scale: 262144.0 | grad norm: 1.293 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.599 | tokens per gpu per second (tgs): 1958.307 | TFLOPs: 15.76 |
g0345: [2024-08-03 01:07:19,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=5470, skipped=3, lr=[9.55777024e-05, 9.55777024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5470 loss: 2.3928 iter time (s): 4.238 samples/sec: 30.205
g0364:  iteration     5470/10000000 | consumed samples:       700160 | consumed tokens:   1433927680 | elapsed time per iteration (ms): 4270.6 | learning rate: 9.558E-05 | global batch size:   128 | lm loss: 2.194962E+00 | loss scale: 262144.0 | grad norm: 0.961 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.973 | tokens per gpu per second (tgs): 1918.244 | TFLOPs: 15.44 |
g0345: [2024-08-03 01:08:01,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=5480, skipped=3, lr=[9.575246506666667e-05, 9.575246506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5480 loss: 2.3103 iter time (s): 4.159 samples/sec: 30.775
g0364:  iteration     5480/10000000 | consumed samples:       701440 | consumed tokens:   1436549120 | elapsed time per iteration (ms): 4192.2 | learning rate: 9.575E-05 | global batch size:   128 | lm loss: 2.189722E+00 | loss scale: 262144.0 | grad norm: 0.887 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.533 | tokens per gpu per second (tgs): 1954.084 | TFLOPs: 15.72 |
g0345: [2024-08-03 01:08:43,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=5490, skipped=3, lr=[9.592722773333335e-05, 9.592722773333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5490 loss: 2.1243 iter time (s): 4.135 samples/sec: 30.956
g0364:  iteration     5490/10000000 | consumed samples:       702720 | consumed tokens:   1439170560 | elapsed time per iteration (ms): 4167.2 | learning rate: 9.593E-05 | global batch size:   128 | lm loss: 2.244868E+00 | loss scale: 262144.0 | grad norm: 0.864 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.716 | tokens per gpu per second (tgs): 1965.820 | TFLOPs: 15.82 |
g0345: [2024-08-03 01:09:25,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=5500, skipped=3, lr=[9.610199040000002e-05, 9.610199040000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5500 loss: 2.3037 iter time (s): 4.158 samples/sec: 30.785
g0364:  iteration     5500/10000000 | consumed samples:       704000 | consumed tokens:   1441792000 | elapsed time per iteration (ms): 4190.8 | learning rate: 9.610E-05 | global batch size:   128 | lm loss: 2.225440E+00 | loss scale: 262144.0 | grad norm: 0.962 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.543 | tokens per gpu per second (tgs): 1954.778 | TFLOPs: 15.73 |
g0345: [2024-08-03 01:10:06,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=5510, skipped=3, lr=[9.627675306666668e-05, 9.627675306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5510 loss: 2.0902 iter time (s): 4.156 samples/sec: 30.797
g0364:  iteration     5510/10000000 | consumed samples:       705280 | consumed tokens:   1444413440 | elapsed time per iteration (ms): 4189.4 | learning rate: 9.628E-05 | global batch size:   128 | lm loss: 2.195908E+00 | loss scale: 262144.0 | grad norm: 1.084 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.554 | tokens per gpu per second (tgs): 1955.428 | TFLOPs: 15.74 |
g0345: [2024-08-03 01:10:48,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=5520, skipped=3, lr=[9.645151573333335e-05, 9.645151573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5520 loss: 2.1303 iter time (s): 4.142 samples/sec: 30.905
g0364:  iteration     5520/10000000 | consumed samples:       706560 | consumed tokens:   1447034880 | elapsed time per iteration (ms): 4174.2 | learning rate: 9.645E-05 | global batch size:   128 | lm loss: 2.136897E+00 | loss scale: 262144.0 | grad norm: 0.896 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.665 | tokens per gpu per second (tgs): 1962.534 | TFLOPs: 15.79 |
g0345: [2024-08-03 01:11:30,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=5530, skipped=3, lr=[9.662627840000001e-05, 9.662627840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5530 loss: 2.2138 iter time (s): 4.183 samples/sec: 30.600
g0364:  iteration     5530/10000000 | consumed samples:       707840 | consumed tokens:   1449656320 | elapsed time per iteration (ms): 4215.7 | learning rate: 9.663E-05 | global batch size:   128 | lm loss: 2.167620E+00 | loss scale: 262144.0 | grad norm: 0.944 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.362 | tokens per gpu per second (tgs): 1943.189 | TFLOPs: 15.64 |
g0345: [2024-08-03 01:12:12,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=5540, skipped=3, lr=[9.680104106666668e-05, 9.680104106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5540 loss: 2.0133 iter time (s): 4.088 samples/sec: 31.311
g0364:  iteration     5540/10000000 | consumed samples:       709120 | consumed tokens:   1452277760 | elapsed time per iteration (ms): 4120.5 | learning rate: 9.680E-05 | global batch size:   128 | lm loss: 2.187075E+00 | loss scale: 262144.0 | grad norm: 0.884 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.064 | tokens per gpu per second (tgs): 1988.097 | TFLOPs: 16.00 |
g0345: [2024-08-03 01:12:54,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=5550, skipped=3, lr=[9.697580373333335e-05, 9.697580373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5550 loss: 2.0639 iter time (s): 4.192 samples/sec: 30.533
g0364:  iteration     5550/10000000 | consumed samples:       710400 | consumed tokens:   1454899200 | elapsed time per iteration (ms): 4225.0 | learning rate: 9.698E-05 | global batch size:   128 | lm loss: 2.118350E+00 | loss scale: 262144.0 | grad norm: 0.900 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.296 | tokens per gpu per second (tgs): 1938.934 | TFLOPs: 15.60 |
g0345: [2024-08-03 01:13:36,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=5560, skipped=3, lr=[9.715056640000001e-05, 9.715056640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5560 loss: 2.1477 iter time (s): 4.149 samples/sec: 30.854
g0364:  iteration     5560/10000000 | consumed samples:       711680 | consumed tokens:   1457520640 | elapsed time per iteration (ms): 4181.0 | learning rate: 9.715E-05 | global batch size:   128 | lm loss: 2.244471E+00 | loss scale: 262144.0 | grad norm: 0.831 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.614 | tokens per gpu per second (tgs): 1959.323 | TFLOPs: 15.77 |
g0345: [2024-08-03 01:14:18,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=5570, skipped=3, lr=[9.732532906666668e-05, 9.732532906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5570 loss: 2.2642 iter time (s): 4.211 samples/sec: 30.394
g0364:  iteration     5570/10000000 | consumed samples:       712960 | consumed tokens:   1460142080 | elapsed time per iteration (ms): 4244.1 | learning rate: 9.733E-05 | global batch size:   128 | lm loss: 2.244691E+00 | loss scale: 262144.0 | grad norm: 1.152 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.159 | tokens per gpu per second (tgs): 1930.190 | TFLOPs: 15.53 |
g0345: [2024-08-03 01:15:00,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=5580, skipped=3, lr=[9.750009173333334e-05, 9.750009173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5580 loss: 2.3380 iter time (s): 4.173 samples/sec: 30.673
g0364:  iteration     5580/10000000 | consumed samples:       714240 | consumed tokens:   1462763520 | elapsed time per iteration (ms): 4206.0 | learning rate: 9.750E-05 | global batch size:   128 | lm loss: 2.188794E+00 | loss scale: 262144.0 | grad norm: 0.986 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.432 | tokens per gpu per second (tgs): 1947.672 | TFLOPs: 15.67 |
g0345: [2024-08-03 01:15:42,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=5590, skipped=3, lr=[9.767485440000001e-05, 9.767485440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5590 loss: 2.2517 iter time (s): 4.144 samples/sec: 30.887
g0364:  iteration     5590/10000000 | consumed samples:       715520 | consumed tokens:   1465384960 | elapsed time per iteration (ms): 4176.7 | learning rate: 9.767E-05 | global batch size:   128 | lm loss: 2.232418E+00 | loss scale: 262144.0 | grad norm: 1.060 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.646 | tokens per gpu per second (tgs): 1961.336 | TFLOPs: 15.78 |
g0345: [2024-08-03 01:16:23,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=5600, skipped=3, lr=[9.784961706666668e-05, 9.784961706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5600 loss: 2.3554 iter time (s): 4.125 samples/sec: 31.030
g0364:  iteration     5600/10000000 | consumed samples:       716800 | consumed tokens:   1468006400 | elapsed time per iteration (ms): 4157.9 | learning rate: 9.785E-05 | global batch size:   128 | lm loss: 2.249812E+00 | loss scale: 262144.0 | grad norm: 1.033 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.785 | tokens per gpu per second (tgs): 1970.247 | TFLOPs: 15.85 |
g0345: [2024-08-03 01:17:05,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=5610, skipped=3, lr=[9.802437973333334e-05, 9.802437973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5610 loss: 2.0263 iter time (s): 4.167 samples/sec: 30.721
g0364:  iteration     5610/10000000 | consumed samples:       718080 | consumed tokens:   1470627840 | elapsed time per iteration (ms): 4199.3 | learning rate: 9.802E-05 | global batch size:   128 | lm loss: 2.163972E+00 | loss scale: 262144.0 | grad norm: 1.022 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.481 | tokens per gpu per second (tgs): 1950.799 | TFLOPs: 15.70 |
g0345: [2024-08-03 01:17:47,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=5620, skipped=3, lr=[9.819914240000001e-05, 9.819914240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5620 loss: 2.3327 iter time (s): 4.169 samples/sec: 30.702
g0364:  iteration     5620/10000000 | consumed samples:       719360 | consumed tokens:   1473249280 | elapsed time per iteration (ms): 4201.5 | learning rate: 9.820E-05 | global batch size:   128 | lm loss: 2.126239E+00 | loss scale: 262144.0 | grad norm: 1.115 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.465 | tokens per gpu per second (tgs): 1949.763 | TFLOPs: 15.69 |
g0345: [2024-08-03 01:18:29,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=5630, skipped=3, lr=[9.837390506666667e-05, 9.837390506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5630 loss: 2.1121 iter time (s): 4.134 samples/sec: 30.965
g0364:  iteration     5630/10000000 | consumed samples:       720640 | consumed tokens:   1475870720 | elapsed time per iteration (ms): 4166.8 | learning rate: 9.837E-05 | global batch size:   128 | lm loss: 2.144230E+00 | loss scale: 262144.0 | grad norm: 1.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.719 | tokens per gpu per second (tgs): 1965.999 | TFLOPs: 15.82 |
g0345: [2024-08-03 01:19:11,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=5640, skipped=3, lr=[9.854866773333334e-05, 9.854866773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5640 loss: 2.1488 iter time (s): 4.161 samples/sec: 30.764
g0364:  iteration     5640/10000000 | consumed samples:       721920 | consumed tokens:   1478492160 | elapsed time per iteration (ms): 4193.0 | learning rate: 9.855E-05 | global batch size:   128 | lm loss: 2.164099E+00 | loss scale: 262144.0 | grad norm: 1.364 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.527 | tokens per gpu per second (tgs): 1953.716 | TFLOPs: 15.72 |
g0345: [2024-08-03 01:19:53,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=5650, skipped=3, lr=[9.87234304e-05, 9.87234304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5650 loss: 2.2177 iter time (s): 4.146 samples/sec: 30.872
g0364:  iteration     5650/10000000 | consumed samples:       723200 | consumed tokens:   1481113600 | elapsed time per iteration (ms): 4178.5 | learning rate: 9.872E-05 | global batch size:   128 | lm loss: 2.142662E+00 | loss scale: 262144.0 | grad norm: 0.773 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.633 | tokens per gpu per second (tgs): 1960.501 | TFLOPs: 15.78 |
g0345: [2024-08-03 01:20:36,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=5660, skipped=3, lr=[9.889819306666667e-05, 9.889819306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5660 loss: 2.1414 iter time (s): 4.241 samples/sec: 30.179
g0364:  iteration     5660/10000000 | consumed samples:       724480 | consumed tokens:   1483735040 | elapsed time per iteration (ms): 4274.1 | learning rate: 9.890E-05 | global batch size:   128 | lm loss: 2.186217E+00 | loss scale: 262144.0 | grad norm: 0.930 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.948 | tokens per gpu per second (tgs): 1916.645 | TFLOPs: 15.42 |
g0345: [2024-08-03 01:21:18,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=5670, skipped=3, lr=[9.907295573333335e-05, 9.907295573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5670 loss: 1.9457 iter time (s): 4.173 samples/sec: 30.675
g0364:  iteration     5670/10000000 | consumed samples:       725760 | consumed tokens:   1486356480 | elapsed time per iteration (ms): 4206.2 | learning rate: 9.907E-05 | global batch size:   128 | lm loss: 2.120072E+00 | loss scale: 262144.0 | grad norm: 0.857 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.432 | tokens per gpu per second (tgs): 1947.617 | TFLOPs: 15.67 |
g0345: [2024-08-03 01:22:00,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=5680, skipped=3, lr=[9.924771840000002e-05, 9.924771840000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5680 loss: 2.2311 iter time (s): 4.163 samples/sec: 30.747
g0364:  iteration     5680/10000000 | consumed samples:       727040 | consumed tokens:   1488977920 | elapsed time per iteration (ms): 4195.5 | learning rate: 9.925E-05 | global batch size:   128 | lm loss: 2.071244E+00 | loss scale: 262144.0 | grad norm: 0.856 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.509 | tokens per gpu per second (tgs): 1952.557 | TFLOPs: 15.71 |
g0345: [2024-08-03 01:22:41,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=5690, skipped=3, lr=[9.942248106666668e-05, 9.942248106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5690 loss: 2.4356 iter time (s): 4.112 samples/sec: 31.128
g0364:  iteration     5690/10000000 | consumed samples:       728320 | consumed tokens:   1491599360 | elapsed time per iteration (ms): 4145.0 | learning rate: 9.942E-05 | global batch size:   128 | lm loss: 2.166339E+00 | loss scale: 262144.0 | grad norm: 0.932 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.881 | tokens per gpu per second (tgs): 1976.367 | TFLOPs: 15.90 |
g0345: [2024-08-03 01:23:23,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=5700, skipped=3, lr=[9.959724373333335e-05, 9.959724373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5700 loss: 2.2208 iter time (s): 4.182 samples/sec: 30.609
g0364:  iteration     5700/10000000 | consumed samples:       729600 | consumed tokens:   1494220800 | elapsed time per iteration (ms): 4214.9 | learning rate: 9.960E-05 | global batch size:   128 | lm loss: 2.110473E+00 | loss scale: 262144.0 | grad norm: 0.891 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.369 | tokens per gpu per second (tgs): 1943.585 | TFLOPs: 15.64 |
g0345: [2024-08-03 01:24:04,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=5710, skipped=3, lr=[9.977200640000002e-05, 9.977200640000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5710 loss: 2.0975 iter time (s): 4.055 samples/sec: 31.563
g0364:  iteration     5710/10000000 | consumed samples:       730880 | consumed tokens:   1496842240 | elapsed time per iteration (ms): 4088.1 | learning rate: 9.977E-05 | global batch size:   128 | lm loss: 2.192838E+00 | loss scale: 262144.0 | grad norm: 0.738 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.310 | tokens per gpu per second (tgs): 2003.857 | TFLOPs: 16.13 |
g0345: [2024-08-03 01:24:46,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=5720, skipped=3, lr=[9.994676906666668e-05, 9.994676906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5720 loss: 2.2538 iter time (s): 4.161 samples/sec: 30.762
g0364:  iteration     5720/10000000 | consumed samples:       732160 | consumed tokens:   1499463680 | elapsed time per iteration (ms): 4193.5 | learning rate: 9.995E-05 | global batch size:   128 | lm loss: 2.184935E+00 | loss scale: 262144.0 | grad norm: 0.941 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.524 | tokens per gpu per second (tgs): 1953.514 | TFLOPs: 15.72 |
g0345: [2024-08-03 01:25:27,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=5730, skipped=3, lr=[0.00010012153173333335, 0.00010012153173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5730 loss: 2.2973 iter time (s): 4.037 samples/sec: 31.704
g0364:  iteration     5730/10000000 | consumed samples:       733440 | consumed tokens:   1502085120 | elapsed time per iteration (ms): 4069.7 | learning rate: 1.001E-04 | global batch size:   128 | lm loss: 2.153344E+00 | loss scale: 262144.0 | grad norm: 0.806 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.452 | tokens per gpu per second (tgs): 2012.928 | TFLOPs: 16.20 |
g0345: [2024-08-03 01:26:07,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=5740, skipped=3, lr=[0.00010029629440000001, 0.00010029629440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5740 loss: 2.3035 iter time (s): 3.974 samples/sec: 32.210
g0364:  iteration     5740/10000000 | consumed samples:       734720 | consumed tokens:   1504706560 | elapsed time per iteration (ms): 4006.6 | learning rate: 1.003E-04 | global batch size:   128 | lm loss: 2.104229E+00 | loss scale: 262144.0 | grad norm: 0.742 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.947 | tokens per gpu per second (tgs): 2044.636 | TFLOPs: 16.45 |
g0345: [2024-08-03 01:26:48,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=5750, skipped=3, lr=[0.00010047105706666668, 0.00010047105706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5750 loss: 2.2704 iter time (s): 4.087 samples/sec: 31.319
g0364:  iteration     5750/10000000 | consumed samples:       736000 | consumed tokens:   1507328000 | elapsed time per iteration (ms): 4119.8 | learning rate: 1.005E-04 | global batch size:   128 | lm loss: 2.141779E+00 | loss scale: 262144.0 | grad norm: 0.824 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.069 | tokens per gpu per second (tgs): 1988.445 | TFLOPs: 16.00 |
g0345: [2024-08-03 01:27:29,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=5760, skipped=3, lr=[0.00010064581973333335, 0.00010064581973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5760 loss: 2.4329 iter time (s): 4.085 samples/sec: 31.335
g0364:  iteration     5760/10000000 | consumed samples:       737280 | consumed tokens:   1509949440 | elapsed time per iteration (ms): 4117.5 | learning rate: 1.006E-04 | global batch size:   128 | lm loss: 2.186670E+00 | loss scale: 262144.0 | grad norm: 0.851 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.087 | tokens per gpu per second (tgs): 1989.538 | TFLOPs: 16.01 |
g0345: [2024-08-03 01:28:10,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=5770, skipped=3, lr=[0.00010082058240000001, 0.00010082058240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5770 loss: 2.5054 iter time (s): 4.053 samples/sec: 31.579
g0364:  iteration     5770/10000000 | consumed samples:       738560 | consumed tokens:   1512570880 | elapsed time per iteration (ms): 4086.9 | learning rate: 1.008E-04 | global batch size:   128 | lm loss: 2.212173E+00 | loss scale: 262144.0 | grad norm: 0.890 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.320 | tokens per gpu per second (tgs): 2004.471 | TFLOPs: 16.13 |
g0345: [2024-08-03 01:28:51,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=5780, skipped=3, lr=[0.00010099534506666668, 0.00010099534506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5780 loss: 2.1256 iter time (s): 4.067 samples/sec: 31.471
g0364:  iteration     5780/10000000 | consumed samples:       739840 | consumed tokens:   1515192320 | elapsed time per iteration (ms): 4100.4 | learning rate: 1.010E-04 | global batch size:   128 | lm loss: 2.184743E+00 | loss scale: 262144.0 | grad norm: 0.688 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.216 | tokens per gpu per second (tgs): 1997.852 | TFLOPs: 16.08 |
g0345: [2024-08-03 01:29:33,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=5790, skipped=3, lr=[0.00010117010773333334, 0.00010117010773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5790 loss: 2.0640 iter time (s): 4.159 samples/sec: 30.774
g0364:  iteration     5790/10000000 | consumed samples:       741120 | consumed tokens:   1517813760 | elapsed time per iteration (ms): 4192.1 | learning rate: 1.012E-04 | global batch size:   128 | lm loss: 2.077109E+00 | loss scale: 262144.0 | grad norm: 0.978 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.533 | tokens per gpu per second (tgs): 1954.130 | TFLOPs: 15.73 |
g0345: [2024-08-03 01:30:14,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=5800, skipped=3, lr=[0.00010134487040000001, 0.00010134487040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5800 loss: 2.1616 iter time (s): 4.052 samples/sec: 31.592
g0364:  iteration     5800/10000000 | consumed samples:       742400 | consumed tokens:   1520435200 | elapsed time per iteration (ms): 4084.4 | learning rate: 1.013E-04 | global batch size:   128 | lm loss: 2.139912E+00 | loss scale: 262144.0 | grad norm: 0.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.339 | tokens per gpu per second (tgs): 2005.684 | TFLOPs: 16.14 |
g0345: [2024-08-03 01:30:55,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=5810, skipped=3, lr=[0.00010151963306666668, 0.00010151963306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5810 loss: 2.0836 iter time (s): 4.074 samples/sec: 31.416
g0364:  iteration     5810/10000000 | consumed samples:       743680 | consumed tokens:   1523056640 | elapsed time per iteration (ms): 4107.0 | learning rate: 1.015E-04 | global batch size:   128 | lm loss: 2.164502E+00 | loss scale: 262144.0 | grad norm: 0.724 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.166 | tokens per gpu per second (tgs): 1994.625 | TFLOPs: 16.05 |
g0345: [2024-08-03 01:31:36,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=5820, skipped=3, lr=[0.00010169439573333333, 0.00010169439573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5820 loss: 2.4617 iter time (s): 4.060 samples/sec: 31.530
g0364:  iteration     5820/10000000 | consumed samples:       744960 | consumed tokens:   1525678080 | elapsed time per iteration (ms): 4092.3 | learning rate: 1.017E-04 | global batch size:   128 | lm loss: 2.065491E+00 | loss scale: 262144.0 | grad norm: 0.685 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.278 | tokens per gpu per second (tgs): 2001.815 | TFLOPs: 16.11 |
g0345: [2024-08-03 01:32:18,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=5830, skipped=3, lr=[0.0001018691584, 0.0001018691584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5830 loss: 2.1096 iter time (s): 4.162 samples/sec: 30.753
g0364:  iteration     5830/10000000 | consumed samples:       746240 | consumed tokens:   1528299520 | elapsed time per iteration (ms): 4194.8 | learning rate: 1.019E-04 | global batch size:   128 | lm loss: 2.038961E+00 | loss scale: 262144.0 | grad norm: 0.863 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.514 | tokens per gpu per second (tgs): 1952.879 | TFLOPs: 15.72 |
g0345: [2024-08-03 01:32:59,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=5840, skipped=3, lr=[0.00010204392106666666, 0.00010204392106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5840 loss: 2.1043 iter time (s): 4.074 samples/sec: 31.422
g0364:  iteration     5840/10000000 | consumed samples:       747520 | consumed tokens:   1530920960 | elapsed time per iteration (ms): 4106.0 | learning rate: 1.020E-04 | global batch size:   128 | lm loss: 2.140485E+00 | loss scale: 262144.0 | grad norm: 0.983 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.174 | tokens per gpu per second (tgs): 1995.138 | TFLOPs: 16.06 |
g0345: [2024-08-03 01:33:40,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=5850, skipped=3, lr=[0.00010221868373333333, 0.00010221868373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5850 loss: 2.2609 iter time (s): 4.093 samples/sec: 31.272
g0364:  iteration     5850/10000000 | consumed samples:       748800 | consumed tokens:   1533542400 | elapsed time per iteration (ms): 4126.1 | learning rate: 1.022E-04 | global batch size:   128 | lm loss: 2.082500E+00 | loss scale: 262144.0 | grad norm: 0.646 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.022 | tokens per gpu per second (tgs): 1985.402 | TFLOPs: 15.98 |
g0345: [2024-08-03 01:34:21,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=5860, skipped=3, lr=[0.0001023934464, 0.0001023934464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5860 loss: 2.2897 iter time (s): 4.074 samples/sec: 31.421
g0364:  iteration     5860/10000000 | consumed samples:       750080 | consumed tokens:   1536163840 | elapsed time per iteration (ms): 4107.0 | learning rate: 1.024E-04 | global batch size:   128 | lm loss: 2.153150E+00 | loss scale: 262144.0 | grad norm: 0.856 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.166 | tokens per gpu per second (tgs): 1994.646 | TFLOPs: 16.05 |
g0345: [2024-08-03 01:35:03,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=5870, skipped=3, lr=[0.00010256820906666666, 0.00010256820906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5870 loss: 2.1349 iter time (s): 4.169 samples/sec: 30.702
g0364:  iteration     5870/10000000 | consumed samples:       751360 | consumed tokens:   1538785280 | elapsed time per iteration (ms): 4203.1 | learning rate: 1.026E-04 | global batch size:   128 | lm loss: 2.060225E+00 | loss scale: 262144.0 | grad norm: 0.840 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.454 | tokens per gpu per second (tgs): 1949.033 | TFLOPs: 15.68 |
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 01:35:15,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 01:35:15,840] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 01:35:15,840] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 01:35:15,840] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 01:35:44,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=5880, skipped=3, lr=[0.00010274297173333333, 0.00010274297173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5880 loss: 1.8593 iter time (s): 4.056 samples/sec: 31.558
g0364:  iteration     5880/10000000 | consumed samples:       752640 | consumed tokens:   1541406720 | elapsed time per iteration (ms): 4089.7 | learning rate: 1.027E-04 | global batch size:   128 | lm loss: 2.077843E+00 | loss scale: 524288.0 | grad norm: 0.840 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.298 | tokens per gpu per second (tgs): 2003.068 | TFLOPs: 16.12 |
g0345: [2024-08-03 01:36:25,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=5890, skipped=3, lr=[0.00010291773439999999, 0.00010291773439999999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5890 loss: 2.4167 iter time (s): 4.082 samples/sec: 31.359
g0364:  iteration     5890/10000000 | consumed samples:       753920 | consumed tokens:   1544028160 | elapsed time per iteration (ms): 4115.7 | learning rate: 1.029E-04 | global batch size:   128 | lm loss: 2.079922E+00 | loss scale: 524288.0 | grad norm: 0.835 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.101 | tokens per gpu per second (tgs): 1990.441 | TFLOPs: 16.02 |
g0345: [2024-08-03 01:37:06,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=5900, skipped=3, lr=[0.00010309249706666666, 0.00010309249706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5900 loss: 1.9761 iter time (s): 4.094 samples/sec: 31.265
g0364:  iteration     5900/10000000 | consumed samples:       755200 | consumed tokens:   1546649600 | elapsed time per iteration (ms): 4126.9 | learning rate: 1.031E-04 | global batch size:   128 | lm loss: 2.142577E+00 | loss scale: 524288.0 | grad norm: 0.865 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.016 | tokens per gpu per second (tgs): 1985.017 | TFLOPs: 15.97 |
g0345: [2024-08-03 01:37:48,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=5910, skipped=3, lr=[0.00010326725973333332, 0.00010326725973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5910 loss: 2.2263 iter time (s): 4.084 samples/sec: 31.339
g0364:  iteration     5910/10000000 | consumed samples:       756480 | consumed tokens:   1549271040 | elapsed time per iteration (ms): 4117.2 | learning rate: 1.033E-04 | global batch size:   128 | lm loss: 2.089626E+00 | loss scale: 524288.0 | grad norm: 1.028 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.089 | tokens per gpu per second (tgs): 1989.689 | TFLOPs: 16.01 |
g0345: [2024-08-03 01:38:28,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=5920, skipped=3, lr=[0.0001034420224, 0.0001034420224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5920 loss: 1.9912 iter time (s): 4.035 samples/sec: 31.725
g0364:  iteration     5920/10000000 | consumed samples:       757760 | consumed tokens:   1551892480 | elapsed time per iteration (ms): 4066.9 | learning rate: 1.034E-04 | global batch size:   128 | lm loss: 2.092854E+00 | loss scale: 524288.0 | grad norm: 0.870 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.473 | tokens per gpu per second (tgs): 2014.294 | TFLOPs: 16.21 |
g0345: [2024-08-03 01:39:09,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=5930, skipped=3, lr=[0.00010361678506666667, 0.00010361678506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5930 loss: 2.0831 iter time (s): 4.052 samples/sec: 31.586
g0364:  iteration     5930/10000000 | consumed samples:       759040 | consumed tokens:   1554513920 | elapsed time per iteration (ms): 4085.3 | learning rate: 1.036E-04 | global batch size:   128 | lm loss: 2.099088E+00 | loss scale: 524288.0 | grad norm: 0.677 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.332 | tokens per gpu per second (tgs): 2005.246 | TFLOPs: 16.14 |
g0345: [2024-08-03 01:39:50,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=5940, skipped=3, lr=[0.00010379154773333334, 0.00010379154773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5940 loss: 2.2162 iter time (s): 4.016 samples/sec: 31.876
g0364:  iteration     5940/10000000 | consumed samples:       760320 | consumed tokens:   1557135360 | elapsed time per iteration (ms): 4048.8 | learning rate: 1.038E-04 | global batch size:   128 | lm loss: 2.076758E+00 | loss scale: 524288.0 | grad norm: 0.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.614 | tokens per gpu per second (tgs): 2023.327 | TFLOPs: 16.28 |
g0345: [2024-08-03 01:40:30,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=5950, skipped=3, lr=[0.0001039663104, 0.0001039663104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5950 loss: 2.0177 iter time (s): 4.042 samples/sec: 31.668
g0364:  iteration     5950/10000000 | consumed samples:       761600 | consumed tokens:   1559756800 | elapsed time per iteration (ms): 4074.6 | learning rate: 1.040E-04 | global batch size:   128 | lm loss: 2.083598E+00 | loss scale: 524288.0 | grad norm: 0.874 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.414 | tokens per gpu per second (tgs): 2010.487 | TFLOPs: 16.18 |
g0345: [2024-08-03 01:41:12,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=5960, skipped=3, lr=[0.00010414107306666667, 0.00010414107306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5960 loss: 2.2110 iter time (s): 4.165 samples/sec: 30.734
g0364:  iteration     5960/10000000 | consumed samples:       762880 | consumed tokens:   1562378240 | elapsed time per iteration (ms): 4197.5 | learning rate: 1.041E-04 | global batch size:   128 | lm loss: 2.083014E+00 | loss scale: 524288.0 | grad norm: 0.914 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.495 | tokens per gpu per second (tgs): 1951.651 | TFLOPs: 15.71 |
g0345: [2024-08-03 01:41:54,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=5970, skipped=3, lr=[0.00010431583573333333, 0.00010431583573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5970 loss: 2.2039 iter time (s): 4.166 samples/sec: 30.729
g0364:  iteration     5970/10000000 | consumed samples:       764160 | consumed tokens:   1564999680 | elapsed time per iteration (ms): 4198.4 | learning rate: 1.043E-04 | global batch size:   128 | lm loss: 2.091101E+00 | loss scale: 524288.0 | grad norm: 0.725 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.488 | tokens per gpu per second (tgs): 1951.220 | TFLOPs: 15.70 |
g0345: [2024-08-03 01:42:36,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=5980, skipped=3, lr=[0.0001044905984, 0.0001044905984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5980 loss: 2.2292 iter time (s): 4.151 samples/sec: 30.837
g0364:  iteration     5980/10000000 | consumed samples:       765440 | consumed tokens:   1567621120 | elapsed time per iteration (ms): 4183.4 | learning rate: 1.045E-04 | global batch size:   128 | lm loss: 2.069247E+00 | loss scale: 524288.0 | grad norm: 0.782 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.597 | tokens per gpu per second (tgs): 1958.206 | TFLOPs: 15.76 |
g0345: [2024-08-03 01:43:17,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=5990, skipped=3, lr=[0.00010466536106666667, 0.00010466536106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 5990 loss: 2.2434 iter time (s): 4.098 samples/sec: 31.237
g0364:  iteration     5990/10000000 | consumed samples:       766720 | consumed tokens:   1570242560 | elapsed time per iteration (ms): 4130.2 | learning rate: 1.047E-04 | global batch size:   128 | lm loss: 2.078782E+00 | loss scale: 524288.0 | grad norm: 0.733 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.991 | tokens per gpu per second (tgs): 1983.435 | TFLOPs: 15.96 |
g0345: [2024-08-03 01:43:59,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=3, lr=[0.00010484012373333333, 0.00010484012373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6000 loss: 1.8216 iter time (s): 4.115 samples/sec: 31.109
g0364:  iteration     6000/10000000 | consumed samples:       768000 | consumed tokens:   1572864000 | elapsed time per iteration (ms): 4147.2 | learning rate: 1.048E-04 | global batch size:   128 | lm loss: 2.058613E+00 | loss scale: 524288.0 | grad norm: 0.780 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.864 | tokens per gpu per second (tgs): 1975.287 | TFLOPs: 15.90 |
g0364: ------------------------------------------------------------------------------------------------
g0364:  validation loss at iteration 6000 | lm loss value: 2.063710E+00 | lm loss PPL: 7.875132E+00 | 
g0364: ------------------------------------------------------------------------------------------------
g0345: saving checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: [2024-08-03 01:50:19,540] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
g0345: [2024-08-03 01:50:19,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0345: [2024-08-03 01:50:19,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0345: [2024-08-03 01:50:19,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0364: [2024-08-03 01:50:19,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0364: [2024-08-03 01:50:19,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0364: [2024-08-03 01:50:19,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0358: [2024-08-03 01:50:19,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0358: [2024-08-03 01:50:19,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0358: [2024-08-03 01:50:19,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0362: [2024-08-03 01:50:19,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0362: [2024-08-03 01:50:19,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0362: [2024-08-03 01:50:19,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0363: [2024-08-03 01:50:19,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0363: [2024-08-03 01:50:19,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0363: [2024-08-03 01:50:19,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0346: [2024-08-03 01:50:19,556] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0346: [2024-08-03 01:50:19,556] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0346: [2024-08-03 01:50:19,556] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0352: [2024-08-03 01:50:19,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0352: [2024-08-03 01:50:19,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0352: [2024-08-03 01:50:19,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0347: [2024-08-03 01:50:19,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0347: [2024-08-03 01:50:19,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0347: [2024-08-03 01:50:19,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0364: [2024-08-03 01:50:19,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt...
g0362: [2024-08-03 01:50:19,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt...
g0358: [2024-08-03 01:50:19,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt...
g0346: [2024-08-03 01:50:19,589] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt...
g0363: [2024-08-03 01:50:19,592] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt...
g0347: [2024-08-03 01:50:19,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt...
g0345: [2024-08-03 01:50:19,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt...
g0352: [2024-08-03 01:50:19,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt...
g0364: [2024-08-03 01:50:19,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt.
g0364: [2024-08-03 01:50:19,710] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt...
g0364: [2024-08-03 01:50:19,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt.
g0362: [2024-08-03 01:50:19,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt.
g0345: [2024-08-03 01:50:19,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt.
g0346: [2024-08-03 01:50:19,733] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt.
g0347: [2024-08-03 01:50:19,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt.
g0345: [2024-08-03 01:50:19,746] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt...
g0363: [2024-08-03 01:50:19,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt.
g0362: [2024-08-03 01:50:19,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt...
g0352: [2024-08-03 01:50:19,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt.
g0364: [2024-08-03 01:50:19,758] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt...
g0346: [2024-08-03 01:50:19,769] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt...
g0347: [2024-08-03 01:50:19,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt...
g0363: [2024-08-03 01:50:19,789] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt...
g0352: [2024-08-03 01:50:19,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt...
g0345: [2024-08-03 01:50:19,843] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt.
g0345: [2024-08-03 01:50:19,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt...
g0358: [2024-08-03 01:50:19,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt.
g0347: [2024-08-03 01:50:19,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt.
g0362: [2024-08-03 01:50:19,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt.
g0346: [2024-08-03 01:50:19,897] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt.
g0347: [2024-08-03 01:50:19,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt...
g0358: [2024-08-03 01:50:19,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt...
g0352: [2024-08-03 01:50:19,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt.
g0362: [2024-08-03 01:50:19,924] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt...
g0346: [2024-08-03 01:50:19,924] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt...
g0364: [2024-08-03 01:50:19,957] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt.
g0364: [2024-08-03 01:50:19,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt...
g0352: [2024-08-03 01:50:19,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt...
g0347: [2024-08-03 01:50:19,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt.
g0347: [2024-08-03 01:50:19,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt...
g0363: [2024-08-03 01:50:20,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt.
g0345: [2024-08-03 01:50:20,044] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt.
g0345: [2024-08-03 01:50:20,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt...
g0363: [2024-08-03 01:50:20,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt...
g0363: [2024-08-03 01:50:20,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt.
g0363: [2024-08-03 01:50:20,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt...
g0352: [2024-08-03 01:50:20,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt.
g0352: [2024-08-03 01:50:20,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt...
g0346: [2024-08-03 01:50:20,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt.
g0346: [2024-08-03 01:50:20,254] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt...
g0345: [2024-08-03 01:50:20,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt.
g0345: [2024-08-03 01:50:20,258] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt
g0345: [2024-08-03 01:50:20,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt...
g0362: [2024-08-03 01:50:20,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt.
g0362: [2024-08-03 01:50:20,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt...
g0358: [2024-08-03 01:50:21,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt.
g0358: [2024-08-03 01:50:21,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt...
g0358: [2024-08-03 01:50:21,652] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt.
g0358: [2024-08-03 01:50:21,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt...
g0364: [2024-08-03 01:50:21,799] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt.
g0364: [2024-08-03 01:50:21,800] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0347: [2024-08-03 01:50:22,356] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt.
g0347: [2024-08-03 01:50:22,357] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0363: [2024-08-03 01:50:22,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt.
g0363: [2024-08-03 01:50:22,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0346: [2024-08-03 01:50:22,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt.
g0346: [2024-08-03 01:50:22,534] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0352: [2024-08-03 01:50:22,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt.
g0352: [2024-08-03 01:50:22,610] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0362: [2024-08-03 01:50:22,800] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt.
g0362: [2024-08-03 01:50:22,800] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0345: [2024-08-03 01:50:23,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt.
g0345: [2024-08-03 01:50:23,539] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0358: [2024-08-03 01:50:23,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt.
g0358: [2024-08-03 01:50:23,952] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0345:   successfully saved checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: Checkpoint Save GB: 22.521, GB/Sec: 4.67, Latency(second): 4.824
g0364: (min, max) time across ranks (ms):
g0364:     save-checkpoint ................................: (4822.49, 4823.80)
g0345: [2024-08-03 01:51:05,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=6010, skipped=3, lr=[0.0001050148864, 0.0001050148864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6010 loss: 2.0875 iter time (s): 4.108 samples/sec: 31.156
g0364:  iteration     6010/10000000 | consumed samples:       769280 | consumed tokens:   1575485440 | elapsed time per iteration (ms): 42628.0 | learning rate: 1.050E-04 | global batch size:   128 | lm loss: 2.022430E+00 | loss scale: 524288.0 | grad norm: 1.025 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.003 | tokens per gpu per second (tgs): 192.174 | TFLOPs: 1.55 |
g0345: [2024-08-03 01:51:47,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=6020, skipped=3, lr=[0.00010518964906666666, 0.00010518964906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6020 loss: 1.9526 iter time (s): 4.172 samples/sec: 30.680
g0364:  iteration     6020/10000000 | consumed samples:       770560 | consumed tokens:   1578106880 | elapsed time per iteration (ms): 4205.5 | learning rate: 1.052E-04 | global batch size:   128 | lm loss: 2.065504E+00 | loss scale: 524288.0 | grad norm: 0.797 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.437 | tokens per gpu per second (tgs): 1947.946 | TFLOPs: 15.68 |
g0345: [2024-08-03 01:52:31,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=6030, skipped=3, lr=[0.00010536441173333333, 0.00010536441173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6030 loss: 2.1372 iter time (s): 4.310 samples/sec: 29.700
g0364:  iteration     6030/10000000 | consumed samples:       771840 | consumed tokens:   1580728320 | elapsed time per iteration (ms): 4342.4 | learning rate: 1.054E-04 | global batch size:   128 | lm loss: 2.099462E+00 | loss scale: 524288.0 | grad norm: 0.737 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.477 | tokens per gpu per second (tgs): 1886.498 | TFLOPs: 15.18 |
g0345: [2024-08-03 01:53:12,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=6040, skipped=3, lr=[0.0001055391744, 0.0001055391744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6040 loss: 2.1364 iter time (s): 4.114 samples/sec: 31.111
g0364:  iteration     6040/10000000 | consumed samples:       773120 | consumed tokens:   1583349760 | elapsed time per iteration (ms): 4146.7 | learning rate: 1.055E-04 | global batch size:   128 | lm loss: 2.089736E+00 | loss scale: 524288.0 | grad norm: 0.731 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.868 | tokens per gpu per second (tgs): 1975.540 | TFLOPs: 15.90 |
g0345: [2024-08-03 01:53:54,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=6050, skipped=3, lr=[0.00010571393706666666, 0.00010571393706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6050 loss: 1.8804 iter time (s): 4.157 samples/sec: 30.788
g0364:  iteration     6050/10000000 | consumed samples:       774400 | consumed tokens:   1585971200 | elapsed time per iteration (ms): 4190.3 | learning rate: 1.057E-04 | global batch size:   128 | lm loss: 2.045196E+00 | loss scale: 524288.0 | grad norm: 0.768 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.547 | tokens per gpu per second (tgs): 1954.994 | TFLOPs: 15.73 |
g0345: [2024-08-03 01:54:35,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=6060, skipped=3, lr=[0.00010588869973333333, 0.00010588869973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6060 loss: 1.9024 iter time (s): 4.062 samples/sec: 31.514
g0364:  iteration     6060/10000000 | consumed samples:       775680 | consumed tokens:   1588592640 | elapsed time per iteration (ms): 4094.2 | learning rate: 1.059E-04 | global batch size:   128 | lm loss: 2.050076E+00 | loss scale: 524288.0 | grad norm: 0.719 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.264 | tokens per gpu per second (tgs): 2000.891 | TFLOPs: 16.10 |
g0345: [2024-08-03 01:55:16,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=6070, skipped=3, lr=[0.0001060634624, 0.0001060634624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6070 loss: 2.2014 iter time (s): 4.014 samples/sec: 31.887
g0364:  iteration     6070/10000000 | consumed samples:       776960 | consumed tokens:   1591214080 | elapsed time per iteration (ms): 4055.1 | learning rate: 1.061E-04 | global batch size:   128 | lm loss: 2.063079E+00 | loss scale: 524288.0 | grad norm: 0.846 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.565 | tokens per gpu per second (tgs): 2020.149 | TFLOPs: 16.26 |
g0345: [2024-08-03 01:55:56,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=6080, skipped=3, lr=[0.00010623822506666666, 0.00010623822506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6080 loss: 2.1327 iter time (s): 4.044 samples/sec: 31.655
g0364:  iteration     6080/10000000 | consumed samples:       778240 | consumed tokens:   1593835520 | elapsed time per iteration (ms): 4076.0 | learning rate: 1.062E-04 | global batch size:   128 | lm loss: 2.051923E+00 | loss scale: 524288.0 | grad norm: 0.808 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.404 | tokens per gpu per second (tgs): 2009.833 | TFLOPs: 16.17 |
g0345: [2024-08-03 01:56:37,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=6090, skipped=3, lr=[0.00010641298773333333, 0.00010641298773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6090 loss: 1.9072 iter time (s): 4.015 samples/sec: 31.881
g0364:  iteration     6090/10000000 | consumed samples:       779520 | consumed tokens:   1596456960 | elapsed time per iteration (ms): 4047.2 | learning rate: 1.064E-04 | global batch size:   128 | lm loss: 2.012393E+00 | loss scale: 524288.0 | grad norm: 0.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.626 | tokens per gpu per second (tgs): 2024.096 | TFLOPs: 16.29 |
g0345: [2024-08-03 01:57:18,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=6100, skipped=3, lr=[0.0001065877504, 0.0001065877504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6100 loss: 2.0563 iter time (s): 4.078 samples/sec: 31.392
g0364:  iteration     6100/10000000 | consumed samples:       780800 | consumed tokens:   1599078400 | elapsed time per iteration (ms): 4110.2 | learning rate: 1.066E-04 | global batch size:   128 | lm loss: 2.096789E+00 | loss scale: 524288.0 | grad norm: 0.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.142 | tokens per gpu per second (tgs): 1993.081 | TFLOPs: 16.04 |
g0345: [2024-08-03 01:57:59,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=6110, skipped=3, lr=[0.00010676251306666667, 0.00010676251306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6110 loss: 1.9337 iter time (s): 4.028 samples/sec: 31.781
g0364:  iteration     6110/10000000 | consumed samples:       782080 | consumed tokens:   1601699840 | elapsed time per iteration (ms): 4060.0 | learning rate: 1.068E-04 | global batch size:   128 | lm loss: 1.938670E+00 | loss scale: 524288.0 | grad norm: 0.701 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.527 | tokens per gpu per second (tgs): 2017.744 | TFLOPs: 16.24 |
g0345: [2024-08-03 01:58:40,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=6120, skipped=3, lr=[0.00010693727573333334, 0.00010693727573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6120 loss: 1.9766 iter time (s): 4.132 samples/sec: 30.981
g0364:  iteration     6120/10000000 | consumed samples:       783360 | consumed tokens:   1604321280 | elapsed time per iteration (ms): 4164.0 | learning rate: 1.069E-04 | global batch size:   128 | lm loss: 2.098626E+00 | loss scale: 524288.0 | grad norm: 1.094 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.739 | tokens per gpu per second (tgs): 1967.326 | TFLOPs: 15.83 |
g0345: [2024-08-03 01:59:21,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=6130, skipped=3, lr=[0.0001071120384, 0.0001071120384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6130 loss: 1.9650 iter time (s): 4.075 samples/sec: 31.410
g0364:  iteration     6130/10000000 | consumed samples:       784640 | consumed tokens:   1606942720 | elapsed time per iteration (ms): 4107.6 | learning rate: 1.071E-04 | global batch size:   128 | lm loss: 2.084128E+00 | loss scale: 524288.0 | grad norm: 0.640 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.162 | tokens per gpu per second (tgs): 1994.346 | TFLOPs: 16.05 |
g0345: [2024-08-03 02:00:03,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=6140, skipped=3, lr=[0.00010728680106666667, 0.00010728680106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6140 loss: 1.9266 iter time (s): 4.171 samples/sec: 30.685
g0364:  iteration     6140/10000000 | consumed samples:       785920 | consumed tokens:   1609564160 | elapsed time per iteration (ms): 4203.9 | learning rate: 1.073E-04 | global batch size:   128 | lm loss: 2.040557E+00 | loss scale: 524288.0 | grad norm: 0.677 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.448 | tokens per gpu per second (tgs): 1948.676 | TFLOPs: 15.68 |
g0345: [2024-08-03 02:00:45,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=6150, skipped=3, lr=[0.00010746156373333334, 0.00010746156373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6150 loss: 2.1937 iter time (s): 4.169 samples/sec: 30.702
g0364:  iteration     6150/10000000 | consumed samples:       787200 | consumed tokens:   1612185600 | elapsed time per iteration (ms): 4201.5 | learning rate: 1.075E-04 | global batch size:   128 | lm loss: 2.032012E+00 | loss scale: 524288.0 | grad norm: 0.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.466 | tokens per gpu per second (tgs): 1949.796 | TFLOPs: 15.69 |
g0345: [2024-08-03 02:01:26,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=6160, skipped=3, lr=[0.0001076363264, 0.0001076363264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6160 loss: 2.0368 iter time (s): 4.041 samples/sec: 31.679
g0364:  iteration     6160/10000000 | consumed samples:       788480 | consumed tokens:   1614807040 | elapsed time per iteration (ms): 4074.5 | learning rate: 1.076E-04 | global batch size:   128 | lm loss: 2.104374E+00 | loss scale: 524288.0 | grad norm: 0.900 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.415 | tokens per gpu per second (tgs): 2010.541 | TFLOPs: 16.18 |
g0345: [2024-08-03 02:02:08,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=6170, skipped=3, lr=[0.00010781108906666667, 0.00010781108906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6170 loss: 1.7154 iter time (s): 4.129 samples/sec: 30.998
g0364:  iteration     6170/10000000 | consumed samples:       789760 | consumed tokens:   1617428480 | elapsed time per iteration (ms): 4161.8 | learning rate: 1.078E-04 | global batch size:   128 | lm loss: 2.028265E+00 | loss scale: 524288.0 | grad norm: 0.808 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.756 | tokens per gpu per second (tgs): 1968.390 | TFLOPs: 15.84 |
g0345: [2024-08-03 02:02:49,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=6180, skipped=3, lr=[0.00010798585173333333, 0.00010798585173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6180 loss: 2.0767 iter time (s): 4.111 samples/sec: 31.134
g0364:  iteration     6180/10000000 | consumed samples:       791040 | consumed tokens:   1620049920 | elapsed time per iteration (ms): 4143.9 | learning rate: 1.080E-04 | global batch size:   128 | lm loss: 2.007175E+00 | loss scale: 524288.0 | grad norm: 0.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.889 | tokens per gpu per second (tgs): 1976.900 | TFLOPs: 15.91 |
g0345: [2024-08-03 02:03:31,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=6190, skipped=3, lr=[0.0001081606144, 0.0001081606144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6190 loss: 2.1017 iter time (s): 4.119 samples/sec: 31.077
g0364:  iteration     6190/10000000 | consumed samples:       792320 | consumed tokens:   1622671360 | elapsed time per iteration (ms): 4151.4 | learning rate: 1.082E-04 | global batch size:   128 | lm loss: 2.038447E+00 | loss scale: 524288.0 | grad norm: 0.755 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.833 | tokens per gpu per second (tgs): 1973.308 | TFLOPs: 15.88 |
g0345: [2024-08-03 02:04:12,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=6200, skipped=3, lr=[0.00010833537706666667, 0.00010833537706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6200 loss: 2.2451 iter time (s): 4.100 samples/sec: 31.221
g0364:  iteration     6200/10000000 | consumed samples:       793600 | consumed tokens:   1625292800 | elapsed time per iteration (ms): 4132.7 | learning rate: 1.083E-04 | global batch size:   128 | lm loss: 2.029856E+00 | loss scale: 524288.0 | grad norm: 0.855 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.972 | tokens per gpu per second (tgs): 1982.220 | TFLOPs: 15.95 |
g0345: [2024-08-03 02:04:54,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=6210, skipped=3, lr=[0.00010851013973333333, 0.00010851013973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6210 loss: 1.9883 iter time (s): 4.151 samples/sec: 30.835
g0364:  iteration     6210/10000000 | consumed samples:       794880 | consumed tokens:   1627914240 | elapsed time per iteration (ms): 4184.1 | learning rate: 1.085E-04 | global batch size:   128 | lm loss: 2.073264E+00 | loss scale: 524288.0 | grad norm: 0.734 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.592 | tokens per gpu per second (tgs): 1957.909 | TFLOPs: 15.76 |
g0345: [2024-08-03 02:05:35,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=6220, skipped=3, lr=[0.0001086849024, 0.0001086849024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6220 loss: 2.2371 iter time (s): 4.080 samples/sec: 31.371
g0364:  iteration     6220/10000000 | consumed samples:       796160 | consumed tokens:   1630535680 | elapsed time per iteration (ms): 4112.5 | learning rate: 1.087E-04 | global batch size:   128 | lm loss: 2.073257E+00 | loss scale: 524288.0 | grad norm: 0.911 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.125 | tokens per gpu per second (tgs): 1991.995 | TFLOPs: 16.03 |
g0345: [2024-08-03 02:06:16,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=6230, skipped=3, lr=[0.00010885966506666666, 0.00010885966506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6230 loss: 1.9370 iter time (s): 4.115 samples/sec: 31.106
g0364:  iteration     6230/10000000 | consumed samples:       797440 | consumed tokens:   1633157120 | elapsed time per iteration (ms): 4148.5 | learning rate: 1.089E-04 | global batch size:   128 | lm loss: 2.036337E+00 | loss scale: 524288.0 | grad norm: 0.815 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.854 | tokens per gpu per second (tgs): 1974.668 | TFLOPs: 15.89 |
g0363: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 6237
g0363: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 6237
g0363: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 6237
g0363: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 6237
g0345: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 6237
g0345: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 6237
g0345: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 6237
g0345: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 6237
g0345: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 6237
g0347: Grad overflow on iteration 6237
g0358: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 6237
g0358: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 6237
g0358: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: Grad overflow on iteration 6237
g0352: Grad overflow on iteration 6237
g0352: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 6237
g0362: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 6237
g0345: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 6237
g0362: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 6237
g0364: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 6237
g0362: Grad overflow on iteration 6237
g0362: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: Grad overflow on iteration 6237
g0352: Grad overflow on iteration 6237
g0347: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: Grad overflow on iteration 6237
g0352: Grad overflow on iteration 6237
g0347: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 6237
g0352: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 6237
g0352: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: Grad overflow on iteration 6237
g0358: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 6237
g0347: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: Grad overflow on iteration 6237
g0346: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 6237
g0345: [2024-08-03 02:06:49,876] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0346: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 6237
g0346: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 6237
g0346: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 02:06:49,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:06:49,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 02:06:58,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=6240, skipped=4, lr=[0.00010903442773333333, 0.00010903442773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6240 loss: 2.3021 iter time (s): 4.106 samples/sec: 31.171
g0364:  iteration     6240/10000000 | consumed samples:       798720 | consumed tokens:   1635778560 | elapsed time per iteration (ms): 4139.0 | learning rate: 1.090E-04 | global batch size:   128 | lm loss: 2.064592E+00 | loss scale: 262144.0 | grad norm: 0.586 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.925 | tokens per gpu per second (tgs): 1979.218 | TFLOPs: 15.93 |
g0345: [2024-08-03 02:07:41,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=6250, skipped=4, lr=[0.0001092091904, 0.0001092091904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6250 loss: 1.8078 iter time (s): 4.310 samples/sec: 29.698
g0364:  iteration     6250/10000000 | consumed samples:       800000 | consumed tokens:   1638400000 | elapsed time per iteration (ms): 4342.3 | learning rate: 1.092E-04 | global batch size:   128 | lm loss: 1.992864E+00 | loss scale: 262144.0 | grad norm: 0.808 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.477 | tokens per gpu per second (tgs): 1886.554 | TFLOPs: 15.18 |
g0345: [2024-08-03 02:08:23,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=6260, skipped=4, lr=[0.00010938395306666666, 0.00010938395306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6260 loss: 2.1135 iter time (s): 4.153 samples/sec: 30.824
g0364:  iteration     6260/10000000 | consumed samples:       801280 | consumed tokens:   1641021440 | elapsed time per iteration (ms): 4184.9 | learning rate: 1.094E-04 | global batch size:   128 | lm loss: 2.018596E+00 | loss scale: 262144.0 | grad norm: 0.698 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.586 | tokens per gpu per second (tgs): 1957.531 | TFLOPs: 15.75 |
g0345: [2024-08-03 02:09:04,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=6270, skipped=4, lr=[0.00010955871573333333, 0.00010955871573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6270 loss: 2.0209 iter time (s): 4.075 samples/sec: 31.411
g0364:  iteration     6270/10000000 | consumed samples:       802560 | consumed tokens:   1643642880 | elapsed time per iteration (ms): 4107.2 | learning rate: 1.096E-04 | global batch size:   128 | lm loss: 2.042915E+00 | loss scale: 262144.0 | grad norm: 0.629 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.165 | tokens per gpu per second (tgs): 1994.544 | TFLOPs: 16.05 |
g0345: [2024-08-03 02:09:46,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=6280, skipped=4, lr=[0.00010973347840000001, 0.00010973347840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6280 loss: 1.7788 iter time (s): 4.123 samples/sec: 31.047
g0364:  iteration     6280/10000000 | consumed samples:       803840 | consumed tokens:   1646264320 | elapsed time per iteration (ms): 4155.1 | learning rate: 1.097E-04 | global batch size:   128 | lm loss: 2.034569E+00 | loss scale: 262144.0 | grad norm: 0.765 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.806 | tokens per gpu per second (tgs): 1971.552 | TFLOPs: 15.87 |
g0345: [2024-08-03 02:10:28,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=6290, skipped=4, lr=[0.00010990824106666667, 0.00010990824106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6290 loss: 1.9650 iter time (s): 4.225 samples/sec: 30.297
g0364:  iteration     6290/10000000 | consumed samples:       805120 | consumed tokens:   1648885760 | elapsed time per iteration (ms): 4257.2 | learning rate: 1.099E-04 | global batch size:   128 | lm loss: 2.007482E+00 | loss scale: 262144.0 | grad norm: 0.978 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.067 | tokens per gpu per second (tgs): 1924.286 | TFLOPs: 15.49 |
g0345: [2024-08-03 02:11:10,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=6300, skipped=4, lr=[0.00011008300373333334, 0.00011008300373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6300 loss: 1.7395 iter time (s): 4.142 samples/sec: 30.903
g0364:  iteration     6300/10000000 | consumed samples:       806400 | consumed tokens:   1651507200 | elapsed time per iteration (ms): 4174.2 | learning rate: 1.101E-04 | global batch size:   128 | lm loss: 2.074659E+00 | loss scale: 262144.0 | grad norm: 0.669 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.664 | tokens per gpu per second (tgs): 1962.526 | TFLOPs: 15.79 |
g0345: [2024-08-03 02:11:52,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=6310, skipped=4, lr=[0.00011025776640000001, 0.00011025776640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6310 loss: 1.9133 iter time (s): 4.132 samples/sec: 30.977
g0364:  iteration     6310/10000000 | consumed samples:       807680 | consumed tokens:   1654128640 | elapsed time per iteration (ms): 4164.4 | learning rate: 1.103E-04 | global batch size:   128 | lm loss: 2.011202E+00 | loss scale: 262144.0 | grad norm: 0.818 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.737 | tokens per gpu per second (tgs): 1967.139 | TFLOPs: 15.83 |
g0345: [2024-08-03 02:12:34,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=6320, skipped=4, lr=[0.00011043252906666667, 0.00011043252906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6320 loss: 2.0222 iter time (s): 4.178 samples/sec: 30.635
g0364:  iteration     6320/10000000 | consumed samples:       808960 | consumed tokens:   1656750080 | elapsed time per iteration (ms): 4211.2 | learning rate: 1.104E-04 | global batch size:   128 | lm loss: 2.071636E+00 | loss scale: 262144.0 | grad norm: 0.563 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.395 | tokens per gpu per second (tgs): 1945.277 | TFLOPs: 15.65 |
g0345: [2024-08-03 02:13:16,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=6330, skipped=4, lr=[0.00011060729173333334, 0.00011060729173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6330 loss: 2.0444 iter time (s): 4.149 samples/sec: 30.848
g0364:  iteration     6330/10000000 | consumed samples:       810240 | consumed tokens:   1659371520 | elapsed time per iteration (ms): 4182.4 | learning rate: 1.106E-04 | global batch size:   128 | lm loss: 2.013532E+00 | loss scale: 262144.0 | grad norm: 0.812 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.605 | tokens per gpu per second (tgs): 1958.707 | TFLOPs: 15.76 |
g0345: [2024-08-03 02:13:57,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=6340, skipped=4, lr=[0.0001107820544, 0.0001107820544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6340 loss: 1.7682 iter time (s): 4.150 samples/sec: 30.841
g0364:  iteration     6340/10000000 | consumed samples:       811520 | consumed tokens:   1661992960 | elapsed time per iteration (ms): 4182.8 | learning rate: 1.108E-04 | global batch size:   128 | lm loss: 2.008974E+00 | loss scale: 262144.0 | grad norm: 0.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.601 | tokens per gpu per second (tgs): 1958.478 | TFLOPs: 15.76 |
g0345: [2024-08-03 02:14:39,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=6350, skipped=4, lr=[0.00011095681706666667, 0.00011095681706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6350 loss: 2.1548 iter time (s): 4.125 samples/sec: 31.027
g0364:  iteration     6350/10000000 | consumed samples:       812800 | consumed tokens:   1664614400 | elapsed time per iteration (ms): 4157.9 | learning rate: 1.110E-04 | global batch size:   128 | lm loss: 1.943306E+00 | loss scale: 262144.0 | grad norm: 0.745 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.785 | tokens per gpu per second (tgs): 1970.231 | TFLOPs: 15.85 |
g0345: [2024-08-03 02:15:20,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=6360, skipped=4, lr=[0.00011113157973333334, 0.00011113157973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6360 loss: 1.9250 iter time (s): 4.100 samples/sec: 31.216
g0364:  iteration     6360/10000000 | consumed samples:       814080 | consumed tokens:   1667235840 | elapsed time per iteration (ms): 4132.9 | learning rate: 1.111E-04 | global batch size:   128 | lm loss: 1.957746E+00 | loss scale: 262144.0 | grad norm: 0.870 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.971 | tokens per gpu per second (tgs): 1982.143 | TFLOPs: 15.95 |
g0345: [2024-08-03 02:16:03,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=6370, skipped=4, lr=[0.0001113063424, 0.0001113063424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6370 loss: 2.0723 iter time (s): 4.212 samples/sec: 30.390
g0364:  iteration     6370/10000000 | consumed samples:       815360 | consumed tokens:   1669857280 | elapsed time per iteration (ms): 4244.6 | learning rate: 1.113E-04 | global batch size:   128 | lm loss: 1.989252E+00 | loss scale: 262144.0 | grad norm: 0.787 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.156 | tokens per gpu per second (tgs): 1929.971 | TFLOPs: 15.53 |
g0345: [2024-08-03 02:16:45,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=6380, skipped=4, lr=[0.00011148110506666667, 0.00011148110506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6380 loss: 1.8648 iter time (s): 4.147 samples/sec: 30.869
g0364:  iteration     6380/10000000 | consumed samples:       816640 | consumed tokens:   1672478720 | elapsed time per iteration (ms): 4179.0 | learning rate: 1.115E-04 | global batch size:   128 | lm loss: 1.960785E+00 | loss scale: 262144.0 | grad norm: 1.051 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.629 | tokens per gpu per second (tgs): 1960.282 | TFLOPs: 15.77 |
g0345: [2024-08-03 02:17:27,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=6390, skipped=4, lr=[0.00011165586773333334, 0.00011165586773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6390 loss: 1.9696 iter time (s): 4.168 samples/sec: 30.709
g0364:  iteration     6390/10000000 | consumed samples:       817920 | consumed tokens:   1675100160 | elapsed time per iteration (ms): 4200.6 | learning rate: 1.117E-04 | global batch size:   128 | lm loss: 2.042068E+00 | loss scale: 262144.0 | grad norm: 0.798 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.472 | tokens per gpu per second (tgs): 1950.187 | TFLOPs: 15.69 |
g0345: [2024-08-03 02:18:08,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=6400, skipped=4, lr=[0.0001118306304, 0.0001118306304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6400 loss: 1.9557 iter time (s): 4.120 samples/sec: 31.067
g0364:  iteration     6400/10000000 | consumed samples:       819200 | consumed tokens:   1677721600 | elapsed time per iteration (ms): 4153.6 | learning rate: 1.118E-04 | global batch size:   128 | lm loss: 2.032888E+00 | loss scale: 262144.0 | grad norm: 0.884 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.817 | tokens per gpu per second (tgs): 1972.262 | TFLOPs: 15.87 |
g0345: [2024-08-03 02:18:51,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=6410, skipped=4, lr=[0.00011200539306666667, 0.00011200539306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6410 loss: 2.1835 iter time (s): 4.227 samples/sec: 30.284
g0364:  iteration     6410/10000000 | consumed samples:       820480 | consumed tokens:   1680343040 | elapsed time per iteration (ms): 4259.2 | learning rate: 1.120E-04 | global batch size:   128 | lm loss: 2.064057E+00 | loss scale: 262144.0 | grad norm: 0.681 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.052 | tokens per gpu per second (tgs): 1923.349 | TFLOPs: 15.48 |
g0345: [2024-08-03 02:19:33,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=6420, skipped=4, lr=[0.00011218015573333333, 0.00011218015573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6420 loss: 1.9697 iter time (s): 4.173 samples/sec: 30.674
g0364:  iteration     6420/10000000 | consumed samples:       821760 | consumed tokens:   1682964480 | elapsed time per iteration (ms): 4205.1 | learning rate: 1.122E-04 | global batch size:   128 | lm loss: 2.014312E+00 | loss scale: 262144.0 | grad norm: 0.653 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.439 | tokens per gpu per second (tgs): 1948.121 | TFLOPs: 15.68 |
g0345: [2024-08-03 02:20:14,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=6430, skipped=4, lr=[0.0001123549184, 0.0001123549184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6430 loss: 2.0571 iter time (s): 4.126 samples/sec: 31.023
g0364:  iteration     6430/10000000 | consumed samples:       823040 | consumed tokens:   1685585920 | elapsed time per iteration (ms): 4158.5 | learning rate: 1.124E-04 | global batch size:   128 | lm loss: 1.968795E+00 | loss scale: 262144.0 | grad norm: 0.791 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.781 | tokens per gpu per second (tgs): 1969.964 | TFLOPs: 15.85 |
g0345: [2024-08-03 02:20:56,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=6440, skipped=4, lr=[0.00011252968106666667, 0.00011252968106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6440 loss: 1.8902 iter time (s): 4.161 samples/sec: 30.761
g0364:  iteration     6440/10000000 | consumed samples:       824320 | consumed tokens:   1688207360 | elapsed time per iteration (ms): 4193.6 | learning rate: 1.125E-04 | global batch size:   128 | lm loss: 2.024577E+00 | loss scale: 262144.0 | grad norm: 0.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.523 | tokens per gpu per second (tgs): 1953.464 | TFLOPs: 15.72 |
g0345: [2024-08-03 02:21:37,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=6450, skipped=4, lr=[0.00011270444373333333, 0.00011270444373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6450 loss: 1.9000 iter time (s): 4.088 samples/sec: 31.308
g0364:  iteration     6450/10000000 | consumed samples:       825600 | consumed tokens:   1690828800 | elapsed time per iteration (ms): 4121.7 | learning rate: 1.127E-04 | global batch size:   128 | lm loss: 1.927853E+00 | loss scale: 262144.0 | grad norm: 0.670 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.055 | tokens per gpu per second (tgs): 1987.523 | TFLOPs: 15.99 |
g0345: [2024-08-03 02:22:19,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=6460, skipped=4, lr=[0.00011287920640000001, 0.00011287920640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6460 loss: 1.9950 iter time (s): 4.130 samples/sec: 30.990
g0364:  iteration     6460/10000000 | consumed samples:       826880 | consumed tokens:   1693450240 | elapsed time per iteration (ms): 4163.5 | learning rate: 1.129E-04 | global batch size:   128 | lm loss: 1.992962E+00 | loss scale: 262144.0 | grad norm: 0.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.744 | tokens per gpu per second (tgs): 1967.592 | TFLOPs: 15.83 |
g0345: [2024-08-03 02:23:01,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=6470, skipped=4, lr=[0.00011305396906666668, 0.00011305396906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6470 loss: 1.9162 iter time (s): 4.161 samples/sec: 30.759
g0364:  iteration     6470/10000000 | consumed samples:       828160 | consumed tokens:   1696071680 | elapsed time per iteration (ms): 4193.8 | learning rate: 1.131E-04 | global batch size:   128 | lm loss: 1.981371E+00 | loss scale: 262144.0 | grad norm: 0.699 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.521 | tokens per gpu per second (tgs): 1953.350 | TFLOPs: 15.72 |
g0345: [2024-08-03 02:23:43,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=6480, skipped=4, lr=[0.00011322873173333334, 0.00011322873173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6480 loss: 2.2661 iter time (s): 4.141 samples/sec: 30.911
g0364:  iteration     6480/10000000 | consumed samples:       829440 | consumed tokens:   1698693120 | elapsed time per iteration (ms): 4173.3 | learning rate: 1.132E-04 | global batch size:   128 | lm loss: 1.972317E+00 | loss scale: 262144.0 | grad norm: 0.680 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.671 | tokens per gpu per second (tgs): 1962.936 | TFLOPs: 15.80 |
g0345: [2024-08-03 02:24:24,982] [INFO] [logging.py:96:log_dist] [Rank 0] step=6490, skipped=4, lr=[0.00011340349440000001, 0.00011340349440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6490 loss: 1.9533 iter time (s): 4.137 samples/sec: 30.938
g0364:  iteration     6490/10000000 | consumed samples:       830720 | consumed tokens:   1701314560 | elapsed time per iteration (ms): 4171.1 | learning rate: 1.134E-04 | global batch size:   128 | lm loss: 2.026150E+00 | loss scale: 262144.0 | grad norm: 0.596 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.687 | tokens per gpu per second (tgs): 1963.992 | TFLOPs: 15.80 |
g0345: [2024-08-03 02:25:05,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=6500, skipped=4, lr=[0.00011357825706666668, 0.00011357825706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6500 loss: 2.0573 iter time (s): 4.062 samples/sec: 31.515
g0364:  iteration     6500/10000000 | consumed samples:       832000 | consumed tokens:   1703936000 | elapsed time per iteration (ms): 4094.4 | learning rate: 1.136E-04 | global batch size:   128 | lm loss: 2.012854E+00 | loss scale: 262144.0 | grad norm: 0.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.262 | tokens per gpu per second (tgs): 2000.771 | TFLOPs: 16.10 |
g0345: [2024-08-03 02:25:46,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=6510, skipped=4, lr=[0.00011375301973333334, 0.00011375301973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6510 loss: 1.8188 iter time (s): 4.072 samples/sec: 31.433
g0364:  iteration     6510/10000000 | consumed samples:       833280 | consumed tokens:   1706557440 | elapsed time per iteration (ms): 4104.8 | learning rate: 1.138E-04 | global batch size:   128 | lm loss: 2.022905E+00 | loss scale: 262144.0 | grad norm: 0.729 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.183 | tokens per gpu per second (tgs): 1995.704 | TFLOPs: 16.06 |
g0345: [2024-08-03 02:26:28,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=6520, skipped=4, lr=[0.00011392778240000001, 0.00011392778240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6520 loss: 1.7833 iter time (s): 4.113 samples/sec: 31.121
g0364:  iteration     6520/10000000 | consumed samples:       834560 | consumed tokens:   1709178880 | elapsed time per iteration (ms): 4145.7 | learning rate: 1.139E-04 | global batch size:   128 | lm loss: 2.001221E+00 | loss scale: 262144.0 | grad norm: 0.648 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.876 | tokens per gpu per second (tgs): 1976.047 | TFLOPs: 15.90 |
g0345: [2024-08-03 02:27:10,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=6530, skipped=4, lr=[0.00011410254506666667, 0.00011410254506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6530 loss: 1.7338 iter time (s): 4.126 samples/sec: 31.026
g0364:  iteration     6530/10000000 | consumed samples:       835840 | consumed tokens:   1711800320 | elapsed time per iteration (ms): 4158.0 | learning rate: 1.141E-04 | global batch size:   128 | lm loss: 2.006358E+00 | loss scale: 262144.0 | grad norm: 0.713 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.784 | tokens per gpu per second (tgs): 1970.160 | TFLOPs: 15.85 |
g0345: [2024-08-03 02:27:50,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=6540, skipped=4, lr=[0.00011427730773333334, 0.00011427730773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6540 loss: 2.0211 iter time (s): 4.034 samples/sec: 31.732
g0364:  iteration     6540/10000000 | consumed samples:       837120 | consumed tokens:   1714421760 | elapsed time per iteration (ms): 4066.5 | learning rate: 1.143E-04 | global batch size:   128 | lm loss: 1.988097E+00 | loss scale: 262144.0 | grad norm: 0.555 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.476 | tokens per gpu per second (tgs): 2014.485 | TFLOPs: 16.21 |
g0345: [2024-08-03 02:28:31,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=6550, skipped=4, lr=[0.0001144520704, 0.0001144520704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6550 loss: 1.8239 iter time (s): 4.090 samples/sec: 31.299
g0364:  iteration     6550/10000000 | consumed samples:       838400 | consumed tokens:   1717043200 | elapsed time per iteration (ms): 4122.0 | learning rate: 1.145E-04 | global batch size:   128 | lm loss: 1.978561E+00 | loss scale: 262144.0 | grad norm: 0.887 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.053 | tokens per gpu per second (tgs): 1987.366 | TFLOPs: 15.99 |
g0345: [2024-08-03 02:29:14,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=6560, skipped=4, lr=[0.00011462683306666667, 0.00011462683306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6560 loss: 1.8816 iter time (s): 4.191 samples/sec: 30.540
g0364:  iteration     6560/10000000 | consumed samples:       839680 | consumed tokens:   1719664640 | elapsed time per iteration (ms): 4224.0 | learning rate: 1.146E-04 | global batch size:   128 | lm loss: 1.931516E+00 | loss scale: 262144.0 | grad norm: 0.625 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.303 | tokens per gpu per second (tgs): 1939.413 | TFLOPs: 15.61 |
g0345: [2024-08-03 02:29:56,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=6570, skipped=4, lr=[0.00011480159573333334, 0.00011480159573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6570 loss: 1.9535 iter time (s): 4.204 samples/sec: 30.448
g0364:  iteration     6570/10000000 | consumed samples:       840960 | consumed tokens:   1722286080 | elapsed time per iteration (ms): 4237.2 | learning rate: 1.148E-04 | global batch size:   128 | lm loss: 2.025683E+00 | loss scale: 262144.0 | grad norm: 0.674 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.208 | tokens per gpu per second (tgs): 1933.332 | TFLOPs: 15.56 |
g0345: [2024-08-03 02:30:38,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=6580, skipped=4, lr=[0.0001149763584, 0.0001149763584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6580 loss: 2.0397 iter time (s): 4.150 samples/sec: 30.843
g0364:  iteration     6580/10000000 | consumed samples:       842240 | consumed tokens:   1724907520 | elapsed time per iteration (ms): 4183.8 | learning rate: 1.150E-04 | global batch size:   128 | lm loss: 1.965003E+00 | loss scale: 262144.0 | grad norm: 0.720 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.594 | tokens per gpu per second (tgs): 1958.045 | TFLOPs: 15.76 |
g0345: [2024-08-03 02:31:20,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=6590, skipped=4, lr=[0.00011515112106666667, 0.00011515112106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6590 loss: 1.9353 iter time (s): 4.149 samples/sec: 30.852
g0364:  iteration     6590/10000000 | consumed samples:       843520 | consumed tokens:   1727528960 | elapsed time per iteration (ms): 4181.3 | learning rate: 1.152E-04 | global batch size:   128 | lm loss: 1.964569E+00 | loss scale: 262144.0 | grad norm: 0.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.613 | tokens per gpu per second (tgs): 1959.222 | TFLOPs: 15.77 |
g0345: [2024-08-03 02:32:01,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=6600, skipped=4, lr=[0.00011532588373333334, 0.00011532588373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6600 loss: 1.7053 iter time (s): 4.132 samples/sec: 30.976
g0364:  iteration     6600/10000000 | consumed samples:       844800 | consumed tokens:   1730150400 | elapsed time per iteration (ms): 4164.7 | learning rate: 1.153E-04 | global batch size:   128 | lm loss: 1.929844E+00 | loss scale: 262144.0 | grad norm: 0.801 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.735 | tokens per gpu per second (tgs): 1967.027 | TFLOPs: 15.83 |
g0345: [2024-08-03 02:32:43,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=6610, skipped=4, lr=[0.0001155006464, 0.0001155006464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6610 loss: 1.7774 iter time (s): 4.180 samples/sec: 30.621
g0364:  iteration     6610/10000000 | consumed samples:       846080 | consumed tokens:   1732771840 | elapsed time per iteration (ms): 4212.9 | learning rate: 1.155E-04 | global batch size:   128 | lm loss: 1.973764E+00 | loss scale: 262144.0 | grad norm: 0.691 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.383 | tokens per gpu per second (tgs): 1944.494 | TFLOPs: 15.65 |
g0345: [2024-08-03 02:33:26,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=6620, skipped=4, lr=[0.00011567540906666667, 0.00011567540906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6620 loss: 2.2468 iter time (s): 4.180 samples/sec: 30.621
g0364:  iteration     6620/10000000 | consumed samples:       847360 | consumed tokens:   1735393280 | elapsed time per iteration (ms): 4212.9 | learning rate: 1.157E-04 | global batch size:   128 | lm loss: 2.009089E+00 | loss scale: 262144.0 | grad norm: 1.128 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.383 | tokens per gpu per second (tgs): 1944.482 | TFLOPs: 15.65 |
g0345: [2024-08-03 02:34:07,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=6630, skipped=4, lr=[0.00011585017173333333, 0.00011585017173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6630 loss: 2.2860 iter time (s): 4.143 samples/sec: 30.895
g0364:  iteration     6630/10000000 | consumed samples:       848640 | consumed tokens:   1738014720 | elapsed time per iteration (ms): 4175.8 | learning rate: 1.159E-04 | global batch size:   128 | lm loss: 1.955183E+00 | loss scale: 262144.0 | grad norm: 0.816 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.800 | TFLOPs: 15.79 |
g0345: [2024-08-03 02:34:49,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=6640, skipped=4, lr=[0.00011602493440000001, 0.00011602493440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6640 loss: 1.7836 iter time (s): 4.150 samples/sec: 30.846
g0364:  iteration     6640/10000000 | consumed samples:       849920 | consumed tokens:   1740636160 | elapsed time per iteration (ms): 4182.2 | learning rate: 1.160E-04 | global batch size:   128 | lm loss: 2.008986E+00 | loss scale: 262144.0 | grad norm: 0.639 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.606 | tokens per gpu per second (tgs): 1958.767 | TFLOPs: 15.76 |
g0345: [2024-08-03 02:35:32,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=6650, skipped=4, lr=[0.00011619969706666668, 0.00011619969706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6650 loss: 1.7591 iter time (s): 4.211 samples/sec: 30.395
g0364:  iteration     6650/10000000 | consumed samples:       851200 | consumed tokens:   1743257600 | elapsed time per iteration (ms): 4243.6 | learning rate: 1.162E-04 | global batch size:   128 | lm loss: 1.912237E+00 | loss scale: 262144.0 | grad norm: 0.704 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.163 | tokens per gpu per second (tgs): 1930.448 | TFLOPs: 15.53 |
g0345: [2024-08-03 02:36:13,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=6660, skipped=4, lr=[0.00011637445973333335, 0.00011637445973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6660 loss: 2.0721 iter time (s): 4.147 samples/sec: 30.866
g0364:  iteration     6660/10000000 | consumed samples:       852480 | consumed tokens:   1745879040 | elapsed time per iteration (ms): 4179.6 | learning rate: 1.164E-04 | global batch size:   128 | lm loss: 2.007308E+00 | loss scale: 262144.0 | grad norm: 0.792 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.625 | tokens per gpu per second (tgs): 1960.007 | TFLOPs: 15.77 |
g0345: [2024-08-03 02:36:55,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=6670, skipped=4, lr=[0.00011654922240000001, 0.00011654922240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6670 loss: 1.8933 iter time (s): 4.170 samples/sec: 30.695
g0364:  iteration     6670/10000000 | consumed samples:       853760 | consumed tokens:   1748500480 | elapsed time per iteration (ms): 4203.2 | learning rate: 1.165E-04 | global batch size:   128 | lm loss: 1.924864E+00 | loss scale: 262144.0 | grad norm: 0.610 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.453 | tokens per gpu per second (tgs): 1949.008 | TFLOPs: 15.68 |
g0345: [2024-08-03 02:37:37,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=6680, skipped=4, lr=[0.00011672398506666668, 0.00011672398506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6680 loss: 1.9657 iter time (s): 4.087 samples/sec: 31.316
g0364:  iteration     6680/10000000 | consumed samples:       855040 | consumed tokens:   1751121920 | elapsed time per iteration (ms): 4120.1 | learning rate: 1.167E-04 | global batch size:   128 | lm loss: 1.972003E+00 | loss scale: 262144.0 | grad norm: 0.705 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.067 | tokens per gpu per second (tgs): 1988.305 | TFLOPs: 16.00 |
g0345: [2024-08-03 02:38:19,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=6690, skipped=4, lr=[0.00011689874773333334, 0.00011689874773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6690 loss: 1.7688 iter time (s): 4.164 samples/sec: 30.738
g0364:  iteration     6690/10000000 | consumed samples:       856320 | consumed tokens:   1753743360 | elapsed time per iteration (ms): 4196.5 | learning rate: 1.169E-04 | global batch size:   128 | lm loss: 1.911263E+00 | loss scale: 262144.0 | grad norm: 0.854 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.502 | tokens per gpu per second (tgs): 1952.096 | TFLOPs: 15.71 |
g0345: [2024-08-03 02:39:00,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=6700, skipped=4, lr=[0.00011707351040000001, 0.00011707351040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6700 loss: 2.0709 iter time (s): 4.137 samples/sec: 30.941
g0364:  iteration     6700/10000000 | consumed samples:       857600 | consumed tokens:   1756364800 | elapsed time per iteration (ms): 4169.4 | learning rate: 1.171E-04 | global batch size:   128 | lm loss: 1.972014E+00 | loss scale: 262144.0 | grad norm: 0.737 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.700 | tokens per gpu per second (tgs): 1964.789 | TFLOPs: 15.81 |
g0345: [2024-08-03 02:39:42,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=6710, skipped=4, lr=[0.00011724827306666668, 0.00011724827306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6710 loss: 1.9847 iter time (s): 4.162 samples/sec: 30.757
g0364:  iteration     6710/10000000 | consumed samples:       858880 | consumed tokens:   1758986240 | elapsed time per iteration (ms): 4194.1 | learning rate: 1.172E-04 | global batch size:   128 | lm loss: 1.904183E+00 | loss scale: 262144.0 | grad norm: 0.673 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.519 | tokens per gpu per second (tgs): 1953.216 | TFLOPs: 15.72 |
g0345: [2024-08-03 02:40:24,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=6720, skipped=4, lr=[0.00011742303573333334, 0.00011742303573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6720 loss: 1.9668 iter time (s): 4.096 samples/sec: 31.249
g0364:  iteration     6720/10000000 | consumed samples:       860160 | consumed tokens:   1761607680 | elapsed time per iteration (ms): 4128.9 | learning rate: 1.174E-04 | global batch size:   128 | lm loss: 2.081172E+00 | loss scale: 262144.0 | grad norm: 0.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.001 | tokens per gpu per second (tgs): 1984.059 | TFLOPs: 15.97 |
g0345: [2024-08-03 02:41:05,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=6730, skipped=4, lr=[0.00011759779840000001, 0.00011759779840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6730 loss: 1.9419 iter time (s): 4.150 samples/sec: 30.841
g0364:  iteration     6730/10000000 | consumed samples:       861440 | consumed tokens:   1764229120 | elapsed time per iteration (ms): 4182.8 | learning rate: 1.176E-04 | global batch size:   128 | lm loss: 1.932354E+00 | loss scale: 262144.0 | grad norm: 0.654 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.601 | tokens per gpu per second (tgs): 1958.477 | TFLOPs: 15.76 |
g0345: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 02:41:43,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 02:41:43,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 02:41:43,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 02:41:43,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 02:41:48,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=6740, skipped=4, lr=[0.00011777256106666667, 0.00011777256106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6740 loss: 2.2481 iter time (s): 4.219 samples/sec: 30.342
g0364:  iteration     6740/10000000 | consumed samples:       862720 | consumed tokens:   1766850560 | elapsed time per iteration (ms): 4251.6 | learning rate: 1.178E-04 | global batch size:   128 | lm loss: 2.061036E+00 | loss scale: 524288.0 | grad norm: 0.756 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.106 | tokens per gpu per second (tgs): 1926.783 | TFLOPs: 15.51 |
g0345: [2024-08-03 02:42:28,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=6750, skipped=4, lr=[0.00011794732373333334, 0.00011794732373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6750 loss: 2.0915 iter time (s): 4.006 samples/sec: 31.949
g0364:  iteration     6750/10000000 | consumed samples:       864000 | consumed tokens:   1769472000 | elapsed time per iteration (ms): 4040.4 | learning rate: 1.179E-04 | global batch size:   128 | lm loss: 1.979162E+00 | loss scale: 524288.0 | grad norm: 0.674 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.680 | tokens per gpu per second (tgs): 2027.500 | TFLOPs: 16.32 |
g0346: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 6754
g0346: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 6754
g0346: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 6754
g0347: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 6754
g0362: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 6754
g0347: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: Grad overflow on iteration 6754
g0362: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 6754
g0362: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: Grad overflow on iteration 6754
g0345: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 6754
g0358: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: Grad overflow on iteration 6754
g0358: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 6754
g0362: Grad overflow on iteration 6754
g0358: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 6754
g0352: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: Grad overflow on iteration 6754
g0363: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 6754
g0362: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 6754
g0362: Grad overflow on iteration 6754
g0346: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 6754
g0345: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: Grad overflow on iteration 6754
g0364: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: Grad overflow on iteration 6754
g0345: [2024-08-03 02:42:48,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: Grad overflow on iteration 6754
g0364: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: Grad overflow on iteration 6754
g0364: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 6754
g0362: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 6754
g0364: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 02:42:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 02:42:48,577] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0347: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 6754
g0347: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 6754
g0346: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 6754
g0362: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 6754
g0345: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: Grad overflow on iteration 6754
g0363: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 6754
g0352: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 6754
g0358: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: Grad overflow on iteration 6754
g0364: [2024-08-03 02:42:48,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 02:43:09,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=6760, skipped=5, lr=[0.00011812208640000001, 0.00011812208640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6760 loss: 1.7614 iter time (s): 3.997 samples/sec: 32.026
g0364:  iteration     6760/10000000 | consumed samples:       865280 | consumed tokens:   1772093440 | elapsed time per iteration (ms): 4029.7 | learning rate: 1.181E-04 | global batch size:   128 | lm loss: 1.926541E+00 | loss scale: 262144.0 | grad norm: 0.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.764 | tokens per gpu per second (tgs): 2032.887 | TFLOPs: 16.36 |
g0345: [2024-08-03 02:43:49,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=6770, skipped=5, lr=[0.00011829684906666667, 0.00011829684906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6770 loss: 1.8966 iter time (s): 4.036 samples/sec: 31.713
g0364:  iteration     6770/10000000 | consumed samples:       866560 | consumed tokens:   1774714880 | elapsed time per iteration (ms): 4068.8 | learning rate: 1.183E-04 | global batch size:   128 | lm loss: 1.948074E+00 | loss scale: 262144.0 | grad norm: 0.587 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.459 | tokens per gpu per second (tgs): 2013.381 | TFLOPs: 16.20 |
g0345: [2024-08-03 02:44:31,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=6780, skipped=5, lr=[0.00011847161173333334, 0.00011847161173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6780 loss: 1.8071 iter time (s): 4.129 samples/sec: 31.001
g0364:  iteration     6780/10000000 | consumed samples:       867840 | consumed tokens:   1777336320 | elapsed time per iteration (ms): 4161.7 | learning rate: 1.185E-04 | global batch size:   128 | lm loss: 1.936568E+00 | loss scale: 262144.0 | grad norm: 0.597 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.757 | tokens per gpu per second (tgs): 1968.428 | TFLOPs: 15.84 |
g0345: [2024-08-03 02:45:12,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=6790, skipped=5, lr=[0.0001186463744, 0.0001186463744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6790 loss: 1.9791 iter time (s): 4.046 samples/sec: 31.640
g0364:  iteration     6790/10000000 | consumed samples:       869120 | consumed tokens:   1779957760 | elapsed time per iteration (ms): 4078.0 | learning rate: 1.186E-04 | global batch size:   128 | lm loss: 1.945083E+00 | loss scale: 262144.0 | grad norm: 0.609 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.388 | tokens per gpu per second (tgs): 2008.817 | TFLOPs: 16.17 |
g0345: [2024-08-03 02:45:53,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=6800, skipped=5, lr=[0.00011882113706666667, 0.00011882113706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6800 loss: 1.8106 iter time (s): 4.151 samples/sec: 30.838
g0364:  iteration     6800/10000000 | consumed samples:       870400 | consumed tokens:   1782579200 | elapsed time per iteration (ms): 4183.5 | learning rate: 1.188E-04 | global batch size:   128 | lm loss: 1.949119E+00 | loss scale: 262144.0 | grad norm: 0.929 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.597 | tokens per gpu per second (tgs): 1958.190 | TFLOPs: 15.76 |
g0345: [2024-08-03 02:46:35,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=6810, skipped=5, lr=[0.00011899589973333335, 0.00011899589973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6810 loss: 1.8173 iter time (s): 4.149 samples/sec: 30.852
g0364:  iteration     6810/10000000 | consumed samples:       871680 | consumed tokens:   1785200640 | elapsed time per iteration (ms): 4181.4 | learning rate: 1.190E-04 | global batch size:   128 | lm loss: 2.003457E+00 | loss scale: 262144.0 | grad norm: 1.777 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.612 | tokens per gpu per second (tgs): 1959.144 | TFLOPs: 15.77 |
g0345: [2024-08-03 02:47:17,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=6820, skipped=5, lr=[0.00011917066240000002, 0.00011917066240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6820 loss: 1.8149 iter time (s): 4.132 samples/sec: 30.981
g0364:  iteration     6820/10000000 | consumed samples:       872960 | consumed tokens:   1787822080 | elapsed time per iteration (ms): 4164.5 | learning rate: 1.192E-04 | global batch size:   128 | lm loss: 1.977272E+00 | loss scale: 262144.0 | grad norm: 0.676 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.736 | tokens per gpu per second (tgs): 1967.105 | TFLOPs: 15.83 |
g0345: [2024-08-03 02:47:59,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=6830, skipped=5, lr=[0.00011934542506666668, 0.00011934542506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6830 loss: 1.9428 iter time (s): 4.177 samples/sec: 30.646
g0364:  iteration     6830/10000000 | consumed samples:       874240 | consumed tokens:   1790443520 | elapsed time per iteration (ms): 4209.6 | learning rate: 1.193E-04 | global batch size:   128 | lm loss: 1.935062E+00 | loss scale: 262144.0 | grad norm: 0.802 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.407 | tokens per gpu per second (tgs): 1946.016 | TFLOPs: 15.66 |
g0345: [2024-08-03 02:48:40,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=6840, skipped=5, lr=[0.00011952018773333335, 0.00011952018773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6840 loss: 2.1007 iter time (s): 4.059 samples/sec: 31.537
g0364:  iteration     6840/10000000 | consumed samples:       875520 | consumed tokens:   1793064960 | elapsed time per iteration (ms): 4091.1 | learning rate: 1.195E-04 | global batch size:   128 | lm loss: 2.014536E+00 | loss scale: 262144.0 | grad norm: 0.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.287 | tokens per gpu per second (tgs): 2002.399 | TFLOPs: 16.11 |
g0345: [2024-08-03 02:49:22,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=6850, skipped=5, lr=[0.00011969495040000001, 0.00011969495040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6850 loss: 1.9865 iter time (s): 4.142 samples/sec: 30.903
g0364:  iteration     6850/10000000 | consumed samples:       876800 | consumed tokens:   1795686400 | elapsed time per iteration (ms): 4174.2 | learning rate: 1.197E-04 | global batch size:   128 | lm loss: 1.881356E+00 | loss scale: 262144.0 | grad norm: 0.629 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.664 | tokens per gpu per second (tgs): 1962.517 | TFLOPs: 15.79 |
g0345: [2024-08-03 02:50:03,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=6860, skipped=5, lr=[0.00011986971306666668, 0.00011986971306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6860 loss: 1.8863 iter time (s): 4.061 samples/sec: 31.522
g0364:  iteration     6860/10000000 | consumed samples:       878080 | consumed tokens:   1798307840 | elapsed time per iteration (ms): 4093.0 | learning rate: 1.199E-04 | global batch size:   128 | lm loss: 1.941115E+00 | loss scale: 262144.0 | grad norm: 0.653 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.273 | tokens per gpu per second (tgs): 2001.468 | TFLOPs: 16.11 |
g0345: [2024-08-03 02:50:44,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=6870, skipped=5, lr=[0.00012004447573333335, 0.00012004447573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6870 loss: 2.1445 iter time (s): 4.098 samples/sec: 31.236
g0364:  iteration     6870/10000000 | consumed samples:       879360 | consumed tokens:   1800929280 | elapsed time per iteration (ms): 4130.3 | learning rate: 1.200E-04 | global batch size:   128 | lm loss: 1.923893E+00 | loss scale: 262144.0 | grad norm: 0.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.990 | tokens per gpu per second (tgs): 1983.386 | TFLOPs: 15.96 |
g0345: [2024-08-03 02:51:25,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=6880, skipped=5, lr=[0.00012021923840000001, 0.00012021923840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6880 loss: 1.8250 iter time (s): 4.050 samples/sec: 31.603
g0364:  iteration     6880/10000000 | consumed samples:       880640 | consumed tokens:   1803550720 | elapsed time per iteration (ms): 4082.6 | learning rate: 1.202E-04 | global batch size:   128 | lm loss: 1.914989E+00 | loss scale: 262144.0 | grad norm: 0.709 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.352 | tokens per gpu per second (tgs): 2006.548 | TFLOPs: 16.15 |
g0345: [2024-08-03 02:52:06,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=6890, skipped=5, lr=[0.00012039400106666668, 0.00012039400106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6890 loss: 1.6630 iter time (s): 4.077 samples/sec: 31.397
g0364:  iteration     6890/10000000 | consumed samples:       881920 | consumed tokens:   1806172160 | elapsed time per iteration (ms): 4109.5 | learning rate: 1.204E-04 | global batch size:   128 | lm loss: 1.937222E+00 | loss scale: 262144.0 | grad norm: 0.645 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.147 | tokens per gpu per second (tgs): 1993.427 | TFLOPs: 16.04 |
g0345: [2024-08-03 02:52:47,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=6900, skipped=5, lr=[0.00012056876373333335, 0.00012056876373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6900 loss: 2.0662 iter time (s): 4.085 samples/sec: 31.337
g0364:  iteration     6900/10000000 | consumed samples:       883200 | consumed tokens:   1808793600 | elapsed time per iteration (ms): 4117.7 | learning rate: 1.206E-04 | global batch size:   128 | lm loss: 1.921827E+00 | loss scale: 262144.0 | grad norm: 0.529 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.085 | tokens per gpu per second (tgs): 1989.438 | TFLOPs: 16.01 |
g0345: [2024-08-03 02:53:28,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=6910, skipped=5, lr=[0.00012074352640000001, 0.00012074352640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6910 loss: 2.0577 iter time (s): 4.060 samples/sec: 31.530
g0364:  iteration     6910/10000000 | consumed samples:       884480 | consumed tokens:   1811415040 | elapsed time per iteration (ms): 4092.8 | learning rate: 1.207E-04 | global batch size:   128 | lm loss: 1.931034E+00 | loss scale: 262144.0 | grad norm: 1.039 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.274 | tokens per gpu per second (tgs): 2001.560 | TFLOPs: 16.11 |
g0345: [2024-08-03 02:54:10,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=6920, skipped=5, lr=[0.00012091828906666668, 0.00012091828906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6920 loss: 1.8279 iter time (s): 4.144 samples/sec: 30.890
g0364:  iteration     6920/10000000 | consumed samples:       885760 | consumed tokens:   1814036480 | elapsed time per iteration (ms): 4192.8 | learning rate: 1.209E-04 | global batch size:   128 | lm loss: 1.963774E+00 | loss scale: 262144.0 | grad norm: 0.718 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.806 | TFLOPs: 15.72 |
g0345: [2024-08-03 02:54:52,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=6930, skipped=5, lr=[0.00012109305173333334, 0.00012109305173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6930 loss: 1.8596 iter time (s): 4.184 samples/sec: 30.590
g0364:  iteration     6930/10000000 | consumed samples:       887040 | consumed tokens:   1816657920 | elapsed time per iteration (ms): 4220.5 | learning rate: 1.211E-04 | global batch size:   128 | lm loss: 1.969968E+00 | loss scale: 262144.0 | grad norm: 0.816 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.328 | tokens per gpu per second (tgs): 1940.988 | TFLOPs: 15.62 |
g0345: [2024-08-03 02:55:33,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=6940, skipped=5, lr=[0.00012126781440000001, 0.00012126781440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6940 loss: 1.9443 iter time (s): 4.050 samples/sec: 31.606
g0364:  iteration     6940/10000000 | consumed samples:       888320 | consumed tokens:   1819279360 | elapsed time per iteration (ms): 4091.8 | learning rate: 1.213E-04 | global batch size:   128 | lm loss: 1.950955E+00 | loss scale: 262144.0 | grad norm: 0.588 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.282 | tokens per gpu per second (tgs): 2002.076 | TFLOPs: 16.11 |
g0345: [2024-08-03 02:56:12,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=6950, skipped=5, lr=[0.00012144257706666668, 0.00012144257706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6950 loss: 1.8382 iter time (s): 3.914 samples/sec: 32.704
g0364:  iteration     6950/10000000 | consumed samples:       889600 | consumed tokens:   1821900800 | elapsed time per iteration (ms): 3946.5 | learning rate: 1.214E-04 | global batch size:   128 | lm loss: 1.882162E+00 | loss scale: 262144.0 | grad norm: 0.692 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.434 | tokens per gpu per second (tgs): 2075.769 | TFLOPs: 16.70 |
g0345: [2024-08-03 02:56:53,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=6960, skipped=5, lr=[0.00012161733973333334, 0.00012161733973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6960 loss: 1.5943 iter time (s): 3.985 samples/sec: 32.124
g0364:  iteration     6960/10000000 | consumed samples:       890880 | consumed tokens:   1824522240 | elapsed time per iteration (ms): 4017.3 | learning rate: 1.216E-04 | global batch size:   128 | lm loss: 1.825675E+00 | loss scale: 262144.0 | grad norm: 0.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.862 | tokens per gpu per second (tgs): 2039.161 | TFLOPs: 16.41 |
g0345: [2024-08-03 02:57:33,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=6970, skipped=5, lr=[0.00012179210240000001, 0.00012179210240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6970 loss: 1.8171 iter time (s): 4.053 samples/sec: 31.585
g0364:  iteration     6970/10000000 | consumed samples:       892160 | consumed tokens:   1827143680 | elapsed time per iteration (ms): 4085.5 | learning rate: 1.218E-04 | global batch size:   128 | lm loss: 1.863946E+00 | loss scale: 262144.0 | grad norm: 0.612 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.330 | tokens per gpu per second (tgs): 2005.151 | TFLOPs: 16.14 |
g0345: [2024-08-03 02:58:14,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=6980, skipped=5, lr=[0.00012196686506666667, 0.00012196686506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6980 loss: 1.7194 iter time (s): 4.006 samples/sec: 31.952
g0364:  iteration     6980/10000000 | consumed samples:       893440 | consumed tokens:   1829765120 | elapsed time per iteration (ms): 4038.5 | learning rate: 1.220E-04 | global batch size:   128 | lm loss: 1.851910E+00 | loss scale: 262144.0 | grad norm: 0.627 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.695 | tokens per gpu per second (tgs): 2028.473 | TFLOPs: 16.32 |
g0345: [2024-08-03 02:58:55,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=6990, skipped=5, lr=[0.00012214162773333334, 0.00012214162773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 6990 loss: 1.6478 iter time (s): 4.060 samples/sec: 31.530
g0364:  iteration     6990/10000000 | consumed samples:       894720 | consumed tokens:   1832386560 | elapsed time per iteration (ms): 4092.6 | learning rate: 1.221E-04 | global batch size:   128 | lm loss: 1.944736E+00 | loss scale: 262144.0 | grad norm: 0.662 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.276 | tokens per gpu per second (tgs): 2001.671 | TFLOPs: 16.11 |
g0345: [2024-08-03 02:59:37,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=5, lr=[0.0001223163904, 0.0001223163904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7000 loss: 2.2292 iter time (s): 4.161 samples/sec: 30.764
g0364:  iteration     7000/10000000 | consumed samples:       896000 | consumed tokens:   1835008000 | elapsed time per iteration (ms): 4197.8 | learning rate: 1.223E-04 | global batch size:   128 | lm loss: 1.970983E+00 | loss scale: 262144.0 | grad norm: 0.909 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.492 | tokens per gpu per second (tgs): 1951.513 | TFLOPs: 15.70 |
g0364: ------------------------------------------------------------------------------------------------
g0364:  validation loss at iteration 7000 | lm loss value: 1.894615E+00 | lm loss PPL: 6.649986E+00 | 
g0364: ------------------------------------------------------------------------------------------------
g0345: saving checkpoint at iteration    7000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: [2024-08-03 03:05:56,603] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7000 is about to be saved!
g0364: [2024-08-03 03:05:56,610] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0364: [2024-08-03 03:05:56,610] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0364: [2024-08-03 03:05:56,610] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0345: [2024-08-03 03:05:56,610] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0345: [2024-08-03 03:05:56,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0345: [2024-08-03 03:05:56,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0346: [2024-08-03 03:05:56,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0346: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0346: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0358: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0358: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0358: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0362: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0362: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0362: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0363: [2024-08-03 03:05:56,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0363: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0363: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0352: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0352: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0347: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0347: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0347: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0352: [2024-08-03 03:05:56,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0364: [2024-08-03 03:05:56,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0362: [2024-08-03 03:05:56,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0358: [2024-08-03 03:05:56,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0346: [2024-08-03 03:05:56,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0352: [2024-08-03 03:05:56,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0347: [2024-08-03 03:05:56,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0363: [2024-08-03 03:05:56,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0345: [2024-08-03 03:05:56,660] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0352: [2024-08-03 03:05:56,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0362: [2024-08-03 03:05:56,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0363: [2024-08-03 03:05:56,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0364: [2024-08-03 03:05:56,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0364: [2024-08-03 03:05:56,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0364: [2024-08-03 03:05:56,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0347: [2024-08-03 03:05:56,787] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0358: [2024-08-03 03:05:56,798] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0352: [2024-08-03 03:05:56,799] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0362: [2024-08-03 03:05:56,800] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0363: [2024-08-03 03:05:56,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0347: [2024-08-03 03:05:56,824] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0358: [2024-08-03 03:05:56,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0364: [2024-08-03 03:05:56,836] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0352: [2024-08-03 03:05:56,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0362: [2024-08-03 03:05:56,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0363: [2024-08-03 03:05:56,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0352: [2024-08-03 03:05:56,935] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0362: [2024-08-03 03:05:56,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0363: [2024-08-03 03:05:56,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0347: [2024-08-03 03:05:56,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0347: [2024-08-03 03:05:56,991] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0358: [2024-08-03 03:05:57,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0364: [2024-08-03 03:05:57,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0364: [2024-08-03 03:05:57,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt...
g0358: [2024-08-03 03:05:57,048] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0352: [2024-08-03 03:05:57,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0352: [2024-08-03 03:05:57,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt...
g0363: [2024-08-03 03:05:57,095] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0363: [2024-08-03 03:05:57,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt...
g0358: [2024-08-03 03:05:57,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0347: [2024-08-03 03:05:57,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0358: [2024-08-03 03:05:57,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt...
g0347: [2024-08-03 03:05:57,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt...
g0345: [2024-08-03 03:05:57,192] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0345: [2024-08-03 03:05:57,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0345: [2024-08-03 03:05:57,312] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0362: [2024-08-03 03:05:57,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0362: [2024-08-03 03:05:57,319] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt...
g0345: [2024-08-03 03:05:57,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0346: [2024-08-03 03:05:57,363] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0346: [2024-08-03 03:05:57,399] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0345: [2024-08-03 03:05:57,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0345: [2024-08-03 03:05:57,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0346: [2024-08-03 03:05:57,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0346: [2024-08-03 03:05:57,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0345: [2024-08-03 03:05:57,660] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0345: [2024-08-03 03:05:57,661] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt
g0345: [2024-08-03 03:05:57,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0346: [2024-08-03 03:05:57,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0346: [2024-08-03 03:05:57,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt...
g0347: [2024-08-03 03:05:59,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt.
g0347: [2024-08-03 03:05:59,539] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0363: [2024-08-03 03:05:59,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt.
g0363: [2024-08-03 03:05:59,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0362: [2024-08-03 03:05:59,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt.
g0362: [2024-08-03 03:05:59,584] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0352: [2024-08-03 03:05:59,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt.
g0352: [2024-08-03 03:05:59,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0364: [2024-08-03 03:05:59,886] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt.
g0364: [2024-08-03 03:05:59,886] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0358: [2024-08-03 03:05:59,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt.
g0358: [2024-08-03 03:05:59,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0346: [2024-08-03 03:06:00,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt.
g0346: [2024-08-03 03:06:00,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0345: [2024-08-03 03:06:01,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0345: [2024-08-03 03:06:01,100] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0345:   successfully saved checkpoint at iteration    7000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0345: Checkpoint Save GB: 22.521, GB/Sec: 4.98, Latency(second): 4.519
g0364: (min, max) time across ranks (ms):
g0364:     save-checkpoint ................................: (4518.54, 4519.63)
g0345: [2024-08-03 03:06:43,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=7010, skipped=5, lr=[0.00012249115306666667, 0.00012249115306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7010 loss: 1.9716 iter time (s): 4.163 samples/sec: 30.746
g0364:  iteration     7010/10000000 | consumed samples:       897280 | consumed tokens:   1837629440 | elapsed time per iteration (ms): 42577.6 | learning rate: 1.225E-04 | global batch size:   128 | lm loss: 1.886221E+00 | loss scale: 262144.0 | grad norm: 0.692 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.006 | tokens per gpu per second (tgs): 192.402 | TFLOPs: 1.55 |
g0345: [2024-08-03 03:07:23,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=7020, skipped=5, lr=[0.00012266591573333334, 0.00012266591573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7020 loss: 1.8540 iter time (s): 3.987 samples/sec: 32.101
g0364:  iteration     7020/10000000 | consumed samples:       898560 | consumed tokens:   1840250880 | elapsed time per iteration (ms): 4020.5 | learning rate: 1.227E-04 | global batch size:   128 | lm loss: 1.862927E+00 | loss scale: 262144.0 | grad norm: 0.687 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.837 | tokens per gpu per second (tgs): 2037.577 | TFLOPs: 16.40 |
g0345: [2024-08-03 03:08:04,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=7030, skipped=5, lr=[0.0001228406784, 0.0001228406784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7030 loss: 1.9232 iter time (s): 4.055 samples/sec: 31.566
g0364:  iteration     7030/10000000 | consumed samples:       899840 | consumed tokens:   1842872320 | elapsed time per iteration (ms): 4087.5 | learning rate: 1.228E-04 | global batch size:   128 | lm loss: 1.925746E+00 | loss scale: 262144.0 | grad norm: 0.627 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.315 | tokens per gpu per second (tgs): 2004.180 | TFLOPs: 16.13 |
g0345: [2024-08-03 03:08:46,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=7040, skipped=5, lr=[0.00012301544106666667, 0.00012301544106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7040 loss: 1.8358 iter time (s): 4.173 samples/sec: 30.674
g0364:  iteration     7040/10000000 | consumed samples:       901120 | consumed tokens:   1845493760 | elapsed time per iteration (ms): 4205.4 | learning rate: 1.230E-04 | global batch size:   128 | lm loss: 1.854631E+00 | loss scale: 262144.0 | grad norm: 0.761 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.437 | tokens per gpu per second (tgs): 1947.981 | TFLOPs: 15.68 |
g0345: [2024-08-03 03:09:27,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=7050, skipped=5, lr=[0.00012319020373333334, 0.00012319020373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7050 loss: 1.9527 iter time (s): 4.127 samples/sec: 31.017
g0364:  iteration     7050/10000000 | consumed samples:       902400 | consumed tokens:   1848115200 | elapsed time per iteration (ms): 4159.4 | learning rate: 1.232E-04 | global batch size:   128 | lm loss: 1.891785E+00 | loss scale: 262144.0 | grad norm: 0.605 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.773 | tokens per gpu per second (tgs): 1969.496 | TFLOPs: 15.85 |
g0345: [2024-08-03 03:10:09,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=7060, skipped=5, lr=[0.0001233649664, 0.0001233649664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7060 loss: 1.7798 iter time (s): 4.105 samples/sec: 31.184
g0364:  iteration     7060/10000000 | consumed samples:       903680 | consumed tokens:   1850736640 | elapsed time per iteration (ms): 4137.5 | learning rate: 1.234E-04 | global batch size:   128 | lm loss: 1.966923E+00 | loss scale: 262144.0 | grad norm: 0.718 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.937 | tokens per gpu per second (tgs): 1979.961 | TFLOPs: 15.93 |
g0345: [2024-08-03 03:10:50,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=7070, skipped=5, lr=[0.00012353972906666667, 0.00012353972906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7070 loss: 2.0419 iter time (s): 4.088 samples/sec: 31.308
g0364:  iteration     7070/10000000 | consumed samples:       904960 | consumed tokens:   1853358080 | elapsed time per iteration (ms): 4120.9 | learning rate: 1.235E-04 | global batch size:   128 | lm loss: 1.929515E+00 | loss scale: 262144.0 | grad norm: 0.634 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.062 | tokens per gpu per second (tgs): 1987.938 | TFLOPs: 16.00 |
g0345: [2024-08-03 03:11:31,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=7080, skipped=5, lr=[0.00012371449173333336, 0.00012371449173333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7080 loss: 1.8004 iter time (s): 4.056 samples/sec: 31.556
g0364:  iteration     7080/10000000 | consumed samples:       906240 | consumed tokens:   1855979520 | elapsed time per iteration (ms): 4088.5 | learning rate: 1.237E-04 | global batch size:   128 | lm loss: 1.897858E+00 | loss scale: 262144.0 | grad norm: 0.616 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.308 | tokens per gpu per second (tgs): 2003.682 | TFLOPs: 16.12 |
g0345: [2024-08-03 03:12:12,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=7090, skipped=5, lr=[0.00012388925440000003, 0.00012388925440000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7090 loss: 1.8618 iter time (s): 4.108 samples/sec: 31.156
g0364:  iteration     7090/10000000 | consumed samples:       907520 | consumed tokens:   1858600960 | elapsed time per iteration (ms): 4140.7 | learning rate: 1.239E-04 | global batch size:   128 | lm loss: 1.865980E+00 | loss scale: 262144.0 | grad norm: 0.660 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.913 | tokens per gpu per second (tgs): 1978.404 | TFLOPs: 15.92 |
g0345: [2024-08-03 03:12:53,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=7100, skipped=5, lr=[0.0001240640170666667, 0.0001240640170666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7100 loss: 1.9931 iter time (s): 4.094 samples/sec: 31.268
g0364:  iteration     7100/10000000 | consumed samples:       908800 | consumed tokens:   1861222400 | elapsed time per iteration (ms): 4126.1 | learning rate: 1.241E-04 | global batch size:   128 | lm loss: 2.030751E+00 | loss scale: 262144.0 | grad norm: 0.997 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.022 | tokens per gpu per second (tgs): 1985.420 | TFLOPs: 15.98 |
g0345: [2024-08-03 03:13:35,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=7110, skipped=5, lr=[0.00012423877973333336, 0.00012423877973333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7110 loss: 2.0359 iter time (s): 4.087 samples/sec: 31.320
g0364:  iteration     7110/10000000 | consumed samples:       910080 | consumed tokens:   1863843840 | elapsed time per iteration (ms): 4120.0 | learning rate: 1.242E-04 | global batch size:   128 | lm loss: 1.828723E+00 | loss scale: 262144.0 | grad norm: 0.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.068 | tokens per gpu per second (tgs): 1988.335 | TFLOPs: 16.00 |
g0345: [2024-08-03 03:14:16,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=7120, skipped=5, lr=[0.00012441354240000003, 0.00012441354240000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7120 loss: 1.9231 iter time (s): 4.147 samples/sec: 30.862
g0364:  iteration     7120/10000000 | consumed samples:       911360 | consumed tokens:   1866465280 | elapsed time per iteration (ms): 4181.1 | learning rate: 1.244E-04 | global batch size:   128 | lm loss: 1.943375E+00 | loss scale: 262144.0 | grad norm: 0.620 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.614 | tokens per gpu per second (tgs): 1959.289 | TFLOPs: 15.77 |
g0345: [2024-08-03 03:14:58,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=7130, skipped=5, lr=[0.0001245883050666667, 0.0001245883050666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7130 loss: 2.0316 iter time (s): 4.096 samples/sec: 31.251
g0364:  iteration     7130/10000000 | consumed samples:       912640 | consumed tokens:   1869086720 | elapsed time per iteration (ms): 4128.2 | learning rate: 1.246E-04 | global batch size:   128 | lm loss: 1.934438E+00 | loss scale: 262144.0 | grad norm: 0.768 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.006 | tokens per gpu per second (tgs): 1984.398 | TFLOPs: 15.97 |
g0345: [2024-08-03 03:15:39,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=7140, skipped=5, lr=[0.00012476306773333333, 0.00012476306773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7140 loss: 1.8313 iter time (s): 4.099 samples/sec: 31.226
g0364:  iteration     7140/10000000 | consumed samples:       913920 | consumed tokens:   1871708160 | elapsed time per iteration (ms): 4131.7 | learning rate: 1.248E-04 | global batch size:   128 | lm loss: 1.937835E+00 | loss scale: 262144.0 | grad norm: 0.844 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.980 | tokens per gpu per second (tgs): 1982.720 | TFLOPs: 15.96 |
g0345: [2024-08-03 03:16:22,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=7150, skipped=5, lr=[0.0001249378304, 0.0001249378304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7150 loss: 1.9595 iter time (s): 4.235 samples/sec: 30.221
g0364:  iteration     7150/10000000 | consumed samples:       915200 | consumed tokens:   1874329600 | elapsed time per iteration (ms): 4267.8 | learning rate: 1.249E-04 | global batch size:   128 | lm loss: 1.887410E+00 | loss scale: 262144.0 | grad norm: 0.682 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.992 | tokens per gpu per second (tgs): 1919.508 | TFLOPs: 15.45 |
g0345: [2024-08-03 03:17:04,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=7160, skipped=5, lr=[0.00012511259306666666, 0.00012511259306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7160 loss: 1.9544 iter time (s): 4.160 samples/sec: 30.770
g0364:  iteration     7160/10000000 | consumed samples:       916480 | consumed tokens:   1876951040 | elapsed time per iteration (ms): 4193.0 | learning rate: 1.251E-04 | global batch size:   128 | lm loss: 1.952763E+00 | loss scale: 262144.0 | grad norm: 0.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.527 | tokens per gpu per second (tgs): 1953.734 | TFLOPs: 15.72 |
g0345: [2024-08-03 03:17:46,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=7170, skipped=5, lr=[0.00012528735573333333, 0.00012528735573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7170 loss: 1.9314 iter time (s): 4.162 samples/sec: 30.754
g0364:  iteration     7170/10000000 | consumed samples:       917760 | consumed tokens:   1879572480 | elapsed time per iteration (ms): 4195.4 | learning rate: 1.253E-04 | global batch size:   128 | lm loss: 2.004163E+00 | loss scale: 262144.0 | grad norm: 0.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.510 | tokens per gpu per second (tgs): 1952.629 | TFLOPs: 15.71 |
g0345: [2024-08-03 03:18:27,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=7180, skipped=5, lr=[0.0001254621184, 0.0001254621184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7180 loss: 1.8613 iter time (s): 4.066 samples/sec: 31.483
g0364:  iteration     7180/10000000 | consumed samples:       919040 | consumed tokens:   1882193920 | elapsed time per iteration (ms): 4098.2 | learning rate: 1.255E-04 | global batch size:   128 | lm loss: 1.944798E+00 | loss scale: 262144.0 | grad norm: 0.490 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.233 | tokens per gpu per second (tgs): 1998.935 | TFLOPs: 16.09 |
g0345: [2024-08-03 03:19:09,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=7190, skipped=5, lr=[0.00012563688106666666, 0.00012563688106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7190 loss: 1.7603 iter time (s): 4.164 samples/sec: 30.740
g0364:  iteration     7190/10000000 | consumed samples:       920320 | consumed tokens:   1884815360 | elapsed time per iteration (ms): 4196.8 | learning rate: 1.256E-04 | global batch size:   128 | lm loss: 1.863643E+00 | loss scale: 262144.0 | grad norm: 0.916 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.500 | tokens per gpu per second (tgs): 1951.970 | TFLOPs: 15.71 |
g0345: [2024-08-03 03:19:49,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=7200, skipped=5, lr=[0.00012581164373333333, 0.00012581164373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7200 loss: 1.9567 iter time (s): 4.031 samples/sec: 31.753
g0364:  iteration     7200/10000000 | consumed samples:       921600 | consumed tokens:   1887436800 | elapsed time per iteration (ms): 4063.7 | learning rate: 1.258E-04 | global batch size:   128 | lm loss: 1.921816E+00 | loss scale: 262144.0 | grad norm: 0.599 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.498 | tokens per gpu per second (tgs): 2015.894 | TFLOPs: 16.22 |
g0345: [2024-08-03 03:20:30,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=7210, skipped=5, lr=[0.0001259864064, 0.0001259864064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7210 loss: 1.7714 iter time (s): 4.099 samples/sec: 31.228
g0364:  iteration     7210/10000000 | consumed samples:       922880 | consumed tokens:   1890058240 | elapsed time per iteration (ms): 4131.2 | learning rate: 1.260E-04 | global batch size:   128 | lm loss: 1.918993E+00 | loss scale: 262144.0 | grad norm: 0.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.984 | tokens per gpu per second (tgs): 1982.958 | TFLOPs: 15.96 |
g0345: [2024-08-03 03:21:11,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=7220, skipped=5, lr=[0.00012616116906666666, 0.00012616116906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7220 loss: 2.0527 iter time (s): 4.025 samples/sec: 31.804
g0364:  iteration     7220/10000000 | consumed samples:       924160 | consumed tokens:   1892679680 | elapsed time per iteration (ms): 4056.9 | learning rate: 1.262E-04 | global batch size:   128 | lm loss: 1.862226E+00 | loss scale: 262144.0 | grad norm: 0.654 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.551 | tokens per gpu per second (tgs): 2019.259 | TFLOPs: 16.25 |
g0345: [2024-08-03 03:21:53,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=7230, skipped=5, lr=[0.00012633593173333333, 0.00012633593173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7230 loss: 1.5496 iter time (s): 4.174 samples/sec: 30.663
g0364:  iteration     7230/10000000 | consumed samples:       925440 | consumed tokens:   1895301120 | elapsed time per iteration (ms): 4208.0 | learning rate: 1.263E-04 | global batch size:   128 | lm loss: 1.847659E+00 | loss scale: 262144.0 | grad norm: 0.543 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.418 | tokens per gpu per second (tgs): 1946.784 | TFLOPs: 15.67 |
g0345: [2024-08-03 03:22:34,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=7240, skipped=5, lr=[0.0001265106944, 0.0001265106944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7240 loss: 1.7445 iter time (s): 4.080 samples/sec: 31.373
g0364:  iteration     7240/10000000 | consumed samples:       926720 | consumed tokens:   1897922560 | elapsed time per iteration (ms): 4113.6 | learning rate: 1.265E-04 | global batch size:   128 | lm loss: 1.941157E+00 | loss scale: 262144.0 | grad norm: 0.644 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.116 | tokens per gpu per second (tgs): 1991.449 | TFLOPs: 16.03 |
g0345: [2024-08-03 03:23:17,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=7250, skipped=5, lr=[0.00012668545706666666, 0.00012668545706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7250 loss: 1.9282 iter time (s): 4.212 samples/sec: 30.393
g0364:  iteration     7250/10000000 | consumed samples:       928000 | consumed tokens:   1900544000 | elapsed time per iteration (ms): 4245.0 | learning rate: 1.267E-04 | global batch size:   128 | lm loss: 1.901297E+00 | loss scale: 262144.0 | grad norm: 0.756 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.153 | tokens per gpu per second (tgs): 1929.790 | TFLOPs: 15.53 |
g0345: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0363: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0352: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0362: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0347: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0358: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0346: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0345: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0364: [2024-08-03 03:23:42,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0362: [2024-08-03 03:23:42,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 03:23:42,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0345: [2024-08-03 03:23:58,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=7260, skipped=5, lr=[0.00012686021973333332, 0.00012686021973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7260 loss: 2.0164 iter time (s): 4.140 samples/sec: 30.919
g0364:  iteration     7260/10000000 | consumed samples:       929280 | consumed tokens:   1903165440 | elapsed time per iteration (ms): 4172.7 | learning rate: 1.269E-04 | global batch size:   128 | lm loss: 1.869441E+00 | loss scale: 524288.0 | grad norm: 0.516 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.676 | tokens per gpu per second (tgs): 1963.242 | TFLOPs: 15.80 |
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 7266
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 7266
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 7266
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 7266
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 7266
g0363: Grad overflow on iteration 7266
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: Grad overflow on iteration 7266
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: Grad overflow on iteration 7266
g0364: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: Grad overflow on iteration 7266
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: Grad overflow on iteration 7266
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: Grad overflow on iteration 7266
g0345: Grad overflow on iteration 7266
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0352: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: Grad overflow on iteration 7266
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: Grad overflow on iteration 7266
g0347: Grad overflow on iteration 7266
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: Grad overflow on iteration 7266
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 7266
g0363: Grad overflow on iteration 7266
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: Grad overflow on iteration 7266
g0363: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: Grad overflow on iteration 7266
g0364: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 7266
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 7266
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: Grad overflow on iteration 7266
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 7266
g0364: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0346: Grad overflow on iteration 7266
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0364: Grad overflow on iteration 7266
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0364: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0358: Grad overflow on iteration 7266
g0364: Grad overflow on iteration 7266
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0362: Grad overflow on iteration 7266
g0364: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0362: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: Grad overflow on iteration 7266
g0358: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: Grad overflow on iteration 7266
g0345: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0346: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:344:_update_scale] 
g0347: Grad overflow on iteration 7266
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0347: [2024-08-03 03:24:27,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 03:24:27,914] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0364: [2024-08-03 03:24:27,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0345: [2024-08-03 03:24:40,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=7270, skipped=6, lr=[0.0001270349824, 0.0001270349824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7270 loss: 1.8748 iter time (s): 4.092 samples/sec: 31.283
g0364:  iteration     7270/10000000 | consumed samples:       930560 | consumed tokens:   1905786880 | elapsed time per iteration (ms): 4124.4 | learning rate: 1.270E-04 | global batch size:   128 | lm loss: 1.875289E+00 | loss scale: 262144.0 | grad norm: 0.578 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.035 | tokens per gpu per second (tgs): 1986.221 | TFLOPs: 15.98 |
g0345: [2024-08-03 03:25:21,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=7280, skipped=6, lr=[0.00012720974506666666, 0.00012720974506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7280 loss: 1.8053 iter time (s): 4.101 samples/sec: 31.213
g0364:  iteration     7280/10000000 | consumed samples:       931840 | consumed tokens:   1908408320 | elapsed time per iteration (ms): 4134.3 | learning rate: 1.272E-04 | global batch size:   128 | lm loss: 1.855345E+00 | loss scale: 262144.0 | grad norm: 0.842 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.961 | tokens per gpu per second (tgs): 1981.478 | TFLOPs: 15.95 |
g0345: [2024-08-03 03:26:03,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=7290, skipped=6, lr=[0.00012738450773333332, 0.00012738450773333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7290 loss: 1.7788 iter time (s): 4.198 samples/sec: 30.494
g0364:  iteration     7290/10000000 | consumed samples:       933120 | consumed tokens:   1911029760 | elapsed time per iteration (ms): 4230.4 | learning rate: 1.274E-04 | global batch size:   128 | lm loss: 1.923461E+00 | loss scale: 262144.0 | grad norm: 0.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.257 | tokens per gpu per second (tgs): 1936.442 | TFLOPs: 15.58 |
g0345: [2024-08-03 03:26:46,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=7300, skipped=6, lr=[0.0001275592704, 0.0001275592704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0345: steps: 7300 loss: 1.8008 iter time (s): 4.226 samples/sec: 30.291
g0364:  iteration     7300/10000000 | consumed samples:       934400 | consumed tokens:   1913651200 | elapsed time per iteration (ms): 4258.6 | learning rate: 1.276E-04 | global batch size:   128 | lm loss: 1.938096E+00 | loss scale: 262144.0 | grad norm: 0.516 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.057 | tokens per gpu per second (tgs): 1923.636 | TFLOPs: 15.48 |
