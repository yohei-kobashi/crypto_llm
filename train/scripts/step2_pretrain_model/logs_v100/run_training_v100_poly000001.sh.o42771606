
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0314
    HostName g0314
    Port 2222
    StrictHostKeyChecking no

Host g0316
    HostName g0316
    Port 2222
    StrictHostKeyChecking no

Host g0318
    HostName g0318
    Port 2222
    StrictHostKeyChecking no

Host g0319
    HostName g0319
    Port 2222
    StrictHostKeyChecking no

Host g0320
    HostName g0320
    Port 2222
    StrictHostKeyChecking no

Host g0325
    HostName g0325
    Port 2222
    StrictHostKeyChecking no

Host g0329
    HostName g0329
    Port 2222
    StrictHostKeyChecking no

Host g0332
    HostName g0332
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000001_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000001_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000001_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42771606
g0314 slots=4
g0316 slots=4
g0318 slots=4
g0319 slots=4
g0320 slots=4
g0325 slots=4
g0329 slots=4
g0332 slots=4

[2024-08-02 18:04:24,547] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 18:04:30,281] [INFO] [runner.py:463:main] Using IP address of 10.1.10.8 for node g0314
[2024-08-02 18:04:30,298] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0314,g0316,g0318,g0319,g0320,g0325,g0329,g0332
[2024-08-02 18:04:30,299] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0314,g0316,g0318,g0319,g0320,g0325,g0329,g0332 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDMxNCI6IFswLCAxLCAyLCAzXSwgImcwMzE2IjogWzAsIDEsIDIsIDNdLCAiZzAzMTgiOiBbMCwgMSwgMiwgM10sICJnMDMxOSI6IFswLCAxLCAyLCAzXSwgImcwMzIwIjogWzAsIDEsIDIsIDNdLCAiZzAzMjUiOiBbMCwgMSwgMiwgM10sICJnMDMyOSI6IFswLCAxLCAyLCAzXSwgImcwMzMyIjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.10.8 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000001_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000001_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000001_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000001_1234_True' --wandb_tag 'other_gpu'
g0314: [2024-08-02 18:04:33,751] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0314: [2024-08-02 18:04:36,014] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0314: [2024-08-02 18:04:36,014] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0314: [2024-08-02 18:04:36,014] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0314: [2024-08-02 18:04:36,014] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0314: [2024-08-02 18:04:36,014] [INFO] [launch.py:163:main] dist_world_size=32
g0314: [2024-08-02 18:04:36,014] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0314: [2024-08-02 18:04:39,080] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0314: [2024-08-02 18:04:39,080] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0314: [2024-08-02 18:04:39,180] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0314: [2024-08-02 18:04:39,298] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0319: [2024-08-02 18:04:40,280] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0325: [2024-08-02 18:04:40,821] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0316: [2024-08-02 18:04:40,942] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0332: [2024-08-02 18:04:41,138] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0329: [2024-08-02 18:04:41,187] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0318: [2024-08-02 18:04:41,812] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0320: [2024-08-02 18:04:42,196] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0314: --------------------------------------------------
g0314: DeepSpeed C++/CUDA extension op report
g0314: --------------------------------------------------
g0314: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0314:       runtime if needed. Op compatibility means that your system
g0314:       meet the required dependencies to JIT install the op.
g0314: --------------------------------------------------
g0314: JIT compiled ops requires ninja
g0314: --------------------------------------------------
g0314: DeepSpeed C++/CUDA extension op report
g0314: --------------------------------------------------
g0314: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0314:       runtime if needed. Op compatibility means that your system
g0314:       meet the required dependencies to JIT install the op.
g0314: --------------------------------------------------
g0314: JIT compiled ops requires ninja
g0314: --------------------------------------------------
g0314: DeepSpeed C++/CUDA extension op report
g0314: --------------------------------------------------
g0314: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0314:       runtime if needed. Op compatibility means that your system
g0314:       meet the required dependencies to JIT install the op.
g0314: --------------------------------------------------
g0314: JIT compiled ops requires ninja
g0314: --------------------------------------------------
g0314: DeepSpeed C++/CUDA extension op report
g0314: --------------------------------------------------
g0314: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0314:       runtime if needed. Op compatibility means that your system
g0314:       meet the required dependencies to JIT install the op.
g0314: --------------------------------------------------
g0314: JIT compiled ops requires ninja
g0314: ninjaninjaninjaninja    ........................................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0314: 
g0314: 
g0314: 
g0314: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0314: 
g0314: 
g0314: 
g0314: op nameop nameop nameop name    ................................................................    installedinstalledinstalledinstalled    ........    compatiblecompatiblecompatiblecompatible
g0314: 
g0314: 
g0314: 
g0314: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0314: 
g0314: 
g0314: 
g0314: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0314: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0314: fused_lamb ............. [92m[YES][0m ......async_io [92m[OKAY][0m 
g0314: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: fused_lion ............. [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0314: [92m[YES][0m ...... [92m[OKAY][0m
g0314: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0314: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0314: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: cpu_adam async_io...............  [92m[YES][0m .....................  [92m[YES][0m[92m[OKAY][0m 
g0314: ...... [92m[OKAY][0mcpu_adagrad
g0314:  ............ [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0314:  ............. [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0314:  ...... [92m[OKAY][0mcpu_adam
g0314:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0314: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0314: cpu_adagrad ............evoformer_attn  [92m[YES][0m.........  ......[93m[NO][0m  [92m[OKAY][0m.......
g0314:  [93m[NO][0m
g0314: cpu_lion fused_lamb...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0314: [92m[OKAY][0m
g0314: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0314: fused_lion evoformer_attn.............  .........[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0314: [93m[NO][0m
g0314: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0314: inference_core_ops .....inference_core_ops  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0314:  [92m[OKAY][0m
g0314: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0314: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0314: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0314: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0314: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0314: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0314: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0314: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0314: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0314: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0314: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0314: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0314: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0314: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0314: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0314: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0314: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0314: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0314: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0314: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0314: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0314: --------------------------------------------------
g0314: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0314: --------------------------------------------------
g0314: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0314: --------------------------------------------------
g0314: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0314: --------------------------------------------------
g0314: DeepSpeed general environment info:
g0314: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0314: torch version .................... 2.0.1+cu118
g0314: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0314: deepspeed info ................... 0.12.4, unknown, unknown
g0314: torch cuda version ............... 11.8
g0314: torch hip version ................ None
g0314: nvcc version ..................... 11.8
g0314: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0314: shared memory (/dev/shm) size .... 188.13 GB
g0314: DeepSpeed general environment info:
g0314: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0314: torch version .................... 2.0.1+cu118
g0314: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0314: deepspeed info ................... 0.12.4, unknown, unknown
g0314: torch cuda version ............... 11.8
g0314: torch hip version ................ None
g0314: nvcc version ..................... 11.8
g0314: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0314: shared memory (/dev/shm) size .... 188.13 GB
g0314: DeepSpeed general environment info:
g0314: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0314: torch version .................... 2.0.1+cu118
g0314: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0314: deepspeed info ................... 0.12.4, unknown, unknown
g0314: torch cuda version ............... 11.8
g0314: torch hip version ................ None
g0314: nvcc version ..................... 11.8
g0314: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0314: shared memory (/dev/shm) size .... 188.13 GB
g0314: DeepSpeed general environment info:
g0314: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0314: torch version .................... 2.0.1+cu118
g0314: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0314: deepspeed info ................... 0.12.4, unknown, unknown
g0314: torch cuda version ............... 11.8
g0314: torch hip version ................ None
g0314: nvcc version ..................... 11.8
g0314: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0314: shared memory (/dev/shm) size .... 188.13 GB
g0314: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0314: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0314: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0314: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0314: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0314: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0314: using torch.float32 for parameters ...
g0314: ------------------------ arguments ------------------------
g0314:   accumulate_allreduce_grads_in_fp32 .............. False
g0314:   adam_beta1 ...................................... 0.9
g0314:   adam_beta2 ...................................... 0.95
g0314:   adam_eps ........................................ 1e-08
g0314:   add_bias_linear ................................. False
g0314:   add_position_embedding .......................... False
g0314:   adlr_autoresume ................................. False
g0314:   adlr_autoresume_interval ........................ 1000
g0314:   aml_data_download_path .......................... None
g0314:   apply_layernorm_1p .............................. False
g0314:   apply_query_key_layer_scaling ................... False
g0314:   apply_residual_connection_post_layernorm ........ False
g0314:   async_tensor_model_parallel_allreduce ........... False
g0314:   attention_dropout ............................... 0.0
g0314:   attention_softmax_in_fp32 ....................... False
g0314:   barrier_with_L1_time ............................ True
g0314:   bert_binary_head ................................ True
g0314:   bert_embedder_type .............................. megatron
g0314:   bert_load ....................................... None
g0314:   bf16 ............................................ False
g0314:   bias_dropout_fusion ............................. True
g0314:   bias_gelu_fusion ................................ False
g0314:   biencoder_projection_dim ........................ 0
g0314:   biencoder_shared_query_context_model ............ False
g0314:   block_data_path ................................. None
g0314:   checkpoint_activations .......................... False
g0314:   checkpoint_in_cpu ............................... False
g0314:   checkpoint_num_layers ........................... 1
g0314:   classes_fraction ................................ 1.0
g0314:   clip_grad ....................................... 1.0
g0314:   compression_training ............................ False
g0314:   consumed_train_samples .......................... 0
g0314:   consumed_train_tokens ........................... 0
g0314:   consumed_valid_samples .......................... 0
g0314:   contigious_checkpointing ........................ False
g0314:   cpu_optimizer ................................... False
g0314:   cpu_torch_adam .................................. False
g0314:   create_moe_param_group .......................... False
g0314:   curriculum_learning_legacy ...................... False
g0314:   data_cache_path ................................. None
g0314:   data_efficiency_curriculum_learning ............. False
g0314:   data_impl ....................................... mmap
g0314:   data_parallel_random_init ....................... False
g0314:   data_parallel_size .............................. 4
g0314:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000001_1234_True_text_document']
g0314:   data_per_class_fraction ......................... 1.0
g0314:   data_sharding ................................... True
g0314:   dataloader_type ................................. single
g0314:   DDP_impl ........................................ local
g0314:   decoder_num_layers .............................. None
g0314:   decoder_seq_length .............................. None
g0314:   deepscale ....................................... False
g0314:   deepscale_config ................................ None
g0314:   deepspeed ....................................... True
g0314:   deepspeed_activation_checkpointing .............. False
g0314:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0314:   deepspeed_mpi ................................... False
g0314:   dino_bottleneck_size ............................ 256
g0314:   dino_freeze_last_layer .......................... 1
g0314:   dino_head_hidden_size ........................... 2048
g0314:   dino_local_crops_number ......................... 10
g0314:   dino_local_img_size ............................. 96
g0314:   dino_norm_last_layer ............................ False
g0314:   dino_teacher_temp ............................... 0.07
g0314:   dino_warmup_teacher_temp ........................ 0.04
g0314:   dino_warmup_teacher_temp_epochs ................. 30
g0314:   distribute_checkpointed_activations ............. False
g0314:   distribute_saved_activations .................... False
g0314:   distributed_backend ............................. nccl
g0314:   distributed_timeout_minutes ..................... 10
g0314:   ds_fused_adam ................................... False
g0314:   ds_inference .................................... False
g0314:   ds_pipeline_enabled ............................. True
g0314:   ds_sequence_parallel_size ....................... 1
g0314:   embedding_path .................................. None
g0314:   embedding_weights_in_fp32 ....................... False
g0314:   empty_unused_memory_level ....................... 0
g0314:   enable_expert_tensor_parallelism ................ False
g0314:   encoder_num_layers .............................. 22
g0314:   encoder_seq_length .............................. 2048
g0314:   end_weight_decay ................................ 0.1
g0314:   eod_mask_loss ................................... False
g0314:   eval_interval ................................... 1000
g0314:   eval_iters ...................................... 100
g0314:   evidence_data_path .............................. None
g0314:   exit_duration_in_mins ........................... 30000000
g0314:   exit_interval ................................... None
g0314:   exit_on_missing_checkpoint ...................... False
g0314:   exit_signal_handler ............................. False
g0314:   expert_interval ................................. 2
g0314:   ffn_hidden_size ................................. 5632
g0314:   finetune ........................................ False
g0314:   force_ds_sequence_parallel ...................... False
g0314:   fp16 ............................................ False
g0314:   fp16_lm_cross_entropy ........................... False
g0314:   fp32_residual_connection ........................ False
g0314:   fp8_amax_compute_algo ........................... most_recent
g0314:   fp8_amax_history_len ............................ 1
g0314:   fp8_e4m3 ........................................ False
g0314:   fp8_hybrid ...................................... False
g0314:   fp8_interval .................................... 1
g0314:   fp8_margin ...................................... 0
g0314:   fp8_wgrad ....................................... True
g0314:   global_batch_size ............................... 128
g0314:   gradient_accumulation_fusion .................... True
g0314:   head_lr_mult .................................... 1.0
g0314:   hidden_dropout .................................. 0.0
g0314:   hidden_size ..................................... 2048
g0314:   hidden_size_teacher ............................. None
g0314:   hysteresis ...................................... 2
g0314:   ict_head_size ................................... None
g0314:   ict_load ........................................ None
g0314:   img_h ........................................... 224
g0314:   img_w ........................................... 224
g0314:   indexer_batch_size .............................. 128
g0314:   indexer_log_interval ............................ 1000
g0314:   inference ....................................... False
g0314:   inference_batch_times_seqlen_threshold .......... 512
g0314:   init_method_std ................................. 0.013
g0314:   init_method_xavier_uniform ...................... False
g0314:   initial_loss_scale .............................. 4294967296
g0314:   iter_per_epoch .................................. 1250
g0314:   kd .............................................. False
g0314:   kd_alpha_ce ..................................... 1
g0314:   kd_beta_ce ...................................... 1
g0314:   kd_temp ......................................... 1.0
g0314:   kv_channels ..................................... 128
g0314:   layernorm_epsilon ............................... 1e-05
g0314:   lazy_mpu_init ................................... None
g0314:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314:   load_teacher .................................... None
g0314:   local_rank ...................................... 0
g0314:   log_batch_size_to_tensorboard ................... True
g0314:   log_interval .................................... 10
g0314:   log_learning_rate_to_tensorboard ................ True
g0314:   log_loss_scale_to_tensorboard ................... True
g0314:   log_memory_to_tensorboard ....................... False
g0314:   log_num_zeros_in_grad ........................... False
g0314:   log_optimizer_states_to_tensorboard ............. True
g0314:   log_params_norm ................................. False
g0314:   log_timers_to_tensorboard ....................... True
g0314:   log_validation_ppl_to_tensorboard ............... True
g0314:   log_world_size_to_tensorboard ................... False
g0314:   loss_scale ...................................... None
g0314:   loss_scale_window ............................... 1000
g0314:   lr .............................................. 0.0002
g0314:   lr_decay_iters .................................. None
g0314:   lr_decay_samples ................................ None
g0314:   lr_decay_style .................................. cosine
g0314:   lr_decay_tokens ................................. 300000000000
g0314:   lr_warmup_fraction .............................. None
g0314:   lr_warmup_iters ................................. 0
g0314:   lr_warmup_samples ............................... 0
g0314:   lr_warmup_tokens ................................ 3000000000
g0314:   make_vocab_size_divisible_by .................... 128
g0314:   mask_factor ..................................... 1.0
g0314:   mask_prob ....................................... 0.15
g0314:   mask_type ....................................... random
g0314:   masked_softmax_fusion ........................... True
g0314:   max_position_embeddings ......................... 2048
g0314:   max_tokens_to_oom ............................... 12000
g0314:   mem_efficient_ln ................................ True
g0314:   memory_centric_tiled_linear ..................... False
g0314:   merge_file ...................................... None
g0314:   micro_batch_size ................................ 1
g0314:   min_loss_scale .................................. 1.0
g0314:   min_lr .......................................... 1e-05
g0314:   mlp_type ........................................ standard
g0314:   mmap_warmup ..................................... False
g0314:   moe_eval_capacity_factor ........................ 1.0
g0314:   moe_expert_parallel_size ........................ 1
g0314:   moe_loss_coeff .................................. 0.1
g0314:   moe_min_capacity ................................ 4
g0314:   moe_token_dropping .............................. True
g0314:   moe_train_capacity_factor ....................... 1.0
g0314:   mos ............................................. False
g0314:   no_load_lr_state ................................ False
g0314:   no_load_optim ................................... None
g0314:   no_load_rng ..................................... None
g0314:   no_persist_layer_norm ........................... False
g0314:   no_pipeline_parallel ............................ False
g0314:   no_save_optim ................................... None
g0314:   no_save_rng ..................................... None
g0314:   normalization ................................... rmsnorm
g0314:   num_attention_heads ............................. 16
g0314:   num_attention_heads_teacher ..................... None
g0314:   num_channels .................................... 3
g0314:   num_classes ..................................... 1000
g0314:   num_experts ..................................... [1]
g0314:   num_experts_switch .............................. None
g0314:   num_experts_teacher ............................. [1]
g0314:   num_key_value_heads ............................. 4
g0314:   num_layers ...................................... 22
g0314:   num_layers_per_virtual_pipeline_stage ........... None
g0314:   num_layers_teacher .............................. None
g0314:   num_workers ..................................... 0
g0314:   onnx_safe ....................................... None
g0314:   openai_gelu ..................................... False
g0314:   optimizer ....................................... adam
g0314:   output_bert_embeddings .......................... False
g0314:   overlap_p2p_comm ................................ False
g0314:   override_opt_param_scheduler .................... True
g0314:   params_dtype .................................... torch.float32
g0314:   partition_activations ........................... False
g0314:   patch_dim ....................................... 16
g0314:   perform_initialization .......................... True
g0314:   pipeline_model_parallel_size .................... 8
g0314:   pipeline_model_parallel_split_rank .............. None
g0314:   profile_backward ................................ False
g0314:   query_in_block_prob ............................. 0.1
g0314:   rampup_batch_size ............................... None
g0314:   random_ltd ...................................... False
g0314:   rank ............................................ 0
g0314:   recompute_granularity ........................... None
g0314:   recompute_method ................................ None
g0314:   recompute_num_layers ............................ 1
g0314:   remote_device ................................... none
g0314:   repeated_dataloader ............................. False
g0314:   reset_attention_mask ............................ False
g0314:   reset_iteration ................................. False
g0314:   reset_position_ids .............................. False
g0314:   retriever_report_topk_accuracies ................ []
g0314:   retriever_score_scaling ......................... False
g0314:   retriever_seq_length ............................ 256
g0314:   retro_add_retriever ............................. False
g0314:   retro_cyclic_train_iters ........................ None
g0314:   retro_encoder_attention_dropout ................. 0.1
g0314:   retro_encoder_hidden_dropout .................... 0.1
g0314:   retro_encoder_layers ............................ 2
g0314:   retro_num_neighbors ............................. 2
g0314:   retro_num_retrieved_chunks ...................... 2
g0314:   retro_return_doc_ids ............................ False
g0314:   retro_workdir ................................... None
g0314:   return_data_index ............................... False
g0314:   rotary_percent .................................. 1.0
g0314:   sample_rate ..................................... 1.0
g0314:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314:   save_interval ................................... 1000
g0314:   scatter_gather_tensors_in_pipeline .............. True
g0314:   scattered_embeddings ............................ False
g0314:   seed ............................................ 1234
g0314:   seq_length ...................................... 2048
g0314:   sequence_parallel ............................... False
g0314:   sgd_momentum .................................... 0.9
g0314:   short_seq_prob .................................. 0.1
g0314:   skip_train ...................................... False
g0314:   split ........................................... 949,50,1
g0314:   split_transformers .............................. False
g0314:   squared_relu .................................... False
g0314:   standalone_embedding_stage ...................... False
g0314:   start_weight_decay .............................. 0.1
g0314:   swiglu .......................................... True
g0314:   swin_backbone_type .............................. tiny
g0314:   synchronize_each_layer .......................... False
g0314:   tensor_model_parallel_size ...................... 1
g0314:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000001_1234_True
g0314:   tensorboard_log_interval ........................ 1
g0314:   tensorboard_queue_size .......................... 1
g0314:   test_data_path .................................. None
g0314:   tf32 ............................................ False
g0314:   tile_factor ..................................... 1
g0314:   timing_log_level ................................ 0
g0314:   timing_log_option ............................... minmax
g0314:   titles_data_path ................................ None
g0314:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000001_1234_True.model
g0314:   tokenizer_type .................................. SentencePieceTokenizer
g0314:   topk ............................................ 1
g0314:   train_data_exact_num_epochs ..................... 1
g0314:   train_data_path ................................. None
g0314:   train_desc_path ................................. None
g0314:   train_doc_idx_path .............................. None
g0314:   train_idx_path .................................. None
g0314:   train_iters ..................................... None
g0314:   train_sample_idx_path ........................... None
g0314:   train_samples ................................... 1280000000
g0314:   train_shuffle_idx_path .......................... None
g0314:   train_tokens .................................... 2621440000000
g0314:   transformer_impl ................................ local
g0314:   transformer_pipeline_model_parallel_size ........ 8
g0314:   universal_checkpoint ............................ False
g0314:   untie_embeddings_and_output_weights ............. True
g0314:   use_checkpoint_args ............................. False
g0314:   use_checkpoint_opt_param_scheduler .............. False
g0314:   use_contiguous_buffers_in_local_ddp ............. True
g0314:   use_cpu_initialization .......................... None
g0314:   use_dataset_only ................................ False
g0314:   use_distributed_optimizer ....................... False
g0314:   use_flash_attn .................................. False
g0314:   use_flash_attn_triton ........................... False
g0314:   use_flash_attn_v1 ............................... False
g0314:   use_flash_attn_v2 ............................... False
g0314:   use_one_sent_docs ............................... False
g0314:   use_pin_memory .................................. False
g0314:   use_ring_exchange_p2p ........................... False
g0314:   use_rotary_position_embeddings .................. True
g0314:   use_tutel ....................................... False
g0314:   use_wandb ....................................... True
g0314:   valid_data_path ................................. None
g0314:   variable_seq_lengths ............................ False
g0314:   virtual_pipeline_model_parallel_size ............ None
g0314:   vision_backbone_type ............................ vit
g0314:   vision_pretraining .............................. False
g0314:   vision_pretraining_type ......................... classify
g0314:   vocab_extra_ids ................................. 0
g0314:   vocab_file ...................................... None
g0314:   vocab_size ...................................... None
g0314:   wandb_entity .................................... yohei-kobashi
g0314:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000001_1234_True
g0314:   wandb_project ................................... encrypted_data_LLM
g0314:   wandb_tag ....................................... other_gpu
g0314:   weight_decay .................................... 0.1
g0314:   weight_decay_incr_style ......................... constant
g0314:   world_size ...................................... 32
g0314:   zero_allgather_bucket_size ...................... 0.0
g0314:   zero_contigious_gradients ....................... False
g0314:   zero_reduce_bucket_size ......................... 0.0
g0314:   zero_reduce_scatter ............................. False
g0314:   zero_stage ...................................... 0
g0314: -------------------- end of arguments ---------------------
g0314: setting number of micro-batches to constant 32
g0314: > building SentencePieceTokenizer tokenizer ...
g0314: [2024-08-02 18:04:43,569] [INFO] [comm.py:637:init_distributed] cdb=None
g0314:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0314: > initializing torch distributed ...
g0314: [2024-08-02 18:04:43,571] [INFO] [comm.py:637:init_distributed] cdb=None
g0314: [2024-08-02 18:04:43,571] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0314: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [2024-08-02 18:04:43,610] [INFO] [comm.py:637:init_distributed] cdb=None
g0314: [2024-08-02 18:04:43,611] [INFO] [comm.py:637:init_distributed] cdb=None
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [2024-08-02 18:04:44,109] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0319: [2024-08-02 18:04:44,109] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0319: [2024-08-02 18:04:44,110] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0319: [2024-08-02 18:04:44,110] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0319: [2024-08-02 18:04:44,110] [INFO] [launch.py:163:main] dist_world_size=32
g0319: [2024-08-02 18:04:44,110] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0325: [2024-08-02 18:04:44,971] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0325: [2024-08-02 18:04:44,971] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0325: [2024-08-02 18:04:44,971] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0325: [2024-08-02 18:04:44,972] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0325: [2024-08-02 18:04:44,972] [INFO] [launch.py:163:main] dist_world_size=32
g0325: [2024-08-02 18:04:44,972] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0316: [2024-08-02 18:04:45,088] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0316: [2024-08-02 18:04:45,088] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0316: [2024-08-02 18:04:45,088] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0316: [2024-08-02 18:04:45,089] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0316: [2024-08-02 18:04:45,089] [INFO] [launch.py:163:main] dist_world_size=32
g0316: [2024-08-02 18:04:45,089] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0329: [2024-08-02 18:04:45,224] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0329: [2024-08-02 18:04:45,225] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0329: [2024-08-02 18:04:45,225] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0329: [2024-08-02 18:04:45,225] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0329: [2024-08-02 18:04:45,225] [INFO] [launch.py:163:main] dist_world_size=32
g0329: [2024-08-02 18:04:45,225] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0332: [2024-08-02 18:04:45,256] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0332: [2024-08-02 18:04:45,256] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0332: [2024-08-02 18:04:45,256] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0332: [2024-08-02 18:04:45,256] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0332: [2024-08-02 18:04:45,256] [INFO] [launch.py:163:main] dist_world_size=32
g0332: [2024-08-02 18:04:45,256] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0318: [2024-08-02 18:04:45,872] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0318: [2024-08-02 18:04:45,872] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0318: [2024-08-02 18:04:45,872] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0318: [2024-08-02 18:04:45,872] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0318: [2024-08-02 18:04:45,873] [INFO] [launch.py:163:main] dist_world_size=32
g0318: [2024-08-02 18:04:45,873] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0320: [2024-08-02 18:04:46,271] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0320: [2024-08-02 18:04:46,271] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0314': [0, 1, 2, 3], 'g0316': [0, 1, 2, 3], 'g0318': [0, 1, 2, 3], 'g0319': [0, 1, 2, 3], 'g0320': [0, 1, 2, 3], 'g0325': [0, 1, 2, 3], 'g0329': [0, 1, 2, 3], 'g0332': [0, 1, 2, 3]}
g0320: [2024-08-02 18:04:46,271] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0320: [2024-08-02 18:04:46,271] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0314': [0, 1, 2, 3], 'g0316': [4, 5, 6, 7], 'g0318': [8, 9, 10, 11], 'g0319': [12, 13, 14, 15], 'g0320': [16, 17, 18, 19], 'g0325': [20, 21, 22, 23], 'g0329': [24, 25, 26, 27], 'g0332': [28, 29, 30, 31]})
g0320: [2024-08-02 18:04:46,271] [INFO] [launch.py:163:main] dist_world_size=32
g0320: [2024-08-02 18:04:46,271] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0319: [2024-08-02 18:04:47,236] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0319: [2024-08-02 18:04:47,251] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0319: [2024-08-02 18:04:47,330] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0319: [2024-08-02 18:04:47,479] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0325: [2024-08-02 18:04:48,064] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0325: [2024-08-02 18:04:48,065] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0325: [2024-08-02 18:04:48,090] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0325: [2024-08-02 18:04:48,108] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0316: [2024-08-02 18:04:48,182] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0316: [2024-08-02 18:04:48,201] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0316: [2024-08-02 18:04:48,290] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0329: [2024-08-02 18:04:48,370] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0329: [2024-08-02 18:04:48,370] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0329: [2024-08-02 18:04:48,370] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0316: [2024-08-02 18:04:48,415] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0332: [2024-08-02 18:04:48,440] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0332: [2024-08-02 18:04:48,441] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0332: [2024-08-02 18:04:48,441] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0329: [2024-08-02 18:04:48,467] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0332: [2024-08-02 18:04:48,565] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0318: [2024-08-02 18:04:49,024] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0318: [2024-08-02 18:04:49,032] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0318: [2024-08-02 18:04:49,079] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0318: [2024-08-02 18:04:49,174] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0320: [2024-08-02 18:04:49,387] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0320: [2024-08-02 18:04:49,436] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0320: [2024-08-02 18:04:49,466] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0320: [2024-08-02 18:04:49,541] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0319: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0319: 
g0319: 
g0319: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0319: 
g0319: 
g0319: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0319: 
g0319: 
g0319: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0319:       runtime if needed. Op compatibility means that your system
g0319:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0319:       runtime if needed. Op compatibility means that your system
g0319:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0319:       runtime if needed. Op compatibility means that your system
g0319:       meet the required dependencies to JIT install the op.
g0319: 
g0319: 
g0319: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0319: 
g0319: 
g0319: JIT compiled ops requires ninjaJIT compiled ops requires ninjaJIT compiled ops requires ninja
g0319: 
g0319: 
g0319: --------------------------------------------------
g0319: DeepSpeed C++/CUDA extension op report
g0319: --------------------------------------------------
g0319: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0319:       runtime if needed. Op compatibility means that your system
g0319:       meet the required dependencies to JIT install the op.
g0319: --------------------------------------------------
g0319: JIT compiled ops requires ninja
g0319: ninjaninjaninja  .................. ..................ninja  ..................[92m[OKAY][0m [92m[OKAY][0m 
g0319: ..................
g0319: [92m[OKAY][0m --------------------------------------------------[92m[OKAY][0m
g0319: --------------------------------------------------
g0319: 
g0319: 
g0319: --------------------------------------------------op name--------------------------------------------------op name
g0319:  
g0319:  ................................op nameop name    installedinstalled................................    ....installedinstalled    compatiblecompatible....
g0319: 
g0319:   compatible--------------------------------------------------compatible--------------------------------------------------
g0319: 
g0319: 
g0319: 
g0319: ----------------------------------------------------------------------------------------------------
g0319: 
g0319: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0319: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0319: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0319: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0319: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0319: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: async_io ............... [92m[YES][0m async_io......  [92m[OKAY][0m............... 
g0319: [92m[YES][0m ...... [92m[OKAY][0m
g0319: fused_adam ............. [92m[YES][0m ...... fused_adam[92m[OKAY][0m 
g0319: ............. [92m[YES][0m cpu_adam......  [92m[OKAY][0m...............
g0319:  [92m[YES][0m ...... cpu_adam[92m[OKAY][0m 
g0319: ............... [92m[YES][0m cpu_adagrad......  ............[92m[OKAY][0m 
g0319: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0319: ............ [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0319:  ...... cpu_lion[92m[OKAY][0m 
g0319: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0319: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0319: evoformer_attn .........[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0319: [93m[NO][0m evoformer_attn.......  .........[93m[NO][0m 
g0319: [93m[NO][0m .......fused_lamb  [93m[NO][0m.............
g0319:  [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0319: [92m[YES][0m ...... [92m[OKAY][0m
g0319: fused_lion ............. [92m[YES][0mfused_lion  ...................  [92m[OKAY][0m[92m[YES][0m
g0319:  ...... [92m[OKAY][0m
g0319: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0319: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0319: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0319: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0319: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0319: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0319: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0319: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0319: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0319: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0319: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0319: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0319: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0319: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0319: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0319: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0319: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0319: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0319: random_ltdsparse_attn  .........................  [93m[NO][0m[92m[YES][0m  ....... ......[93m[NO][0m 
g0319: [92m[OKAY][0m
g0319: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0319: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0319: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0319: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0319: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0319: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0319: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0319: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0319: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0319: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0319: --------------------------------------------------
g0319: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0319: --------------------------------------------------
g0319: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0319: --------------------------------------------------
g0319: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0319: --------------------------------------------------
g0319: DeepSpeed general environment info:
g0319: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0319: torch version .................... 2.0.1+cu118
g0319: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0319: deepspeed info ................... 0.12.4, unknown, unknown
g0319: torch cuda version ............... 11.8
g0319: torch hip version ................ None
g0319: nvcc version ..................... 11.8
g0319: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0319: shared memory (/dev/shm) size .... 188.13 GB
g0319: DeepSpeed general environment info:
g0319: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0319: torch version .................... 2.0.1+cu118
g0319: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0319: deepspeed info ................... 0.12.4, unknown, unknown
g0319: torch cuda version ............... 11.8
g0319: torch hip version ................ None
g0319: nvcc version ..................... 11.8
g0319: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0319: shared memory (/dev/shm) size .... 188.13 GB
g0319: DeepSpeed general environment info:
g0319: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0319: torch version .................... 2.0.1+cu118
g0319: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0319: deepspeed info ................... 0.12.4, unknown, unknown
g0319: torch cuda version ............... 11.8
g0319: torch hip version ................ None
g0319: nvcc version ..................... 11.8
g0319: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0319: shared memory (/dev/shm) size .... 188.13 GB
g0319: DeepSpeed general environment info:
g0319: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0319: torch version .................... 2.0.1+cu118
g0319: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0319: deepspeed info ................... 0.12.4, unknown, unknown
g0319: torch cuda version ............... 11.8
g0319: torch hip version ................ None
g0319: nvcc version ..................... 11.8
g0319: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0319: shared memory (/dev/shm) size .... 188.13 GB
g0319: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0319: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0319: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0319: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0319: [2024-08-02 18:04:51,740] [INFO] [comm.py:637:init_distributed] cdb=None
g0319: [2024-08-02 18:04:51,740] [INFO] [comm.py:637:init_distributed] cdb=None
g0319: [2024-08-02 18:04:51,741] [INFO] [comm.py:637:init_distributed] cdb=None
g0319: [2024-08-02 18:04:51,741] [INFO] [comm.py:637:init_distributed] cdb=None
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0319: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0325: --------------------------------------------------
g0325: DeepSpeed C++/CUDA extension op report
g0325: --------------------------------------------------
g0325: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0325:       runtime if needed. Op compatibility means that your system
g0325:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0325: ----------------------------------------------------------------------------------------------------
g0325: 
g0325: 
g0325: DeepSpeed C++/CUDA extension op reportJIT compiled ops requires ninjaDeepSpeed C++/CUDA extension op report
g0325: 
g0325: 
g0325: ----------------------------------------------------------------------------------------------------
g0325: 
g0325: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0325:       runtime if needed. Op compatibility means that your system
g0325:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0325:       runtime if needed. Op compatibility means that your system
g0325:       meet the required dependencies to JIT install the op.
g0325: 
g0325: ----------------------------------------------------------------------------------------------------
g0325: 
g0325: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0325: 
g0325: --------------------------------------------------
g0325: DeepSpeed C++/CUDA extension op report
g0325: --------------------------------------------------
g0325: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0325:       runtime if needed. Op compatibility means that your system
g0325:       meet the required dependencies to JIT install the op.
g0325: --------------------------------------------------
g0325: JIT compiled ops requires ninja
g0325: ninjaninjaninja ninja  ......................................................    ..................[92m[OKAY][0m[92m[OKAY][0m [92m[OKAY][0m
g0325: [92m[OKAY][0m
g0325: --------------------------------------------------
g0325: 
g0325: 
g0325: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0325: op name
g0325: 
g0325:  op name................op nameop name   ................ installed ................................ installed  .. installed ..installed  compatible.. compatible
g0325:  ..
g0325: compatible-------------------------------------------------- 
g0325: --------------------------------------------------
g0325: compatible
g0325: --------------------------------------------------
g0325: 
g0325: --------------------------------------------------
g0325: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0325: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0325: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0325: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHasync_io
g0325:  ...............evoformer_attn  [92m[YES][0m.........  ......[93m[NO][0m  [92m[OKAY][0m....... 
g0325: [93m[NO][0m
g0325: fused_lamb fused_adam.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0325: [92m[OKAY][0m
g0325: cpu_adam ............... [92m[YES][0m fused_lion......async_io  .............[92m[OKAY][0m  
g0325: ...............[92m[YES][0m  [92m[YES][0m......cpu_adagrad   ......[92m[OKAY][0m............ 
g0325:  [92m[OKAY][0m[92m[YES][0m
g0325:  ...... [92m[OKAY][0m
g0325: cpu_lion fused_adam...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0325: [92m[OKAY][0m
g0325: cpu_adam ...............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0325: [92m[YES][0m ......evoformer_attn  [92m[OKAY][0m.........
g0325:  [93m[NO][0m .......cpu_adagrad  async_io[93m[NO][0m............
g0325:   [92m[YES][0m...............  ......fused_lamb[92m[YES][0m   [92m[OKAY][0m...................
g0325:   [92m[YES][0m[92m[OKAY][0m 
g0325: cpu_lion......  [92m[OKAY][0m...............
g0325:  [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0325: [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0325: [92m[YES][0m ...... cpu_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m 
g0325: 
g0325: ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0325: ....... [93m[NO][0mcpu_adagrad
g0325:  ............ fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0325: ...... [92m[OKAY][0mcpu_lion
g0325:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0325: fused_lion ............. [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0325: 
g0325: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0325: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0325: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0325: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0325: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0316: --------------------------------------------------
g0316: DeepSpeed C++/CUDA extension op report
g0316: --------------------------------------------------
g0316: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0316:       runtime if needed. Op compatibility means that your system
g0316:       meet the required dependencies to JIT install the op.
g0316: --------------------------------------------------
g0316: JIT compiled ops requires ninja
g0316: --------------------------------------------------
g0316: DeepSpeed C++/CUDA extension op report
g0316: --------------------------------------------------
g0316: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0316:       runtime if needed. Op compatibility means that your system
g0316:       meet the required dependencies to JIT install the op.
g0316: --------------------------------------------------
g0316: JIT compiled ops requires ninja
g0316: --------------------------------------------------
g0316: DeepSpeed C++/CUDA extension op report
g0316: --------------------------------------------------
g0316: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0316:       runtime if needed. Op compatibility means that your system
g0316:       meet the required dependencies to JIT install the op.
g0316: --------------------------------------------------
g0316: JIT compiled ops requires ninja
g0316: --------------------------------------------------
g0316: DeepSpeed C++/CUDA extension op report
g0316: --------------------------------------------------
g0316: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0316:       runtime if needed. Op compatibility means that your system
g0316:       meet the required dependencies to JIT install the op.
g0316: --------------------------------------------------
g0316: JIT compiled ops requires ninja
g0325: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: ninjaninjaninjaninja   .................. .................................... ..................  [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0316: 
g0316: 
g0316: 
g0316: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0316: --------------------------------------------------
g0316: 
g0316: 
g0316: op nameop nameop nameop name    ................................................................    installedinstalledinstalled installed  .. .... ..  compatible compatiblecompatible
g0316: compatible
g0316: 
g0316: --------------------------------------------------
g0316: ----------------------------------------------------------------------------------------------------
g0316: 
g0316: 
g0316: --------------------------------------------------
g0325: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0325: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0325: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0325: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0325: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0325: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0325: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0325: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0325: sparse_attn ............ [93m[NO][0m ragged_ops.......  .............[93m[NO][0m 
g0325: [92m[YES][0m ...... [92m[OKAY][0m
g0325: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0325: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0325: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0325: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0325: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0325: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0325: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0325: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0325: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0325: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0325: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0325: --------------------------------------------------
g0325: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0325: --------------------------------------------------
g0325: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0325: --------------------------------------------------
g0325: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0325: --------------------------------------------------
g0325: DeepSpeed general environment info:
g0325: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0325: torch version .................... 2.0.1+cu118
g0325: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0325: deepspeed info ................... 0.12.4, unknown, unknown
g0325: torch cuda version ............... 11.8
g0325: torch hip version ................ None
g0325: nvcc version ..................... 11.8
g0325: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0325: shared memory (/dev/shm) size .... 188.13 GB
g0325: DeepSpeed general environment info:
g0325: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0325: torch version .................... 2.0.1+cu118
g0325: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0325: deepspeed info ................... 0.12.4, unknown, unknown
g0325: torch cuda version ............... 11.8
g0325: torch hip version ................ None
g0325: nvcc version ..................... 11.8
g0325: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0325: shared memory (/dev/shm) size .... 188.13 GB
g0325: DeepSpeed general environment info:
g0325: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0325: torch version .................... 2.0.1+cu118
g0325: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0325: deepspeed info ................... 0.12.4, unknown, unknown
g0325: torch cuda version ............... 11.8
g0325: torch hip version ................ None
g0325: nvcc version ..................... 11.8
g0325: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0325: shared memory (/dev/shm) size .... 188.13 GB
g0325: DeepSpeed general environment info:
g0325: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0325: torch version .................... 2.0.1+cu118
g0325: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0325: deepspeed info ................... 0.12.4, unknown, unknown
g0325: torch cuda version ............... 11.8
g0325: torch hip version ................ None
g0325: nvcc version ..................... 11.8
g0325: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0325: shared memory (/dev/shm) size .... 188.13 GB
g0325: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0325: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0325: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0325: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0316: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0316: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0316: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0316: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0316: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0316: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: async_io ............... fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0316: ...... [92m[OKAY][0m
g0316: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0316: cpu_adagrad ............ [92m[YES][0m async_io...... [92m[OKAY][0m 
g0316: ............... [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0masync_io[92m[YES][0m
g0316:   .....................  [92m[OKAY][0m[92m[YES][0m
g0316:  fused_adam......  .............[92m[OKAY][0m 
g0316: [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0316: 
g0316: fused_adamevoformer_attn  cpu_adam......... ............. ............... [93m[NO][0m [92m[YES][0m [92m[YES][0m ....... ...... ...... [93m[NO][0m [92m[OKAY][0m
g0316: [92m[OKAY][0m
g0316: 
g0316: fused_lambcpu_adamcpu_adagrad   ........................................   [92m[YES][0m[92m[YES][0m[92m[YES][0m   ..................   [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0316: 
g0316: 
g0316: cpu_lioncpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  fused_lion............   .............[92m[OKAY][0m[92m[OKAY][0m 
g0316: 
g0316: [92m[YES][0m ...... cpu_lion[92m[OKAY][0m 
g0316: ............... [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0316:  [92m[OKAY][0mevoformer_attn
g0316:  ......... [93m[NO][0m ....... [93m[NO][0m
g0316: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0316: fused_lambevoformer_attn  ......................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0316: 
g0316: fused_lamb ............. [92m[YES][0mfused_lion  ...................  [92m[OKAY][0m[92m[YES][0m
g0316:  ...... [92m[OKAY][0m
g0316: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0316: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0316: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0316: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0316: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0316: ragged_device_ops ...... ragged_device_opsragged_device_ops[92m[YES][0m   ..................   [92m[YES][0m[92m[OKAY][0m [92m[YES][0m
g0316: ......  ......[92m[OKAY][0m 
g0316: [92m[OKAY][0m
g0316: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0316: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0316: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0316: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0316: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: random_ltd ............. [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[92m[YES][0m 
g0316: ...... [92m[OKAY][0msparse_attn
g0316:  ............ [93m[NO][0m ....... [93m[NO][0m
g0316: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0316: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0316: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0316: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0316: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0316: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0316: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0316: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0316: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0316: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0316: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0316: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0316: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0316: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0316: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0316: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0316: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0316: --------------------------------------------------
g0316: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0316: --------------------------------------------------
g0316: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0316: --------------------------------------------------
g0316: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0316: --------------------------------------------------
g0316: DeepSpeed general environment info:
g0316: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0316: torch version .................... 2.0.1+cu118
g0316: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0316: deepspeed info ................... 0.12.4, unknown, unknown
g0316: torch cuda version ............... 11.8
g0316: torch hip version ................ None
g0316: nvcc version ..................... 11.8
g0316: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0316: shared memory (/dev/shm) size .... 188.13 GB
g0316: DeepSpeed general environment info:
g0316: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0316: torch version .................... 2.0.1+cu118
g0316: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0316: deepspeed info ................... 0.12.4, unknown, unknown
g0316: torch cuda version ............... 11.8
g0316: torch hip version ................ None
g0316: nvcc version ..................... 11.8
g0316: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0316: shared memory (/dev/shm) size .... 188.13 GB
g0316: DeepSpeed general environment info:
g0316: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0316: torch version .................... 2.0.1+cu118
g0316: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0316: deepspeed info ................... 0.12.4, unknown, unknown
g0316: torch cuda version ............... 11.8
g0316: torch hip version ................ None
g0316: nvcc version ..................... 11.8
g0316: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0316: shared memory (/dev/shm) size .... 188.13 GB
g0316: DeepSpeed general environment info:
g0316: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0316: torch version .................... 2.0.1+cu118
g0316: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0316: deepspeed info ................... 0.12.4, unknown, unknown
g0316: torch cuda version ............... 11.8
g0316: torch hip version ................ None
g0316: nvcc version ..................... 11.8
g0316: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0316: shared memory (/dev/shm) size .... 188.13 GB
g0325: [2024-08-02 18:04:52,620] [INFO] [comm.py:637:init_distributed] cdb=None
g0325: [2024-08-02 18:04:52,622] [INFO] [comm.py:637:init_distributed] cdb=None
g0325: [2024-08-02 18:04:52,624] [INFO] [comm.py:637:init_distributed] cdb=None
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0316: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0316: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0325: [2024-08-02 18:04:52,630] [INFO] [comm.py:637:init_distributed] cdb=None
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0325: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: --------------------------------------------------
g0329: DeepSpeed C++/CUDA extension op report
g0329: --------------------------------------------------
g0329: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0329:       runtime if needed. Op compatibility means that your system
g0329:       meet the required dependencies to JIT install the op.
g0329: --------------------------------------------------
g0329: JIT compiled ops requires ninja
g0329: --------------------------------------------------
g0329: DeepSpeed C++/CUDA extension op report
g0329: --------------------------------------------------
g0329: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0329:       runtime if needed. Op compatibility means that your system
g0329:       meet the required dependencies to JIT install the op.
g0329: --------------------------------------------------
g0329: JIT compiled ops requires ninja
g0329: --------------------------------------------------
g0329: DeepSpeed C++/CUDA extension op report
g0329: --------------------------------------------------
g0329: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0329:       runtime if needed. Op compatibility means that your system
g0329:       meet the required dependencies to JIT install the op.
g0329: ----------------------------------------------------------------------------------------------------
g0329: 
g0329: JIT compiled ops requires ninjaDeepSpeed C++/CUDA extension op report
g0329: 
g0329: --------------------------------------------------
g0329: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0329:       runtime if needed. Op compatibility means that your system
g0329:       meet the required dependencies to JIT install the op.
g0329: --------------------------------------------------
g0329: JIT compiled ops requires ninja
g0329: ninjaninjaninja   ....................................ninja..................   [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m
g0329: ..................
g0329: 
g0329:  --------------------------------------------------[92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0329: 
g0329: 
g0329: 
g0329: op nameop name--------------------------------------------------op name  
g0329:  ................................................ op name  installed installedinstalled ................  .. .... installed  compatible compatiblecompatible
g0329: ..
g0329: 
g0329:  ------------------------------------------------------------------------------------------------------------------------------------------------------compatible
g0329: 
g0329: 
g0329: 
g0329: --------------------------------------------------
g0316: [2024-08-02 18:04:52,725] [INFO] [comm.py:637:init_distributed] cdb=None
g0316: [2024-08-02 18:04:52,725] [INFO] [comm.py:637:init_distributed] cdb=None
g0316: [2024-08-02 18:04:52,725] [INFO] [comm.py:637:init_distributed] cdb=None
g0316: [2024-08-02 18:04:52,726] [INFO] [comm.py:637:init_distributed] cdb=None
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0316: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0329: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0329: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0329: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0329: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0329: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0329: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0329: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0329: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0329: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0329: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: async_io ............... [92m[YES][0m ...... async_io[92m[OKAY][0m
g0329:  ............... [92m[YES][0m ......fused_adam  [92m[OKAY][0m.............
g0329:  [92m[YES][0m ...... [92m[OKAY][0m
g0329: fused_adam ............. cpu_adam[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0329: ...... [92m[OKAY][0m
g0329: cpu_adam ...............cpu_adagrad  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m
g0329: ...... [92m[OKAY][0mcpu_adagrad
g0329:  ............ cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0329: ...... [92m[OKAY][0mcpu_lion
g0329:  ............... [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0329: 
g0329: evoformer_attn ......... [93m[NO][0m .......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0329: [93m[NO][0m
g0329: evoformer_attn .........fused_lamb  [93m[NO][0m.............  .......[92m[YES][0m  [93m[NO][0m......
g0329:  [92m[OKAY][0m
g0329: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0mfused_lion
g0329:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0minference_core_ops
g0329:  ..... [92m[YES][0m ...... [92m[OKAY][0m
g0329: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0329: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0329: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0329: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0329: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0329: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0329: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0329: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0329: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0329: 
g0329: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0329: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0329: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0329: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0329: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0329: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0329: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0329: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0329: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0329: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0329: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0332: --------------------------------------------------
g0332: DeepSpeed C++/CUDA extension op report
g0332: --------------------------------------------------
g0332: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0332:       runtime if needed. Op compatibility means that your system
g0332:       meet the required dependencies to JIT install the op.
g0332: --------------------------------------------------
g0332: JIT compiled ops requires ninja
g0332: --------------------------------------------------
g0332: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0332: 
g0332: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0332: 
g0332: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0332:       runtime if needed. Op compatibility means that your system
g0332:       meet the required dependencies to JIT install the op.
g0332: --------------------------------------------------
g0332: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0332:       runtime if needed. Op compatibility means that your system
g0332:       meet the required dependencies to JIT install the op.
g0332: 
g0332: JIT compiled ops requires ninja--------------------------------------------------
g0332: 
g0332: JIT compiled ops requires ninja
g0332: --------------------------------------------------
g0332: DeepSpeed C++/CUDA extension op report
g0332: --------------------------------------------------
g0332: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0332:       runtime if needed. Op compatibility means that your system
g0332:       meet the required dependencies to JIT install the op.
g0332: --------------------------------------------------
g0332: JIT compiled ops requires ninja
g0329: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0329: --------------------------------------------------
g0329: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0329: --------------------------------------------------
g0329: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0329: --------------------------------------------------
g0329: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0329: --------------------------------------------------
g0329: DeepSpeed general environment info:
g0329: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0329: torch version .................... 2.0.1+cu118
g0329: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0329: deepspeed info ................... 0.12.4, unknown, unknown
g0329: torch cuda version ............... 11.8
g0329: torch hip version ................ None
g0329: nvcc version ..................... 11.8
g0329: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0329: shared memory (/dev/shm) size .... 188.13 GB
g0332: ninjaninjaninjaninja   .................................... ..................  ..................  [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0332: 
g0332: 
g0332: 
g0332: ----------------------------------------------------------------------------------------------------
g0332: ----------------------------------------------------------------------------------------------------
g0332: 
g0332: op name
g0332: op nameop nameop name    ................................................................    installedinstalledinstalled installed  .. ....   ..compatiblecompatiblecompatible 
g0332: 
g0332: 
g0332: compatible--------------------------------------------------
g0332: ----------------------------------------------------------------------------------------------------
g0332: 
g0332: 
g0332: --------------------------------------------------
g0329: DeepSpeed general environment info:
g0329: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0329: torch version .................... 2.0.1+cu118
g0329: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0329: deepspeed info ................... 0.12.4, unknown, unknown
g0329: torch cuda version ............... 11.8
g0329: torch hip version ................ None
g0329: nvcc version ..................... 11.8
g0329: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0329: shared memory (/dev/shm) size .... 188.13 GB
g0329: DeepSpeed general environment info:
g0329: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0329: torch version .................... 2.0.1+cu118
g0329: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0329: deepspeed info ................... 0.12.4, unknown, unknown
g0329: torch cuda version ............... 11.8
g0329: torch hip version ................ None
g0329: nvcc version ..................... 11.8
g0329: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0329: shared memory (/dev/shm) size .... 188.13 GB
g0329: DeepSpeed general environment info:
g0329: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0329: torch version .................... 2.0.1+cu118
g0329: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0329: deepspeed info ................... 0.12.4, unknown, unknown
g0329: torch cuda version ............... 11.8
g0329: torch hip version ................ None
g0329: nvcc version ..................... 11.8
g0329: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0329: shared memory (/dev/shm) size .... 188.13 GB
g0329: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0329: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0329: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0329: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0329: [2024-08-02 18:04:52,941] [INFO] [comm.py:637:init_distributed] cdb=None
g0329: [2024-08-02 18:04:52,942] [INFO] [comm.py:637:init_distributed] cdb=None
g0329: [2024-08-02 18:04:52,944] [INFO] [comm.py:637:init_distributed] cdb=None
g0329: [2024-08-02 18:04:52,944] [INFO] [comm.py:637:init_distributed] cdb=None
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0329: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0332: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0332: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0332: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0332: async_io [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0332:  [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0332:  ....... [93m[NO][0m
g0332: fused_adamfused_lamb  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0332: 
g0332: cpu_adam ............... [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0332: ............. [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0332:  ...... [92m[OKAY][0m
g0332: cpu_lion ............... [92m[YES][0m ......async_io [92m[OKAY][0m
g0332:  ............... [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0332: async_io[92m[OKAY][0mevoformer_attn 
g0332:  ........................  [93m[NO][0m[92m[YES][0m  ....... ......[93m[NO][0m 
g0332: fused_adam[92m[OKAY][0m 
g0332: fused_lamb.............  .............[92m[YES][0m [92m[YES][0m  ............  fused_adam[92m[OKAY][0m[92m[OKAY][0m 
g0332: 
g0332: ............. [92m[YES][0m ......cpu_adam  [92m[OKAY][0mfused_lion...............
g0332:   ............. [92m[YES][0m[92m[YES][0m  cpu_adam............   [92m[OKAY][0m...............[92m[OKAY][0m
g0332:  
g0332: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0332: ............ [92m[YES][0m cpu_adagrad......  ............[92m[OKAY][0m 
g0332: [92m[YES][0m ...... cpu_lion[92m[OKAY][0m 
g0332: ............... [92m[YES][0m cpu_lion......  ...............[92m[OKAY][0m 
g0332: [92m[YES][0m ...... [92m[OKAY][0m
g0332: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0332: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHevoformer_attn
g0332:  ......... evoformer_attn[93m[NO][0m  ................  [93m[NO][0m[93m[NO][0m 
g0332: ....... [93m[NO][0m
g0332: fused_lamb .............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0332:  [92m[OKAY][0m
g0332: fused_lion fused_lion.............  .............[92m[YES][0m  [92m[YES][0m...... ......  [92m[OKAY][0m[92m[OKAY][0m
g0332: 
g0332: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0332: inference_core_ops .....inference_core_ops [92m[YES][0m  ...........  [92m[YES][0m[92m[OKAY][0m 
g0332: ...... [92m[OKAY][0m
g0332: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0332: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: cutlass_ops ............quantizer  [92m[YES][0m..............  ......[92m[YES][0m  [92m[OKAY][0m......
g0332:  [92m[OKAY][0m
g0332: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0332: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0332: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0332: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0332: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0332: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0332: sparse_attn ............ ragged_ops[93m[NO][0m  ....................  [93m[NO][0m[92m[YES][0m
g0332:  ...... [92m[OKAY][0m
g0332: random_ltd ............. [92m[YES][0m ragged_ops......  [92m[OKAY][0m.............
g0332:  [92m[YES][0m ...... [92m[OKAY][0m
g0332: random_ltd .............[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0 
g0332: [92m[YES][0m ...... [92m[OKAY][0m
g0332: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0332: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0332: sparse_attn ............[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0332: [93m[NO][0m .......sparse_attn  [93m[NO][0m............
g0332:  [93m[NO][0m ....... [93m[NO][0m
g0332: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0332: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0332: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0332: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0332: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0332: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0332: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0332: --------------------------------------------------
g0332: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0332: --------------------------------------------------
g0332: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0332: --------------------------------------------------
g0332: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0332: --------------------------------------------------
g0332: DeepSpeed general environment info:
g0332: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0332: torch version .................... 2.0.1+cu118
g0332: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0332: deepspeed info ................... 0.12.4, unknown, unknown
g0332: torch cuda version ............... 11.8
g0332: torch hip version ................ None
g0332: nvcc version ..................... 11.8
g0332: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0332: shared memory (/dev/shm) size .... 188.13 GB
g0332: DeepSpeed general environment info:
g0332: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0332: torch version .................... 2.0.1+cu118
g0332: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0332: deepspeed info ................... 0.12.4, unknown, unknown
g0332: torch cuda version ............... 11.8
g0332: torch hip version ................ None
g0332: nvcc version ..................... 11.8
g0332: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0332: shared memory (/dev/shm) size .... 188.13 GB
g0332: DeepSpeed general environment info:
g0332: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0332: torch version .................... 2.0.1+cu118
g0332: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0332: deepspeed info ................... 0.12.4, unknown, unknown
g0332: torch cuda version ............... 11.8
g0332: torch hip version ................ None
g0332: nvcc version ..................... 11.8
g0332: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0332: shared memory (/dev/shm) size .... 188.13 GB
g0332: DeepSpeed general environment info:
g0332: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0332: torch version .................... 2.0.1+cu118
g0332: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0332: deepspeed info ................... 0.12.4, unknown, unknown
g0332: torch cuda version ............... 11.8
g0332: torch hip version ................ None
g0332: nvcc version ..................... 11.8
g0332: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0332: shared memory (/dev/shm) size .... 188.13 GB
g0332: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0332: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0332: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0332: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0332: [2024-08-02 18:04:53,156] [INFO] [comm.py:637:init_distributed] cdb=None
g0332: [2024-08-02 18:04:53,157] [INFO] [comm.py:637:init_distributed] cdb=None
g0332: [2024-08-02 18:04:53,158] [INFO] [comm.py:637:init_distributed] cdb=None
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: --------------------------------------------------
g0318: DeepSpeed C++/CUDA extension op report
g0318: --------------------------------------------------
g0318: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0318:       runtime if needed. Op compatibility means that your system
g0318:       meet the required dependencies to JIT install the op.
g0318: --------------------------------------------------
g0318: JIT compiled ops requires ninja
g0318: ----------------------------------------------------------------------------------------------------
g0318: 
g0318: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0318: 
g0318: ----------------------------------------------------------------------------------------------------
g0318: 
g0318: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0318:       runtime if needed. Op compatibility means that your system
g0318:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0318:       runtime if needed. Op compatibility means that your system
g0318:       meet the required dependencies to JIT install the op.
g0318: 
g0318: ----------------------------------------------------------------------------------------------------
g0318: 
g0318: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0318: 
g0318: --------------------------------------------------
g0318: DeepSpeed C++/CUDA extension op report
g0318: --------------------------------------------------
g0318: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0318:       runtime if needed. Op compatibility means that your system
g0318:       meet the required dependencies to JIT install the op.
g0318: --------------------------------------------------
g0318: JIT compiled ops requires ninja
g0318: ninjaninjaninja  ninja.................. .................. ..................   [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m 
g0318: 
g0318: 
g0318: [92m[OKAY][0m
g0318: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0318: 
g0318: 
g0318: --------------------------------------------------
g0318: op nameop nameop name   op name................................................    ................installedinstalledinstalled    ..installed....    compatiblecompatible..
g0318: compatible
g0318:  
g0318: --------------------------------------------------compatible--------------------------------------------------
g0318: --------------------------------------------------
g0318: 
g0318: 
g0318: --------------------------------------------------
g0318: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0318: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0masync_io
g0318:  ...............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m...... 
g0318: [92m[OKAY][0m
g0318: fused_adam ............. [92m[YES][0mfused_lion  ...................  [92m[OKAY][0m[92m[YES][0m
g0318:  ...... [92m[OKAY][0mcpu_adam
g0318:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0318: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0318: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0318: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0318: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0318: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0318: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0318: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0318: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0318: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0318: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0318: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0318: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0318: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0318: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0318: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0318: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0318: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0318: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0318: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0318: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0318: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0318: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0318: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0318: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0318: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0318: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0318: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0318: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0318: --------------------------------------------------
g0318: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0318: --------------------------------------------------
g0318: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0318: --------------------------------------------------
g0318: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0318: --------------------------------------------------
g0318: DeepSpeed general environment info:
g0318: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0318: torch version .................... 2.0.1+cu118
g0318: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0318: deepspeed info ................... 0.12.4, unknown, unknown
g0318: torch cuda version ............... 11.8
g0318: torch hip version ................ None
g0318: nvcc version ..................... 11.8
g0318: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0318: shared memory (/dev/shm) size .... 188.13 GB
g0318: DeepSpeed general environment info:
g0318: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0318: torch version .................... 2.0.1+cu118
g0318: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0318: deepspeed info ................... 0.12.4, unknown, unknown
g0318: torch cuda version ............... 11.8
g0318: torch hip version ................ None
g0318: nvcc version ..................... 11.8
g0318: DeepSpeed general environment info:deepspeed wheel compiled w. 
g0318: ...... torch 2.0, cuda 11.8torch install path
g0318:  shared memory (/dev/shm) size...............  .... 188.13 GB['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0318: 
g0318: torch version .................... 2.0.1+cu118
g0318: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0318: deepspeed info ................... 0.12.4, unknown, unknown
g0318: torch cuda version ............... 11.8
g0318: torch hip version ................ None
g0318: nvcc version ..................... 11.8
g0318: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0318: shared memory (/dev/shm) size .... 188.13 GB
g0318: DeepSpeed general environment info:
g0318: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0318: torch version .................... 2.0.1+cu118
g0318: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0318: deepspeed info ................... 0.12.4, unknown, unknown
g0318: torch cuda version ............... 11.8
g0318: torch hip version ................ None
g0318: nvcc version ..................... 11.8
g0318: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0318: shared memory (/dev/shm) size .... 188.13 GB
g0332: > setting tensorboard ...
g0332: [2024-08-02 18:04:53,568] [INFO] [comm.py:637:init_distributed] cdb=None
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0332: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: --------------------------------------------------
g0320: DeepSpeed C++/CUDA extension op report
g0320: --------------------------------------------------
g0320: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0320:       runtime if needed. Op compatibility means that your system
g0320:       meet the required dependencies to JIT install the op.
g0320: --------------------------------------------------
g0320: JIT compiled ops requires ninja
g0320: --------------------------------------------------
g0320: DeepSpeed C++/CUDA extension op report
g0320: --------------------------------------------------
g0320: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0320:       runtime if needed. Op compatibility means that your system
g0320:       meet the required dependencies to JIT install the op.
g0320: --------------------------------------------------
g0320: JIT compiled ops requires ninja
g0318: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0318: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0318: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0318: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0320: --------------------------------------------------
g0320: DeepSpeed C++/CUDA extension op report
g0320: --------------------------------------------------
g0320: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0320:       runtime if needed. Op compatibility means that your system
g0320:       meet the required dependencies to JIT install the op.
g0320: --------------------------------------------------
g0320: JIT compiled ops requires ninja
g0320: --------------------------------------------------
g0320: DeepSpeed C++/CUDA extension op report
g0320: --------------------------------------------------
g0320: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0320:       runtime if needed. Op compatibility means that your system
g0320:       meet the required dependencies to JIT install the op.
g0320: --------------------------------------------------
g0320: JIT compiled ops requires ninja
g0320: ninja ninja..................ninja  [92m[OKAY][0m ninja....................................
g0320:    [92m[OKAY][0m--------------------------------------------------..................
g0320: [92m[OKAY][0m
g0320:  
g0320: [92m[OKAY][0m--------------------------------------------------op name
g0320: 
g0320: -------------------------------------------------- --------------------------------------------------................op name
g0320: 
g0320:   op name................installedop name    ..................installed  ................ compatibleinstalled..
g0320:    --------------------------------------------------installed..
g0320: compatible  
g0320: compatible..--------------------------------------------------
g0320: 
g0320:  --------------------------------------------------
g0320: compatible
g0320: --------------------------------------------------
g0320: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0320: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0320: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0320: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0320: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: cpu_lion ...............async_io [92m[YES][0m ...............  ......[92m[YES][0m  ......[92m[OKAY][0m 
g0320: [92m[OKAY][0m
g0320: fused_adam ............. [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...... 
g0320: [92m[OKAY][0m
g0320: evoformer_attn cpu_adam......... ...............  [93m[NO][0m[92m[YES][0m  ............. [92m[OKAY][0m 
g0320: [93m[NO][0m
g0320: cpu_adagrad ............ [92m[YES][0m fused_lamb......  [92m[OKAY][0m.............
g0320:  [92m[YES][0mcpu_lion  .....................  [92m[YES][0m [92m[OKAY][0m......
g0320:  [92m[OKAY][0m
g0320: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0320: evoformer_attnfused_lion .........  .............[93m[NO][0m  .......[92m[YES][0m [93m[NO][0m 
g0320: ...... fused_lamb[92m[OKAY][0m .............
g0320:  [92m[YES][0m ...... [92m[OKAY][0m
g0320: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0318: [2024-08-02 18:04:53,680] [INFO] [comm.py:637:init_distributed] cdb=None
g0318: [2024-08-02 18:04:53,680] [INFO] [comm.py:637:init_distributed] cdb=None
g0318: [2024-08-02 18:04:53,681] [INFO] [comm.py:637:init_distributed] cdb=None
g0318: [2024-08-02 18:04:53,683] [INFO] [comm.py:637:init_distributed] cdb=None
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0318: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0320: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0320: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0320: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0320: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: cutlass_ops ............cutlass_ops [92m[YES][0m  ..................  [92m[OKAY][0m
g0320: [92m[YES][0m ......quantizer  ..............[92m[OKAY][0m [92m[YES][0m
g0320:  ...... [92m[OKAY][0m
g0320: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0320: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0320: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0320: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0320: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0320: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0320: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0320: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: random_ltd ............. ragged_ops[92m[YES][0m  ...... .............[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0[92m[OKAY][0m 
g0320: 
g0320: [92m[YES][0m ...... [92m[OKAY][0m
g0320: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0320: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0320: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0320: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0320: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0320: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0320: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0320: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0320: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0320: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0320: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0320: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0320: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0320: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0320: spatial_inferencetransformer  ..................  [92m[YES][0m[92m[YES][0m ......  [92m[OKAY][0m......
g0320:  [92m[OKAY][0mstochastic_transformer
g0320:  . [92m[YES][0m ...... transformer[92m[OKAY][0m
g0320:  ............ [92m[YES][0m ...... [92m[OKAY][0m
g0320: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0320: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0320: --------------------------------------------------
g0320: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0320: --------------------------------------------------
g0320: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0320: --------------------------------------------------
g0320: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0320: --------------------------------------------------
g0320: DeepSpeed general environment info:
g0320: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0320: torch version .................... 2.0.1+cu118
g0320: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0320: deepspeed info ................... 0.12.4, unknown, unknown
g0320: torch cuda version ............... 11.8
g0320: torch hip version ................ None
g0320: nvcc version ..................... 11.8
g0320: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0320: shared memory (/dev/shm) size .... 188.13 GB
g0320: DeepSpeed general environment info:
g0320: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0320: torch version .................... 2.0.1+cu118
g0320: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0320: deepspeed info ................... 0.12.4, unknown, unknown
g0320: torch cuda version ............... 11.8
g0320: torch hip version ................DeepSpeed general environment info: 
g0320: None
g0320: nvcc versiontorch install path  ....................................  11.8
g0320: deepspeed wheel compiled w.['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch'] 
g0320: ...... torch 2.0, cuda 11.8torch version
g0320:  shared memory (/dev/shm) size....................  ....2.0.1+cu118 
g0320: 188.13 GB
g0320: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0320: deepspeed info ................... 0.12.4, unknown, unknown
g0320: torch cuda version ............... 11.8
g0320: torch hip version ................ None
g0320: nvcc version ..................... 11.8
g0320: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0320: shared memory (/dev/shm) size .... 188.13 GB
g0320: DeepSpeed general environment info:
g0320: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0320: torch version .................... 2.0.1+cu118
g0320: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0320: deepspeed info ................... 0.12.4, unknown, unknown
g0320: torch cuda version ............... 11.8
g0320: torch hip version ................ None
g0320: nvcc version ..................... 11.8
g0320: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0320: shared memory (/dev/shm) size .... 188.13 GB
g0320: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0320: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0320: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0320: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0320: [2024-08-02 18:04:53,866] [INFO] [comm.py:637:init_distributed] cdb=None
g0320: [2024-08-02 18:04:53,870] [INFO] [comm.py:637:init_distributed] cdb=None
g0320: [2024-08-02 18:04:53,871] [INFO] [comm.py:637:init_distributed] cdb=None
g0320: [2024-08-02 18:04:53,873] [INFO] [comm.py:637:init_distributed] cdb=None
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0320: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0314-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0314: > initialized tensor model parallel with size 1
g0314: > initialized pipeline model parallel with size 8
g0314: > setting random seeds to 1234 ...
g0314: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0314: > compiling dataset index builder ...
g0314: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0314: make: Nothing to be done for 'default'.
g0314: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0314: >>> done with dataset index builder. Compilation time: 0.080 seconds
g0314: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0314: > compiling and loading fused kernels ...
g0314: Detected CUDA files, patching ldflags
g0314: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0314: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0314: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0314: ninja: no work to do.
g0314: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0314: Detected CUDA files, patching ldflags
g0314: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0314: Building extension module scaled_masked_softmax_cuda...
g0314: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0314: ninja: no work to do.
g0314: Loading extension module scaled_masked_softmax_cuda...
g0314: Detected CUDA files, patching ldflags
g0314: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0314: Building extension module scaled_softmax_cuda...
g0314: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0314: ninja: no work to do.
g0314: Loading extension module scaled_softmax_cuda...
g0314: >>> done with compiling and loading fused kernels. Compilation time: 7.876 seconds
g0314: time to initialize megatron (seconds): 22.750
g0314: [after megatron is initialized] datetime: 2024-08-02 18:05:04 
g0329: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0320: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0318: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0325: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0319: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0332: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0316: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0314: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0329: wandb: Tracking run with wandb version 0.17.5
g0329: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-av6qi2ee
g0329: wandb: Run `wandb offline` to turn off syncing.
g0329: wandb: Syncing run g0329.abci.local
g0329: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0329: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/av6qi2ee
g0325: wandb: Tracking run with wandb version 0.17.5
g0325: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-ivtek894
g0325: wandb: Run `wandb offline` to turn off syncing.
g0325: wandb: Syncing run g0325.abci.local
g0325: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0325: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ivtek894
g0332: wandb: Tracking run with wandb version 0.17.5
g0332: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-36b5xm2z
g0332: wandb: Run `wandb offline` to turn off syncing.
g0332: wandb: Syncing run g0332.abci.local
g0332: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0332: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/36b5xm2z
g0314: wandb: Tracking run with wandb version 0.17.5
g0314: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-51r23kr9
g0314: wandb: Run `wandb offline` to turn off syncing.
g0314: wandb: Syncing run g0314.abci.local
g0314: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0314: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/51r23kr9
g0319: wandb: Tracking run with wandb version 0.17.5
g0319: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-hadj25u2
g0319: wandb: Run `wandb offline` to turn off syncing.
g0319: wandb: Syncing run g0319.abci.local
g0319: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0319: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/hadj25u2
g0318: wandb: Tracking run with wandb version 0.17.5
g0318: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-kqgkwq2f
g0318: wandb: Run `wandb offline` to turn off syncing.
g0318: wandb: Syncing run g0318.abci.local
g0318: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0318: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/kqgkwq2f
g0320: wandb: Tracking run with wandb version 0.17.5
g0320: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-umq2ycyh
g0320: wandb: Run `wandb offline` to turn off syncing.
g0320: wandb: Syncing run g0320.abci.local
g0320: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0320: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/umq2ycyh
g0316: wandb: Tracking run with wandb version 0.17.5
g0316: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180505-7s5xf97h
g0316: wandb: Run `wandb offline` to turn off syncing.
g0316: wandb: Syncing run g0316.abci.local
g0316: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0316: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/7s5xf97h
g0314: building GPT model ...
g0314: [2024-08-02 18:05:07,152] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0314: [2024-08-02 18:05:07,153] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0314: [2024-08-02 18:05:07,153] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 36.17 GB, percent = 9.6%
g0314: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0314: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0314: [2024-08-02 18:05:07,668] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0314: stage=0 layers=5
g0314:      0: _to_float16
g0314:      1: EmbeddingPipe
g0314:      2: ParallelTransformerLayerPipe
g0314:      3: ParallelTransformerLayerPipe
g0314:      4: ParallelTransformerLayerPipe
g0314: stage=1 layers=3
g0314:      5: ParallelTransformerLayerPipe
g0314:      6: ParallelTransformerLayerPipe
g0314:      7: ParallelTransformerLayerPipe
g0314: stage=2 layers=3
g0314:      8: ParallelTransformerLayerPipe
g0314:      9: ParallelTransformerLayerPipe
g0314:     10: ParallelTransformerLayerPipe
g0314: stage=3 layers=3
g0314:     11: ParallelTransformerLayerPipe
g0314:     12: ParallelTransformerLayerPipe
g0314:     13: ParallelTransformerLayerPipe
g0314: stage=4 layers=3
g0314:     14: ParallelTransformerLayerPipe
g0314:     15: ParallelTransformerLayerPipe
g0314:     16: ParallelTransformerLayerPipe
g0314: stage=5 layers=3
g0314:     17: ParallelTransformerLayerPipe
g0314:     18: ParallelTransformerLayerPipe
g0314:     19: ParallelTransformerLayerPipe
g0314: stage=6 layers=3
g0314:     20: ParallelTransformerLayerPipe
g0314:     21: ParallelTransformerLayerPipe
g0314:     22: ParallelTransformerLayerPipe
g0314: stage=7 layers=3
g0314:     23: ParallelTransformerLayerPipe
g0314:     24: MixedFusedRMSNorm
g0314:     25: LMHeadPipe
g0314:   loss: CrossEntropy
g0318:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0332:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0329:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0325:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0316:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0319:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0320:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0314: [2024-08-02 18:05:08,185] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0314: [2024-08-02 18:05:08,186] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0314: [2024-08-02 18:05:08,186] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 36.23 GB, percent = 9.6%
g0314:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0314: setting training iterations to 10000000
g0314: > learning rate decay style: cosine
g0314: DeepSpeed is enabled.
g0314: [2024-08-02 18:05:08,189] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0332: [2024-08-02 18:05:08,277] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0332: [2024-08-02 18:05:08,278] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0332: [2024-08-02 18:05:08,278] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0332: [2024-08-02 18:05:08,278] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0318: [2024-08-02 18:05:08,284] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0318: [2024-08-02 18:05:08,285] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0318: [2024-08-02 18:05:08,285] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0318: [2024-08-02 18:05:08,285] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0329: [2024-08-02 18:05:08,285] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0329: [2024-08-02 18:05:08,285] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0329: [2024-08-02 18:05:08,285] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0329: [2024-08-02 18:05:08,285] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0325: [2024-08-02 18:05:08,315] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0325: [2024-08-02 18:05:08,315] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0325: [2024-08-02 18:05:08,316] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0325: [2024-08-02 18:05:08,316] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0316: [2024-08-02 18:05:08,319] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0316: [2024-08-02 18:05:08,319] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0316: [2024-08-02 18:05:08,320] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0316: [2024-08-02 18:05:08,320] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0319: [2024-08-02 18:05:08,322] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0319: [2024-08-02 18:05:08,322] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0319: [2024-08-02 18:05:08,322] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0319: [2024-08-02 18:05:08,322] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0320: [2024-08-02 18:05:08,343] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0320: [2024-08-02 18:05:08,343] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0320: [2024-08-02 18:05:08,343] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0320: [2024-08-02 18:05:08,343] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0314: [2024-08-02 18:05:08,377] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0314: [2024-08-02 18:05:08,378] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0314: [2024-08-02 18:05:08,378] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0314: [2024-08-02 18:05:08,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0314: [2024-08-02 18:05:08,378] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0314: [2024-08-02 18:05:08,413] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0314: [2024-08-02 18:05:08,413] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0314: [2024-08-02 18:05:08,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0314: [2024-08-02 18:05:08,414] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0314: [2024-08-02 18:05:08,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f314f610910>
g0314: [2024-08-02 18:05:08,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: [2024-08-02 18:05:08,414] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0314: [2024-08-02 18:05:08,414] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0314: [2024-08-02 18:05:08,415] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0314:     "partition_activations": false, 
g0314:     "contiguous_memory_optimization": false, 
g0314:     "cpu_checkpointing": false, 
g0314:     "number_checkpoints": null, 
g0314:     "synchronize_checkpoint_boundary": false, 
g0314:     "profile": false
g0314: }
g0314: [2024-08-02 18:05:08,415] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0314: [2024-08-02 18:05:08,415] [INFO] [config.py:983:print]   amp_enabled .................. False
g0314: [2024-08-02 18:05:08,415] [INFO] [config.py:983:print]   amp_params ................... False
g0314: [2024-08-02 18:05:08,415] [INFO] [config.py:983:print]   autotuning_config ............ {
g0314:     "enabled": false, 
g0314:     "start_step": null, 
g0314:     "end_step": null, 
g0314:     "metric_path": null, 
g0314:     "arg_mappings": null, 
g0314:     "metric": "throughput", 
g0314:     "model_info": null, 
g0314:     "results_dir": "autotuning_results", 
g0314:     "exps_dir": "autotuning_exps", 
g0314:     "overwrite": true, 
g0314:     "fast": true, 
g0314:     "start_profile_step": 3, 
g0314:     "end_profile_step": 5, 
g0314:     "tuner_type": "gridsearch", 
g0314:     "tuner_early_stopping": 5, 
g0314:     "tuner_num_trials": 50, 
g0314:     "model_info_path": null, 
g0314:     "mp_size": 1, 
g0314:     "max_train_batch_size": null, 
g0314:     "min_train_batch_size": 1, 
g0314:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0314:     "min_train_micro_batch_size_per_gpu": 1, 
g0314:     "num_tuning_micro_batch_sizes": 3
g0314: }
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f31481e3b50>
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   communication_data_type ...... None
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0314: [2024-08-02 18:05:08,416] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   disable_allgather ............ False
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   dump_state ................... False
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0314: [2024-08-02 18:05:08,417] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0314: [2024-08-02 18:05:08,418] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0314:     "enabled": false, 
g0314:     "recompute_fwd_factor": 0.0, 
g0314:     "profile_step": 1, 
g0314:     "module_depth": -1, 
g0314:     "top_modules": 1, 
g0314:     "detailed": true, 
g0314:     "output_file": null
g0314: }
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   global_rank .................. 0
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0314: [2024-08-02 18:05:08,419] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   loss_scale ................... 0
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0314: [2024-08-02 18:05:08,420] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0314: [2024-08-02 18:05:08,421] [INFO] [config.py:983:print]   nebula_config ................ {
g0314:     "enabled": false, 
g0314:     "persistent_storage_path": null, 
g0314:     "persistent_time_interval": 100, 
g0314:     "num_of_version_in_retention": 2, 
g0314:     "enable_nebula_load": true, 
g0314:     "load_path": null
g0314: }
g0314: [2024-08-02 18:05:08,421] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0314: [2024-08-02 18:05:08,421] [INFO] [config.py:983:print]   optimizer_name ............... None
g0314: [2024-08-02 18:05:08,421] [INFO] [config.py:983:print]   optimizer_params ............. None
g0314: [2024-08-02 18:05:08,421] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0314: [2024-08-02 18:05:08,421] [INFO] [config.py:983:print]   pld_enabled .................. False
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   pld_params ................... False
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   scheduler_name ............... None
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   scheduler_params ............. None
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   sparse_attention ............. None
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0314: [2024-08-02 18:05:08,422] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   world_size ................... 4
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0314: [2024-08-02 18:05:08,423] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0314: [2024-08-02 18:05:08,424] [INFO] [config.py:983:print]   zero_enabled ................. False
g0314: [2024-08-02 18:05:08,424] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0314: [2024-08-02 18:05:08,424] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0314: [2024-08-02 18:05:08,424] [INFO] [config.py:969:print_user_config]   json = {
g0314:     "train_batch_size": 128, 
g0314:     "train_micro_batch_size_per_gpu": 1, 
g0314:     "steps_per_print": 10, 
g0314:     "zero_optimization": {
g0314:         "stage": 0
g0314:     }, 
g0314:     "gradient_clipping": 1.0, 
g0314:     "prescale_gradients": true, 
g0314:     "fp16": {
g0314:         "enabled": true, 
g0314:         "loss_scale": 0, 
g0314:         "loss_scale_window": 500, 
g0314:         "hysteresis": 2, 
g0314:         "min_loss_scale": 1, 
g0314:         "initial_scale_power": 11
g0314:     }, 
g0314:     "wall_clock_breakdown": false
g0314: }
g0314: [2024-08-02 18:05:08,424] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0314: [2024-08-02 18:05:08,424] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0314: [2024-08-02 18:05:09,136] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0320: [2024-08-02 18:05:09,136] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0329: [2024-08-02 18:05:09,136] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0318: [2024-08-02 18:05:09,136] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0319: [2024-08-02 18:05:09,137] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0332: [2024-08-02 18:05:09,137] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0325: [2024-08-02 18:05:09,137] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0316: [2024-08-02 18:05:09,138] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0329: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0329: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0329: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0329: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0319: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0319: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0319: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0319: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0318: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0318: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0318: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0316: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0316: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0316: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0316: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0314: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0320: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0320: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0320: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0332: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0332: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0325: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0325: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0332: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0314: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0314: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0314: WARNING: could not find the metadata file /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase 
g0314: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0314:     will not load any checkpoints and will start from random
g0318: [2024-08-02 18:05:09,835] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0320: [2024-08-02 18:05:09,835] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0325: [2024-08-02 18:05:09,834] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0325: [2024-08-02 18:05:09,835] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0332: [2024-08-02 18:05:09,835] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0332: (min, max) time across ranks (ms):
g0332:     load-checkpoint ................................: (1.49, 2.68)
g0314: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-02 18:05:09 
g0314: > building train, validation, and test datasets ...
g0314:  > datasets target sizes (minimum size):
g0314:     train:      1280000000
g0314:     validation: 128012800
g0314:     test:       12800
g0314: > building train, validation, and test datasets for GPT ...
g0314: Single data path provided for train, valid & test
g0314:  > building dataset index ...
g0314:     reading sizes...
g0314:     reading pointers...
g0314:     reading document index...
g0314:     creating numpy buffer of mmap...
g0314:     creating memory view of numpy buffer...
g0314:  > finished creating indexed dataset in 0.338662 seconds
g0314:     number of documents: 2237032
g0314:  > dataset split:
g0314:     train:
g0314:      document indices in [0, 2122943) total of 2122943 documents
g0314:     validation:
g0314:      document indices in [2122943, 2234795) total of 111852 documents
g0314:     test:
g0314:      document indices in [2234795, 2237032) total of 2237 documents
g0314:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0314:  > only one epoch required, setting separate_last_epoch to False
g0314:  > elasped time to build and save doc-idx mapping (seconds): 0.085907
g0314:     using:
g0314:      number of documents:       2122943
g0314:      number of epochs:          1
g0314:      sequence length:           2048
g0314:      total number of samples:   3567637
g0314:  > elasped time to build and save sample-idx mapping (seconds): 0.104671
g0314:  > building shuffle index with split [0, 3567637) and [3567637, 3567637) ...
g0314:  > elasped time to build and save shuffle-idx mapping (seconds): 0.096688
g0314:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/0389f3f62cb35f21a15d811a32a36d66_doc_idx.npy
g0314:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/0389f3f62cb35f21a15d811a32a36d66_sample_idx.npy
g0314:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/0389f3f62cb35f21a15d811a32a36d66_shuffle_idx.npy
g0314:     loaded indexed file in 0.012 seconds
g0314:     total number of samples: 3567638
g0314:     total number of epochs: 1
g0314:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0314:  > last epoch number of samples (34382) is smaller than 80% of number of samples per epoch (186829), setting separate_last_epoch to True
g0314:  > elasped time to build and save doc-idx mapping (seconds): 5.463318
g0314:     using:
g0314:      number of documents:       111852
g0314:      number of epochs:          686
g0314:      sequence length:           2048
g0314:      total number of samples:   128165248
g0314:  > elasped time to build and save sample-idx mapping (seconds): 3.062584
g0314:  > building shuffle index with split [0, 127978418) and [127978418, 128165248) ...
g0314:  > elasped time to build and save shuffle-idx mapping (seconds): 7.704232
g0314:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/87913291b337168ee8adee517c41c63a_doc_idx.npy
g0314:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/87913291b337168ee8adee517c41c63a_sample_idx.npy
g0314:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/87913291b337168ee8adee517c41c63a_shuffle_idx.npy
g0314:     loaded indexed file in 0.019 seconds
g0314:     total number of samples: 128165249
g0314:     total number of epochs: 686
g0314:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0314:  > last epoch number of samples (646) is smaller than 80% of number of samples per epoch (2430), setting separate_last_epoch to True
g0314:  > elasped time to build and save doc-idx mapping (seconds): 0.001997
g0314:     using:
g0314:      number of documents:       2237
g0314:      number of epochs:          6
g0314:      sequence length:           2048
g0314:      total number of samples:   14585
g0314:  > elasped time to build and save sample-idx mapping (seconds): 0.001500
g0314:  > building shuffle index with split [0, 12154) and [12154, 14585) ...
g0314:  > elasped time to build and save shuffle-idx mapping (seconds): 0.001953
g0314:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/a6053d29e80ff52629cab17a6afbf3dd_doc_idx.npy
g0314:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/a6053d29e80ff52629cab17a6afbf3dd_sample_idx.npy
g0314:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/a6053d29e80ff52629cab17a6afbf3dd_shuffle_idx.npy
g0314:     loaded indexed file in 0.003 seconds
g0314:     total number of samples: 14586
g0314:     total number of epochs: 6
g0314: > finished creating GPT datasets ...
g0314: [after dataloaders are built] datetime: 2024-08-02 18:05:36 
g0314: done with setup ...
g0314: training ...
g0332: (min, max) time across ranks (ms):
g0332:     model-and-optimizer-setup ......................: (2943.52, 2953.01)
g0332:     train/valid/test-data-iterators-setup ..........: (26739.62, 26758.65)
g0314: [before the start of training step] datetime: 2024-08-02 18:05:36 
g0314: [2024-08-02 18:07:13,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.5728640000000002e-07, 1.5728640000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 10 loss: 10.5020 iter time (s): 9.650 samples/sec: 13.264
g0332:  iteration       10/10000000 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 9687.2 | learning rate: 1.573E-07 | global batch size:   128 | lm loss: 1.053726E+01 | loss scale: 2048.0 | grad norm: 36.799 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.213 | tokens per gpu per second (tgs): 845.652 | TFLOPs: 6.81 |
g0316: [Rank 4] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8986.0 | max reserved: 8986.0
g0319: [Rank 12] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7198.0 | max reserved: 7198.0
g0332: [Rank 28] (after 10 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0314: [Rank 0] (after 10 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11650.0 | max reserved: 11650.0
g0329: [Rank 24] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 4662.0 | max reserved: 4662.0
g0325: [Rank 20] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5410.0 | max reserved: 5410.0
g0320: [Rank 16] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6304.0 | max reserved: 6304.0
g0318: [Rank 8] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8092.0 | max reserved: 8092.0
g0314: [2024-08-02 18:08:36,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[3.3204906666666666e-07, 3.3204906666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 20 loss: 9.9395 iter time (s): 8.084 samples/sec: 15.834
g0332:  iteration       20/10000000 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 8256.7 | learning rate: 3.320E-07 | global batch size:   128 | lm loss: 1.020494E+01 | loss scale: 2048.0 | grad norm: 33.660 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.503 | tokens per gpu per second (tgs): 992.169 | TFLOPs: 7.98 |
g0314: [2024-08-02 18:09:44,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[5.068117333333334e-07, 5.068117333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 30 loss: 9.1892 iter time (s): 6.764 samples/sec: 18.925
g0332:  iteration       30/10000000 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 6797.5 | learning rate: 5.068E-07 | global batch size:   128 | lm loss: 9.495416E+00 | loss scale: 2048.0 | grad norm: 18.698 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.830 | tokens per gpu per second (tgs): 1205.143 | TFLOPs: 9.70 |
g0314: [2024-08-02 18:10:53,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[6.815744000000001e-07, 6.815744000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 40 loss: 8.8010 iter time (s): 6.919 samples/sec: 18.500
g0332:  iteration       40/10000000 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 6952.1 | learning rate: 6.816E-07 | global batch size:   128 | lm loss: 8.947626E+00 | loss scale: 2048.0 | grad norm: 12.953 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.412 | tokens per gpu per second (tgs): 1178.342 | TFLOPs: 9.48 |
g0314: [2024-08-02 18:11:58,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[8.563370666666667e-07, 8.563370666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 50 loss: 8.5173 iter time (s): 6.474 samples/sec: 19.773
g0332:  iteration       50/10000000 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 6507.0 | learning rate: 8.563E-07 | global batch size:   128 | lm loss: 8.620002E+00 | loss scale: 2048.0 | grad norm: 6.391 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.671 | tokens per gpu per second (tgs): 1258.958 | TFLOPs: 10.13 |
g0314: [2024-08-02 18:12:57,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0310997333333332e-06, 1.0310997333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 60 loss: 8.3005 iter time (s): 5.831 samples/sec: 21.950
g0332:  iteration       60/10000000 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 5864.2 | learning rate: 1.031E-06 | global batch size:   128 | lm loss: 8.407394E+00 | loss scale: 2048.0 | grad norm: 3.701 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.827 | tokens per gpu per second (tgs): 1396.941 | TFLOPs: 11.24 |
g0314: [2024-08-02 18:13:53,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.2058624000000002e-06, 1.2058624000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 70 loss: 8.1798 iter time (s): 5.626 samples/sec: 22.753
g0332:  iteration       70/10000000 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 5658.6 | learning rate: 1.206E-06 | global batch size:   128 | lm loss: 8.240939E+00 | loss scale: 2048.0 | grad norm: 3.092 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.620 | tokens per gpu per second (tgs): 1447.696 | TFLOPs: 11.65 |
g0314: [2024-08-02 18:14:49,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.3806250666666669e-06, 1.3806250666666669e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 80 loss: 8.0937 iter time (s): 5.562 samples/sec: 23.014
g0332:  iteration       80/10000000 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 5595.4 | learning rate: 1.381E-06 | global batch size:   128 | lm loss: 8.117810E+00 | loss scale: 2048.0 | grad norm: 4.972 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.876 | tokens per gpu per second (tgs): 1464.072 | TFLOPs: 11.78 |
g0314: [2024-08-02 18:15:38,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.5553877333333333e-06, 1.5553877333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 90 loss: 8.0256 iter time (s): 4.872 samples/sec: 26.271
g0332:  iteration       90/10000000 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 4905.3 | learning rate: 1.555E-06 | global batch size:   128 | lm loss: 8.008246E+00 | loss scale: 2048.0 | grad norm: 4.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.094 | tokens per gpu per second (tgs): 1670.046 | TFLOPs: 13.44 |
g0314: [2024-08-02 18:16:28,728] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.7301504e-06, 1.7301504e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 100 loss: 7.8943 iter time (s): 4.941 samples/sec: 25.905
g0332:  iteration      100/10000000 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 4975.2 | learning rate: 1.730E-06 | global batch size:   128 | lm loss: 7.930575E+00 | loss scale: 2048.0 | grad norm: 7.654 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.728 | tokens per gpu per second (tgs): 1646.565 | TFLOPs: 13.25 |
g0314: [2024-08-02 18:17:14,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.9049130666666667e-06, 1.9049130666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 110 loss: 7.8361 iter time (s): 4.532 samples/sec: 28.242
g0332:  iteration      110/10000000 | consumed samples:        14080 | consumed tokens:     28835840 | elapsed time per iteration (ms): 4565.1 | learning rate: 1.905E-06 | global batch size:   128 | lm loss: 7.825886E+00 | loss scale: 2048.0 | grad norm: 9.971 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.039 | tokens per gpu per second (tgs): 1794.495 | TFLOPs: 14.44 |
g0314: [2024-08-02 18:18:01,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[2.0796757333333334e-06, 2.0796757333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 120 loss: 7.7366 iter time (s): 4.707 samples/sec: 27.194
g0332:  iteration      120/10000000 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 4739.8 | learning rate: 2.080E-06 | global batch size:   128 | lm loss: 7.748452E+00 | loss scale: 2048.0 | grad norm: 7.007 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.005 | tokens per gpu per second (tgs): 1728.331 | TFLOPs: 13.91 |
g0314: [2024-08-02 18:18:45,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[2.2544384e-06, 2.2544384e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 130 loss: 7.6436 iter time (s): 4.387 samples/sec: 29.180
g0332:  iteration      130/10000000 | consumed samples:        16640 | consumed tokens:     34078720 | elapsed time per iteration (ms): 4420.2 | learning rate: 2.254E-06 | global batch size:   128 | lm loss: 7.688825E+00 | loss scale: 2048.0 | grad norm: 11.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.958 | tokens per gpu per second (tgs): 1853.290 | TFLOPs: 14.91 |
g0314: [2024-08-02 18:19:27,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[2.429201066666667e-06, 2.429201066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 140 loss: 7.5798 iter time (s): 4.094 samples/sec: 31.262
g0332:  iteration      140/10000000 | consumed samples:        17920 | consumed tokens:     36700160 | elapsed time per iteration (ms): 4127.7 | learning rate: 2.429E-06 | global batch size:   128 | lm loss: 7.601078E+00 | loss scale: 2048.0 | grad norm: 9.266 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.010 | tokens per gpu per second (tgs): 1984.662 | TFLOPs: 15.97 |
g0314: [2024-08-02 18:20:11,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[2.6039637333333333e-06, 2.6039637333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 150 loss: 7.5675 iter time (s): 4.397 samples/sec: 29.113
g0332:  iteration      150/10000000 | consumed samples:        19200 | consumed tokens:     39321600 | elapsed time per iteration (ms): 4429.7 | learning rate: 2.604E-06 | global batch size:   128 | lm loss: 7.563393E+00 | loss scale: 2048.0 | grad norm: 9.061 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.896 | tokens per gpu per second (tgs): 1849.318 | TFLOPs: 14.88 |
g0314: [2024-08-02 18:20:58,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[2.7787264000000002e-06, 2.7787264000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 160 loss: 7.4100 iter time (s): 4.650 samples/sec: 27.527
g0332:  iteration      160/10000000 | consumed samples:        20480 | consumed tokens:     41943040 | elapsed time per iteration (ms): 4683.2 | learning rate: 2.779E-06 | global batch size:   128 | lm loss: 7.482644E+00 | loss scale: 2048.0 | grad norm: 6.477 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.332 | tokens per gpu per second (tgs): 1749.237 | TFLOPs: 14.08 |
g0314: [2024-08-02 18:21:41,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[2.953489066666667e-06, 2.953489066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 170 loss: 7.3351 iter time (s): 4.272 samples/sec: 29.962
g0332:  iteration      170/10000000 | consumed samples:        21760 | consumed tokens:     44564480 | elapsed time per iteration (ms): 4305.5 | learning rate: 2.953E-06 | global batch size:   128 | lm loss: 7.387027E+00 | loss scale: 2048.0 | grad norm: 7.425 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.729 | tokens per gpu per second (tgs): 1902.665 | TFLOPs: 15.31 |
g0314: [2024-08-02 18:22:24,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[3.128251733333333e-06, 3.128251733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 180 loss: 7.2467 iter time (s): 4.282 samples/sec: 29.896
g0332:  iteration      180/10000000 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 4314.7 | learning rate: 3.128E-06 | global batch size:   128 | lm loss: 7.306068E+00 | loss scale: 2048.0 | grad norm: 17.127 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.666 | tokens per gpu per second (tgs): 1898.624 | TFLOPs: 15.28 |
g0314: [2024-08-02 18:23:07,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[3.3030144e-06, 3.3030144e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 190 loss: 7.1757 iter time (s): 4.266 samples/sec: 30.004
g0332:  iteration      190/10000000 | consumed samples:        24320 | consumed tokens:     49807360 | elapsed time per iteration (ms): 4299.1 | learning rate: 3.303E-06 | global batch size:   128 | lm loss: 7.255377E+00 | loss scale: 2048.0 | grad norm: 7.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.774 | tokens per gpu per second (tgs): 1905.509 | TFLOPs: 15.33 |
g0314: [2024-08-02 18:23:50,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[3.477777066666667e-06, 3.477777066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 200 loss: 7.1375 iter time (s): 4.288 samples/sec: 29.851
g0332:  iteration      200/10000000 | consumed samples:        25600 | consumed tokens:     52428800 | elapsed time per iteration (ms): 4320.8 | learning rate: 3.478E-06 | global batch size:   128 | lm loss: 7.186529E+00 | loss scale: 2048.0 | grad norm: 7.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.624 | tokens per gpu per second (tgs): 1895.933 | TFLOPs: 15.26 |
g0314: [2024-08-02 18:24:31,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[3.6525397333333335e-06, 3.6525397333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 210 loss: 7.1067 iter time (s): 4.083 samples/sec: 31.351
g0332:  iteration      210/10000000 | consumed samples:        26880 | consumed tokens:     55050240 | elapsed time per iteration (ms): 4115.6 | learning rate: 3.653E-06 | global batch size:   128 | lm loss: 7.128281E+00 | loss scale: 2048.0 | grad norm: 7.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.101 | tokens per gpu per second (tgs): 1990.487 | TFLOPs: 16.02 |
g0314: [2024-08-02 18:25:11,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[3.8273024e-06, 3.8273024e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 220 loss: 7.0652 iter time (s): 3.963 samples/sec: 32.300
g0332:  iteration      220/10000000 | consumed samples:        28160 | consumed tokens:     57671680 | elapsed time per iteration (ms): 3995.4 | learning rate: 3.827E-06 | global batch size:   128 | lm loss: 7.040215E+00 | loss scale: 2048.0 | grad norm: 5.151 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.037 | tokens per gpu per second (tgs): 2050.346 | TFLOPs: 16.50 |
g0314: [2024-08-02 18:25:58,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[4.002065066666667e-06, 4.002065066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 230 loss: 6.9546 iter time (s): 4.643 samples/sec: 27.569
g0332:  iteration      230/10000000 | consumed samples:        29440 | consumed tokens:     60293120 | elapsed time per iteration (ms): 4675.7 | learning rate: 4.002E-06 | global batch size:   128 | lm loss: 6.980527E+00 | loss scale: 2048.0 | grad norm: 7.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.376 | tokens per gpu per second (tgs): 1752.042 | TFLOPs: 14.10 |
g0314: [2024-08-02 18:27:07,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[4.176827733333334e-06, 4.176827733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 240 loss: 6.9020 iter time (s): 6.840 samples/sec: 18.715
g0332:  iteration      240/10000000 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 6872.7 | learning rate: 4.177E-06 | global batch size:   128 | lm loss: 6.893565E+00 | loss scale: 2048.0 | grad norm: 7.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.624 | tokens per gpu per second (tgs): 1191.965 | TFLOPs: 9.59 |
g0314: [2024-08-02 18:28:19,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[4.351590400000001e-06, 4.351590400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 250 loss: 6.7634 iter time (s): 7.189 samples/sec: 17.806
g0332:  iteration      250/10000000 | consumed samples:        32000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 7222.6 | learning rate: 4.352E-06 | global batch size:   128 | lm loss: 6.843470E+00 | loss scale: 2048.0 | grad norm: 8.834 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.722 | tokens per gpu per second (tgs): 1134.211 | TFLOPs: 9.13 |
g0314: [2024-08-02 18:29:26,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[4.526353066666667e-06, 4.526353066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 260 loss: 6.7446 iter time (s): 6.673 samples/sec: 19.183
g0332:  iteration      260/10000000 | consumed samples:        33280 | consumed tokens:     68157440 | elapsed time per iteration (ms): 6706.4 | learning rate: 4.526E-06 | global batch size:   128 | lm loss: 6.775434E+00 | loss scale: 2048.0 | grad norm: 5.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.086 | tokens per gpu per second (tgs): 1221.521 | TFLOPs: 9.83 |
g0314: [2024-08-02 18:30:33,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[4.701115733333334e-06, 4.701115733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 270 loss: 6.7292 iter time (s): 6.697 samples/sec: 19.113
g0332:  iteration      270/10000000 | consumed samples:        34560 | consumed tokens:     70778880 | elapsed time per iteration (ms): 6730.3 | learning rate: 4.701E-06 | global batch size:   128 | lm loss: 6.710565E+00 | loss scale: 2048.0 | grad norm: 6.466 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.018 | tokens per gpu per second (tgs): 1217.178 | TFLOPs: 9.79 |
g0314: [2024-08-02 18:31:31,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[4.875878400000001e-06, 4.875878400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 280 loss: 6.6116 iter time (s): 5.737 samples/sec: 22.311
g0332:  iteration      280/10000000 | consumed samples:        35840 | consumed tokens:     73400320 | elapsed time per iteration (ms): 5770.7 | learning rate: 4.876E-06 | global batch size:   128 | lm loss: 6.629396E+00 | loss scale: 2048.0 | grad norm: 5.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.181 | tokens per gpu per second (tgs): 1419.574 | TFLOPs: 11.42 |
g0314: [2024-08-02 18:32:34,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[5.050641066666667e-06, 5.050641066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 290 loss: 6.5603 iter time (s): 6.211 samples/sec: 20.609
g0332:  iteration      290/10000000 | consumed samples:        37120 | consumed tokens:     76021760 | elapsed time per iteration (ms): 6244.3 | learning rate: 5.051E-06 | global batch size:   128 | lm loss: 6.556990E+00 | loss scale: 2048.0 | grad norm: 5.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.499 | tokens per gpu per second (tgs): 1311.911 | TFLOPs: 10.56 |
g0314: [2024-08-02 18:33:29,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[5.225403733333334e-06, 5.225403733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 300 loss: 6.5461 iter time (s): 5.487 samples/sec: 23.327
g0332:  iteration      300/10000000 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 5520.4 | learning rate: 5.225E-06 | global batch size:   128 | lm loss: 6.512427E+00 | loss scale: 2048.0 | grad norm: 7.046 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.187 | tokens per gpu per second (tgs): 1483.958 | TFLOPs: 11.94 |
g0314: [2024-08-02 18:34:22,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[5.4001664e-06, 5.4001664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 310 loss: 6.4035 iter time (s): 5.324 samples/sec: 24.044
g0332:  iteration      310/10000000 | consumed samples:        39680 | consumed tokens:     81264640 | elapsed time per iteration (ms): 5357.0 | learning rate: 5.400E-06 | global batch size:   128 | lm loss: 6.432393E+00 | loss scale: 2048.0 | grad norm: 6.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.894 | tokens per gpu per second (tgs): 1529.203 | TFLOPs: 12.31 |
g0314: [2024-08-02 18:35:13,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[5.574929066666667e-06, 5.574929066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 320 loss: 6.3163 iter time (s): 5.000 samples/sec: 25.601
g0332:  iteration      320/10000000 | consumed samples:        40960 | consumed tokens:     83886080 | elapsed time per iteration (ms): 5033.2 | learning rate: 5.575E-06 | global batch size:   128 | lm loss: 6.382270E+00 | loss scale: 2048.0 | grad norm: 4.139 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.431 | tokens per gpu per second (tgs): 1627.585 | TFLOPs: 13.10 |
g0314: [2024-08-02 18:36:03,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[5.7496917333333335e-06, 5.7496917333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 330 loss: 6.3398 iter time (s): 5.014 samples/sec: 25.528
g0332:  iteration      330/10000000 | consumed samples:        42240 | consumed tokens:     86507520 | elapsed time per iteration (ms): 5047.6 | learning rate: 5.750E-06 | global batch size:   128 | lm loss: 6.304379E+00 | loss scale: 2048.0 | grad norm: 7.149 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.358 | tokens per gpu per second (tgs): 1622.935 | TFLOPs: 13.06 |
g0314: [2024-08-02 18:36:47,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[5.9244543999999995e-06, 5.9244543999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 340 loss: 6.2453 iter time (s): 4.377 samples/sec: 29.246
g0332:  iteration      340/10000000 | consumed samples:        43520 | consumed tokens:     89128960 | elapsed time per iteration (ms): 4409.9 | learning rate: 5.924E-06 | global batch size:   128 | lm loss: 6.251424E+00 | loss scale: 2048.0 | grad norm: 8.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.026 | tokens per gpu per second (tgs): 1857.639 | TFLOPs: 14.95 |
g0314: [2024-08-02 18:37:33,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[6.0992170666666664e-06, 6.0992170666666664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 350 loss: 6.1869 iter time (s): 4.535 samples/sec: 28.227
g0332:  iteration      350/10000000 | consumed samples:        44800 | consumed tokens:     91750400 | elapsed time per iteration (ms): 4570.3 | learning rate: 6.099E-06 | global batch size:   128 | lm loss: 6.197620E+00 | loss scale: 2048.0 | grad norm: 5.897 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.007 | tokens per gpu per second (tgs): 1792.424 | TFLOPs: 14.42 |
g0314: [2024-08-02 18:38:37,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[6.273979733333333e-06, 6.273979733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 360 loss: 5.9888 iter time (s): 6.371 samples/sec: 20.092
g0332:  iteration      360/10000000 | consumed samples:        46080 | consumed tokens:     94371840 | elapsed time per iteration (ms): 6404.2 | learning rate: 6.274E-06 | global batch size:   128 | lm loss: 6.135668E+00 | loss scale: 2048.0 | grad norm: 8.568 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.987 | tokens per gpu per second (tgs): 1279.163 | TFLOPs: 10.29 |
g0314: [2024-08-02 18:39:26,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[6.4487424e-06, 6.4487424e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 370 loss: 6.0806 iter time (s): 4.871 samples/sec: 26.279
g0332:  iteration      370/10000000 | consumed samples:        47360 | consumed tokens:     96993280 | elapsed time per iteration (ms): 4903.7 | learning rate: 6.449E-06 | global batch size:   128 | lm loss: 6.038508E+00 | loss scale: 2048.0 | grad norm: 6.692 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.103 | tokens per gpu per second (tgs): 1670.587 | TFLOPs: 13.44 |
g0314: [2024-08-02 18:40:12,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[6.623505066666667e-06, 6.623505066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 380 loss: 6.0052 iter time (s): 4.535 samples/sec: 28.227
g0332:  iteration      380/10000000 | consumed samples:        48640 | consumed tokens:     99614720 | elapsed time per iteration (ms): 4567.8 | learning rate: 6.624E-06 | global batch size:   128 | lm loss: 6.006117E+00 | loss scale: 2048.0 | grad norm: 8.835 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.022 | tokens per gpu per second (tgs): 1793.434 | TFLOPs: 14.43 |
g0314: [2024-08-02 18:40:54,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[6.798267733333334e-06, 6.798267733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 390 loss: 5.9479 iter time (s): 4.144 samples/sec: 30.886
g0332:  iteration      390/10000000 | consumed samples:        49920 | consumed tokens:    102236160 | elapsed time per iteration (ms): 4177.1 | learning rate: 6.798E-06 | global batch size:   128 | lm loss: 5.961259E+00 | loss scale: 2048.0 | grad norm: 8.054 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.643 | tokens per gpu per second (tgs): 1961.149 | TFLOPs: 15.78 |
g0314: [2024-08-02 18:41:38,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[6.973030400000001e-06, 6.973030400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 400 loss: 5.7994 iter time (s): 4.462 samples/sec: 28.690
g0332:  iteration      400/10000000 | consumed samples:        51200 | consumed tokens:    104857600 | elapsed time per iteration (ms): 4494.9 | learning rate: 6.973E-06 | global batch size:   128 | lm loss: 5.888361E+00 | loss scale: 2048.0 | grad norm: 7.808 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.477 | tokens per gpu per second (tgs): 1822.505 | TFLOPs: 14.67 |
g0314: [2024-08-02 18:42:22,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[7.147793066666666e-06, 7.147793066666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 410 loss: 5.7859 iter time (s): 4.291 samples/sec: 29.827
g0332:  iteration      410/10000000 | consumed samples:        52480 | consumed tokens:    107479040 | elapsed time per iteration (ms): 4324.5 | learning rate: 7.148E-06 | global batch size:   128 | lm loss: 5.824812E+00 | loss scale: 2048.0 | grad norm: 10.701 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.599 | tokens per gpu per second (tgs): 1894.334 | TFLOPs: 15.24 |
g0314: [2024-08-02 18:43:04,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[7.322555733333333e-06, 7.322555733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 420 loss: 5.7099 iter time (s): 4.194 samples/sec: 30.519
g0332:  iteration      420/10000000 | consumed samples:        53760 | consumed tokens:    110100480 | elapsed time per iteration (ms): 4227.3 | learning rate: 7.323E-06 | global batch size:   128 | lm loss: 5.746822E+00 | loss scale: 2048.0 | grad norm: 7.029 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.280 | tokens per gpu per second (tgs): 1937.897 | TFLOPs: 15.59 |
g0314: [2024-08-02 18:43:45,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[7.4973184e-06, 7.4973184e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 430 loss: 5.6551 iter time (s): 4.081 samples/sec: 31.361
g0332:  iteration      430/10000000 | consumed samples:        55040 | consumed tokens:    112721920 | elapsed time per iteration (ms): 4114.3 | learning rate: 7.497E-06 | global batch size:   128 | lm loss: 5.664729E+00 | loss scale: 2048.0 | grad norm: 8.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.111 | tokens per gpu per second (tgs): 1991.108 | TFLOPs: 16.02 |
g0314: [2024-08-02 18:44:26,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[7.672081066666667e-06, 7.672081066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 440 loss: 5.5872 iter time (s): 4.035 samples/sec: 31.725
g0332:  iteration      440/10000000 | consumed samples:        56320 | consumed tokens:    115343360 | elapsed time per iteration (ms): 4067.8 | learning rate: 7.672E-06 | global batch size:   128 | lm loss: 5.623890E+00 | loss scale: 2048.0 | grad norm: 8.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.467 | tokens per gpu per second (tgs): 2013.885 | TFLOPs: 16.21 |
g0314: [2024-08-02 18:45:07,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[7.846843733333333e-06, 7.846843733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 450 loss: 5.5771 iter time (s): 4.125 samples/sec: 31.032
g0332:  iteration      450/10000000 | consumed samples:        57600 | consumed tokens:    117964800 | elapsed time per iteration (ms): 4158.1 | learning rate: 7.847E-06 | global batch size:   128 | lm loss: 5.546548E+00 | loss scale: 2048.0 | grad norm: 7.985 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.783 | tokens per gpu per second (tgs): 1970.131 | TFLOPs: 15.85 |
g0314: [2024-08-02 18:45:51,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[8.0216064e-06, 8.0216064e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 460 loss: 5.4571 iter time (s): 4.328 samples/sec: 29.573
g0332:  iteration      460/10000000 | consumed samples:        58880 | consumed tokens:    120586240 | elapsed time per iteration (ms): 4361.6 | learning rate: 8.022E-06 | global batch size:   128 | lm loss: 5.491718E+00 | loss scale: 2048.0 | grad norm: 10.913 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.347 | tokens per gpu per second (tgs): 1878.216 | TFLOPs: 15.11 |
g0314: [2024-08-02 18:46:33,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[8.196369066666667e-06, 8.196369066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 470 loss: 5.3615 iter time (s): 4.190 samples/sec: 30.548
g0332:  iteration      470/10000000 | consumed samples:        60160 | consumed tokens:    123207680 | elapsed time per iteration (ms): 4223.9 | learning rate: 8.196E-06 | global batch size:   128 | lm loss: 5.457296E+00 | loss scale: 2048.0 | grad norm: 7.442 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.304 | tokens per gpu per second (tgs): 1939.455 | TFLOPs: 15.61 |
g0314: [2024-08-02 18:47:14,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[8.371131733333335e-06, 8.371131733333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 480 loss: 5.3828 iter time (s): 4.087 samples/sec: 31.318
g0332:  iteration      480/10000000 | consumed samples:        61440 | consumed tokens:    125829120 | elapsed time per iteration (ms): 4120.1 | learning rate: 8.371E-06 | global batch size:   128 | lm loss: 5.393847E+00 | loss scale: 2048.0 | grad norm: 8.936 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.067 | tokens per gpu per second (tgs): 1988.293 | TFLOPs: 16.00 |
g0314: [2024-08-02 18:47:56,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[8.5458944e-06, 8.5458944e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 490 loss: 5.2768 iter time (s): 4.076 samples/sec: 31.405
g0332:  iteration      490/10000000 | consumed samples:        62720 | consumed tokens:    128450560 | elapsed time per iteration (ms): 4108.9 | learning rate: 8.546E-06 | global batch size:   128 | lm loss: 5.323031E+00 | loss scale: 2048.0 | grad norm: 9.979 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.152 | tokens per gpu per second (tgs): 1993.723 | TFLOPs: 16.04 |
g0314: [2024-08-02 18:48:37,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[8.720657066666667e-06, 8.720657066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 500 loss: 5.2596 iter time (s): 4.144 samples/sec: 30.888
g0332:  iteration      500/10000000 | consumed samples:        64000 | consumed tokens:    131072000 | elapsed time per iteration (ms): 4176.8 | learning rate: 8.721E-06 | global batch size:   128 | lm loss: 5.270611E+00 | loss scale: 2048.0 | grad norm: 7.986 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.645 | tokens per gpu per second (tgs): 1961.305 | TFLOPs: 15.78 |
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0325: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0325: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0329: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0319: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0332: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0320: [2024-08-02 18:48:42,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0325: [2024-08-02 18:48:42,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0314: [2024-08-02 18:49:22,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[8.895419733333333e-06, 8.895419733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 510 loss: 5.1162 iter time (s): 4.409 samples/sec: 29.029
g0332:  iteration      510/10000000 | consumed samples:        65280 | consumed tokens:    133693440 | elapsed time per iteration (ms): 4442.3 | learning rate: 8.895E-06 | global batch size:   128 | lm loss: 5.163405E+00 | loss scale: 4096.0 | grad norm: 8.875 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.814 | tokens per gpu per second (tgs): 1844.091 | TFLOPs: 14.84 |
g0314: [2024-08-02 18:50:03,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[9.0701824e-06, 9.0701824e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 520 loss: 5.1371 iter time (s): 4.095 samples/sec: 31.260
g0332:  iteration      520/10000000 | consumed samples:        66560 | consumed tokens:    136314880 | elapsed time per iteration (ms): 4127.5 | learning rate: 9.070E-06 | global batch size:   128 | lm loss: 5.138557E+00 | loss scale: 4096.0 | grad norm: 11.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.011 | tokens per gpu per second (tgs): 1984.720 | TFLOPs: 15.97 |
g0314: [2024-08-02 18:50:44,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[9.244945066666667e-06, 9.244945066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 530 loss: 5.1261 iter time (s): 4.090 samples/sec: 31.294
g0332:  iteration      530/10000000 | consumed samples:        67840 | consumed tokens:    138936320 | elapsed time per iteration (ms): 4123.2 | learning rate: 9.245E-06 | global batch size:   128 | lm loss: 5.130341E+00 | loss scale: 4096.0 | grad norm: 10.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.044 | tokens per gpu per second (tgs): 1986.787 | TFLOPs: 15.99 |
g0314: [2024-08-02 18:51:28,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[9.419707733333334e-06, 9.419707733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 540 loss: 5.0218 iter time (s): 4.314 samples/sec: 29.670
g0332:  iteration      540/10000000 | consumed samples:        69120 | consumed tokens:    141557760 | elapsed time per iteration (ms): 4347.0 | learning rate: 9.420E-06 | global batch size:   128 | lm loss: 5.074923E+00 | loss scale: 4096.0 | grad norm: 8.314 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.446 | tokens per gpu per second (tgs): 1884.529 | TFLOPs: 15.17 |
g0314: [2024-08-02 18:52:11,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[9.5944704e-06, 9.5944704e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 550 loss: 4.9165 iter time (s): 4.259 samples/sec: 30.056
g0332:  iteration      550/10000000 | consumed samples:        70400 | consumed tokens:    144179200 | elapsed time per iteration (ms): 4291.4 | learning rate: 9.594E-06 | global batch size:   128 | lm loss: 5.011442E+00 | loss scale: 4096.0 | grad norm: 10.703 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.827 | tokens per gpu per second (tgs): 1908.936 | TFLOPs: 15.36 |
g0314: [2024-08-02 18:52:56,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[9.769233066666668e-06, 9.769233066666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 560 loss: 4.9055 iter time (s): 4.548 samples/sec: 28.141
g0332:  iteration      560/10000000 | consumed samples:        71680 | consumed tokens:    146800640 | elapsed time per iteration (ms): 4583.2 | learning rate: 9.769E-06 | global batch size:   128 | lm loss: 4.940444E+00 | loss scale: 4096.0 | grad norm: 8.321 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.928 | tokens per gpu per second (tgs): 1787.411 | TFLOPs: 14.38 |
g0314: [2024-08-02 18:53:41,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[9.943995733333334e-06, 9.943995733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 570 loss: 4.8927 iter time (s): 4.378 samples/sec: 29.235
g0332:  iteration      570/10000000 | consumed samples:        72960 | consumed tokens:    149422080 | elapsed time per iteration (ms): 4411.2 | learning rate: 9.944E-06 | global batch size:   128 | lm loss: 4.897688E+00 | loss scale: 4096.0 | grad norm: 7.357 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.017 | tokens per gpu per second (tgs): 1857.086 | TFLOPs: 14.94 |
g0314: [2024-08-02 18:54:23,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[1.01187584e-05, 1.01187584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 580 loss: 4.7952 iter time (s): 4.233 samples/sec: 30.235
g0332:  iteration      580/10000000 | consumed samples:        74240 | consumed tokens:    152043520 | elapsed time per iteration (ms): 4266.3 | learning rate: 1.012E-05 | global batch size:   128 | lm loss: 4.868710E+00 | loss scale: 4096.0 | grad norm: 6.756 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.003 | tokens per gpu per second (tgs): 1920.162 | TFLOPs: 15.45 |
g0314: [2024-08-02 18:55:05,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[1.0293521066666666e-05, 1.0293521066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 590 loss: 4.8250 iter time (s): 4.155 samples/sec: 30.806
g0332:  iteration      590/10000000 | consumed samples:        75520 | consumed tokens:    154664960 | elapsed time per iteration (ms): 4187.8 | learning rate: 1.029E-05 | global batch size:   128 | lm loss: 4.828522E+00 | loss scale: 4096.0 | grad norm: 10.114 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.565 | tokens per gpu per second (tgs): 1956.164 | TFLOPs: 15.74 |
g0314: [2024-08-02 18:55:48,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[1.0468283733333334e-05, 1.0468283733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 600 loss: 4.7121 iter time (s): 4.274 samples/sec: 29.948
g0332:  iteration      600/10000000 | consumed samples:        76800 | consumed tokens:    157286400 | elapsed time per iteration (ms): 4307.5 | learning rate: 1.047E-05 | global batch size:   128 | lm loss: 4.786688E+00 | loss scale: 4096.0 | grad norm: 8.303 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.716 | tokens per gpu per second (tgs): 1901.802 | TFLOPs: 15.30 |
g0314: [2024-08-02 18:56:31,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[1.06430464e-05, 1.06430464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 610 loss: 4.7078 iter time (s): 4.204 samples/sec: 30.444
g0332:  iteration      610/10000000 | consumed samples:        78080 | consumed tokens:    159907840 | elapsed time per iteration (ms): 4237.1 | learning rate: 1.064E-05 | global batch size:   128 | lm loss: 4.762707E+00 | loss scale: 4096.0 | grad norm: 7.739 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.209 | tokens per gpu per second (tgs): 1933.390 | TFLOPs: 15.56 |
g0314: [2024-08-02 18:57:13,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[1.0817809066666668e-05, 1.0817809066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 620 loss: 4.6802 iter time (s): 4.230 samples/sec: 30.260
g0332:  iteration      620/10000000 | consumed samples:        79360 | consumed tokens:    162529280 | elapsed time per iteration (ms): 4263.5 | learning rate: 1.082E-05 | global batch size:   128 | lm loss: 4.729909E+00 | loss scale: 4096.0 | grad norm: 10.366 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.022 | tokens per gpu per second (tgs): 1921.421 | TFLOPs: 15.46 |
g0314: [2024-08-02 18:57:56,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[1.0992571733333332e-05, 1.0992571733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 630 loss: 4.6980 iter time (s): 4.273 samples/sec: 29.952
g0332:  iteration      630/10000000 | consumed samples:        80640 | consumed tokens:    165150720 | elapsed time per iteration (ms): 4306.4 | learning rate: 1.099E-05 | global batch size:   128 | lm loss: 4.669638E+00 | loss scale: 4096.0 | grad norm: 7.626 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.723 | tokens per gpu per second (tgs): 1902.267 | TFLOPs: 15.31 |
g0314: [2024-08-02 18:58:39,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[1.11673344e-05, 1.11673344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 640 loss: 4.6768 iter time (s): 4.265 samples/sec: 30.014
g0332:  iteration      640/10000000 | consumed samples:        81920 | consumed tokens:    167772160 | elapsed time per iteration (ms): 4297.9 | learning rate: 1.117E-05 | global batch size:   128 | lm loss: 4.655739E+00 | loss scale: 4096.0 | grad norm: 5.752 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.782 | tokens per gpu per second (tgs): 1906.048 | TFLOPs: 15.34 |
g0314: [2024-08-02 18:59:22,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[1.1342097066666666e-05, 1.1342097066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 650 loss: 4.6125 iter time (s): 4.240 samples/sec: 30.191
g0332:  iteration      650/10000000 | consumed samples:        83200 | consumed tokens:    170393600 | elapsed time per iteration (ms): 4273.1 | learning rate: 1.134E-05 | global batch size:   128 | lm loss: 4.601253E+00 | loss scale: 4096.0 | grad norm: 7.631 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.955 | tokens per gpu per second (tgs): 1917.116 | TFLOPs: 15.43 |
g0314: [2024-08-02 19:00:05,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[1.1516859733333334e-05, 1.1516859733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 660 loss: 4.5607 iter time (s): 4.224 samples/sec: 30.304
g0332:  iteration      660/10000000 | consumed samples:        84480 | consumed tokens:    173015040 | elapsed time per iteration (ms): 4256.6 | learning rate: 1.152E-05 | global batch size:   128 | lm loss: 4.560772E+00 | loss scale: 4096.0 | grad norm: 6.047 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.071 | tokens per gpu per second (tgs): 1924.562 | TFLOPs: 15.49 |
g0314: [2024-08-02 19:00:46,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[1.16916224e-05, 1.16916224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 670 loss: 4.5546 iter time (s): 4.154 samples/sec: 30.811
g0332:  iteration      670/10000000 | consumed samples:        85760 | consumed tokens:    175636480 | elapsed time per iteration (ms): 4187.2 | learning rate: 1.169E-05 | global batch size:   128 | lm loss: 4.541615E+00 | loss scale: 4096.0 | grad norm: 6.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.569 | tokens per gpu per second (tgs): 1956.418 | TFLOPs: 15.74 |
g0314: [2024-08-02 19:01:29,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[1.1866385066666668e-05, 1.1866385066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 680 loss: 4.4917 iter time (s): 4.252 samples/sec: 30.107
g0332:  iteration      680/10000000 | consumed samples:        87040 | consumed tokens:    178257920 | elapsed time per iteration (ms): 4284.2 | learning rate: 1.187E-05 | global batch size:   128 | lm loss: 4.502436E+00 | loss scale: 4096.0 | grad norm: 9.956 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.877 | tokens per gpu per second (tgs): 1912.134 | TFLOPs: 15.39 |
g0314: [2024-08-02 19:02:11,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[1.2041147733333334e-05, 1.2041147733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 690 loss: 4.5058 iter time (s): 4.146 samples/sec: 30.873
g0332:  iteration      690/10000000 | consumed samples:        88320 | consumed tokens:    180879360 | elapsed time per iteration (ms): 4178.7 | learning rate: 1.204E-05 | global batch size:   128 | lm loss: 4.471271E+00 | loss scale: 4096.0 | grad norm: 6.641 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.631 | tokens per gpu per second (tgs): 1960.409 | TFLOPs: 15.78 |
g0314: [2024-08-02 19:02:54,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[1.22159104e-05, 1.22159104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 700 loss: 4.3917 iter time (s): 4.270 samples/sec: 29.975
g0332:  iteration      700/10000000 | consumed samples:        89600 | consumed tokens:    183500800 | elapsed time per iteration (ms): 4303.2 | learning rate: 1.222E-05 | global batch size:   128 | lm loss: 4.429476E+00 | loss scale: 4096.0 | grad norm: 5.771 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.746 | tokens per gpu per second (tgs): 1903.716 | TFLOPs: 15.32 |
g0314: [2024-08-02 19:03:36,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[1.2390673066666668e-05, 1.2390673066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 710 loss: 4.3561 iter time (s): 4.193 samples/sec: 30.528
g0332:  iteration      710/10000000 | consumed samples:        90880 | consumed tokens:    186122240 | elapsed time per iteration (ms): 4225.6 | learning rate: 1.239E-05 | global batch size:   128 | lm loss: 4.403533E+00 | loss scale: 4096.0 | grad norm: 6.110 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.292 | tokens per gpu per second (tgs): 1938.676 | TFLOPs: 15.60 |
g0314: [2024-08-02 19:04:18,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[1.2565435733333334e-05, 1.2565435733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 720 loss: 4.2709 iter time (s): 4.137 samples/sec: 30.941
g0332:  iteration      720/10000000 | consumed samples:        92160 | consumed tokens:    188743680 | elapsed time per iteration (ms): 4170.4 | learning rate: 1.257E-05 | global batch size:   128 | lm loss: 4.336254E+00 | loss scale: 4096.0 | grad norm: 8.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.693 | tokens per gpu per second (tgs): 1964.325 | TFLOPs: 15.81 |
g0314: [2024-08-02 19:05:01,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[1.2740198400000001e-05, 1.2740198400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 730 loss: 4.4068 iter time (s): 4.270 samples/sec: 29.975
g0332:  iteration      730/10000000 | consumed samples:        93440 | consumed tokens:    191365120 | elapsed time per iteration (ms): 4304.9 | learning rate: 1.274E-05 | global batch size:   128 | lm loss: 4.361551E+00 | loss scale: 4096.0 | grad norm: 8.321 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.734 | tokens per gpu per second (tgs): 1902.969 | TFLOPs: 15.31 |
g0314: [2024-08-02 19:05:45,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[1.2914961066666667e-05, 1.2914961066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 740 loss: 4.2508 iter time (s): 4.405 samples/sec: 29.059
g0332:  iteration      740/10000000 | consumed samples:        94720 | consumed tokens:    193986560 | elapsed time per iteration (ms): 4439.4 | learning rate: 1.291E-05 | global batch size:   128 | lm loss: 4.307798E+00 | loss scale: 4096.0 | grad norm: 6.908 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.833 | tokens per gpu per second (tgs): 1845.296 | TFLOPs: 14.85 |
g0314: [2024-08-02 19:06:30,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[1.3089723733333335e-05, 1.3089723733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 750 loss: 4.1371 iter time (s): 4.460 samples/sec: 28.700
g0332:  iteration      750/10000000 | consumed samples:        96000 | consumed tokens:    196608000 | elapsed time per iteration (ms): 4493.0 | learning rate: 1.309E-05 | global batch size:   128 | lm loss: 4.256737E+00 | loss scale: 4096.0 | grad norm: 6.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.489 | tokens per gpu per second (tgs): 1823.293 | TFLOPs: 14.67 |
g0314: [2024-08-02 19:07:16,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[1.3264486400000001e-05, 1.3264486400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 760 loss: 4.2773 iter time (s): 4.488 samples/sec: 28.519
g0332:  iteration      760/10000000 | consumed samples:        97280 | consumed tokens:    199229440 | elapsed time per iteration (ms): 4521.0 | learning rate: 1.326E-05 | global batch size:   128 | lm loss: 4.237243E+00 | loss scale: 4096.0 | grad norm: 5.947 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.312 | tokens per gpu per second (tgs): 1811.977 | TFLOPs: 14.58 |
g0314: [2024-08-02 19:07:59,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[1.3439249066666669e-05, 1.3439249066666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 770 loss: 4.1059 iter time (s): 4.289 samples/sec: 29.840
g0332:  iteration      770/10000000 | consumed samples:        98560 | consumed tokens:    201850880 | elapsed time per iteration (ms): 4322.3 | learning rate: 1.344E-05 | global batch size:   128 | lm loss: 4.243768E+00 | loss scale: 4096.0 | grad norm: 6.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.614 | tokens per gpu per second (tgs): 1895.266 | TFLOPs: 15.25 |
g0314: [2024-08-02 19:08:42,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[1.3614011733333333e-05, 1.3614011733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 780 loss: 4.2260 iter time (s): 4.250 samples/sec: 30.118
g0332:  iteration      780/10000000 | consumed samples:        99840 | consumed tokens:    204472320 | elapsed time per iteration (ms): 4282.8 | learning rate: 1.361E-05 | global batch size:   128 | lm loss: 4.224428E+00 | loss scale: 4096.0 | grad norm: 6.463 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.887 | tokens per gpu per second (tgs): 1912.747 | TFLOPs: 15.39 |
g0314: [2024-08-02 19:09:25,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[1.37887744e-05, 1.37887744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 790 loss: 4.2171 iter time (s): 4.328 samples/sec: 29.572
g0332:  iteration      790/10000000 | consumed samples:       101120 | consumed tokens:    207093760 | elapsed time per iteration (ms): 4361.0 | learning rate: 1.379E-05 | global batch size:   128 | lm loss: 4.169104E+00 | loss scale: 4096.0 | grad norm: 6.636 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.351 | tokens per gpu per second (tgs): 1878.480 | TFLOPs: 15.12 |
g0314: [2024-08-02 19:10:09,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[1.3963537066666667e-05, 1.3963537066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 800 loss: 4.2087 iter time (s): 4.343 samples/sec: 29.476
g0332:  iteration      800/10000000 | consumed samples:       102400 | consumed tokens:    209715200 | elapsed time per iteration (ms): 4377.2 | learning rate: 1.396E-05 | global batch size:   128 | lm loss: 4.185576E+00 | loss scale: 4096.0 | grad norm: 4.905 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.243 | tokens per gpu per second (tgs): 1871.530 | TFLOPs: 15.06 |
g0314: [2024-08-02 19:11:21,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[1.4138299733333333e-05, 1.4138299733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 810 loss: 4.1107 iter time (s): 7.177 samples/sec: 17.835
g0332:  iteration      810/10000000 | consumed samples:       103680 | consumed tokens:    212336640 | elapsed time per iteration (ms): 7219.1 | learning rate: 1.414E-05 | global batch size:   128 | lm loss: 4.142586E+00 | loss scale: 4096.0 | grad norm: 5.674 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.731 | tokens per gpu per second (tgs): 1134.769 | TFLOPs: 9.13 |
g0314: [2024-08-02 19:12:17,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[1.43130624e-05, 1.43130624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 820 loss: 4.0828 iter time (s): 5.543 samples/sec: 23.092
g0332:  iteration      820/10000000 | consumed samples:       104960 | consumed tokens:    214958080 | elapsed time per iteration (ms): 5579.3 | learning rate: 1.431E-05 | global batch size:   128 | lm loss: 4.091533E+00 | loss scale: 4096.0 | grad norm: 5.625 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.942 | tokens per gpu per second (tgs): 1468.278 | TFLOPs: 11.82 |
g0314: [2024-08-02 19:13:05,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[1.4487825066666667e-05, 1.4487825066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 830 loss: 4.1495 iter time (s): 4.774 samples/sec: 26.813
g0332:  iteration      830/10000000 | consumed samples:       106240 | consumed tokens:    217579520 | elapsed time per iteration (ms): 4806.5 | learning rate: 1.449E-05 | global batch size:   128 | lm loss: 4.112865E+00 | loss scale: 4096.0 | grad norm: 4.911 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.630 | tokens per gpu per second (tgs): 1704.350 | TFLOPs: 13.72 |
g0314: [2024-08-02 19:13:46,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[1.4662587733333333e-05, 1.4662587733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 840 loss: 3.9027 iter time (s): 4.052 samples/sec: 31.591
g0332:  iteration      840/10000000 | consumed samples:       107520 | consumed tokens:    220200960 | elapsed time per iteration (ms): 4084.8 | learning rate: 1.466E-05 | global batch size:   128 | lm loss: 4.058000E+00 | loss scale: 4096.0 | grad norm: 5.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.336 | tokens per gpu per second (tgs): 2005.506 | TFLOPs: 16.14 |
g0314: [2024-08-02 19:14:27,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[1.4837350400000001e-05, 1.4837350400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 850 loss: 4.0185 iter time (s): 4.085 samples/sec: 31.330
g0332:  iteration      850/10000000 | consumed samples:       108800 | consumed tokens:    222822400 | elapsed time per iteration (ms): 4118.3 | learning rate: 1.484E-05 | global batch size:   128 | lm loss: 4.048272E+00 | loss scale: 4096.0 | grad norm: 5.751 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.081 | tokens per gpu per second (tgs): 1989.175 | TFLOPs: 16.01 |
g0314: [2024-08-02 19:15:09,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.5012113066666667e-05, 1.5012113066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 860 loss: 4.0360 iter time (s): 4.148 samples/sec: 30.859
g0332:  iteration      860/10000000 | consumed samples:       110080 | consumed tokens:    225443840 | elapsed time per iteration (ms): 4180.9 | learning rate: 1.501E-05 | global batch size:   128 | lm loss: 3.988923E+00 | loss scale: 4096.0 | grad norm: 5.027 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.615 | tokens per gpu per second (tgs): 1959.376 | TFLOPs: 15.77 |
g0314: [2024-08-02 19:15:53,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.5186875733333335e-05, 1.5186875733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 870 loss: 3.9613 iter time (s): 4.329 samples/sec: 29.567
g0332:  iteration      870/10000000 | consumed samples:       111360 | consumed tokens:    228065280 | elapsed time per iteration (ms): 4364.5 | learning rate: 1.519E-05 | global batch size:   128 | lm loss: 3.988102E+00 | loss scale: 4096.0 | grad norm: 5.559 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.328 | tokens per gpu per second (tgs): 1876.960 | TFLOPs: 15.10 |
g0314: [2024-08-02 19:16:35,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.5361638400000003e-05, 1.5361638400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 880 loss: 3.9949 iter time (s): 4.229 samples/sec: 30.268
g0332:  iteration      880/10000000 | consumed samples:       112640 | consumed tokens:    230686720 | elapsed time per iteration (ms): 4262.0 | learning rate: 1.536E-05 | global batch size:   128 | lm loss: 3.976963E+00 | loss scale: 4096.0 | grad norm: 4.643 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.033 | tokens per gpu per second (tgs): 1922.103 | TFLOPs: 15.47 |
g0314: [2024-08-02 19:17:18,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.553640106666667e-05, 1.553640106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 890 loss: 3.9130 iter time (s): 4.202 samples/sec: 30.465
g0332:  iteration      890/10000000 | consumed samples:       113920 | consumed tokens:    233308160 | elapsed time per iteration (ms): 4235.9 | learning rate: 1.554E-05 | global batch size:   128 | lm loss: 3.967604E+00 | loss scale: 4096.0 | grad norm: 5.128 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.218 | tokens per gpu per second (tgs): 1933.968 | TFLOPs: 15.56 |
g0314: [2024-08-02 19:18:00,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.5711163733333335e-05, 1.5711163733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 900 loss: 3.9769 iter time (s): 4.228 samples/sec: 30.271
g0332:  iteration      900/10000000 | consumed samples:       115200 | consumed tokens:    235929600 | elapsed time per iteration (ms): 4261.1 | learning rate: 1.571E-05 | global batch size:   128 | lm loss: 3.937359E+00 | loss scale: 4096.0 | grad norm: 5.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.039 | tokens per gpu per second (tgs): 1922.519 | TFLOPs: 15.47 |
g0314: [2024-08-02 19:18:44,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.58859264e-05, 1.58859264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 910 loss: 3.9668 iter time (s): 4.381 samples/sec: 29.214
g0332:  iteration      910/10000000 | consumed samples:       116480 | consumed tokens:    238551040 | elapsed time per iteration (ms): 4414.3 | learning rate: 1.589E-05 | global batch size:   128 | lm loss: 3.941708E+00 | loss scale: 4096.0 | grad norm: 5.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.997 | tokens per gpu per second (tgs): 1855.782 | TFLOPs: 14.93 |
g0314: [2024-08-02 19:19:27,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[1.6060689066666667e-05, 1.6060689066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 920 loss: 3.9197 iter time (s): 4.227 samples/sec: 30.278
g0332:  iteration      920/10000000 | consumed samples:       117760 | consumed tokens:    241172480 | elapsed time per iteration (ms): 4260.8 | learning rate: 1.606E-05 | global batch size:   128 | lm loss: 3.919430E+00 | loss scale: 4096.0 | grad norm: 5.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.041 | tokens per gpu per second (tgs): 1922.651 | TFLOPs: 15.47 |
g0314: [2024-08-02 19:20:10,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[1.6235451733333336e-05, 1.6235451733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 930 loss: 3.9110 iter time (s): 4.271 samples/sec: 29.970
g0332:  iteration      930/10000000 | consumed samples:       119040 | consumed tokens:    243793920 | elapsed time per iteration (ms): 4304.1 | learning rate: 1.624E-05 | global batch size:   128 | lm loss: 3.879317E+00 | loss scale: 4096.0 | grad norm: 5.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.739 | tokens per gpu per second (tgs): 1903.282 | TFLOPs: 15.32 |
g0314: [2024-08-02 19:20:55,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[1.6410214400000002e-05, 1.6410214400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 940 loss: 3.8816 iter time (s): 4.441 samples/sec: 28.824
g0332:  iteration      940/10000000 | consumed samples:       120320 | consumed tokens:    246415360 | elapsed time per iteration (ms): 4476.1 | learning rate: 1.641E-05 | global batch size:   128 | lm loss: 3.881224E+00 | loss scale: 4096.0 | grad norm: 6.037 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.596 | tokens per gpu per second (tgs): 1830.149 | TFLOPs: 14.73 |
g0314: [2024-08-02 19:21:38,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[1.6584977066666665e-05, 1.6584977066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 950 loss: 3.8034 iter time (s): 4.321 samples/sec: 29.626
g0332:  iteration      950/10000000 | consumed samples:       121600 | consumed tokens:    249036800 | elapsed time per iteration (ms): 4353.1 | learning rate: 1.658E-05 | global batch size:   128 | lm loss: 3.800939E+00 | loss scale: 4096.0 | grad norm: 4.586 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.404 | tokens per gpu per second (tgs): 1881.885 | TFLOPs: 15.14 |
g0314: [2024-08-02 19:22:21,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[1.6759739733333334e-05, 1.6759739733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 960 loss: 3.7907 iter time (s): 4.209 samples/sec: 30.411
g0332:  iteration      960/10000000 | consumed samples:       122880 | consumed tokens:    251658240 | elapsed time per iteration (ms): 4241.9 | learning rate: 1.676E-05 | global batch size:   128 | lm loss: 3.839429E+00 | loss scale: 4096.0 | grad norm: 4.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.175 | tokens per gpu per second (tgs): 1931.202 | TFLOPs: 15.54 |
g0314: [2024-08-02 19:23:12,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[1.69345024e-05, 1.69345024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 970 loss: 3.7661 iter time (s): 5.124 samples/sec: 24.980
g0332:  iteration      970/10000000 | consumed samples:       124160 | consumed tokens:    254279680 | elapsed time per iteration (ms): 5156.8 | learning rate: 1.693E-05 | global batch size:   128 | lm loss: 3.830473E+00 | loss scale: 4096.0 | grad norm: 4.688 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.822 | tokens per gpu per second (tgs): 1588.586 | TFLOPs: 12.78 |
g0314: [2024-08-02 19:24:10,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[1.7109265066666667e-05, 1.7109265066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 980 loss: 3.7660 iter time (s): 5.717 samples/sec: 22.388
g0332:  iteration      980/10000000 | consumed samples:       125440 | consumed tokens:    256901120 | elapsed time per iteration (ms): 5750.6 | learning rate: 1.711E-05 | global batch size:   128 | lm loss: 3.790676E+00 | loss scale: 4096.0 | grad norm: 4.008 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.258 | tokens per gpu per second (tgs): 1424.540 | TFLOPs: 11.46 |
g0314: [2024-08-02 19:24:54,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[1.7284027733333333e-05, 1.7284027733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 990 loss: 3.7367 iter time (s): 4.358 samples/sec: 29.370
g0332:  iteration      990/10000000 | consumed samples:       126720 | consumed tokens:    259522560 | elapsed time per iteration (ms): 4391.9 | learning rate: 1.728E-05 | global batch size:   128 | lm loss: 3.769820E+00 | loss scale: 4096.0 | grad norm: 3.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.144 | tokens per gpu per second (tgs): 1865.239 | TFLOPs: 15.01 |
g0314: [2024-08-02 19:25:36,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.7458790400000002e-05, 1.7458790400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1000 loss: 3.7452 iter time (s): 4.226 samples/sec: 30.288
g0332:  iteration     1000/10000000 | consumed samples:       128000 | consumed tokens:    262144000 | elapsed time per iteration (ms): 4259.2 | learning rate: 1.746E-05 | global batch size:   128 | lm loss: 3.747299E+00 | loss scale: 4096.0 | grad norm: 5.417 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.053 | tokens per gpu per second (tgs): 1923.371 | TFLOPs: 15.48 |
g0332: ------------------------------------------------------------------------------------------------
g0332:  validation loss at iteration 1000 | lm loss value: 3.752594E+00 | lm loss PPL: 4.263153E+01 | 
g0332: ------------------------------------------------------------------------------------------------
g0314: saving checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: [2024-08-02 19:32:16,668] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
g0314: [2024-08-02 19:32:16,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0332: [2024-08-02 19:32:16,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0314: [2024-08-02 19:32:16,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0332: [2024-08-02 19:32:16,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0332: [2024-08-02 19:32:16,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0314: [2024-08-02 19:32:16,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0319: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0319: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0319: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0325: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0325: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0325: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0320: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0316: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0320: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0320: [2024-08-02 19:32:16,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0316: [2024-08-02 19:32:16,682] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0316: [2024-08-02 19:32:16,682] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0329: [2024-08-02 19:32:16,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0329: [2024-08-02 19:32:16,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0329: [2024-08-02 19:32:16,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0318: [2024-08-02 19:32:16,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0318: [2024-08-02 19:32:16,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0318: [2024-08-02 19:32:16,687] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0332: [2024-08-02 19:32:16,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt...
g0316: [2024-08-02 19:32:16,713] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt...
g0325: [2024-08-02 19:32:16,715] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt...
g0319: [2024-08-02 19:32:16,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt...
g0320: [2024-08-02 19:32:16,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt...
g0329: [2024-08-02 19:32:16,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt...
g0318: [2024-08-02 19:32:16,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt...
g0314: [2024-08-02 19:32:16,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt...
g0320: [2024-08-02 19:32:16,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt.
g0332: [2024-08-02 19:32:16,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt.
g0332: [2024-08-02 19:32:16,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt...
g0332: [2024-08-02 19:32:16,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt.
g0316: [2024-08-02 19:32:16,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt.
g0318: [2024-08-02 19:32:16,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt.
g0320: [2024-08-02 19:32:16,867] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt...
g0329: [2024-08-02 19:32:16,868] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt.
g0316: [2024-08-02 19:32:16,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt...
g0319: [2024-08-02 19:32:16,890] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt.
g0318: [2024-08-02 19:32:16,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt...
g0332: [2024-08-02 19:32:16,898] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt...
g0329: [2024-08-02 19:32:16,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt...
g0319: [2024-08-02 19:32:16,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt...
g0325: [2024-08-02 19:32:16,930] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt.
g0325: [2024-08-02 19:32:16,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt...
g0314: [2024-08-02 19:32:16,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt.
g0314: [2024-08-02 19:32:17,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt...
g0316: [2024-08-02 19:32:17,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt.
g0329: [2024-08-02 19:32:17,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt.
g0316: [2024-08-02 19:32:17,054] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt...
g0329: [2024-08-02 19:32:17,075] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt...
g0325: [2024-08-02 19:32:17,089] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt.
g0318: [2024-08-02 19:32:17,121] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt.
g0325: [2024-08-02 19:32:17,121] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt...
g0314: [2024-08-02 19:32:17,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt.
g0318: [2024-08-02 19:32:17,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt...
g0314: [2024-08-02 19:32:17,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt...
g0329: [2024-08-02 19:32:17,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt.
g0329: [2024-08-02 19:32:17,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt...
g0332: [2024-08-02 19:32:17,184] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt.
g0332: [2024-08-02 19:32:17,186] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt...
g0325: [2024-08-02 19:32:17,214] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt.
g0325: [2024-08-02 19:32:17,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt...
g0319: [2024-08-02 19:32:17,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt.
g0318: [2024-08-02 19:32:17,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt.
g0318: [2024-08-02 19:32:17,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt...
g0319: [2024-08-02 19:32:17,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt...
g0320: [2024-08-02 19:32:17,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt.
g0320: [2024-08-02 19:32:17,381] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt...
g0314: [2024-08-02 19:32:17,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt.
g0319: [2024-08-02 19:32:17,404] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt.
g0319: [2024-08-02 19:32:17,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt...
g0314: [2024-08-02 19:32:17,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt...
g0320: [2024-08-02 19:32:17,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt.
g0320: [2024-08-02 19:32:17,513] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt...
g0314: [2024-08-02 19:32:17,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt.
g0314: [2024-08-02 19:32:17,538] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt
g0314: [2024-08-02 19:32:17,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt...
g0316: [2024-08-02 19:32:17,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt.
g0316: [2024-08-02 19:32:17,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt...
g0332: [2024-08-02 19:32:19,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt.
g0332: [2024-08-02 19:32:19,172] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0325: [2024-08-02 19:32:19,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt.
g0325: [2024-08-02 19:32:19,508] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0316: [2024-08-02 19:32:19,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt.
g0316: [2024-08-02 19:32:19,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0319: [2024-08-02 19:32:19,818] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt.
g0319: [2024-08-02 19:32:19,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0320: [2024-08-02 19:32:19,931] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt.
g0320: [2024-08-02 19:32:19,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0318: [2024-08-02 19:32:20,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt.
g0318: [2024-08-02 19:32:20,115] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0329: [2024-08-02 19:32:20,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt.
g0329: [2024-08-02 19:32:20,252] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0314: [2024-08-02 19:32:21,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt.
g0314: [2024-08-02 19:32:21,041] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0314:   successfully saved checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: Checkpoint Save GB: 22.521, GB/Sec: 5.13, Latency(second): 4.389
g0332: (min, max) time across ranks (ms):
g0332:     save-checkpoint ................................: (4389.05, 4389.30)
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0325: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0318: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0318: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0318: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0319: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0329: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0318: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0325: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0325: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0332: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0332: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0325: [2024-08-02 19:32:25,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0332: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 19:32:25,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0320: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0332: [2024-08-02 19:32:25,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0318: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0329: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0329: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0314: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0329: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0329: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0318: [2024-08-02 19:32:25,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0316: [2024-08-02 19:32:25,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0319: [2024-08-02 19:32:25,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0329: [2024-08-02 19:32:25,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0314: [2024-08-02 19:33:05,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[1.7633553066666668e-05, 1.7633553066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1010 loss: 3.7711 iter time (s): 4.405 samples/sec: 29.057
g0332:  iteration     1010/10000000 | consumed samples:       129280 | consumed tokens:    264765440 | elapsed time per iteration (ms): 44862.1 | learning rate: 1.763E-05 | global batch size:   128 | lm loss: 3.746984E+00 | loss scale: 8192.0 | grad norm: 4.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.853 | tokens per gpu per second (tgs): 182.604 | TFLOPs: 1.47 |
g0314: [2024-08-02 19:33:51,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[1.7808315733333334e-05, 1.7808315733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1020 loss: 3.7380 iter time (s): 4.538 samples/sec: 28.204
g0332:  iteration     1020/10000000 | consumed samples:       130560 | consumed tokens:    267386880 | elapsed time per iteration (ms): 4570.9 | learning rate: 1.781E-05 | global batch size:   128 | lm loss: 3.761729E+00 | loss scale: 8192.0 | grad norm: 4.659 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.003 | tokens per gpu per second (tgs): 1792.197 | TFLOPs: 14.42 |
g0314: [2024-08-02 19:34:34,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[1.79830784e-05, 1.79830784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1030 loss: 3.6112 iter time (s): 4.315 samples/sec: 29.661
g0332:  iteration     1030/10000000 | consumed samples:       131840 | consumed tokens:    270008320 | elapsed time per iteration (ms): 4350.0 | learning rate: 1.798E-05 | global batch size:   128 | lm loss: 3.694182E+00 | loss scale: 8192.0 | grad norm: 4.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.425 | tokens per gpu per second (tgs): 1883.222 | TFLOPs: 15.15 |
g0314: [2024-08-02 19:35:17,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[1.8157841066666666e-05, 1.8157841066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1040 loss: 3.7217 iter time (s): 4.254 samples/sec: 30.089
g0332:  iteration     1040/10000000 | consumed samples:       133120 | consumed tokens:    272629760 | elapsed time per iteration (ms): 4286.7 | learning rate: 1.816E-05 | global batch size:   128 | lm loss: 3.692569E+00 | loss scale: 8192.0 | grad norm: 4.460 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.860 | tokens per gpu per second (tgs): 1911.025 | TFLOPs: 15.38 |
g0314: [2024-08-02 19:36:00,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[1.8332603733333336e-05, 1.8332603733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1050 loss: 3.6335 iter time (s): 4.283 samples/sec: 29.883
g0332:  iteration     1050/10000000 | consumed samples:       134400 | consumed tokens:    275251200 | elapsed time per iteration (ms): 4316.4 | learning rate: 1.833E-05 | global batch size:   128 | lm loss: 3.658799E+00 | loss scale: 8192.0 | grad norm: 4.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.654 | tokens per gpu per second (tgs): 1897.887 | TFLOPs: 15.27 |
g0314: [2024-08-02 19:36:43,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[1.8507366400000002e-05, 1.8507366400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1060 loss: 3.7270 iter time (s): 4.283 samples/sec: 29.888
g0332:  iteration     1060/10000000 | consumed samples:       135680 | consumed tokens:    277872640 | elapsed time per iteration (ms): 4315.2 | learning rate: 1.851E-05 | global batch size:   128 | lm loss: 3.658746E+00 | loss scale: 8192.0 | grad norm: 5.107 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.662 | tokens per gpu per second (tgs): 1898.384 | TFLOPs: 15.28 |
g0314: [2024-08-02 19:37:27,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[1.8682129066666668e-05, 1.8682129066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1070 loss: 3.6925 iter time (s): 4.289 samples/sec: 29.844
g0332:  iteration     1070/10000000 | consumed samples:       136960 | consumed tokens:    280494080 | elapsed time per iteration (ms): 4323.8 | learning rate: 1.868E-05 | global batch size:   128 | lm loss: 3.662873E+00 | loss scale: 8192.0 | grad norm: 4.127 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.603 | tokens per gpu per second (tgs): 1894.614 | TFLOPs: 15.25 |
g0314: [2024-08-02 19:38:09,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[1.8856891733333334e-05, 1.8856891733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1080 loss: 3.5683 iter time (s): 4.257 samples/sec: 30.067
g0332:  iteration     1080/10000000 | consumed samples:       138240 | consumed tokens:    283115520 | elapsed time per iteration (ms): 4289.9 | learning rate: 1.886E-05 | global batch size:   128 | lm loss: 3.628769E+00 | loss scale: 8192.0 | grad norm: 3.888 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.837 | tokens per gpu per second (tgs): 1909.593 | TFLOPs: 15.37 |
g0314: [2024-08-02 19:38:52,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[1.9031654400000003e-05, 1.9031654400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1090 loss: 3.5784 iter time (s): 4.268 samples/sec: 29.991
g0332:  iteration     1090/10000000 | consumed samples:       139520 | consumed tokens:    285736960 | elapsed time per iteration (ms): 4301.2 | learning rate: 1.903E-05 | global batch size:   128 | lm loss: 3.646032E+00 | loss scale: 8192.0 | grad norm: 3.966 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.759 | tokens per gpu per second (tgs): 1904.595 | TFLOPs: 15.33 |
g0314: [2024-08-02 19:39:35,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[1.920641706666667e-05, 1.920641706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1100 loss: 3.6092 iter time (s): 4.237 samples/sec: 30.209
g0332:  iteration     1100/10000000 | consumed samples:       140800 | consumed tokens:    288358400 | elapsed time per iteration (ms): 4269.9 | learning rate: 1.921E-05 | global batch size:   128 | lm loss: 3.599593E+00 | loss scale: 8192.0 | grad norm: 4.435 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.977 | tokens per gpu per second (tgs): 1918.535 | TFLOPs: 15.44 |
g0314: [2024-08-02 19:40:19,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[1.9381179733333332e-05, 1.9381179733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1110 loss: 3.7061 iter time (s): 4.315 samples/sec: 29.664
g0332:  iteration     1110/10000000 | consumed samples:       142080 | consumed tokens:    290979840 | elapsed time per iteration (ms): 4348.2 | learning rate: 1.938E-05 | global batch size:   128 | lm loss: 3.590984E+00 | loss scale: 8192.0 | grad norm: 4.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.437 | tokens per gpu per second (tgs): 1883.990 | TFLOPs: 15.16 |
g0314: [2024-08-02 19:41:02,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[1.95559424e-05, 1.95559424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1120 loss: 3.5688 iter time (s): 4.351 samples/sec: 29.417
g0332:  iteration     1120/10000000 | consumed samples:       143360 | consumed tokens:    293601280 | elapsed time per iteration (ms): 4384.1 | learning rate: 1.956E-05 | global batch size:   128 | lm loss: 3.545776E+00 | loss scale: 8192.0 | grad norm: 4.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.196 | tokens per gpu per second (tgs): 1868.567 | TFLOPs: 15.04 |
g0314: [2024-08-02 19:41:46,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[1.9730705066666668e-05, 1.9730705066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1130 loss: 3.5475 iter time (s): 4.280 samples/sec: 29.908
g0332:  iteration     1130/10000000 | consumed samples:       144640 | consumed tokens:    296222720 | elapsed time per iteration (ms): 4313.3 | learning rate: 1.973E-05 | global batch size:   128 | lm loss: 3.583337E+00 | loss scale: 8192.0 | grad norm: 3.830 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.676 | tokens per gpu per second (tgs): 1899.259 | TFLOPs: 15.28 |
g0314: [2024-08-02 19:42:28,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[1.9905467733333334e-05, 1.9905467733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1140 loss: 3.5501 iter time (s): 4.250 samples/sec: 30.118
g0332:  iteration     1140/10000000 | consumed samples:       145920 | consumed tokens:    298844160 | elapsed time per iteration (ms): 4283.2 | learning rate: 1.991E-05 | global batch size:   128 | lm loss: 3.528992E+00 | loss scale: 8192.0 | grad norm: 3.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.884 | tokens per gpu per second (tgs): 1912.579 | TFLOPs: 15.39 |
g0314: [2024-08-02 19:43:12,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[2.00802304e-05, 2.00802304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1150 loss: 3.4803 iter time (s): 4.286 samples/sec: 29.864
g0332:  iteration     1150/10000000 | consumed samples:       147200 | consumed tokens:    301465600 | elapsed time per iteration (ms): 4318.9 | learning rate: 2.008E-05 | global batch size:   128 | lm loss: 3.511540E+00 | loss scale: 8192.0 | grad norm: 3.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.638 | tokens per gpu per second (tgs): 1896.801 | TFLOPs: 15.26 |
g0314: [2024-08-02 19:43:55,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[2.0254993066666666e-05, 2.0254993066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1160 loss: 3.5750 iter time (s): 4.290 samples/sec: 29.836
g0332:  iteration     1160/10000000 | consumed samples:       148480 | consumed tokens:    304087040 | elapsed time per iteration (ms): 4324.8 | learning rate: 2.025E-05 | global batch size:   128 | lm loss: 3.513354E+00 | loss scale: 8192.0 | grad norm: 4.473 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.597 | tokens per gpu per second (tgs): 1894.198 | TFLOPs: 15.24 |
g0314: [2024-08-02 19:44:38,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[2.0429755733333335e-05, 2.0429755733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1170 loss: 3.4719 iter time (s): 4.319 samples/sec: 29.634
g0332:  iteration     1170/10000000 | consumed samples:       149760 | consumed tokens:    306708480 | elapsed time per iteration (ms): 4352.2 | learning rate: 2.043E-05 | global batch size:   128 | lm loss: 3.503035E+00 | loss scale: 8192.0 | grad norm: 3.886 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.410 | tokens per gpu per second (tgs): 1882.250 | TFLOPs: 15.15 |
g0314: [2024-08-02 19:45:23,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[2.06045184e-05, 2.06045184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1180 loss: 3.2383 iter time (s): 4.447 samples/sec: 28.783
g0332:  iteration     1180/10000000 | consumed samples:       151040 | consumed tokens:    309329920 | elapsed time per iteration (ms): 4480.0 | learning rate: 2.060E-05 | global batch size:   128 | lm loss: 3.440237E+00 | loss scale: 8192.0 | grad norm: 4.074 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.571 | tokens per gpu per second (tgs): 1828.576 | TFLOPs: 14.71 |
g0314: [2024-08-02 19:46:08,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[2.0779281066666667e-05, 2.0779281066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1190 loss: 3.4153 iter time (s): 4.437 samples/sec: 28.850
g0332:  iteration     1190/10000000 | consumed samples:       152320 | consumed tokens:    311951360 | elapsed time per iteration (ms): 4472.0 | learning rate: 2.078E-05 | global batch size:   128 | lm loss: 3.447852E+00 | loss scale: 8192.0 | grad norm: 3.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.622 | tokens per gpu per second (tgs): 1831.826 | TFLOPs: 14.74 |
g0314: [2024-08-02 19:46:51,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[2.0954043733333333e-05, 2.0954043733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1200 loss: 3.4864 iter time (s): 4.290 samples/sec: 29.840
g0332:  iteration     1200/10000000 | consumed samples:       153600 | consumed tokens:    314572800 | elapsed time per iteration (ms): 4322.9 | learning rate: 2.095E-05 | global batch size:   128 | lm loss: 3.454360E+00 | loss scale: 8192.0 | grad norm: 3.750 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.610 | tokens per gpu per second (tgs): 1895.021 | TFLOPs: 15.25 |
g0314: [2024-08-02 19:47:34,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[2.1128806400000003e-05, 2.1128806400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1210 loss: 3.2866 iter time (s): 4.298 samples/sec: 29.782
g0332:  iteration     1210/10000000 | consumed samples:       154880 | consumed tokens:    317194240 | elapsed time per iteration (ms): 4332.8 | learning rate: 2.113E-05 | global batch size:   128 | lm loss: 3.422218E+00 | loss scale: 8192.0 | grad norm: 3.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.542 | tokens per gpu per second (tgs): 1890.713 | TFLOPs: 15.21 |
g0314: [2024-08-02 19:48:18,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[2.130356906666667e-05, 2.130356906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1220 loss: 3.3838 iter time (s): 4.312 samples/sec: 29.687
g0332:  iteration     1220/10000000 | consumed samples:       156160 | consumed tokens:    319815680 | elapsed time per iteration (ms): 4344.3 | learning rate: 2.130E-05 | global batch size:   128 | lm loss: 3.416864E+00 | loss scale: 8192.0 | grad norm: 3.335 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.464 | tokens per gpu per second (tgs): 1885.704 | TFLOPs: 15.17 |
g0314: [2024-08-02 19:49:00,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[2.1478331733333335e-05, 2.1478331733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1230 loss: 3.4550 iter time (s): 4.147 samples/sec: 30.864
g0332:  iteration     1230/10000000 | consumed samples:       157440 | consumed tokens:    322437120 | elapsed time per iteration (ms): 4180.4 | learning rate: 2.148E-05 | global batch size:   128 | lm loss: 3.425047E+00 | loss scale: 8192.0 | grad norm: 3.820 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.619 | tokens per gpu per second (tgs): 1959.613 | TFLOPs: 15.77 |
g0314: [2024-08-02 19:49:42,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[2.16530944e-05, 2.16530944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1240 loss: 3.4337 iter time (s): 4.230 samples/sec: 30.259
g0332:  iteration     1240/10000000 | consumed samples:       158720 | consumed tokens:    325058560 | elapsed time per iteration (ms): 4263.2 | learning rate: 2.165E-05 | global batch size:   128 | lm loss: 3.404696E+00 | loss scale: 8192.0 | grad norm: 3.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.025 | tokens per gpu per second (tgs): 1921.570 | TFLOPs: 15.46 |
g0314: [2024-08-02 19:50:26,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[2.1827857066666667e-05, 2.1827857066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1250 loss: 3.3020 iter time (s): 4.294 samples/sec: 29.809
g0332:  iteration     1250/10000000 | consumed samples:       160000 | consumed tokens:    327680000 | elapsed time per iteration (ms): 4327.0 | learning rate: 2.183E-05 | global batch size:   128 | lm loss: 3.338159E+00 | loss scale: 8192.0 | grad norm: 3.628 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.582 | tokens per gpu per second (tgs): 1893.218 | TFLOPs: 15.24 |
g0314: [2024-08-02 19:51:09,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[2.2002619733333337e-05, 2.2002619733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1260 loss: 3.3141 iter time (s): 4.283 samples/sec: 29.886
g0332:  iteration     1260/10000000 | consumed samples:       161280 | consumed tokens:    330301440 | elapsed time per iteration (ms): 4315.5 | learning rate: 2.200E-05 | global batch size:   128 | lm loss: 3.331811E+00 | loss scale: 8192.0 | grad norm: 3.071 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.660 | tokens per gpu per second (tgs): 1898.258 | TFLOPs: 15.28 |
g0314: [2024-08-02 19:51:52,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[2.2177382400000003e-05, 2.2177382400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1270 loss: 3.3203 iter time (s): 4.304 samples/sec: 29.739
g0332:  iteration     1270/10000000 | consumed samples:       162560 | consumed tokens:    332922880 | elapsed time per iteration (ms): 4337.6 | learning rate: 2.218E-05 | global batch size:   128 | lm loss: 3.331963E+00 | loss scale: 8192.0 | grad norm: 3.114 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.509 | tokens per gpu per second (tgs): 1888.581 | TFLOPs: 15.20 |
g0314: [2024-08-02 19:52:35,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[2.235214506666667e-05, 2.235214506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1280 loss: 3.2820 iter time (s): 4.219 samples/sec: 30.336
g0332:  iteration     1280/10000000 | consumed samples:       163840 | consumed tokens:    335544320 | elapsed time per iteration (ms): 4252.8 | learning rate: 2.235E-05 | global batch size:   128 | lm loss: 3.340331E+00 | loss scale: 8192.0 | grad norm: 3.655 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.098 | tokens per gpu per second (tgs): 1926.255 | TFLOPs: 15.50 |
g0314: [2024-08-02 19:53:18,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[2.2526907733333335e-05, 2.2526907733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1290 loss: 3.3014 iter time (s): 4.319 samples/sec: 29.638
g0332:  iteration     1290/10000000 | consumed samples:       165120 | consumed tokens:    338165760 | elapsed time per iteration (ms): 4352.0 | learning rate: 2.253E-05 | global batch size:   128 | lm loss: 3.320169E+00 | loss scale: 8192.0 | grad norm: 3.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.412 | tokens per gpu per second (tgs): 1882.360 | TFLOPs: 15.15 |
g0314: [2024-08-02 19:54:14,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[2.2701670400000004e-05, 2.2701670400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1300 loss: 3.2912 iter time (s): 5.587 samples/sec: 22.911
g0332:  iteration     1300/10000000 | consumed samples:       166400 | consumed tokens:    340787200 | elapsed time per iteration (ms): 5619.9 | learning rate: 2.270E-05 | global batch size:   128 | lm loss: 3.260928E+00 | loss scale: 8192.0 | grad norm: 3.107 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.776 | tokens per gpu per second (tgs): 1457.689 | TFLOPs: 11.73 |
g0314: [2024-08-02 19:55:10,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[2.287643306666667e-05, 2.287643306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1310 loss: 3.1543 iter time (s): 5.515 samples/sec: 23.209
g0332:  iteration     1310/10000000 | consumed samples:       167680 | consumed tokens:    343408640 | elapsed time per iteration (ms): 5548.2 | learning rate: 2.288E-05 | global batch size:   128 | lm loss: 3.257668E+00 | loss scale: 8192.0 | grad norm: 2.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.071 | tokens per gpu per second (tgs): 1476.521 | TFLOPs: 11.88 |
g0314: [2024-08-02 19:55:55,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[2.3051195733333336e-05, 2.3051195733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1320 loss: 3.2338 iter time (s): 4.477 samples/sec: 28.593
g0332:  iteration     1320/10000000 | consumed samples:       168960 | consumed tokens:    346030080 | elapsed time per iteration (ms): 4510.0 | learning rate: 2.305E-05 | global batch size:   128 | lm loss: 3.250303E+00 | loss scale: 8192.0 | grad norm: 3.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.381 | tokens per gpu per second (tgs): 1816.410 | TFLOPs: 14.62 |
g0314: [2024-08-02 19:56:39,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[2.3225958400000002e-05, 2.3225958400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1330 loss: 3.2609 iter time (s): 4.384 samples/sec: 29.197
g0332:  iteration     1330/10000000 | consumed samples:       170240 | consumed tokens:    348651520 | elapsed time per iteration (ms): 4416.9 | learning rate: 2.323E-05 | global batch size:   128 | lm loss: 3.223440E+00 | loss scale: 8192.0 | grad norm: 3.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.979 | tokens per gpu per second (tgs): 1854.681 | TFLOPs: 14.92 |
g0314: [2024-08-02 19:57:22,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[2.340072106666667e-05, 2.340072106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1340 loss: 3.1896 iter time (s): 4.286 samples/sec: 29.868
g0332:  iteration     1340/10000000 | consumed samples:       171520 | consumed tokens:    351272960 | elapsed time per iteration (ms): 4318.6 | learning rate: 2.340E-05 | global batch size:   128 | lm loss: 3.214311E+00 | loss scale: 8192.0 | grad norm: 2.705 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.639 | tokens per gpu per second (tgs): 1896.925 | TFLOPs: 15.26 |
g0314: [2024-08-02 19:58:05,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[2.3575483733333338e-05, 2.3575483733333338e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1350 loss: 3.1420 iter time (s): 4.199 samples/sec: 30.484
g0332:  iteration     1350/10000000 | consumed samples:       172800 | consumed tokens:    353894400 | elapsed time per iteration (ms): 4231.7 | learning rate: 2.358E-05 | global batch size:   128 | lm loss: 3.205202E+00 | loss scale: 8192.0 | grad norm: 3.052 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.248 | tokens per gpu per second (tgs): 1935.887 | TFLOPs: 15.58 |
g0314: [2024-08-02 19:58:47,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[2.3750246399999997e-05, 2.3750246399999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1360 loss: 3.1655 iter time (s): 4.175 samples/sec: 30.659
g0332:  iteration     1360/10000000 | consumed samples:       174080 | consumed tokens:    356515840 | elapsed time per iteration (ms): 4207.6 | learning rate: 2.375E-05 | global batch size:   128 | lm loss: 3.201781E+00 | loss scale: 8192.0 | grad norm: 3.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.421 | tokens per gpu per second (tgs): 1946.947 | TFLOPs: 15.67 |
g0314: [2024-08-02 19:59:32,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[2.3925009066666667e-05, 2.3925009066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1370 loss: 3.1177 iter time (s): 4.513 samples/sec: 28.360
g0332:  iteration     1370/10000000 | consumed samples:       175360 | consumed tokens:    359137280 | elapsed time per iteration (ms): 4546.5 | learning rate: 2.393E-05 | global batch size:   128 | lm loss: 3.158981E+00 | loss scale: 8192.0 | grad norm: 3.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.153 | tokens per gpu per second (tgs): 1801.809 | TFLOPs: 14.50 |
g0314: [2024-08-02 20:00:15,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[2.4099771733333333e-05, 2.4099771733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1380 loss: 3.1418 iter time (s): 4.254 samples/sec: 30.089
g0332:  iteration     1380/10000000 | consumed samples:       176640 | consumed tokens:    361758720 | elapsed time per iteration (ms): 4286.9 | learning rate: 2.410E-05 | global batch size:   128 | lm loss: 3.138541E+00 | loss scale: 8192.0 | grad norm: 3.520 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.858 | tokens per gpu per second (tgs): 1910.935 | TFLOPs: 15.38 |
g0314: [2024-08-02 20:00:58,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[2.42745344e-05, 2.42745344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1390 loss: 3.0880 iter time (s): 4.300 samples/sec: 29.768
g0332:  iteration     1390/10000000 | consumed samples:       177920 | consumed tokens:    364380160 | elapsed time per iteration (ms): 4332.7 | learning rate: 2.427E-05 | global batch size:   128 | lm loss: 3.124472E+00 | loss scale: 8192.0 | grad norm: 2.906 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.543 | tokens per gpu per second (tgs): 1890.741 | TFLOPs: 15.22 |
g0314: [2024-08-02 20:01:42,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[2.4449297066666665e-05, 2.4449297066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1400 loss: 2.9942 iter time (s): 4.282 samples/sec: 29.893
g0332:  iteration     1400/10000000 | consumed samples:       179200 | consumed tokens:    367001600 | elapsed time per iteration (ms): 4315.5 | learning rate: 2.445E-05 | global batch size:   128 | lm loss: 3.097729E+00 | loss scale: 8192.0 | grad norm: 3.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.660 | tokens per gpu per second (tgs): 1898.258 | TFLOPs: 15.28 |
g0314: [2024-08-02 20:02:25,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[2.4624059733333334e-05, 2.4624059733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1410 loss: 3.1597 iter time (s): 4.290 samples/sec: 29.838
g0332:  iteration     1410/10000000 | consumed samples:       180480 | consumed tokens:    369623040 | elapsed time per iteration (ms): 4323.0 | learning rate: 2.462E-05 | global batch size:   128 | lm loss: 3.114090E+00 | loss scale: 8192.0 | grad norm: 3.311 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.609 | tokens per gpu per second (tgs): 1894.962 | TFLOPs: 15.25 |
g0314: [2024-08-02 20:03:07,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[2.47988224e-05, 2.47988224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1420 loss: 3.0306 iter time (s): 4.237 samples/sec: 30.213
g0332:  iteration     1420/10000000 | consumed samples:       181760 | consumed tokens:    372244480 | elapsed time per iteration (ms): 4269.3 | learning rate: 2.480E-05 | global batch size:   128 | lm loss: 3.084654E+00 | loss scale: 8192.0 | grad norm: 3.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.982 | tokens per gpu per second (tgs): 1918.826 | TFLOPs: 15.44 |
g0314: [2024-08-02 20:03:51,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[2.4973585066666666e-05, 2.4973585066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1430 loss: 3.0700 iter time (s): 4.332 samples/sec: 29.545
g0332:  iteration     1430/10000000 | consumed samples:       183040 | consumed tokens:    374865920 | elapsed time per iteration (ms): 4365.0 | learning rate: 2.497E-05 | global batch size:   128 | lm loss: 3.059539E+00 | loss scale: 8192.0 | grad norm: 2.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.324 | tokens per gpu per second (tgs): 1876.752 | TFLOPs: 15.10 |
g0314: [2024-08-02 20:04:34,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[2.5148347733333333e-05, 2.5148347733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1440 loss: 3.0031 iter time (s): 4.223 samples/sec: 30.311
g0332:  iteration     1440/10000000 | consumed samples:       184320 | consumed tokens:    377487360 | elapsed time per iteration (ms): 4255.6 | learning rate: 2.515E-05 | global batch size:   128 | lm loss: 3.029934E+00 | loss scale: 8192.0 | grad norm: 2.811 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.078 | tokens per gpu per second (tgs): 1924.979 | TFLOPs: 15.49 |
g0314: [2024-08-02 20:05:17,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[2.53231104e-05, 2.53231104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1450 loss: 3.1092 iter time (s): 4.334 samples/sec: 29.531
g0332:  iteration     1450/10000000 | consumed samples:       185600 | consumed tokens:    380108800 | elapsed time per iteration (ms): 4367.4 | learning rate: 2.532E-05 | global batch size:   128 | lm loss: 3.044913E+00 | loss scale: 8192.0 | grad norm: 2.467 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.308 | tokens per gpu per second (tgs): 1875.702 | TFLOPs: 15.09 |
g0314: [2024-08-02 20:06:01,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[2.5497873066666668e-05, 2.5497873066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1460 loss: 3.0290 iter time (s): 4.356 samples/sec: 29.383
g0332:  iteration     1460/10000000 | consumed samples:       186880 | consumed tokens:    382730240 | elapsed time per iteration (ms): 4388.9 | learning rate: 2.550E-05 | global batch size:   128 | lm loss: 2.993217E+00 | loss scale: 8192.0 | grad norm: 2.757 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.164 | tokens per gpu per second (tgs): 1866.507 | TFLOPs: 15.02 |
g0314: [2024-08-02 20:06:46,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[2.5672635733333334e-05, 2.5672635733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1470 loss: 2.9647 iter time (s): 4.394 samples/sec: 29.131
g0332:  iteration     1470/10000000 | consumed samples:       188160 | consumed tokens:    385351680 | elapsed time per iteration (ms): 4426.9 | learning rate: 2.567E-05 | global batch size:   128 | lm loss: 2.974779E+00 | loss scale: 8192.0 | grad norm: 2.928 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.914 | tokens per gpu per second (tgs): 1850.508 | TFLOPs: 14.89 |
g0314: [2024-08-02 20:07:29,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[2.58473984e-05, 2.58473984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1480 loss: 2.9952 iter time (s): 4.270 samples/sec: 29.975
g0332:  iteration     1480/10000000 | consumed samples:       189440 | consumed tokens:    387973120 | elapsed time per iteration (ms): 4302.9 | learning rate: 2.585E-05 | global batch size:   128 | lm loss: 2.973335E+00 | loss scale: 8192.0 | grad norm: 2.880 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.747 | tokens per gpu per second (tgs): 1903.836 | TFLOPs: 15.32 |
g0314: [2024-08-02 20:08:12,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[2.6022161066666666e-05, 2.6022161066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1490 loss: 2.9350 iter time (s): 4.269 samples/sec: 29.986
g0332:  iteration     1490/10000000 | consumed samples:       190720 | consumed tokens:    390594560 | elapsed time per iteration (ms): 4301.5 | learning rate: 2.602E-05 | global batch size:   128 | lm loss: 2.957045E+00 | loss scale: 8192.0 | grad norm: 2.416 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.757 | tokens per gpu per second (tgs): 1904.442 | TFLOPs: 15.33 |
g0314: [2024-08-02 20:08:55,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[2.6196923733333336e-05, 2.6196923733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1500 loss: 2.8878 iter time (s): 4.270 samples/sec: 29.975
g0332:  iteration     1500/10000000 | consumed samples:       192000 | consumed tokens:    393216000 | elapsed time per iteration (ms): 4302.9 | learning rate: 2.620E-05 | global batch size:   128 | lm loss: 2.897499E+00 | loss scale: 8192.0 | grad norm: 2.532 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.747 | tokens per gpu per second (tgs): 1903.835 | TFLOPs: 15.32 |
g0314: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0314: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0314: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0314: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0318: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0332: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0319: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0320: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0320: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0332: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0320: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0332: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0320: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0316: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0320: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0316: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0319: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0325: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0325: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0325: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0319: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0316: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0329: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0316: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0325: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0329: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0319: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0329: [2024-08-02 20:08:59,438] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0329: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0329: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0329: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0318: [2024-08-02 20:08:59,439] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:08:59,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0314: [2024-08-02 20:09:37,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[2.6371686400000002e-05, 2.6371686400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1510 loss: 2.9875 iter time (s): 4.187 samples/sec: 30.574
g0332:  iteration     1510/10000000 | consumed samples:       193280 | consumed tokens:    395837440 | elapsed time per iteration (ms): 4219.6 | learning rate: 2.637E-05 | global batch size:   128 | lm loss: 2.914820E+00 | loss scale: 16384.0 | grad norm: 3.037 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.334 | tokens per gpu per second (tgs): 1941.397 | TFLOPs: 15.62 |
g0314: [2024-08-02 20:10:19,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[2.6546449066666668e-05, 2.6546449066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1520 loss: 2.9244 iter time (s): 4.238 samples/sec: 30.206
g0332:  iteration     1520/10000000 | consumed samples:       194560 | consumed tokens:    398458880 | elapsed time per iteration (ms): 4270.6 | learning rate: 2.655E-05 | global batch size:   128 | lm loss: 2.904781E+00 | loss scale: 16384.0 | grad norm: 2.573 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.973 | tokens per gpu per second (tgs): 1918.254 | TFLOPs: 15.44 |
g0314: [2024-08-02 20:11:02,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[2.6721211733333334e-05, 2.6721211733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1530 loss: 2.8602 iter time (s): 4.189 samples/sec: 30.556
g0332:  iteration     1530/10000000 | consumed samples:       195840 | consumed tokens:    401080320 | elapsed time per iteration (ms): 4222.1 | learning rate: 2.672E-05 | global batch size:   128 | lm loss: 2.871200E+00 | loss scale: 16384.0 | grad norm: 2.713 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.317 | tokens per gpu per second (tgs): 1940.277 | TFLOPs: 15.61 |
g0314: [2024-08-02 20:11:44,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[2.68959744e-05, 2.68959744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1540 loss: 2.8501 iter time (s): 4.222 samples/sec: 30.321
g0332:  iteration     1540/10000000 | consumed samples:       197120 | consumed tokens:    403701760 | elapsed time per iteration (ms): 4254.8 | learning rate: 2.690E-05 | global batch size:   128 | lm loss: 2.864759E+00 | loss scale: 16384.0 | grad norm: 2.485 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.083 | tokens per gpu per second (tgs): 1925.337 | TFLOPs: 15.49 |
g0314: [2024-08-02 20:12:29,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[2.707073706666667e-05, 2.707073706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1550 loss: 2.8368 iter time (s): 4.398 samples/sec: 29.107
g0332:  iteration     1550/10000000 | consumed samples:       198400 | consumed tokens:    406323200 | elapsed time per iteration (ms): 4430.3 | learning rate: 2.707E-05 | global batch size:   128 | lm loss: 2.836252E+00 | loss scale: 16384.0 | grad norm: 2.392 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.892 | tokens per gpu per second (tgs): 1849.087 | TFLOPs: 14.88 |
g0314: [2024-08-02 20:13:12,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[2.7245499733333335e-05, 2.7245499733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1560 loss: 2.8238 iter time (s): 4.324 samples/sec: 29.600
g0332:  iteration     1560/10000000 | consumed samples:       199680 | consumed tokens:    408944640 | elapsed time per iteration (ms): 4357.4 | learning rate: 2.725E-05 | global batch size:   128 | lm loss: 2.839464E+00 | loss scale: 16384.0 | grad norm: 2.430 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.375 | tokens per gpu per second (tgs): 1880.023 | TFLOPs: 15.13 |
g0314: [2024-08-02 20:13:55,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[2.74202624e-05, 2.74202624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1570 loss: 2.8064 iter time (s): 4.211 samples/sec: 30.393
g0332:  iteration     1570/10000000 | consumed samples:       200960 | consumed tokens:    411566080 | elapsed time per iteration (ms): 4244.2 | learning rate: 2.742E-05 | global batch size:   128 | lm loss: 2.833227E+00 | loss scale: 16384.0 | grad norm: 2.458 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.158 | tokens per gpu per second (tgs): 1930.143 | TFLOPs: 15.53 |
g0314: [2024-08-02 20:14:38,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[2.7595025066666668e-05, 2.7595025066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1580 loss: 2.8354 iter time (s): 4.274 samples/sec: 29.950
g0332:  iteration     1580/10000000 | consumed samples:       202240 | consumed tokens:    414187520 | elapsed time per iteration (ms): 4306.8 | learning rate: 2.760E-05 | global batch size:   128 | lm loss: 2.783225E+00 | loss scale: 16384.0 | grad norm: 2.365 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.720 | tokens per gpu per second (tgs): 1902.096 | TFLOPs: 15.31 |
g0314: [2024-08-02 20:15:22,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[2.7769787733333337e-05, 2.7769787733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1590 loss: 2.7998 iter time (s): 4.368 samples/sec: 29.306
g0332:  iteration     1590/10000000 | consumed samples:       203520 | consumed tokens:    416808960 | elapsed time per iteration (ms): 4417.1 | learning rate: 2.777E-05 | global batch size:   128 | lm loss: 2.805785E+00 | loss scale: 16384.0 | grad norm: 2.511 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.978 | tokens per gpu per second (tgs): 1854.592 | TFLOPs: 14.92 |
g0314: [2024-08-02 20:16:05,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[2.7944550400000003e-05, 2.7944550400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1600 loss: 2.8134 iter time (s): 4.311 samples/sec: 29.692
g0332:  iteration     1600/10000000 | consumed samples:       204800 | consumed tokens:    419430400 | elapsed time per iteration (ms): 4344.6 | learning rate: 2.794E-05 | global batch size:   128 | lm loss: 2.787720E+00 | loss scale: 16384.0 | grad norm: 2.678 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.462 | tokens per gpu per second (tgs): 1885.579 | TFLOPs: 15.17 |
g0314: [2024-08-02 20:16:49,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[2.811931306666667e-05, 2.811931306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1610 loss: 2.8319 iter time (s): 4.369 samples/sec: 29.300
g0332:  iteration     1610/10000000 | consumed samples:       206080 | consumed tokens:    422051840 | elapsed time per iteration (ms): 4401.6 | learning rate: 2.812E-05 | global batch size:   128 | lm loss: 2.783374E+00 | loss scale: 16384.0 | grad norm: 2.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.080 | tokens per gpu per second (tgs): 1861.126 | TFLOPs: 14.98 |
g0314: [2024-08-02 20:17:34,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[2.8294075733333335e-05, 2.8294075733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1620 loss: 2.7212 iter time (s): 4.464 samples/sec: 28.673
g0332:  iteration     1620/10000000 | consumed samples:       207360 | consumed tokens:    424673280 | elapsed time per iteration (ms): 4497.1 | learning rate: 2.829E-05 | global batch size:   128 | lm loss: 2.765904E+00 | loss scale: 16384.0 | grad norm: 2.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.463 | tokens per gpu per second (tgs): 1821.606 | TFLOPs: 14.66 |
g0314: [2024-08-02 20:18:17,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[2.84688384e-05, 2.84688384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1630 loss: 2.6113 iter time (s): 4.289 samples/sec: 29.842
g0332:  iteration     1630/10000000 | consumed samples:       208640 | consumed tokens:    427294720 | elapsed time per iteration (ms): 4322.6 | learning rate: 2.847E-05 | global batch size:   128 | lm loss: 2.725472E+00 | loss scale: 16384.0 | grad norm: 2.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.612 | tokens per gpu per second (tgs): 1895.165 | TFLOPs: 15.25 |
g0314: [2024-08-02 20:19:01,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[2.864360106666667e-05, 2.864360106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1640 loss: 2.7035 iter time (s): 4.366 samples/sec: 29.321
g0332:  iteration     1640/10000000 | consumed samples:       209920 | consumed tokens:    429916160 | elapsed time per iteration (ms): 4398.7 | learning rate: 2.864E-05 | global batch size:   128 | lm loss: 2.736027E+00 | loss scale: 16384.0 | grad norm: 2.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.100 | tokens per gpu per second (tgs): 1862.374 | TFLOPs: 14.99 |
g0314: [2024-08-02 20:19:45,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[2.8818363733333337e-05, 2.8818363733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1650 loss: 2.6968 iter time (s): 4.320 samples/sec: 29.629
g0332:  iteration     1650/10000000 | consumed samples:       211200 | consumed tokens:    432537600 | elapsed time per iteration (ms): 4353.1 | learning rate: 2.882E-05 | global batch size:   128 | lm loss: 2.712293E+00 | loss scale: 16384.0 | grad norm: 2.315 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.405 | tokens per gpu per second (tgs): 1881.899 | TFLOPs: 15.14 |
g0314: [2024-08-02 20:20:28,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[2.8993126400000003e-05, 2.8993126400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1660 loss: 2.7483 iter time (s): 4.276 samples/sec: 29.936
g0332:  iteration     1660/10000000 | consumed samples:       212480 | consumed tokens:    435159040 | elapsed time per iteration (ms): 4309.0 | learning rate: 2.899E-05 | global batch size:   128 | lm loss: 2.709540E+00 | loss scale: 16384.0 | grad norm: 2.339 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.705 | tokens per gpu per second (tgs): 1901.121 | TFLOPs: 15.30 |
g0314: [2024-08-02 20:21:11,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[2.916788906666667e-05, 2.916788906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1670 loss: 2.6385 iter time (s): 4.281 samples/sec: 29.899
g0332:  iteration     1670/10000000 | consumed samples:       213760 | consumed tokens:    437780480 | elapsed time per iteration (ms): 4314.1 | learning rate: 2.917E-05 | global batch size:   128 | lm loss: 2.719473E+00 | loss scale: 16384.0 | grad norm: 2.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.670 | tokens per gpu per second (tgs): 1898.871 | TFLOPs: 15.28 |
g0314: [2024-08-02 20:21:56,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[2.934265173333334e-05, 2.934265173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1680 loss: 2.6815 iter time (s): 4.405 samples/sec: 29.058
g0332:  iteration     1680/10000000 | consumed samples:       215040 | consumed tokens:    440401920 | elapsed time per iteration (ms): 4438.0 | learning rate: 2.934E-05 | global batch size:   128 | lm loss: 2.695625E+00 | loss scale: 16384.0 | grad norm: 1.990 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.842 | tokens per gpu per second (tgs): 1845.886 | TFLOPs: 14.85 |
g0314: [2024-08-02 20:22:39,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[2.9517414399999998e-05, 2.9517414399999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1690 loss: 2.7652 iter time (s): 4.313 samples/sec: 29.679
g0332:  iteration     1690/10000000 | consumed samples:       216320 | consumed tokens:    443023360 | elapsed time per iteration (ms): 4345.6 | learning rate: 2.952E-05 | global batch size:   128 | lm loss: 2.674123E+00 | loss scale: 16384.0 | grad norm: 2.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.455 | tokens per gpu per second (tgs): 1885.116 | TFLOPs: 15.17 |
g0314: [2024-08-02 20:23:24,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[2.9692177066666667e-05, 2.9692177066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1700 loss: 2.7052 iter time (s): 4.425 samples/sec: 28.925
g0332:  iteration     1700/10000000 | consumed samples:       217600 | consumed tokens:    445644800 | elapsed time per iteration (ms): 4459.2 | learning rate: 2.969E-05 | global batch size:   128 | lm loss: 2.663325E+00 | loss scale: 16384.0 | grad norm: 1.891 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.704 | tokens per gpu per second (tgs): 1837.087 | TFLOPs: 14.78 |
g0314: [2024-08-02 20:24:10,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[2.9866939733333333e-05, 2.9866939733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1710 loss: 2.6951 iter time (s): 4.606 samples/sec: 27.787
g0332:  iteration     1710/10000000 | consumed samples:       218880 | consumed tokens:    448266240 | elapsed time per iteration (ms): 4639.1 | learning rate: 2.987E-05 | global batch size:   128 | lm loss: 2.651667E+00 | loss scale: 16384.0 | grad norm: 2.109 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.592 | tokens per gpu per second (tgs): 1765.863 | TFLOPs: 14.21 |
g0314: [2024-08-02 20:24:55,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[3.00417024e-05, 3.00417024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1720 loss: 2.6810 iter time (s): 4.418 samples/sec: 28.973
g0332:  iteration     1720/10000000 | consumed samples:       220160 | consumed tokens:    450887680 | elapsed time per iteration (ms): 4450.5 | learning rate: 3.004E-05 | global batch size:   128 | lm loss: 2.609503E+00 | loss scale: 16384.0 | grad norm: 2.049 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.761 | tokens per gpu per second (tgs): 1840.705 | TFLOPs: 14.81 |
g0314: [2024-08-02 20:25:38,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[3.0216465066666665e-05, 3.0216465066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1730 loss: 2.6523 iter time (s): 4.345 samples/sec: 29.459
g0332:  iteration     1730/10000000 | consumed samples:       221440 | consumed tokens:    453509120 | elapsed time per iteration (ms): 4377.8 | learning rate: 3.022E-05 | global batch size:   128 | lm loss: 2.637162E+00 | loss scale: 16384.0 | grad norm: 2.145 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.238 | tokens per gpu per second (tgs): 1871.243 | TFLOPs: 15.06 |
g0314: [2024-08-02 20:26:25,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[3.0391227733333335e-05, 3.0391227733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1740 loss: 2.6182 iter time (s): 4.642 samples/sec: 27.572
g0332:  iteration     1740/10000000 | consumed samples:       222720 | consumed tokens:    456130560 | elapsed time per iteration (ms): 4675.2 | learning rate: 3.039E-05 | global batch size:   128 | lm loss: 2.626531E+00 | loss scale: 16384.0 | grad norm: 2.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.379 | tokens per gpu per second (tgs): 1752.231 | TFLOPs: 14.10 |
g0314: [2024-08-02 20:27:09,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[3.05659904e-05, 3.05659904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1750 loss: 2.6295 iter time (s): 4.312 samples/sec: 29.683
g0332:  iteration     1750/10000000 | consumed samples:       224000 | consumed tokens:    458752000 | elapsed time per iteration (ms): 4344.6 | learning rate: 3.057E-05 | global batch size:   128 | lm loss: 2.620992E+00 | loss scale: 16384.0 | grad norm: 2.078 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.462 | tokens per gpu per second (tgs): 1885.554 | TFLOPs: 15.17 |
g0314: [2024-08-02 20:27:52,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[3.0740753066666664e-05, 3.0740753066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1760 loss: 2.7039 iter time (s): 4.317 samples/sec: 29.651
g0332:  iteration     1760/10000000 | consumed samples:       225280 | consumed tokens:    461373440 | elapsed time per iteration (ms): 4349.3 | learning rate: 3.074E-05 | global batch size:   128 | lm loss: 2.631429E+00 | loss scale: 16384.0 | grad norm: 1.982 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.430 | tokens per gpu per second (tgs): 1883.501 | TFLOPs: 15.16 |
g0314: [2024-08-02 20:28:34,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[3.0915515733333336e-05, 3.0915515733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1770 loss: 2.6432 iter time (s): 4.185 samples/sec: 30.582
g0332:  iteration     1770/10000000 | consumed samples:       226560 | consumed tokens:    463994880 | elapsed time per iteration (ms): 4218.0 | learning rate: 3.092E-05 | global batch size:   128 | lm loss: 2.617581E+00 | loss scale: 16384.0 | grad norm: 1.937 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.160 | TFLOPs: 15.63 |
g0314: [2024-08-02 20:29:19,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[3.10902784e-05, 3.10902784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1780 loss: 2.5940 iter time (s): 4.447 samples/sec: 28.786
g0332:  iteration     1780/10000000 | consumed samples:       227840 | consumed tokens:    466616320 | elapsed time per iteration (ms): 4479.3 | learning rate: 3.109E-05 | global batch size:   128 | lm loss: 2.589323E+00 | loss scale: 16384.0 | grad norm: 1.988 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.576 | tokens per gpu per second (tgs): 1828.874 | TFLOPs: 14.72 |
g0314: [2024-08-02 20:30:06,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[3.126504106666667e-05, 3.126504106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1790 loss: 2.5323 iter time (s): 4.639 samples/sec: 27.592
g0332:  iteration     1790/10000000 | consumed samples:       229120 | consumed tokens:    469237760 | elapsed time per iteration (ms): 4671.5 | learning rate: 3.127E-05 | global batch size:   128 | lm loss: 2.573963E+00 | loss scale: 16384.0 | grad norm: 1.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.400 | tokens per gpu per second (tgs): 1753.608 | TFLOPs: 14.11 |
g0314: [2024-08-02 20:30:49,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[3.1439803733333335e-05, 3.1439803733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1800 loss: 2.5911 iter time (s): 4.301 samples/sec: 29.757
g0332:  iteration     1800/10000000 | consumed samples:       230400 | consumed tokens:    471859200 | elapsed time per iteration (ms): 4335.8 | learning rate: 3.144E-05 | global batch size:   128 | lm loss: 2.587166E+00 | loss scale: 16384.0 | grad norm: 1.900 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.522 | tokens per gpu per second (tgs): 1889.386 | TFLOPs: 15.20 |
g0314: [2024-08-02 20:32:04,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[3.16145664e-05, 3.16145664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1810 loss: 2.4837 iter time (s): 7.424 samples/sec: 17.242
g0332:  iteration     1810/10000000 | consumed samples:       231680 | consumed tokens:    474480640 | elapsed time per iteration (ms): 7456.2 | learning rate: 3.161E-05 | global batch size:   128 | lm loss: 2.553102E+00 | loss scale: 16384.0 | grad norm: 1.713 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.167 | tokens per gpu per second (tgs): 1098.681 | TFLOPs: 8.84 |
g0314: [2024-08-02 20:33:02,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[3.178932906666667e-05, 3.178932906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1820 loss: 2.5928 iter time (s): 5.829 samples/sec: 21.961
g0332:  iteration     1820/10000000 | consumed samples:       232960 | consumed tokens:    477102080 | elapsed time per iteration (ms): 5861.8 | learning rate: 3.179E-05 | global batch size:   128 | lm loss: 2.534260E+00 | loss scale: 16384.0 | grad norm: 2.070 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.836 | tokens per gpu per second (tgs): 1397.530 | TFLOPs: 11.25 |
g0314: [2024-08-02 20:33:58,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[3.196409173333333e-05, 3.196409173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1830 loss: 2.6215 iter time (s): 5.549 samples/sec: 23.067
g0332:  iteration     1830/10000000 | consumed samples:       234240 | consumed tokens:    479723520 | elapsed time per iteration (ms): 5582.3 | learning rate: 3.196E-05 | global batch size:   128 | lm loss: 2.556586E+00 | loss scale: 16384.0 | grad norm: 1.799 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.930 | tokens per gpu per second (tgs): 1467.506 | TFLOPs: 11.81 |
g0314: [2024-08-02 20:34:46,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[3.21388544e-05, 3.21388544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1840 loss: 2.4760 iter time (s): 4.743 samples/sec: 26.989
g0332:  iteration     1840/10000000 | consumed samples:       235520 | consumed tokens:    482344960 | elapsed time per iteration (ms): 4775.2 | learning rate: 3.214E-05 | global batch size:   128 | lm loss: 2.532099E+00 | loss scale: 16384.0 | grad norm: 1.837 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.805 | tokens per gpu per second (tgs): 1715.547 | TFLOPs: 13.81 |
g0314: [2024-08-02 20:35:28,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[3.2313617066666665e-05, 3.2313617066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1850 loss: 2.5579 iter time (s): 4.197 samples/sec: 30.501
g0332:  iteration     1850/10000000 | consumed samples:       236800 | consumed tokens:    484966400 | elapsed time per iteration (ms): 4228.9 | learning rate: 3.231E-05 | global batch size:   128 | lm loss: 2.541831E+00 | loss scale: 16384.0 | grad norm: 1.824 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.268 | tokens per gpu per second (tgs): 1937.150 | TFLOPs: 15.59 |
g0314: [2024-08-02 20:36:10,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[3.248837973333334e-05, 3.248837973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1860 loss: 2.5198 iter time (s): 4.163 samples/sec: 30.747
g0332:  iteration     1860/10000000 | consumed samples:       238080 | consumed tokens:    487587840 | elapsed time per iteration (ms): 4195.6 | learning rate: 3.249E-05 | global batch size:   128 | lm loss: 2.524572E+00 | loss scale: 16384.0 | grad norm: 3.113 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.508 | tokens per gpu per second (tgs): 1952.540 | TFLOPs: 15.71 |
g0314: [2024-08-02 20:36:51,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[3.2663142400000004e-05, 3.2663142400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1870 loss: 2.5065 iter time (s): 4.051 samples/sec: 31.594
g0332:  iteration     1870/10000000 | consumed samples:       239360 | consumed tokens:    490209280 | elapsed time per iteration (ms): 4083.9 | learning rate: 3.266E-05 | global batch size:   128 | lm loss: 2.516846E+00 | loss scale: 16384.0 | grad norm: 1.973 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.342 | tokens per gpu per second (tgs): 2005.908 | TFLOPs: 16.14 |
g0314: [2024-08-02 20:37:33,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[3.283790506666667e-05, 3.283790506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1880 loss: 2.4969 iter time (s): 4.191 samples/sec: 30.541
g0332:  iteration     1880/10000000 | consumed samples:       240640 | consumed tokens:    492830720 | elapsed time per iteration (ms): 4224.3 | learning rate: 3.284E-05 | global batch size:   128 | lm loss: 2.487350E+00 | loss scale: 16384.0 | grad norm: 2.125 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.301 | tokens per gpu per second (tgs): 1939.272 | TFLOPs: 15.61 |
g0314: [2024-08-02 20:38:17,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[3.3012667733333336e-05, 3.3012667733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1890 loss: 2.4675 iter time (s): 4.327 samples/sec: 29.585
g0332:  iteration     1890/10000000 | consumed samples:       241920 | consumed tokens:    495452160 | elapsed time per iteration (ms): 4359.4 | learning rate: 3.301E-05 | global batch size:   128 | lm loss: 2.513824E+00 | loss scale: 16384.0 | grad norm: 1.827 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.362 | tokens per gpu per second (tgs): 1879.159 | TFLOPs: 15.12 |
g0314: [2024-08-02 20:38:59,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[3.31874304e-05, 3.31874304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1900 loss: 2.5410 iter time (s): 4.144 samples/sec: 30.889
g0332:  iteration     1900/10000000 | consumed samples:       243200 | consumed tokens:    498073600 | elapsed time per iteration (ms): 4176.6 | learning rate: 3.319E-05 | global batch size:   128 | lm loss: 2.485360E+00 | loss scale: 16384.0 | grad norm: 1.950 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.647 | tokens per gpu per second (tgs): 1961.418 | TFLOPs: 15.78 |
g0314: [2024-08-02 20:39:41,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[3.336219306666667e-05, 3.336219306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1910 loss: 2.4232 iter time (s): 4.210 samples/sec: 30.407
g0332:  iteration     1910/10000000 | consumed samples:       244480 | consumed tokens:    500695040 | elapsed time per iteration (ms): 4242.4 | learning rate: 3.336E-05 | global batch size:   128 | lm loss: 2.458541E+00 | loss scale: 16384.0 | grad norm: 1.805 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.172 | tokens per gpu per second (tgs): 1931.004 | TFLOPs: 15.54 |
g0314: [2024-08-02 20:40:23,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[3.3536955733333334e-05, 3.3536955733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1920 loss: 2.4917 iter time (s): 4.155 samples/sec: 30.803
g0332:  iteration     1920/10000000 | consumed samples:       245760 | consumed tokens:    503316480 | elapsed time per iteration (ms): 4187.9 | learning rate: 3.354E-05 | global batch size:   128 | lm loss: 2.497836E+00 | loss scale: 16384.0 | grad norm: 1.747 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.564 | tokens per gpu per second (tgs): 1956.095 | TFLOPs: 15.74 |
g0314: [2024-08-02 20:41:06,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[3.37117184e-05, 3.37117184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1930 loss: 2.4975 iter time (s): 4.309 samples/sec: 29.707
g0332:  iteration     1930/10000000 | consumed samples:       247040 | consumed tokens:    505937920 | elapsed time per iteration (ms): 4342.4 | learning rate: 3.371E-05 | global batch size:   128 | lm loss: 2.475711E+00 | loss scale: 16384.0 | grad norm: 1.795 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.477 | tokens per gpu per second (tgs): 1886.512 | TFLOPs: 15.18 |
g0314: [2024-08-02 20:41:49,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[3.388648106666667e-05, 3.388648106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1940 loss: 2.4204 iter time (s): 4.248 samples/sec: 30.133
g0332:  iteration     1940/10000000 | consumed samples:       248320 | consumed tokens:    508559360 | elapsed time per iteration (ms): 4281.0 | learning rate: 3.389E-05 | global batch size:   128 | lm loss: 2.434915E+00 | loss scale: 16384.0 | grad norm: 1.755 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.899 | tokens per gpu per second (tgs): 1913.556 | TFLOPs: 15.40 |
g0314: [2024-08-02 20:42:39,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[3.406124373333334e-05, 3.406124373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1950 loss: 2.4320 iter time (s): 4.983 samples/sec: 25.687
g0332:  iteration     1950/10000000 | consumed samples:       249600 | consumed tokens:    511180800 | elapsed time per iteration (ms): 5017.9 | learning rate: 3.406E-05 | global batch size:   128 | lm loss: 2.454921E+00 | loss scale: 16384.0 | grad norm: 1.731 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.509 | tokens per gpu per second (tgs): 1632.547 | TFLOPs: 13.14 |
g0314: [2024-08-02 20:43:32,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[3.4236006400000005e-05, 3.4236006400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1960 loss: 2.5185 iter time (s): 5.202 samples/sec: 24.606
g0332:  iteration     1960/10000000 | consumed samples:       250880 | consumed tokens:    513802240 | elapsed time per iteration (ms): 5234.6 | learning rate: 3.424E-05 | global batch size:   128 | lm loss: 2.491201E+00 | loss scale: 16384.0 | grad norm: 1.555 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.453 | tokens per gpu per second (tgs): 1564.964 | TFLOPs: 12.59 |
g0314: [2024-08-02 20:44:14,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[3.441076906666667e-05, 3.441076906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1970 loss: 2.4961 iter time (s): 4.187 samples/sec: 30.571
g0332:  iteration     1970/10000000 | consumed samples:       252160 | consumed tokens:    516423680 | elapsed time per iteration (ms): 4219.5 | learning rate: 3.441E-05 | global batch size:   128 | lm loss: 2.465312E+00 | loss scale: 16384.0 | grad norm: 1.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.335 | tokens per gpu per second (tgs): 1941.455 | TFLOPs: 15.62 |
g0314: [2024-08-02 20:44:58,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[3.458553173333334e-05, 3.458553173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1980 loss: 2.5618 iter time (s): 4.433 samples/sec: 28.876
g0332:  iteration     1980/10000000 | consumed samples:       253440 | consumed tokens:    519045120 | elapsed time per iteration (ms): 4465.7 | learning rate: 3.459E-05 | global batch size:   128 | lm loss: 2.448289E+00 | loss scale: 16384.0 | grad norm: 1.853 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.663 | tokens per gpu per second (tgs): 1834.422 | TFLOPs: 14.76 |
g0314: [2024-08-02 20:45:41,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=0, lr=[3.47602944e-05, 3.47602944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 1990 loss: 2.4100 iter time (s): 4.237 samples/sec: 30.210
g0332:  iteration     1990/10000000 | consumed samples:       254720 | consumed tokens:    521666560 | elapsed time per iteration (ms): 4269.6 | learning rate: 3.476E-05 | global batch size:   128 | lm loss: 2.414602E+00 | loss scale: 16384.0 | grad norm: 1.755 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.979 | tokens per gpu per second (tgs): 1918.666 | TFLOPs: 15.44 |
g0314: [2024-08-02 20:46:24,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=0, lr=[3.493505706666667e-05, 3.493505706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2000 loss: 2.4242 iter time (s): 4.220 samples/sec: 30.333
g0332:  iteration     2000/10000000 | consumed samples:       256000 | consumed tokens:    524288000 | elapsed time per iteration (ms): 4252.5 | learning rate: 3.494E-05 | global batch size:   128 | lm loss: 2.440763E+00 | loss scale: 16384.0 | grad norm: 1.685 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.100 | tokens per gpu per second (tgs): 1926.381 | TFLOPs: 15.50 |
g0332: ------------------------------------------------------------------------------------------------
g0332:  validation loss at iteration 2000 | lm loss value: 2.422184E+00 | lm loss PPL: 1.127045E+01 | 
g0332: ------------------------------------------------------------------------------------------------
g0314: saving checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: [2024-08-02 20:52:55,553] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
g0332: [2024-08-02 20:52:55,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0332: [2024-08-02 20:52:55,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0314: [2024-08-02 20:52:55,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0314: [2024-08-02 20:52:55,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0314: [2024-08-02 20:52:55,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0332: [2024-08-02 20:52:55,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0325: [2024-08-02 20:52:55,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0325: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0319: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0319: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0319: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0325: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0316: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0316: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0329: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0329: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0329: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0320: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0320: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0320: [2024-08-02 20:52:55,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0316: [2024-08-02 20:52:55,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0318: [2024-08-02 20:52:55,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0318: [2024-08-02 20:52:55,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0318: [2024-08-02 20:52:55,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0332: [2024-08-02 20:52:55,589] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt...
g0325: [2024-08-02 20:52:55,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt...
g0316: [2024-08-02 20:52:55,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt...
g0319: [2024-08-02 20:52:55,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt...
g0329: [2024-08-02 20:52:55,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt...
g0320: [2024-08-02 20:52:55,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt...
g0318: [2024-08-02 20:52:55,603] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt...
g0314: [2024-08-02 20:52:55,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt...
g0319: [2024-08-02 20:52:55,719] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt.
g0329: [2024-08-02 20:52:55,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt.
g0318: [2024-08-02 20:52:55,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt.
g0314: [2024-08-02 20:52:55,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt.
g0319: [2024-08-02 20:52:55,759] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt...
g0316: [2024-08-02 20:52:55,768] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt.
g0329: [2024-08-02 20:52:55,770] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt...
g0332: [2024-08-02 20:52:55,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt.
g0332: [2024-08-02 20:52:55,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt...
g0332: [2024-08-02 20:52:55,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt.
g0318: [2024-08-02 20:52:55,779] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt...
g0314: [2024-08-02 20:52:55,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt...
g0316: [2024-08-02 20:52:55,803] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt...
g0325: [2024-08-02 20:52:55,807] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt.
g0332: [2024-08-02 20:52:55,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt...
g0325: [2024-08-02 20:52:55,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt...
g0320: [2024-08-02 20:52:55,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt.
g0329: [2024-08-02 20:52:55,892] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt.
g0319: [2024-08-02 20:52:55,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt.
g0320: [2024-08-02 20:52:55,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt...
g0314: [2024-08-02 20:52:55,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt.
g0318: [2024-08-02 20:52:55,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt.
g0329: [2024-08-02 20:52:55,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt...
g0319: [2024-08-02 20:52:55,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt...
g0314: [2024-08-02 20:52:55,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt...
g0316: [2024-08-02 20:52:55,950] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt.
g0318: [2024-08-02 20:52:55,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt...
g0325: [2024-08-02 20:52:55,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt.
g0316: [2024-08-02 20:52:55,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt...
g0325: [2024-08-02 20:52:55,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt...
g0319: [2024-08-02 20:52:56,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt.
g0319: [2024-08-02 20:52:56,032] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt...
g0332: [2024-08-02 20:52:56,060] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt.
g0332: [2024-08-02 20:52:56,061] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt...
g0314: [2024-08-02 20:52:56,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt.
g0320: [2024-08-02 20:52:56,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt.
g0314: [2024-08-02 20:52:56,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt...
g0320: [2024-08-02 20:52:56,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt...
g0329: [2024-08-02 20:52:56,122] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt.
g0329: [2024-08-02 20:52:56,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt...
g0325: [2024-08-02 20:52:56,156] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt.
g0325: [2024-08-02 20:52:56,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt...
g0316: [2024-08-02 20:52:56,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt.
g0316: [2024-08-02 20:52:56,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt...
g0314: [2024-08-02 20:52:56,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt.
g0314: [2024-08-02 20:52:56,212] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt
g0314: [2024-08-02 20:52:56,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt...
g0318: [2024-08-02 20:52:56,227] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt.
g0318: [2024-08-02 20:52:56,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt...
g0320: [2024-08-02 20:52:56,259] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt.
g0320: [2024-08-02 20:52:56,261] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt...
g0332: [2024-08-02 20:52:58,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt.
g0332: [2024-08-02 20:52:58,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0319: [2024-08-02 20:52:58,364] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt.
g0319: [2024-08-02 20:52:58,365] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0325: [2024-08-02 20:52:58,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt.
g0325: [2024-08-02 20:52:58,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0318: [2024-08-02 20:52:58,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt.
g0318: [2024-08-02 20:52:58,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0320: [2024-08-02 20:52:58,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt.
g0320: [2024-08-02 20:52:58,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0329: [2024-08-02 20:52:58,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt.
g0329: [2024-08-02 20:52:58,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0316: [2024-08-02 20:52:59,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt.
g0316: [2024-08-02 20:52:59,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0314: [2024-08-02 20:52:59,793] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt.
g0314: [2024-08-02 20:52:59,794] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0314:   successfully saved checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: Checkpoint Save GB: 22.521, GB/Sec: 5.28, Latency(second): 4.261
g0332: (min, max) time across ranks (ms):
g0332:     save-checkpoint ................................: (4260.43, 4261.41)
g0314: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0314: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0325: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0325: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0325: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0320: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0320: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0314: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0316: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0320: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0316: [2024-08-02 20:53:03,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0314: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0316: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0316: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0318: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0332: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0314: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0325: [2024-08-02 20:53:03,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0314: [2024-08-02 20:53:42,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=0, lr=[3.5109819733333335e-05, 3.5109819733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2010 loss: 2.4657 iter time (s): 4.194 samples/sec: 30.523
g0332:  iteration     2010/10000000 | consumed samples:       257280 | consumed tokens:    526909440 | elapsed time per iteration (ms): 43790.4 | learning rate: 3.511E-05 | global batch size:   128 | lm loss: 2.406544E+00 | loss scale: 32768.0 | grad norm: 1.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.923 | tokens per gpu per second (tgs): 187.073 | TFLOPs: 1.51 |
g0314: [2024-08-02 20:54:24,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=0, lr=[3.52845824e-05, 3.52845824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2020 loss: 2.4854 iter time (s): 4.205 samples/sec: 30.440
g0332:  iteration     2020/10000000 | consumed samples:       258560 | consumed tokens:    529530880 | elapsed time per iteration (ms): 4237.3 | learning rate: 3.528E-05 | global batch size:   128 | lm loss: 2.437971E+00 | loss scale: 32768.0 | grad norm: 1.544 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.208 | tokens per gpu per second (tgs): 1933.312 | TFLOPs: 15.56 |
g0314: [2024-08-02 20:55:07,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=0, lr=[3.545934506666667e-05, 3.545934506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2030 loss: 2.3526 iter time (s): 4.260 samples/sec: 30.046
g0332:  iteration     2030/10000000 | consumed samples:       259840 | consumed tokens:    532152320 | elapsed time per iteration (ms): 4292.4 | learning rate: 3.546E-05 | global batch size:   128 | lm loss: 2.410839E+00 | loss scale: 32768.0 | grad norm: 1.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.820 | tokens per gpu per second (tgs): 1908.510 | TFLOPs: 15.36 |
g0314: [2024-08-02 20:55:50,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=0, lr=[3.5634107733333334e-05, 3.5634107733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2040 loss: 2.3863 iter time (s): 4.310 samples/sec: 29.697
g0332:  iteration     2040/10000000 | consumed samples:       261120 | consumed tokens:    534773760 | elapsed time per iteration (ms): 4342.9 | learning rate: 3.563E-05 | global batch size:   128 | lm loss: 2.388358E+00 | loss scale: 32768.0 | grad norm: 1.702 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.473 | tokens per gpu per second (tgs): 1886.289 | TFLOPs: 15.18 |
g0314: [2024-08-02 20:56:32,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=0, lr=[3.58088704e-05, 3.58088704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2050 loss: 2.3288 iter time (s): 4.174 samples/sec: 30.665
g0332:  iteration     2050/10000000 | consumed samples:       262400 | consumed tokens:    537395200 | elapsed time per iteration (ms): 4206.7 | learning rate: 3.581E-05 | global batch size:   128 | lm loss: 2.376134E+00 | loss scale: 32768.0 | grad norm: 1.536 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.428 | tokens per gpu per second (tgs): 1947.372 | TFLOPs: 15.67 |
g0314: [2024-08-02 20:57:15,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=0, lr=[3.5983633066666666e-05, 3.5983633066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2060 loss: 2.3800 iter time (s): 4.184 samples/sec: 30.594
g0332:  iteration     2060/10000000 | consumed samples:       263680 | consumed tokens:    540016640 | elapsed time per iteration (ms): 4216.3 | learning rate: 3.598E-05 | global batch size:   128 | lm loss: 2.423174E+00 | loss scale: 32768.0 | grad norm: 1.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.358 | tokens per gpu per second (tgs): 1942.933 | TFLOPs: 15.64 |
g0314: [2024-08-02 20:57:57,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=0, lr=[3.615839573333333e-05, 3.615839573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2070 loss: 2.4608 iter time (s): 4.230 samples/sec: 30.259
g0332:  iteration     2070/10000000 | consumed samples:       264960 | consumed tokens:    542638080 | elapsed time per iteration (ms): 4262.5 | learning rate: 3.616E-05 | global batch size:   128 | lm loss: 2.377795E+00 | loss scale: 32768.0 | grad norm: 1.673 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.030 | tokens per gpu per second (tgs): 1921.894 | TFLOPs: 15.47 |
g0314: [2024-08-02 20:58:39,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=0, lr=[3.63331584e-05, 3.63331584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2080 loss: 2.4514 iter time (s): 4.161 samples/sec: 30.765
g0332:  iteration     2080/10000000 | consumed samples:       266240 | consumed tokens:    545259520 | elapsed time per iteration (ms): 4192.9 | learning rate: 3.633E-05 | global batch size:   128 | lm loss: 2.374408E+00 | loss scale: 32768.0 | grad norm: 1.547 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.763 | TFLOPs: 15.72 |
g0314: [2024-08-02 20:59:30,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=0, lr=[3.6507921066666664e-05, 3.6507921066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2090 loss: 2.3590 iter time (s): 5.055 samples/sec: 25.322
g0332:  iteration     2090/10000000 | consumed samples:       267520 | consumed tokens:    547880960 | elapsed time per iteration (ms): 5087.4 | learning rate: 3.651E-05 | global batch size:   128 | lm loss: 2.406558E+00 | loss scale: 32768.0 | grad norm: 1.597 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.160 | tokens per gpu per second (tgs): 1610.264 | TFLOPs: 12.96 |
g0314: [2024-08-02 21:00:12,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=0, lr=[3.668268373333334e-05, 3.668268373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2100 loss: 2.3523 iter time (s): 4.189 samples/sec: 30.553
g0332:  iteration     2100/10000000 | consumed samples:       268800 | consumed tokens:    550502400 | elapsed time per iteration (ms): 4222.0 | learning rate: 3.668E-05 | global batch size:   128 | lm loss: 2.343262E+00 | loss scale: 32768.0 | grad norm: 1.478 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.317 | tokens per gpu per second (tgs): 1940.308 | TFLOPs: 15.61 |
g0314: [2024-08-02 21:00:55,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=0, lr=[3.68574464e-05, 3.68574464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2110 loss: 2.2476 iter time (s): 4.211 samples/sec: 30.394
g0332:  iteration     2110/10000000 | consumed samples:       270080 | consumed tokens:    553123840 | elapsed time per iteration (ms): 4244.0 | learning rate: 3.686E-05 | global batch size:   128 | lm loss: 2.401334E+00 | loss scale: 32768.0 | grad norm: 1.649 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.160 | tokens per gpu per second (tgs): 1930.237 | TFLOPs: 15.53 |
g0314: [2024-08-02 21:01:37,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=0, lr=[3.703220906666667e-05, 3.703220906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2120 loss: 2.3712 iter time (s): 4.227 samples/sec: 30.280
g0332:  iteration     2120/10000000 | consumed samples:       271360 | consumed tokens:    555745280 | elapsed time per iteration (ms): 4259.2 | learning rate: 3.703E-05 | global batch size:   128 | lm loss: 2.373350E+00 | loss scale: 32768.0 | grad norm: 1.582 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.053 | tokens per gpu per second (tgs): 1923.376 | TFLOPs: 15.48 |
g0314: [2024-08-02 21:02:22,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=0, lr=[3.7206971733333335e-05, 3.7206971733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2130 loss: 2.3492 iter time (s): 4.477 samples/sec: 28.590
g0332:  iteration     2130/10000000 | consumed samples:       272640 | consumed tokens:    558366720 | elapsed time per iteration (ms): 4509.7 | learning rate: 3.721E-05 | global batch size:   128 | lm loss: 2.336829E+00 | loss scale: 32768.0 | grad norm: 1.462 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.384 | tokens per gpu per second (tgs): 1816.546 | TFLOPs: 14.62 |
g0314: [2024-08-02 21:03:06,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=0, lr=[3.73817344e-05, 3.73817344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2140 loss: 2.3607 iter time (s): 4.303 samples/sec: 29.745
g0332:  iteration     2140/10000000 | consumed samples:       273920 | consumed tokens:    560988160 | elapsed time per iteration (ms): 4336.3 | learning rate: 3.738E-05 | global batch size:   128 | lm loss: 2.351299E+00 | loss scale: 32768.0 | grad norm: 1.464 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.518 | tokens per gpu per second (tgs): 1889.167 | TFLOPs: 15.20 |
g0314: [2024-08-02 21:03:48,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=0, lr=[3.755649706666667e-05, 3.755649706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2150 loss: 2.3142 iter time (s): 4.211 samples/sec: 30.396
g0332:  iteration     2150/10000000 | consumed samples:       275200 | consumed tokens:    563609600 | elapsed time per iteration (ms): 4243.7 | learning rate: 3.756E-05 | global batch size:   128 | lm loss: 2.318231E+00 | loss scale: 32768.0 | grad norm: 1.395 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.162 | tokens per gpu per second (tgs): 1930.383 | TFLOPs: 15.53 |
g0314: [2024-08-02 21:04:30,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=0, lr=[3.773125973333333e-05, 3.773125973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2160 loss: 2.3421 iter time (s): 4.125 samples/sec: 31.032
g0332:  iteration     2160/10000000 | consumed samples:       276480 | consumed tokens:    566231040 | elapsed time per iteration (ms): 4157.2 | learning rate: 3.773E-05 | global batch size:   128 | lm loss: 2.354682E+00 | loss scale: 32768.0 | grad norm: 1.502 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.790 | tokens per gpu per second (tgs): 1970.569 | TFLOPs: 15.86 |
g0314: [2024-08-02 21:05:13,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=0, lr=[3.79060224e-05, 3.79060224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2170 loss: 2.3769 iter time (s): 4.274 samples/sec: 29.950
g0332:  iteration     2170/10000000 | consumed samples:       277760 | consumed tokens:    568852480 | elapsed time per iteration (ms): 4306.3 | learning rate: 3.791E-05 | global batch size:   128 | lm loss: 2.366478E+00 | loss scale: 32768.0 | grad norm: 1.626 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.724 | tokens per gpu per second (tgs): 1902.345 | TFLOPs: 15.31 |
g0314: [2024-08-02 21:05:55,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=0, lr=[3.8080785066666665e-05, 3.8080785066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2180 loss: 2.3494 iter time (s): 4.220 samples/sec: 30.333
g0332:  iteration     2180/10000000 | consumed samples:       279040 | consumed tokens:    571473920 | elapsed time per iteration (ms): 4252.2 | learning rate: 3.808E-05 | global batch size:   128 | lm loss: 2.333475E+00 | loss scale: 32768.0 | grad norm: 1.519 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.102 | tokens per gpu per second (tgs): 1926.520 | TFLOPs: 15.50 |
g0314: [2024-08-02 21:06:54,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=0, lr=[3.825554773333334e-05, 3.825554773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2190 loss: 2.3668 iter time (s): 5.881 samples/sec: 21.765
g0332:  iteration     2190/10000000 | consumed samples:       280320 | consumed tokens:    574095360 | elapsed time per iteration (ms): 5913.8 | learning rate: 3.826E-05 | global batch size:   128 | lm loss: 2.333529E+00 | loss scale: 32768.0 | grad norm: 1.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.644 | tokens per gpu per second (tgs): 1385.243 | TFLOPs: 11.15 |
g0314: [2024-08-02 21:08:04,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=0, lr=[3.8430310400000004e-05, 3.8430310400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2200 loss: 2.3567 iter time (s): 6.958 samples/sec: 18.395
g0332:  iteration     2200/10000000 | consumed samples:       281600 | consumed tokens:    576716800 | elapsed time per iteration (ms): 6991.1 | learning rate: 3.843E-05 | global batch size:   128 | lm loss: 2.326013E+00 | loss scale: 32768.0 | grad norm: 1.593 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.309 | tokens per gpu per second (tgs): 1171.767 | TFLOPs: 9.43 |
g0314: [2024-08-02 21:09:13,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=0, lr=[3.860507306666667e-05, 3.860507306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2210 loss: 2.3693 iter time (s): 6.795 samples/sec: 18.838
g0332:  iteration     2210/10000000 | consumed samples:       282880 | consumed tokens:    579338240 | elapsed time per iteration (ms): 6827.2 | learning rate: 3.861E-05 | global batch size:   128 | lm loss: 2.325296E+00 | loss scale: 32768.0 | grad norm: 1.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.749 | tokens per gpu per second (tgs): 1199.908 | TFLOPs: 9.66 |
g0314: [2024-08-02 21:10:11,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=0, lr=[3.8779835733333336e-05, 3.8779835733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2220 loss: 2.3796 iter time (s): 5.850 samples/sec: 21.882
g0332:  iteration     2220/10000000 | consumed samples:       284160 | consumed tokens:    581959680 | elapsed time per iteration (ms): 5882.0 | learning rate: 3.878E-05 | global batch size:   128 | lm loss: 2.319613E+00 | loss scale: 32768.0 | grad norm: 1.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.761 | tokens per gpu per second (tgs): 1392.716 | TFLOPs: 11.21 |
g0314: [2024-08-02 21:11:04,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=0, lr=[3.89545984e-05, 3.89545984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2230 loss: 2.2480 iter time (s): 5.275 samples/sec: 24.267
g0332:  iteration     2230/10000000 | consumed samples:       285440 | consumed tokens:    584581120 | elapsed time per iteration (ms): 5308.6 | learning rate: 3.895E-05 | global batch size:   128 | lm loss: 2.312516E+00 | loss scale: 32768.0 | grad norm: 1.300 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.112 | tokens per gpu per second (tgs): 1543.164 | TFLOPs: 12.42 |
g0314: [2024-08-02 21:12:03,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=0, lr=[3.912936106666667e-05, 3.912936106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2240 loss: 2.3333 iter time (s): 5.869 samples/sec: 21.808
g0332:  iteration     2240/10000000 | consumed samples:       286720 | consumed tokens:    587202560 | elapsed time per iteration (ms): 5902.9 | learning rate: 3.913E-05 | global batch size:   128 | lm loss: 2.320097E+00 | loss scale: 32768.0 | grad norm: 1.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.684 | tokens per gpu per second (tgs): 1387.794 | TFLOPs: 11.17 |
g0314: [2024-08-02 21:13:01,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=0, lr=[3.9304123733333334e-05, 3.9304123733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2250 loss: 2.2386 iter time (s): 5.719 samples/sec: 22.381
g0332:  iteration     2250/10000000 | consumed samples:       288000 | consumed tokens:    589824000 | elapsed time per iteration (ms): 5752.5 | learning rate: 3.930E-05 | global batch size:   128 | lm loss: 2.284986E+00 | loss scale: 32768.0 | grad norm: 1.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.251 | tokens per gpu per second (tgs): 1424.073 | TFLOPs: 11.46 |
g0314: [2024-08-02 21:13:58,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=0, lr=[3.94788864e-05, 3.94788864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2260 loss: 2.3404 iter time (s): 5.653 samples/sec: 22.641
g0332:  iteration     2260/10000000 | consumed samples:       289280 | consumed tokens:    592445440 | elapsed time per iteration (ms): 5686.6 | learning rate: 3.948E-05 | global batch size:   128 | lm loss: 2.325445E+00 | loss scale: 32768.0 | grad norm: 1.508 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.509 | tokens per gpu per second (tgs): 1440.588 | TFLOPs: 11.59 |
g0314: [2024-08-02 21:14:55,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=0, lr=[3.965364906666667e-05, 3.965364906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2270 loss: 2.3108 iter time (s): 5.674 samples/sec: 22.560
g0332:  iteration     2270/10000000 | consumed samples:       290560 | consumed tokens:    595066880 | elapsed time per iteration (ms): 5706.8 | learning rate: 3.965E-05 | global batch size:   128 | lm loss: 2.308460E+00 | loss scale: 32768.0 | grad norm: 1.381 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.429 | tokens per gpu per second (tgs): 1435.472 | TFLOPs: 11.55 |
g0314: [2024-08-02 21:15:47,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=0, lr=[3.982841173333334e-05, 3.982841173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2280 loss: 2.2378 iter time (s): 5.194 samples/sec: 24.645
g0332:  iteration     2280/10000000 | consumed samples:       291840 | consumed tokens:    597688320 | elapsed time per iteration (ms): 5227.4 | learning rate: 3.983E-05 | global batch size:   128 | lm loss: 2.280502E+00 | loss scale: 32768.0 | grad norm: 1.370 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.486 | tokens per gpu per second (tgs): 1567.136 | TFLOPs: 12.61 |
g0314: [2024-08-02 21:16:35,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=0, lr=[4.0003174400000006e-05, 4.0003174400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2290 loss: 2.1962 iter time (s): 4.777 samples/sec: 26.793
g0332:  iteration     2290/10000000 | consumed samples:       293120 | consumed tokens:    600309760 | elapsed time per iteration (ms): 4810.1 | learning rate: 4.000E-05 | global batch size:   128 | lm loss: 2.282490E+00 | loss scale: 32768.0 | grad norm: 1.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.611 | tokens per gpu per second (tgs): 1703.094 | TFLOPs: 13.71 |
g0314: [2024-08-02 21:17:21,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=0, lr=[4.017793706666667e-05, 4.017793706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2300 loss: 2.2686 iter time (s): 4.502 samples/sec: 28.432
g0332:  iteration     2300/10000000 | consumed samples:       294400 | consumed tokens:    602931200 | elapsed time per iteration (ms): 4535.0 | learning rate: 4.018E-05 | global batch size:   128 | lm loss: 2.300115E+00 | loss scale: 32768.0 | grad norm: 1.320 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.225 | tokens per gpu per second (tgs): 1806.407 | TFLOPs: 14.54 |
g0314: [2024-08-02 21:18:05,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=0, lr=[4.035269973333334e-05, 4.035269973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2310 loss: 2.3854 iter time (s): 4.437 samples/sec: 28.848
g0332:  iteration     2310/10000000 | consumed samples:       295680 | consumed tokens:    605552640 | elapsed time per iteration (ms): 4469.6 | learning rate: 4.035E-05 | global batch size:   128 | lm loss: 2.285337E+00 | loss scale: 32768.0 | grad norm: 1.445 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.638 | tokens per gpu per second (tgs): 1832.813 | TFLOPs: 14.75 |
g0314: [2024-08-02 21:18:49,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=0, lr=[4.0527462400000004e-05, 4.0527462400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2320 loss: 2.2978 iter time (s): 4.321 samples/sec: 29.620
g0332:  iteration     2320/10000000 | consumed samples:       296960 | consumed tokens:    608174080 | elapsed time per iteration (ms): 4354.4 | learning rate: 4.053E-05 | global batch size:   128 | lm loss: 2.297940E+00 | loss scale: 32768.0 | grad norm: 1.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.396 | tokens per gpu per second (tgs): 1881.322 | TFLOPs: 15.14 |
g0314: [2024-08-02 21:19:31,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=0, lr=[4.070222506666667e-05, 4.070222506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2330 loss: 2.3025 iter time (s): 4.194 samples/sec: 30.523
g0332:  iteration     2330/10000000 | consumed samples:       298240 | consumed tokens:    610795520 | elapsed time per iteration (ms): 4227.0 | learning rate: 4.070E-05 | global batch size:   128 | lm loss: 2.286872E+00 | loss scale: 32768.0 | grad norm: 1.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.281 | tokens per gpu per second (tgs): 1938.010 | TFLOPs: 15.60 |
g0314: [2024-08-02 21:20:13,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=0, lr=[4.0876987733333336e-05, 4.0876987733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2340 loss: 2.3439 iter time (s): 4.152 samples/sec: 30.832
g0332:  iteration     2340/10000000 | consumed samples:       299520 | consumed tokens:    613416960 | elapsed time per iteration (ms): 4192.7 | learning rate: 4.088E-05 | global batch size:   128 | lm loss: 2.269148E+00 | loss scale: 32768.0 | grad norm: 1.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.529 | tokens per gpu per second (tgs): 1953.876 | TFLOPs: 15.72 |
g0314: [2024-08-02 21:20:55,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=0, lr=[4.10517504e-05, 4.10517504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2350 loss: 2.2888 iter time (s): 4.154 samples/sec: 30.816
g0332:  iteration     2350/10000000 | consumed samples:       300800 | consumed tokens:    616038400 | elapsed time per iteration (ms): 4189.0 | learning rate: 4.105E-05 | global batch size:   128 | lm loss: 2.265141E+00 | loss scale: 32768.0 | grad norm: 1.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.556 | tokens per gpu per second (tgs): 1955.595 | TFLOPs: 15.74 |
g0314: [2024-08-02 21:21:38,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=0, lr=[4.122651306666667e-05, 4.122651306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2360 loss: 2.1766 iter time (s): 4.276 samples/sec: 29.934
g0332:  iteration     2360/10000000 | consumed samples:       302080 | consumed tokens:    618659840 | elapsed time per iteration (ms): 4308.7 | learning rate: 4.123E-05 | global batch size:   128 | lm loss: 2.271318E+00 | loss scale: 32768.0 | grad norm: 1.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.707 | tokens per gpu per second (tgs): 1901.272 | TFLOPs: 15.30 |
g0314: [2024-08-02 21:22:20,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=0, lr=[4.1401275733333334e-05, 4.1401275733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2370 loss: 2.2452 iter time (s): 4.160 samples/sec: 30.772
g0332:  iteration     2370/10000000 | consumed samples:       303360 | consumed tokens:    621281280 | elapsed time per iteration (ms): 4192.3 | learning rate: 4.140E-05 | global batch size:   128 | lm loss: 2.238829E+00 | loss scale: 32768.0 | grad norm: 1.307 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.532 | tokens per gpu per second (tgs): 1954.076 | TFLOPs: 15.72 |
g0314: [2024-08-02 21:23:03,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=0, lr=[4.15760384e-05, 4.15760384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2380 loss: 2.2715 iter time (s): 4.281 samples/sec: 29.897
g0332:  iteration     2380/10000000 | consumed samples:       304640 | consumed tokens:    623902720 | elapsed time per iteration (ms): 4314.6 | learning rate: 4.158E-05 | global batch size:   128 | lm loss: 2.280522E+00 | loss scale: 32768.0 | grad norm: 1.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.666 | tokens per gpu per second (tgs): 1898.650 | TFLOPs: 15.28 |
g0314: [2024-08-02 21:23:45,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=0, lr=[4.1750801066666666e-05, 4.1750801066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2390 loss: 2.2489 iter time (s): 4.176 samples/sec: 30.652
g0332:  iteration     2390/10000000 | consumed samples:       305920 | consumed tokens:    626524160 | elapsed time per iteration (ms): 4208.6 | learning rate: 4.175E-05 | global batch size:   128 | lm loss: 2.230859E+00 | loss scale: 32768.0 | grad norm: 1.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.414 | tokens per gpu per second (tgs): 1946.481 | TFLOPs: 15.66 |
g0314: [2024-08-02 21:24:25,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=0, lr=[4.192556373333333e-05, 4.192556373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2400 loss: 2.2259 iter time (s): 3.990 samples/sec: 32.083
g0332:  iteration     2400/10000000 | consumed samples:       307200 | consumed tokens:    629145600 | elapsed time per iteration (ms): 4022.4 | learning rate: 4.193E-05 | global batch size:   128 | lm loss: 2.247684E+00 | loss scale: 32768.0 | grad norm: 1.381 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.822 | tokens per gpu per second (tgs): 2036.602 | TFLOPs: 16.39 |
g0314: [2024-08-02 21:25:08,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=0, lr=[4.21003264e-05, 4.21003264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2410 loss: 2.2905 iter time (s): 4.233 samples/sec: 30.239
g0332:  iteration     2410/10000000 | consumed samples:       308480 | consumed tokens:    631767040 | elapsed time per iteration (ms): 4265.8 | learning rate: 4.210E-05 | global batch size:   128 | lm loss: 2.260547E+00 | loss scale: 32768.0 | grad norm: 1.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.006 | tokens per gpu per second (tgs): 1920.403 | TFLOPs: 15.45 |
g0314: [2024-08-02 21:25:50,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=0, lr=[4.2275089066666664e-05, 4.2275089066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2420 loss: 2.1769 iter time (s): 4.187 samples/sec: 30.573
g0332:  iteration     2420/10000000 | consumed samples:       309760 | consumed tokens:    634388480 | elapsed time per iteration (ms): 4218.9 | learning rate: 4.228E-05 | global batch size:   128 | lm loss: 2.246869E+00 | loss scale: 32768.0 | grad norm: 1.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.340 | tokens per gpu per second (tgs): 1941.752 | TFLOPs: 15.63 |
g0314: [2024-08-02 21:26:33,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=0, lr=[4.244985173333334e-05, 4.244985173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2430 loss: 2.2335 iter time (s): 4.191 samples/sec: 30.543
g0332:  iteration     2430/10000000 | consumed samples:       311040 | consumed tokens:    637009920 | elapsed time per iteration (ms): 4223.3 | learning rate: 4.245E-05 | global batch size:   128 | lm loss: 2.240289E+00 | loss scale: 32768.0 | grad norm: 1.343 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.308 | tokens per gpu per second (tgs): 1939.731 | TFLOPs: 15.61 |
g0314: [2024-08-02 21:27:15,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=0, lr=[4.26246144e-05, 4.26246144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2440 loss: 2.1883 iter time (s): 4.171 samples/sec: 30.686
g0332:  iteration     2440/10000000 | consumed samples:       312320 | consumed tokens:    639631360 | elapsed time per iteration (ms): 4203.6 | learning rate: 4.262E-05 | global batch size:   128 | lm loss: 2.233652E+00 | loss scale: 32768.0 | grad norm: 1.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.450 | tokens per gpu per second (tgs): 1948.810 | TFLOPs: 15.68 |
g0314: [2024-08-02 21:27:57,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=2450, skipped=0, lr=[4.279937706666667e-05, 4.279937706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2450 loss: 2.2684 iter time (s): 4.167 samples/sec: 30.720
g0332:  iteration     2450/10000000 | consumed samples:       313600 | consumed tokens:    642252800 | elapsed time per iteration (ms): 4199.7 | learning rate: 4.280E-05 | global batch size:   128 | lm loss: 2.237291E+00 | loss scale: 32768.0 | grad norm: 1.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.479 | tokens per gpu per second (tgs): 1950.634 | TFLOPs: 15.70 |
g0314: [2024-08-02 21:28:38,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=2460, skipped=0, lr=[4.2974139733333335e-05, 4.2974139733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2460 loss: 2.3023 iter time (s): 4.073 samples/sec: 31.423
g0332:  iteration     2460/10000000 | consumed samples:       314880 | consumed tokens:    644874240 | elapsed time per iteration (ms): 4105.8 | learning rate: 4.297E-05 | global batch size:   128 | lm loss: 2.233000E+00 | loss scale: 32768.0 | grad norm: 1.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.175 | tokens per gpu per second (tgs): 1995.213 | TFLOPs: 16.06 |
g0314: [2024-08-02 21:29:21,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=2470, skipped=0, lr=[4.31489024e-05, 4.31489024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2470 loss: 2.2066 iter time (s): 4.323 samples/sec: 29.610
g0332:  iteration     2470/10000000 | consumed samples:       316160 | consumed tokens:    647495680 | elapsed time per iteration (ms): 4356.1 | learning rate: 4.315E-05 | global batch size:   128 | lm loss: 2.257111E+00 | loss scale: 32768.0 | grad norm: 1.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.384 | tokens per gpu per second (tgs): 1880.602 | TFLOPs: 15.13 |
g0314: [2024-08-02 21:30:03,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=2480, skipped=0, lr=[4.332366506666667e-05, 4.332366506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2480 loss: 2.2333 iter time (s): 4.173 samples/sec: 30.672
g0332:  iteration     2480/10000000 | consumed samples:       317440 | consumed tokens:    650117120 | elapsed time per iteration (ms): 4206.4 | learning rate: 4.332E-05 | global batch size:   128 | lm loss: 2.236939E+00 | loss scale: 32768.0 | grad norm: 1.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.430 | tokens per gpu per second (tgs): 1947.530 | TFLOPs: 15.67 |
g0314: [2024-08-02 21:30:46,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=2490, skipped=0, lr=[4.3498427733333334e-05, 4.3498427733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2490 loss: 2.2442 iter time (s): 4.198 samples/sec: 30.493
g0332:  iteration     2490/10000000 | consumed samples:       318720 | consumed tokens:    652738560 | elapsed time per iteration (ms): 4230.6 | learning rate: 4.350E-05 | global batch size:   128 | lm loss: 2.226147E+00 | loss scale: 32768.0 | grad norm: 1.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.256 | tokens per gpu per second (tgs): 1936.369 | TFLOPs: 15.58 |
g0314: [2024-08-02 21:31:27,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=2500, skipped=0, lr=[4.36731904e-05, 4.36731904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2500 loss: 2.2007 iter time (s): 4.143 samples/sec: 30.893
g0332:  iteration     2500/10000000 | consumed samples:       320000 | consumed tokens:    655360000 | elapsed time per iteration (ms): 4176.3 | learning rate: 4.367E-05 | global batch size:   128 | lm loss: 2.202971E+00 | loss scale: 32768.0 | grad norm: 1.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.649 | tokens per gpu per second (tgs): 1961.551 | TFLOPs: 15.78 |
g0314: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0314: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0329: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0329: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0325: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0332: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0318: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0318: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0325: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0319: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0332: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0325: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0318: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0332: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0320: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0319: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0320: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0319: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0319: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0316: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0316: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0316: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0320: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0329: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0314: [2024-08-02 21:31:32,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0314: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0316: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0318: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0325: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0319: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0329: [2024-08-02 21:31:32,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0332: [2024-08-02 21:31:32,066] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0314: [2024-08-02 21:32:10,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=2510, skipped=0, lr=[4.3847953066666666e-05, 4.3847953066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2510 loss: 2.1382 iter time (s): 4.192 samples/sec: 30.538
g0332:  iteration     2510/10000000 | consumed samples:       321280 | consumed tokens:    657981440 | elapsed time per iteration (ms): 4224.8 | learning rate: 4.385E-05 | global batch size:   128 | lm loss: 2.206709E+00 | loss scale: 65536.0 | grad norm: 1.151 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.297 | tokens per gpu per second (tgs): 1939.017 | TFLOPs: 15.60 |
g0314: [2024-08-02 21:32:52,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=2520, skipped=0, lr=[4.402271573333334e-05, 4.402271573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2520 loss: 2.1812 iter time (s): 4.188 samples/sec: 30.561
g0332:  iteration     2520/10000000 | consumed samples:       322560 | consumed tokens:    660602880 | elapsed time per iteration (ms): 4221.0 | learning rate: 4.402E-05 | global batch size:   128 | lm loss: 2.207362E+00 | loss scale: 65536.0 | grad norm: 1.124 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.325 | tokens per gpu per second (tgs): 1940.784 | TFLOPs: 15.62 |
g0314: [2024-08-02 21:33:34,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=2530, skipped=0, lr=[4.4197478400000005e-05, 4.4197478400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2530 loss: 2.1513 iter time (s): 4.197 samples/sec: 30.499
g0332:  iteration     2530/10000000 | consumed samples:       323840 | consumed tokens:    663224320 | elapsed time per iteration (ms): 4230.2 | learning rate: 4.420E-05 | global batch size:   128 | lm loss: 2.213332E+00 | loss scale: 65536.0 | grad norm: 1.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.259 | tokens per gpu per second (tgs): 1936.571 | TFLOPs: 15.58 |
g0314: [2024-08-02 21:34:16,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=2540, skipped=0, lr=[4.437224106666667e-05, 4.437224106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2540 loss: 2.1630 iter time (s): 4.187 samples/sec: 30.574
g0332:  iteration     2540/10000000 | consumed samples:       325120 | consumed tokens:    665845760 | elapsed time per iteration (ms): 4219.2 | learning rate: 4.437E-05 | global batch size:   128 | lm loss: 2.194306E+00 | loss scale: 65536.0 | grad norm: 1.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.338 | tokens per gpu per second (tgs): 1941.618 | TFLOPs: 15.62 |
g0314: [2024-08-02 21:34:59,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=2550, skipped=0, lr=[4.454700373333334e-05, 4.454700373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2550 loss: 2.1596 iter time (s): 4.271 samples/sec: 29.969
g0332:  iteration     2550/10000000 | consumed samples:       326400 | consumed tokens:    668467200 | elapsed time per iteration (ms): 4303.7 | learning rate: 4.455E-05 | global batch size:   128 | lm loss: 2.195237E+00 | loss scale: 65536.0 | grad norm: 1.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.742 | tokens per gpu per second (tgs): 1903.488 | TFLOPs: 15.32 |
g0314: [2024-08-02 21:35:41,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=2560, skipped=0, lr=[4.47217664e-05, 4.47217664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2560 loss: 2.2089 iter time (s): 4.159 samples/sec: 30.777
g0332:  iteration     2560/10000000 | consumed samples:       327680 | consumed tokens:    671088640 | elapsed time per iteration (ms): 4191.4 | learning rate: 4.472E-05 | global batch size:   128 | lm loss: 2.203737E+00 | loss scale: 65536.0 | grad norm: 1.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.539 | tokens per gpu per second (tgs): 1954.478 | TFLOPs: 15.73 |
g0314: [2024-08-02 21:36:40,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=2570, skipped=0, lr=[4.489652906666667e-05, 4.489652906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2570 loss: 2.1595 iter time (s): 5.856 samples/sec: 21.858
g0332:  iteration     2570/10000000 | consumed samples:       328960 | consumed tokens:    673710080 | elapsed time per iteration (ms): 5891.2 | learning rate: 4.490E-05 | global batch size:   128 | lm loss: 2.179098E+00 | loss scale: 65536.0 | grad norm: 1.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.727 | tokens per gpu per second (tgs): 1390.551 | TFLOPs: 11.19 |
g0314: [2024-08-02 21:37:27,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=2580, skipped=0, lr=[4.5071291733333335e-05, 4.5071291733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2580 loss: 2.1876 iter time (s): 4.656 samples/sec: 27.490
g0332:  iteration     2580/10000000 | consumed samples:       330240 | consumed tokens:    676331520 | elapsed time per iteration (ms): 4688.8 | learning rate: 4.507E-05 | global batch size:   128 | lm loss: 2.212740E+00 | loss scale: 65536.0 | grad norm: 1.142 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.299 | tokens per gpu per second (tgs): 1747.126 | TFLOPs: 14.06 |
g0314: [2024-08-02 21:38:09,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=2590, skipped=0, lr=[4.52460544e-05, 4.52460544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2590 loss: 2.1178 iter time (s): 4.205 samples/sec: 30.440
g0332:  iteration     2590/10000000 | consumed samples:       331520 | consumed tokens:    678952960 | elapsed time per iteration (ms): 4237.7 | learning rate: 4.525E-05 | global batch size:   128 | lm loss: 2.174532E+00 | loss scale: 65536.0 | grad norm: 1.147 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.205 | tokens per gpu per second (tgs): 1933.124 | TFLOPs: 15.56 |
g0314: [2024-08-02 21:38:51,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=2600, skipped=0, lr=[4.542081706666667e-05, 4.542081706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2600 loss: 2.1896 iter time (s): 4.147 samples/sec: 30.869
g0332:  iteration     2600/10000000 | consumed samples:       332800 | consumed tokens:    681574400 | elapsed time per iteration (ms): 4179.1 | learning rate: 4.542E-05 | global batch size:   128 | lm loss: 2.158924E+00 | loss scale: 65536.0 | grad norm: 1.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.629 | tokens per gpu per second (tgs): 1960.237 | TFLOPs: 15.77 |
g0314: [2024-08-02 21:39:34,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=2610, skipped=0, lr=[4.559557973333334e-05, 4.559557973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2610 loss: 2.1700 iter time (s): 4.250 samples/sec: 30.119
g0332:  iteration     2610/10000000 | consumed samples:       334080 | consumed tokens:    684195840 | elapsed time per iteration (ms): 4282.7 | learning rate: 4.560E-05 | global batch size:   128 | lm loss: 2.191371E+00 | loss scale: 65536.0 | grad norm: 1.126 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.888 | tokens per gpu per second (tgs): 1912.828 | TFLOPs: 15.39 |
g0314: [2024-08-02 21:40:17,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=2620, skipped=0, lr=[4.5770342400000006e-05, 4.5770342400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2620 loss: 2.2675 iter time (s): 4.232 samples/sec: 30.247
g0332:  iteration     2620/10000000 | consumed samples:       335360 | consumed tokens:    686817280 | elapsed time per iteration (ms): 4264.3 | learning rate: 4.577E-05 | global batch size:   128 | lm loss: 2.220914E+00 | loss scale: 65536.0 | grad norm: 1.136 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.016 | tokens per gpu per second (tgs): 1921.048 | TFLOPs: 15.46 |
g0314: [2024-08-02 21:40:59,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=2630, skipped=0, lr=[4.594510506666667e-05, 4.594510506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2630 loss: 2.1908 iter time (s): 4.163 samples/sec: 30.744
g0332:  iteration     2630/10000000 | consumed samples:       336640 | consumed tokens:    689438720 | elapsed time per iteration (ms): 4195.8 | learning rate: 4.595E-05 | global batch size:   128 | lm loss: 2.199008E+00 | loss scale: 65536.0 | grad norm: 1.149 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.506 | tokens per gpu per second (tgs): 1952.408 | TFLOPs: 15.71 |
g0314: [2024-08-02 21:41:42,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=2640, skipped=0, lr=[4.611986773333334e-05, 4.611986773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2640 loss: 2.1432 iter time (s): 4.263 samples/sec: 30.029
g0332:  iteration     2640/10000000 | consumed samples:       337920 | consumed tokens:    692060160 | elapsed time per iteration (ms): 4295.3 | learning rate: 4.612E-05 | global batch size:   128 | lm loss: 2.172065E+00 | loss scale: 65536.0 | grad norm: 1.101 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.800 | tokens per gpu per second (tgs): 1907.185 | TFLOPs: 15.35 |
g0314: [2024-08-02 21:42:23,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=2650, skipped=0, lr=[4.6294630400000004e-05, 4.6294630400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2650 loss: 2.1631 iter time (s): 4.068 samples/sec: 31.464
g0332:  iteration     2650/10000000 | consumed samples:       339200 | consumed tokens:    694681600 | elapsed time per iteration (ms): 4100.5 | learning rate: 4.629E-05 | global batch size:   128 | lm loss: 2.192542E+00 | loss scale: 65536.0 | grad norm: 1.146 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.216 | tokens per gpu per second (tgs): 1997.804 | TFLOPs: 16.08 |
g0314: [2024-08-02 21:43:05,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=2660, skipped=0, lr=[4.646939306666667e-05, 4.646939306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2660 loss: 2.1198 iter time (s): 4.238 samples/sec: 30.201
g0332:  iteration     2660/10000000 | consumed samples:       340480 | consumed tokens:    697303040 | elapsed time per iteration (ms): 4271.2 | learning rate: 4.647E-05 | global batch size:   128 | lm loss: 2.178215E+00 | loss scale: 65536.0 | grad norm: 1.112 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.968 | tokens per gpu per second (tgs): 1917.970 | TFLOPs: 15.43 |
g0314: [2024-08-02 21:43:47,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=2670, skipped=0, lr=[4.6644155733333336e-05, 4.6644155733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2670 loss: 2.2319 iter time (s): 4.173 samples/sec: 30.672
g0332:  iteration     2670/10000000 | consumed samples:       341760 | consumed tokens:    699924480 | elapsed time per iteration (ms): 4205.8 | learning rate: 4.664E-05 | global batch size:   128 | lm loss: 2.189367E+00 | loss scale: 65536.0 | grad norm: 1.151 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.434 | tokens per gpu per second (tgs): 1947.770 | TFLOPs: 15.67 |
g0314: [2024-08-02 21:44:30,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=2680, skipped=0, lr=[4.68189184e-05, 4.68189184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2680 loss: 2.1227 iter time (s): 4.191 samples/sec: 30.539
g0332:  iteration     2680/10000000 | consumed samples:       343040 | consumed tokens:    702545920 | elapsed time per iteration (ms): 4224.2 | learning rate: 4.682E-05 | global batch size:   128 | lm loss: 2.156439E+00 | loss scale: 65536.0 | grad norm: 1.080 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.302 | tokens per gpu per second (tgs): 1939.301 | TFLOPs: 15.61 |
g0314: [2024-08-02 21:45:12,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=2690, skipped=0, lr=[4.699368106666667e-05, 4.699368106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2690 loss: 2.2333 iter time (s): 4.175 samples/sec: 30.658
g0332:  iteration     2690/10000000 | consumed samples:       344320 | consumed tokens:    705167360 | elapsed time per iteration (ms): 4207.9 | learning rate: 4.699E-05 | global batch size:   128 | lm loss: 2.200245E+00 | loss scale: 65536.0 | grad norm: 1.105 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.419 | tokens per gpu per second (tgs): 1946.832 | TFLOPs: 15.67 |
g0314: [2024-08-02 21:45:54,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=2700, skipped=0, lr=[4.716844373333334e-05, 4.716844373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2700 loss: 2.2127 iter time (s): 4.242 samples/sec: 30.178
g0332:  iteration     2700/10000000 | consumed samples:       345600 | consumed tokens:    707788800 | elapsed time per iteration (ms): 4274.1 | learning rate: 4.717E-05 | global batch size:   128 | lm loss: 2.187993E+00 | loss scale: 65536.0 | grad norm: 1.133 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.948 | tokens per gpu per second (tgs): 1916.668 | TFLOPs: 15.42 |
g0314: [2024-08-02 21:46:37,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=2710, skipped=0, lr=[4.734320640000001e-05, 4.734320640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2710 loss: 2.0748 iter time (s): 4.243 samples/sec: 30.170
g0332:  iteration     2710/10000000 | consumed samples:       346880 | consumed tokens:    710410240 | elapsed time per iteration (ms): 4275.2 | learning rate: 4.734E-05 | global batch size:   128 | lm loss: 2.146074E+00 | loss scale: 65536.0 | grad norm: 1.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.940 | tokens per gpu per second (tgs): 1916.187 | TFLOPs: 15.42 |
g0314: [2024-08-02 21:47:20,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=2720, skipped=0, lr=[4.751796906666667e-05, 4.751796906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2720 loss: 2.1469 iter time (s): 4.232 samples/sec: 30.244
g0332:  iteration     2720/10000000 | consumed samples:       348160 | consumed tokens:    713031680 | elapsed time per iteration (ms): 4265.2 | learning rate: 4.752E-05 | global batch size:   128 | lm loss: 2.176785E+00 | loss scale: 65536.0 | grad norm: 1.075 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.010 | tokens per gpu per second (tgs): 1920.638 | TFLOPs: 15.46 |
g0314: [2024-08-02 21:48:03,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=2730, skipped=0, lr=[4.769273173333334e-05, 4.769273173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2730 loss: 2.1648 iter time (s): 4.234 samples/sec: 30.234
g0332:  iteration     2730/10000000 | consumed samples:       349440 | consumed tokens:    715653120 | elapsed time per iteration (ms): 4268.5 | learning rate: 4.769E-05 | global batch size:   128 | lm loss: 2.165276E+00 | loss scale: 65536.0 | grad norm: 1.054 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.987 | tokens per gpu per second (tgs): 1919.155 | TFLOPs: 15.44 |
g0314: [2024-08-02 21:48:46,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=2740, skipped=0, lr=[4.7867494400000005e-05, 4.7867494400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2740 loss: 2.1798 iter time (s): 4.286 samples/sec: 29.864
g0332:  iteration     2740/10000000 | consumed samples:       350720 | consumed tokens:    718274560 | elapsed time per iteration (ms): 4319.6 | learning rate: 4.787E-05 | global batch size:   128 | lm loss: 2.177092E+00 | loss scale: 65536.0 | grad norm: 1.062 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.632 | tokens per gpu per second (tgs): 1896.465 | TFLOPs: 15.26 |
g0314: [2024-08-02 21:49:29,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=2750, skipped=0, lr=[4.804225706666667e-05, 4.804225706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2750 loss: 2.1789 iter time (s): 4.274 samples/sec: 29.951
g0332:  iteration     2750/10000000 | consumed samples:       352000 | consumed tokens:    720896000 | elapsed time per iteration (ms): 4306.5 | learning rate: 4.804E-05 | global batch size:   128 | lm loss: 2.182075E+00 | loss scale: 65536.0 | grad norm: 1.126 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.723 | tokens per gpu per second (tgs): 1902.242 | TFLOPs: 15.31 |
g0314: [2024-08-02 21:50:12,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=2760, skipped=0, lr=[4.821701973333334e-05, 4.821701973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2760 loss: 2.0925 iter time (s): 4.256 samples/sec: 30.076
g0332:  iteration     2760/10000000 | consumed samples:       353280 | consumed tokens:    723517440 | elapsed time per iteration (ms): 4288.6 | learning rate: 4.822E-05 | global batch size:   128 | lm loss: 2.143097E+00 | loss scale: 65536.0 | grad norm: 1.056 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.846 | tokens per gpu per second (tgs): 1910.158 | TFLOPs: 15.37 |
g0314: [2024-08-02 21:50:55,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=2770, skipped=0, lr=[4.8391782400000004e-05, 4.8391782400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2770 loss: 2.1862 iter time (s): 4.294 samples/sec: 29.811
g0332:  iteration     2770/10000000 | consumed samples:       354560 | consumed tokens:    726138880 | elapsed time per iteration (ms): 4326.6 | learning rate: 4.839E-05 | global batch size:   128 | lm loss: 2.163780E+00 | loss scale: 65536.0 | grad norm: 1.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.584 | tokens per gpu per second (tgs): 1893.392 | TFLOPs: 15.24 |
g0314: [2024-08-02 21:51:38,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=2780, skipped=0, lr=[4.856654506666667e-05, 4.856654506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2780 loss: 2.1715 iter time (s): 4.278 samples/sec: 29.917
g0332:  iteration     2780/10000000 | consumed samples:       355840 | consumed tokens:    728760320 | elapsed time per iteration (ms): 4311.2 | learning rate: 4.857E-05 | global batch size:   128 | lm loss: 2.151530E+00 | loss scale: 65536.0 | grad norm: 1.022 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.690 | tokens per gpu per second (tgs): 1900.170 | TFLOPs: 15.29 |
g0314: [2024-08-02 21:52:21,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=2790, skipped=0, lr=[4.874130773333334e-05, 4.874130773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2790 loss: 2.1645 iter time (s): 4.299 samples/sec: 29.776
g0332:  iteration     2790/10000000 | consumed samples:       357120 | consumed tokens:    731381760 | elapsed time per iteration (ms): 4331.7 | learning rate: 4.874E-05 | global batch size:   128 | lm loss: 2.162816E+00 | loss scale: 65536.0 | grad norm: 1.053 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.549 | tokens per gpu per second (tgs): 1891.160 | TFLOPs: 15.22 |
g0314: [2024-08-02 21:53:04,812] [INFO] [logging.py:96:log_dist] [Rank 0] step=2800, skipped=0, lr=[4.891607040000001e-05, 4.891607040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2800 loss: 2.0622 iter time (s): 4.253 samples/sec: 30.098
g0332:  iteration     2800/10000000 | consumed samples:       358400 | consumed tokens:    734003200 | elapsed time per iteration (ms): 4294.5 | learning rate: 4.892E-05 | global batch size:   128 | lm loss: 2.123349E+00 | loss scale: 65536.0 | grad norm: 1.031 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.806 | tokens per gpu per second (tgs): 1907.553 | TFLOPs: 15.35 |
g0314: [2024-08-02 21:53:47,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=2810, skipped=0, lr=[4.9090833066666675e-05, 4.9090833066666675e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2810 loss: 2.1615 iter time (s): 4.228 samples/sec: 30.277
g0332:  iteration     2810/10000000 | consumed samples:       359680 | consumed tokens:    736624640 | elapsed time per iteration (ms): 4260.1 | learning rate: 4.909E-05 | global batch size:   128 | lm loss: 2.155542E+00 | loss scale: 65536.0 | grad norm: 1.078 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.046 | tokens per gpu per second (tgs): 1922.946 | TFLOPs: 15.47 |
g0314: [2024-08-02 21:54:31,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=2820, skipped=0, lr=[4.926559573333334e-05, 4.926559573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2820 loss: 2.0645 iter time (s): 4.387 samples/sec: 29.180
g0332:  iteration     2820/10000000 | consumed samples:       360960 | consumed tokens:    739246080 | elapsed time per iteration (ms): 4419.3 | learning rate: 4.927E-05 | global batch size:   128 | lm loss: 2.130101E+00 | loss scale: 65536.0 | grad norm: 1.034 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.964 | tokens per gpu per second (tgs): 1853.706 | TFLOPs: 14.92 |
g0314: [2024-08-02 21:55:15,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=2830, skipped=0, lr=[4.944035840000001e-05, 4.944035840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2830 loss: 2.0657 iter time (s): 4.358 samples/sec: 29.368
g0332:  iteration     2830/10000000 | consumed samples:       362240 | consumed tokens:    741867520 | elapsed time per iteration (ms): 4391.0 | learning rate: 4.944E-05 | global batch size:   128 | lm loss: 2.102983E+00 | loss scale: 65536.0 | grad norm: 1.091 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.151 | tokens per gpu per second (tgs): 1865.644 | TFLOPs: 15.01 |
g0314: [2024-08-02 21:55:58,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=2840, skipped=0, lr=[4.961512106666667e-05, 4.961512106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2840 loss: 2.1787 iter time (s): 4.276 samples/sec: 29.938
g0332:  iteration     2840/10000000 | consumed samples:       363520 | consumed tokens:    744488960 | elapsed time per iteration (ms): 4308.0 | learning rate: 4.962E-05 | global batch size:   128 | lm loss: 2.154080E+00 | loss scale: 65536.0 | grad norm: 1.069 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.712 | tokens per gpu per second (tgs): 1901.576 | TFLOPs: 15.30 |
g0314: [2024-08-02 21:56:41,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=2850, skipped=0, lr=[4.978988373333333e-05, 4.978988373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2850 loss: 2.1134 iter time (s): 4.304 samples/sec: 29.741
g0332:  iteration     2850/10000000 | consumed samples:       364800 | consumed tokens:    747110400 | elapsed time per iteration (ms): 4336.4 | learning rate: 4.979E-05 | global batch size:   128 | lm loss: 2.121934E+00 | loss scale: 65536.0 | grad norm: 1.026 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.517 | tokens per gpu per second (tgs): 1889.109 | TFLOPs: 15.20 |
g0314: [2024-08-02 21:57:25,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=2860, skipped=0, lr=[4.99646464e-05, 4.99646464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2860 loss: 2.1373 iter time (s): 4.319 samples/sec: 29.638
g0332:  iteration     2860/10000000 | consumed samples:       366080 | consumed tokens:    749731840 | elapsed time per iteration (ms): 4353.1 | learning rate: 4.996E-05 | global batch size:   128 | lm loss: 2.130743E+00 | loss scale: 65536.0 | grad norm: 1.058 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.404 | tokens per gpu per second (tgs): 1881.870 | TFLOPs: 15.14 |
g0314: [2024-08-02 21:58:08,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=2870, skipped=0, lr=[5.0139409066666664e-05, 5.0139409066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2870 loss: 2.1673 iter time (s): 4.300 samples/sec: 29.767
g0332:  iteration     2870/10000000 | consumed samples:       367360 | consumed tokens:    752353280 | elapsed time per iteration (ms): 4332.6 | learning rate: 5.014E-05 | global batch size:   128 | lm loss: 2.113079E+00 | loss scale: 65536.0 | grad norm: 1.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.543 | tokens per gpu per second (tgs): 1890.760 | TFLOPs: 15.22 |
g0314: [2024-08-02 21:58:51,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=0, lr=[5.031417173333333e-05, 5.031417173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2880 loss: 2.1191 iter time (s): 4.248 samples/sec: 30.135
g0332:  iteration     2880/10000000 | consumed samples:       368640 | consumed tokens:    754974720 | elapsed time per iteration (ms): 4279.9 | learning rate: 5.031E-05 | global batch size:   128 | lm loss: 2.134629E+00 | loss scale: 65536.0 | grad norm: 0.973 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.907 | tokens per gpu per second (tgs): 1914.066 | TFLOPs: 15.40 |
g0314: [2024-08-02 21:59:33,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=0, lr=[5.0488934399999996e-05, 5.0488934399999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2890 loss: 2.1645 iter time (s): 4.189 samples/sec: 30.554
g0332:  iteration     2890/10000000 | consumed samples:       369920 | consumed tokens:    757596160 | elapsed time per iteration (ms): 4221.7 | learning rate: 5.049E-05 | global batch size:   128 | lm loss: 2.166373E+00 | loss scale: 65536.0 | grad norm: 1.101 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.320 | tokens per gpu per second (tgs): 1940.449 | TFLOPs: 15.62 |
g0314: [2024-08-02 22:00:16,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=0, lr=[5.066369706666666e-05, 5.066369706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2900 loss: 2.1197 iter time (s): 4.230 samples/sec: 30.258
g0332:  iteration     2900/10000000 | consumed samples:       371200 | consumed tokens:    760217600 | elapsed time per iteration (ms): 4262.5 | learning rate: 5.066E-05 | global batch size:   128 | lm loss: 2.134994E+00 | loss scale: 65536.0 | grad norm: 1.016 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.030 | tokens per gpu per second (tgs): 1921.893 | TFLOPs: 15.47 |
g0314: [2024-08-02 22:01:15,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=0, lr=[5.083845973333333e-05, 5.083845973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2910 loss: 2.1626 iter time (s): 5.886 samples/sec: 21.747
g0332:  iteration     2910/10000000 | consumed samples:       372480 | consumed tokens:    762839040 | elapsed time per iteration (ms): 5918.8 | learning rate: 5.084E-05 | global batch size:   128 | lm loss: 2.136656E+00 | loss scale: 65536.0 | grad norm: 0.965 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.626 | tokens per gpu per second (tgs): 1384.059 | TFLOPs: 11.14 |
g0314: [2024-08-02 22:03:44,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=0, lr=[5.10132224e-05, 5.10132224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2920 loss: 2.0624 iter time (s): 14.818 samples/sec: 8.638
g0332:  iteration     2920/10000000 | consumed samples:       373760 | consumed tokens:    765460480 | elapsed time per iteration (ms): 14852.2 | learning rate: 5.101E-05 | global batch size:   128 | lm loss: 2.121062E+00 | loss scale: 65536.0 | grad norm: 0.938 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.618 | tokens per gpu per second (tgs): 551.568 | TFLOPs: 4.44 |
g0314: [2024-08-02 22:05:31,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=0, lr=[5.118798506666667e-05, 5.118798506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2930 loss: 2.1069 iter time (s): 10.725 samples/sec: 11.934
g0332:  iteration     2930/10000000 | consumed samples:       375040 | consumed tokens:    768081920 | elapsed time per iteration (ms): 10758.9 | learning rate: 5.119E-05 | global batch size:   128 | lm loss: 2.123723E+00 | loss scale: 65536.0 | grad norm: 0.977 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.897 | tokens per gpu per second (tgs): 761.416 | TFLOPs: 6.13 |
g0314: [2024-08-02 22:06:45,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=2940, skipped=0, lr=[5.1362747733333334e-05, 5.1362747733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2940 loss: 2.0386 iter time (s): 7.350 samples/sec: 17.416
g0332:  iteration     2940/10000000 | consumed samples:       376320 | consumed tokens:    770703360 | elapsed time per iteration (ms): 7382.6 | learning rate: 5.136E-05 | global batch size:   128 | lm loss: 2.110276E+00 | loss scale: 65536.0 | grad norm: 0.971 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.338 | tokens per gpu per second (tgs): 1109.629 | TFLOPs: 8.93 |
g0314: [2024-08-02 22:07:36,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=2950, skipped=0, lr=[5.15375104e-05, 5.15375104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2950 loss: 2.0544 iter time (s): 5.037 samples/sec: 25.411
g0332:  iteration     2950/10000000 | consumed samples:       377600 | consumed tokens:    773324800 | elapsed time per iteration (ms): 5070.1 | learning rate: 5.154E-05 | global batch size:   128 | lm loss: 2.106521E+00 | loss scale: 65536.0 | grad norm: 0.935 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.246 | tokens per gpu per second (tgs): 1615.751 | TFLOPs: 13.00 |
g0314: [2024-08-02 22:08:19,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=2960, skipped=0, lr=[5.1712273066666666e-05, 5.1712273066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2960 loss: 2.0184 iter time (s): 4.313 samples/sec: 29.677
g0332:  iteration     2960/10000000 | consumed samples:       378880 | consumed tokens:    775946240 | elapsed time per iteration (ms): 4346.1 | learning rate: 5.171E-05 | global batch size:   128 | lm loss: 2.093954E+00 | loss scale: 65536.0 | grad norm: 1.050 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.451 | tokens per gpu per second (tgs): 1884.892 | TFLOPs: 15.17 |
g0314: [2024-08-02 22:09:03,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=2970, skipped=0, lr=[5.188703573333333e-05, 5.188703573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2970 loss: 2.1299 iter time (s): 4.338 samples/sec: 29.507
g0332:  iteration     2970/10000000 | consumed samples:       380160 | consumed tokens:    778567680 | elapsed time per iteration (ms): 4370.7 | learning rate: 5.189E-05 | global batch size:   128 | lm loss: 2.114236E+00 | loss scale: 65536.0 | grad norm: 0.938 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.286 | tokens per gpu per second (tgs): 1874.287 | TFLOPs: 15.08 |
g0314: [2024-08-02 22:09:44,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=2980, skipped=0, lr=[5.20617984e-05, 5.20617984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2980 loss: 2.0541 iter time (s): 4.090 samples/sec: 31.299
g0332:  iteration     2980/10000000 | consumed samples:       381440 | consumed tokens:    781189120 | elapsed time per iteration (ms): 4122.0 | learning rate: 5.206E-05 | global batch size:   128 | lm loss: 2.099514E+00 | loss scale: 65536.0 | grad norm: 1.004 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.053 | tokens per gpu per second (tgs): 1987.384 | TFLOPs: 15.99 |
g0314: [2024-08-02 22:10:28,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=2990, skipped=0, lr=[5.2236561066666664e-05, 5.2236561066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 2990 loss: 2.0703 iter time (s): 4.313 samples/sec: 29.681
g0332:  iteration     2990/10000000 | consumed samples:       382720 | consumed tokens:    783810560 | elapsed time per iteration (ms): 4344.8 | learning rate: 5.224E-05 | global batch size:   128 | lm loss: 2.099964E+00 | loss scale: 65536.0 | grad norm: 1.071 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.460 | tokens per gpu per second (tgs): 1885.455 | TFLOPs: 15.17 |
g0314: [2024-08-02 22:11:10,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=0, lr=[5.241132373333334e-05, 5.241132373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3000 loss: 2.0754 iter time (s): 4.211 samples/sec: 30.397
g0332:  iteration     3000/10000000 | consumed samples:       384000 | consumed tokens:    786432000 | elapsed time per iteration (ms): 4243.6 | learning rate: 5.241E-05 | global batch size:   128 | lm loss: 2.121102E+00 | loss scale: 65536.0 | grad norm: 0.974 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.163 | tokens per gpu per second (tgs): 1930.455 | TFLOPs: 15.53 |
g0332: ------------------------------------------------------------------------------------------------
g0332:  validation loss at iteration 3000 | lm loss value: 2.109782E+00 | lm loss PPL: 8.246447E+00 | 
g0332: ------------------------------------------------------------------------------------------------
g0314: saving checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: [2024-08-02 22:17:44,605] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
g0332: [2024-08-02 22:17:44,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0332: [2024-08-02 22:17:44,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0314: [2024-08-02 22:17:44,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0314: [2024-08-02 22:17:44,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0332: [2024-08-02 22:17:44,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0314: [2024-08-02 22:17:44,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0316: [2024-08-02 22:17:44,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0316: [2024-08-02 22:17:44,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0316: [2024-08-02 22:17:44,614] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0319: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0319: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0318: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0318: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0318: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0319: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0320: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0320: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0320: [2024-08-02 22:17:44,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0329: [2024-08-02 22:17:44,616] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0329: [2024-08-02 22:17:44,616] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0329: [2024-08-02 22:17:44,616] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0325: [2024-08-02 22:17:44,616] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0325: [2024-08-02 22:17:44,616] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0325: [2024-08-02 22:17:44,617] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0332: [2024-08-02 22:17:44,641] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt...
g0316: [2024-08-02 22:17:44,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt...
g0318: [2024-08-02 22:17:44,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt...
g0329: [2024-08-02 22:17:44,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt...
g0325: [2024-08-02 22:17:44,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt...
g0320: [2024-08-02 22:17:44,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt...
g0319: [2024-08-02 22:17:44,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt...
g0314: [2024-08-02 22:17:44,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt...
g0332: [2024-08-02 22:17:44,741] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt.
g0332: [2024-08-02 22:17:44,742] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt...
g0332: [2024-08-02 22:17:44,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt.
g0332: [2024-08-02 22:17:44,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt...
g0314: [2024-08-02 22:17:44,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt.
g0316: [2024-08-02 22:17:44,834] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt.
g0325: [2024-08-02 22:17:44,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt.
g0314: [2024-08-02 22:17:44,848] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt...
g0316: [2024-08-02 22:17:44,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt...
g0325: [2024-08-02 22:17:44,874] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt...
g0329: [2024-08-02 22:17:44,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt.
g0319: [2024-08-02 22:17:44,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt.
g0329: [2024-08-02 22:17:44,920] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt...
g0332: [2024-08-02 22:17:44,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt.
g0332: [2024-08-02 22:17:44,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt...
g0320: [2024-08-02 22:17:44,927] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt.
g0319: [2024-08-02 22:17:44,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt...
g0320: [2024-08-02 22:17:44,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt...
g0318: [2024-08-02 22:17:44,992] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt.
g0318: [2024-08-02 22:17:45,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt...
g0329: [2024-08-02 22:17:45,037] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt.
g0329: [2024-08-02 22:17:45,071] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt...
g0320: [2024-08-02 22:17:45,071] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt.
g0320: [2024-08-02 22:17:45,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt...
g0314: [2024-08-02 22:17:45,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt.
g0314: [2024-08-02 22:17:45,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt...
g0325: [2024-08-02 22:17:45,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt.
g0316: [2024-08-02 22:17:45,169] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt.
g0325: [2024-08-02 22:17:45,175] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt...
g0318: [2024-08-02 22:17:45,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt.
g0319: [2024-08-02 22:17:45,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt.
g0316: [2024-08-02 22:17:45,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt...
g0318: [2024-08-02 22:17:45,207] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt...
g0320: [2024-08-02 22:17:45,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt.
g0320: [2024-08-02 22:17:45,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt...
g0319: [2024-08-02 22:17:45,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt...
g0314: [2024-08-02 22:17:45,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt.
g0329: [2024-08-02 22:17:45,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt.
g0329: [2024-08-02 22:17:45,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt...
g0314: [2024-08-02 22:17:45,257] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt...
g0325: [2024-08-02 22:17:45,310] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt.
g0325: [2024-08-02 22:17:45,312] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt...
g0316: [2024-08-02 22:17:45,320] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt.
g0316: [2024-08-02 22:17:45,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt...
g0318: [2024-08-02 22:17:45,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt.
g0318: [2024-08-02 22:17:45,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt...
g0319: [2024-08-02 22:17:45,389] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt.
g0319: [2024-08-02 22:17:45,391] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt...
g0314: [2024-08-02 22:17:45,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt.
g0314: [2024-08-02 22:17:45,445] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt
g0314: [2024-08-02 22:17:45,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt...
g0332: [2024-08-02 22:17:47,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt.
g0332: [2024-08-02 22:17:47,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0320: [2024-08-02 22:17:47,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt.
g0320: [2024-08-02 22:17:47,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0316: [2024-08-02 22:17:47,602] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt.
g0316: [2024-08-02 22:17:47,602] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0329: [2024-08-02 22:17:47,619] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt.
g0329: [2024-08-02 22:17:47,620] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0319: [2024-08-02 22:17:47,793] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt.
g0319: [2024-08-02 22:17:47,794] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0318: [2024-08-02 22:17:47,889] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt.
g0318: [2024-08-02 22:17:47,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0325: [2024-08-02 22:17:47,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt.
g0325: [2024-08-02 22:17:47,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0314: [2024-08-02 22:17:48,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt.
g0314: [2024-08-02 22:17:48,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0314:   successfully saved checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: Checkpoint Save GB: 22.521, GB/Sec: 5.38, Latency(second): 4.185
g0332: (min, max) time across ranks (ms):
g0332:     save-checkpoint ................................: (4184.97, 4185.78)
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0329: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0329: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0318: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0316: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0319: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0332: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0325: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0314: [2024-08-02 22:17:52,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0329: [2024-08-02 22:17:52,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0314: [2024-08-02 22:18:30,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=3010, skipped=0, lr=[5.25860864e-05, 5.25860864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3010 loss: 2.0703 iter time (s): 4.140 samples/sec: 30.918
g0332:  iteration     3010/10000000 | consumed samples:       385280 | consumed tokens:    789053440 | elapsed time per iteration (ms): 43990.3 | learning rate: 5.259E-05 | global batch size:   128 | lm loss: 2.078553E+00 | loss scale: 131072.0 | grad norm: 1.046 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.910 | tokens per gpu per second (tgs): 186.223 | TFLOPs: 1.50 |
g0314: [2024-08-02 22:19:15,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=3020, skipped=0, lr=[5.276084906666667e-05, 5.276084906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3020 loss: 2.1262 iter time (s): 4.490 samples/sec: 28.508
g0332:  iteration     3020/10000000 | consumed samples:       386560 | consumed tokens:    791674880 | elapsed time per iteration (ms): 4522.6 | learning rate: 5.276E-05 | global batch size:   128 | lm loss: 2.095278E+00 | loss scale: 131072.0 | grad norm: 0.967 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.303 | tokens per gpu per second (tgs): 1811.364 | TFLOPs: 14.58 |
g0314: [2024-08-02 22:20:00,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=3030, skipped=0, lr=[5.2935611733333335e-05, 5.2935611733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3030 loss: 2.0617 iter time (s): 4.476 samples/sec: 28.594
g0332:  iteration     3030/10000000 | consumed samples:       387840 | consumed tokens:    794296320 | elapsed time per iteration (ms): 4509.0 | learning rate: 5.294E-05 | global batch size:   128 | lm loss: 2.111541E+00 | loss scale: 131072.0 | grad norm: 0.920 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.388 | tokens per gpu per second (tgs): 1816.801 | TFLOPs: 14.62 |
g0314: [2024-08-02 22:20:43,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=3040, skipped=0, lr=[5.31103744e-05, 5.31103744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3040 loss: 2.1270 iter time (s): 4.271 samples/sec: 29.972
g0332:  iteration     3040/10000000 | consumed samples:       389120 | consumed tokens:    796917760 | elapsed time per iteration (ms): 4303.2 | learning rate: 5.311E-05 | global batch size:   128 | lm loss: 2.094783E+00 | loss scale: 131072.0 | grad norm: 0.945 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.745 | tokens per gpu per second (tgs): 1903.679 | TFLOPs: 15.32 |
g0314: [2024-08-02 22:21:25,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=3050, skipped=0, lr=[5.328513706666667e-05, 5.328513706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3050 loss: 2.1296 iter time (s): 4.147 samples/sec: 30.866
g0332:  iteration     3050/10000000 | consumed samples:       390400 | consumed tokens:    799539200 | elapsed time per iteration (ms): 4179.9 | learning rate: 5.329E-05 | global batch size:   128 | lm loss: 2.073683E+00 | loss scale: 131072.0 | grad norm: 1.018 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.623 | tokens per gpu per second (tgs): 1959.873 | TFLOPs: 15.77 |
g0314: [2024-08-02 22:22:08,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=3060, skipped=0, lr=[5.345989973333333e-05, 5.345989973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3060 loss: 2.0885 iter time (s): 4.229 samples/sec: 30.268
g0332:  iteration     3060/10000000 | consumed samples:       391680 | consumed tokens:    802160640 | elapsed time per iteration (ms): 4262.1 | learning rate: 5.346E-05 | global batch size:   128 | lm loss: 2.096609E+00 | loss scale: 131072.0 | grad norm: 0.929 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.032 | tokens per gpu per second (tgs): 1922.049 | TFLOPs: 15.47 |
g0314: [2024-08-02 22:22:51,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=3070, skipped=0, lr=[5.36346624e-05, 5.36346624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3070 loss: 2.1502 iter time (s): 4.271 samples/sec: 29.973
g0332:  iteration     3070/10000000 | consumed samples:       392960 | consumed tokens:    804782080 | elapsed time per iteration (ms): 4304.8 | learning rate: 5.363E-05 | global batch size:   128 | lm loss: 2.106210E+00 | loss scale: 131072.0 | grad norm: 0.979 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.734 | tokens per gpu per second (tgs): 1903.007 | TFLOPs: 15.31 |
g0314: [2024-08-02 22:23:34,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=3080, skipped=0, lr=[5.3809425066666665e-05, 5.3809425066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3080 loss: 2.0956 iter time (s): 4.274 samples/sec: 29.948
g0332:  iteration     3080/10000000 | consumed samples:       394240 | consumed tokens:    807403520 | elapsed time per iteration (ms): 4306.9 | learning rate: 5.381E-05 | global batch size:   128 | lm loss: 2.097174E+00 | loss scale: 131072.0 | grad norm: 0.980 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.720 | tokens per gpu per second (tgs): 1902.055 | TFLOPs: 15.31 |
g0314: [2024-08-02 22:24:18,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=3090, skipped=0, lr=[5.398418773333334e-05, 5.398418773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3090 loss: 2.1054 iter time (s): 4.369 samples/sec: 29.296
g0332:  iteration     3090/10000000 | consumed samples:       395520 | consumed tokens:    810024960 | elapsed time per iteration (ms): 4401.7 | learning rate: 5.398E-05 | global batch size:   128 | lm loss: 2.079736E+00 | loss scale: 131072.0 | grad norm: 0.910 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.079 | tokens per gpu per second (tgs): 1861.086 | TFLOPs: 14.98 |
g0314: [2024-08-02 22:25:02,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=3100, skipped=0, lr=[5.4158950400000004e-05, 5.4158950400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3100 loss: 2.1589 iter time (s): 4.332 samples/sec: 29.546
g0332:  iteration     3100/10000000 | consumed samples:       396800 | consumed tokens:    812646400 | elapsed time per iteration (ms): 4368.3 | learning rate: 5.416E-05 | global batch size:   128 | lm loss: 2.098727E+00 | loss scale: 131072.0 | grad norm: 0.946 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.302 | tokens per gpu per second (tgs): 1875.308 | TFLOPs: 15.09 |
g0314: [2024-08-02 22:25:46,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=3110, skipped=0, lr=[5.433371306666667e-05, 5.433371306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3110 loss: 2.0874 iter time (s): 4.391 samples/sec: 29.154
g0332:  iteration     3110/10000000 | consumed samples:       398080 | consumed tokens:    815267840 | elapsed time per iteration (ms): 4433.9 | learning rate: 5.433E-05 | global batch size:   128 | lm loss: 2.045705E+00 | loss scale: 131072.0 | grad norm: 0.925 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.869 | tokens per gpu per second (tgs): 1847.599 | TFLOPs: 14.87 |
g0314: [2024-08-02 22:26:29,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=3120, skipped=0, lr=[5.4508475733333336e-05, 5.4508475733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3120 loss: 2.1178 iter time (s): 4.272 samples/sec: 29.964
g0332:  iteration     3120/10000000 | consumed samples:       399360 | consumed tokens:    817889280 | elapsed time per iteration (ms): 4305.6 | learning rate: 5.451E-05 | global batch size:   128 | lm loss: 2.062570E+00 | loss scale: 131072.0 | grad norm: 0.908 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.728 | tokens per gpu per second (tgs): 1902.622 | TFLOPs: 15.31 |
g0314: [2024-08-02 22:27:11,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=3130, skipped=0, lr=[5.46832384e-05, 5.46832384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3130 loss: 2.0896 iter time (s): 4.207 samples/sec: 30.423
g0332:  iteration     3130/10000000 | consumed samples:       400640 | consumed tokens:    820510720 | elapsed time per iteration (ms): 4239.7 | learning rate: 5.468E-05 | global batch size:   128 | lm loss: 2.086385E+00 | loss scale: 131072.0 | grad norm: 0.885 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.190 | tokens per gpu per second (tgs): 1932.190 | TFLOPs: 15.55 |
g0314: [2024-08-02 22:27:56,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=3140, skipped=0, lr=[5.485800106666667e-05, 5.485800106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3140 loss: 1.9945 iter time (s): 4.428 samples/sec: 28.910
g0332:  iteration     3140/10000000 | consumed samples:       401920 | consumed tokens:    823132160 | elapsed time per iteration (ms): 4460.4 | learning rate: 5.486E-05 | global batch size:   128 | lm loss: 2.084493E+00 | loss scale: 131072.0 | grad norm: 0.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.697 | tokens per gpu per second (tgs): 1836.616 | TFLOPs: 14.78 |
g0314: [2024-08-02 22:28:39,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=3150, skipped=0, lr=[5.5032763733333334e-05, 5.5032763733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3150 loss: 2.0742 iter time (s): 4.305 samples/sec: 29.735
g0332:  iteration     3150/10000000 | consumed samples:       403200 | consumed tokens:    825753600 | elapsed time per iteration (ms): 4337.0 | learning rate: 5.503E-05 | global batch size:   128 | lm loss: 2.093513E+00 | loss scale: 131072.0 | grad norm: 0.882 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.514 | tokens per gpu per second (tgs): 1888.885 | TFLOPs: 15.20 |
g0314: [2024-08-02 22:29:23,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=3160, skipped=0, lr=[5.52075264e-05, 5.52075264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3160 loss: 2.0626 iter time (s): 4.326 samples/sec: 29.585
g0332:  iteration     3160/10000000 | consumed samples:       404480 | consumed tokens:    828375040 | elapsed time per iteration (ms): 4358.8 | learning rate: 5.521E-05 | global batch size:   128 | lm loss: 2.094756E+00 | loss scale: 131072.0 | grad norm: 0.880 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.366 | tokens per gpu per second (tgs): 1879.431 | TFLOPs: 15.12 |
g0314: [2024-08-02 22:30:06,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=3170, skipped=0, lr=[5.5382289066666667e-05, 5.5382289066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3170 loss: 2.0400 iter time (s): 4.229 samples/sec: 30.268
g0332:  iteration     3170/10000000 | consumed samples:       405760 | consumed tokens:    830996480 | elapsed time per iteration (ms): 4261.1 | learning rate: 5.538E-05 | global batch size:   128 | lm loss: 2.077467E+00 | loss scale: 131072.0 | grad norm: 0.916 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.039 | tokens per gpu per second (tgs): 1922.494 | TFLOPs: 15.47 |
g0314: [2024-08-02 22:30:48,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=3180, skipped=0, lr=[5.555705173333334e-05, 5.555705173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3180 loss: 2.0842 iter time (s): 4.201 samples/sec: 30.469
g0332:  iteration     3180/10000000 | consumed samples:       407040 | consumed tokens:    833617920 | elapsed time per iteration (ms): 4233.4 | learning rate: 5.556E-05 | global batch size:   128 | lm loss: 2.079340E+00 | loss scale: 131072.0 | grad norm: 0.908 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.236 | tokens per gpu per second (tgs): 1935.096 | TFLOPs: 15.57 |
g0314: [2024-08-02 22:31:30,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=3190, skipped=0, lr=[5.5731814400000005e-05, 5.5731814400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3190 loss: 2.1303 iter time (s): 4.177 samples/sec: 30.641
g0332:  iteration     3190/10000000 | consumed samples:       408320 | consumed tokens:    836239360 | elapsed time per iteration (ms): 4210.0 | learning rate: 5.573E-05 | global batch size:   128 | lm loss: 2.069810E+00 | loss scale: 131072.0 | grad norm: 0.900 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.404 | tokens per gpu per second (tgs): 1945.861 | TFLOPs: 15.66 |
g0314: [2024-08-02 22:32:13,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=3200, skipped=0, lr=[5.590657706666667e-05, 5.590657706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3200 loss: 2.0419 iter time (s): 4.299 samples/sec: 29.772
g0332:  iteration     3200/10000000 | consumed samples:       409600 | consumed tokens:    838860800 | elapsed time per iteration (ms): 4331.9 | learning rate: 5.591E-05 | global batch size:   128 | lm loss: 2.060244E+00 | loss scale: 131072.0 | grad norm: 0.880 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.548 | tokens per gpu per second (tgs): 1891.072 | TFLOPs: 15.22 |
g0314: [2024-08-02 22:32:57,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=3210, skipped=0, lr=[5.608133973333334e-05, 5.608133973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3210 loss: 2.0843 iter time (s): 4.319 samples/sec: 29.637
g0332:  iteration     3210/10000000 | consumed samples:       410880 | consumed tokens:    841482240 | elapsed time per iteration (ms): 4351.3 | learning rate: 5.608E-05 | global batch size:   128 | lm loss: 2.084551E+00 | loss scale: 131072.0 | grad norm: 0.889 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.416 | tokens per gpu per second (tgs): 1882.654 | TFLOPs: 15.15 |
g0314: [2024-08-02 22:33:41,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=3220, skipped=0, lr=[5.6256102400000004e-05, 5.6256102400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3220 loss: 2.0680 iter time (s): 4.430 samples/sec: 28.893
g0332:  iteration     3220/10000000 | consumed samples:       412160 | consumed tokens:    844103680 | elapsed time per iteration (ms): 4462.4 | learning rate: 5.626E-05 | global batch size:   128 | lm loss: 2.050045E+00 | loss scale: 131072.0 | grad norm: 0.976 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.684 | tokens per gpu per second (tgs): 1835.785 | TFLOPs: 14.77 |
g0314: [2024-08-02 22:34:23,844] [INFO] [logging.py:96:log_dist] [Rank 0] step=3230, skipped=0, lr=[5.643086506666667e-05, 5.643086506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3230 loss: 2.1465 iter time (s): 4.161 samples/sec: 30.762
g0332:  iteration     3230/10000000 | consumed samples:       413440 | consumed tokens:    846725120 | elapsed time per iteration (ms): 4193.6 | learning rate: 5.643E-05 | global batch size:   128 | lm loss: 2.070800E+00 | loss scale: 131072.0 | grad norm: 0.912 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.523 | tokens per gpu per second (tgs): 1953.466 | TFLOPs: 15.72 |
g0314: [2024-08-02 22:35:07,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=3240, skipped=0, lr=[5.6605627733333336e-05, 5.6605627733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3240 loss: 2.0507 iter time (s): 4.305 samples/sec: 29.735
g0332:  iteration     3240/10000000 | consumed samples:       414720 | consumed tokens:    849346560 | elapsed time per iteration (ms): 4336.9 | learning rate: 5.661E-05 | global batch size:   128 | lm loss: 2.038117E+00 | loss scale: 131072.0 | grad norm: 0.854 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.514 | tokens per gpu per second (tgs): 1888.888 | TFLOPs: 15.20 |
g0314: [2024-08-02 22:35:49,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=3250, skipped=0, lr=[5.67803904e-05, 5.67803904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3250 loss: 2.0068 iter time (s): 4.236 samples/sec: 30.219
g0332:  iteration     3250/10000000 | consumed samples:       416000 | consumed tokens:    851968000 | elapsed time per iteration (ms): 4268.4 | learning rate: 5.678E-05 | global batch size:   128 | lm loss: 2.062388E+00 | loss scale: 131072.0 | grad norm: 0.881 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.988 | tokens per gpu per second (tgs): 1919.225 | TFLOPs: 15.44 |
g0314: [2024-08-02 22:36:33,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=3260, skipped=0, lr=[5.695515306666667e-05, 5.695515306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3260 loss: 2.0480 iter time (s): 4.355 samples/sec: 29.392
g0332:  iteration     3260/10000000 | consumed samples:       417280 | consumed tokens:    854589440 | elapsed time per iteration (ms): 4387.2 | learning rate: 5.696E-05 | global batch size:   128 | lm loss: 2.060451E+00 | loss scale: 131072.0 | grad norm: 0.871 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.176 | tokens per gpu per second (tgs): 1867.238 | TFLOPs: 15.03 |
g0314: [2024-08-02 22:37:16,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=3270, skipped=0, lr=[5.712991573333334e-05, 5.712991573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3270 loss: 2.1234 iter time (s): 4.200 samples/sec: 30.473
g0332:  iteration     3270/10000000 | consumed samples:       418560 | consumed tokens:    857210880 | elapsed time per iteration (ms): 4235.7 | learning rate: 5.713E-05 | global batch size:   128 | lm loss: 2.073567E+00 | loss scale: 131072.0 | grad norm: 0.829 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.219 | tokens per gpu per second (tgs): 1934.029 | TFLOPs: 15.56 |
g0314: [2024-08-02 22:37:58,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=3280, skipped=0, lr=[5.730467840000001e-05, 5.730467840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3280 loss: 2.0359 iter time (s): 4.248 samples/sec: 30.130
g0332:  iteration     3280/10000000 | consumed samples:       419840 | consumed tokens:    859832320 | elapsed time per iteration (ms): 4280.6 | learning rate: 5.730E-05 | global batch size:   128 | lm loss: 2.058337E+00 | loss scale: 131072.0 | grad norm: 0.888 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.902 | tokens per gpu per second (tgs): 1913.736 | TFLOPs: 15.40 |
g0314: [2024-08-02 22:38:41,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=3290, skipped=0, lr=[5.747944106666667e-05, 5.747944106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3290 loss: 2.0820 iter time (s): 4.274 samples/sec: 29.950
g0332:  iteration     3290/10000000 | consumed samples:       421120 | consumed tokens:    862453760 | elapsed time per iteration (ms): 4306.0 | learning rate: 5.748E-05 | global batch size:   128 | lm loss: 2.060770E+00 | loss scale: 131072.0 | grad norm: 0.897 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.726 | tokens per gpu per second (tgs): 1902.464 | TFLOPs: 15.31 |
g0314: [2024-08-02 22:39:26,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=3300, skipped=0, lr=[5.765420373333334e-05, 5.765420373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3300 loss: 2.0308 iter time (s): 4.434 samples/sec: 28.865
g0332:  iteration     3300/10000000 | consumed samples:       422400 | consumed tokens:    865075200 | elapsed time per iteration (ms): 4467.4 | learning rate: 5.765E-05 | global batch size:   128 | lm loss: 2.015688E+00 | loss scale: 131072.0 | grad norm: 0.875 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.652 | tokens per gpu per second (tgs): 1833.717 | TFLOPs: 14.76 |
g0314: [2024-08-02 22:40:13,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=3310, skipped=0, lr=[5.7828966400000005e-05, 5.7828966400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3310 loss: 2.1095 iter time (s): 4.601 samples/sec: 27.823
g0332:  iteration     3310/10000000 | consumed samples:       423680 | consumed tokens:    867696640 | elapsed time per iteration (ms): 4633.1 | learning rate: 5.783E-05 | global batch size:   128 | lm loss: 2.065667E+00 | loss scale: 131072.0 | grad norm: 0.845 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.627 | tokens per gpu per second (tgs): 1768.135 | TFLOPs: 14.23 |
g0314: [2024-08-02 22:41:01,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=3320, skipped=0, lr=[5.800372906666667e-05, 5.800372906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3320 loss: 2.0891 iter time (s): 4.843 samples/sec: 26.431
g0332:  iteration     3320/10000000 | consumed samples:       424960 | consumed tokens:    870318080 | elapsed time per iteration (ms): 4877.7 | learning rate: 5.800E-05 | global batch size:   128 | lm loss: 2.022732E+00 | loss scale: 131072.0 | grad norm: 0.885 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.242 | tokens per gpu per second (tgs): 1679.476 | TFLOPs: 13.52 |
g0314: [2024-08-02 22:41:50,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=3330, skipped=0, lr=[5.817849173333334e-05, 5.817849173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3330 loss: 2.0229 iter time (s): 4.793 samples/sec: 26.705
g0332:  iteration     3330/10000000 | consumed samples:       426240 | consumed tokens:    872939520 | elapsed time per iteration (ms): 4825.5 | learning rate: 5.818E-05 | global batch size:   128 | lm loss: 2.022251E+00 | loss scale: 131072.0 | grad norm: 0.848 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.526 | tokens per gpu per second (tgs): 1697.634 | TFLOPs: 13.66 |
g0314: [2024-08-02 22:42:37,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=3340, skipped=0, lr=[5.83532544e-05, 5.83532544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3340 loss: 1.9782 iter time (s): 4.733 samples/sec: 27.044
g0332:  iteration     3340/10000000 | consumed samples:       427520 | consumed tokens:    875560960 | elapsed time per iteration (ms): 4765.9 | learning rate: 5.835E-05 | global batch size:   128 | lm loss: 2.021286E+00 | loss scale: 131072.0 | grad norm: 0.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.857 | tokens per gpu per second (tgs): 1718.868 | TFLOPs: 13.83 |
g0314: [2024-08-02 22:43:25,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=3350, skipped=0, lr=[5.852801706666667e-05, 5.852801706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3350 loss: 2.0158 iter time (s): 4.787 samples/sec: 26.736
g0332:  iteration     3350/10000000 | consumed samples:       428800 | consumed tokens:    878182400 | elapsed time per iteration (ms): 4820.9 | learning rate: 5.853E-05 | global batch size:   128 | lm loss: 2.049603E+00 | loss scale: 131072.0 | grad norm: 0.891 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.551 | tokens per gpu per second (tgs): 1699.282 | TFLOPs: 13.67 |
g0314: [2024-08-02 22:44:12,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=3360, skipped=0, lr=[5.870277973333334e-05, 5.870277973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3360 loss: 2.0331 iter time (s): 4.614 samples/sec: 27.740
g0332:  iteration     3360/10000000 | consumed samples:       430080 | consumed tokens:    880803840 | elapsed time per iteration (ms): 4646.8 | learning rate: 5.870E-05 | global batch size:   128 | lm loss: 2.070345E+00 | loss scale: 131072.0 | grad norm: 0.835 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.546 | tokens per gpu per second (tgs): 1762.936 | TFLOPs: 14.19 |
g0314: [2024-08-02 22:44:58,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=3370, skipped=0, lr=[5.887754240000001e-05, 5.887754240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3370 loss: 2.0708 iter time (s): 4.622 samples/sec: 27.694
g0332:  iteration     3370/10000000 | consumed samples:       431360 | consumed tokens:    883425280 | elapsed time per iteration (ms): 4654.5 | learning rate: 5.888E-05 | global batch size:   128 | lm loss: 2.031322E+00 | loss scale: 131072.0 | grad norm: 0.869 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.500 | tokens per gpu per second (tgs): 1760.013 | TFLOPs: 14.16 |
g0314: [2024-08-02 22:45:44,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=3380, skipped=0, lr=[5.9052305066666674e-05, 5.9052305066666674e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3380 loss: 2.0134 iter time (s): 4.574 samples/sec: 27.983
g0332:  iteration     3380/10000000 | consumed samples:       432640 | consumed tokens:    886046720 | elapsed time per iteration (ms): 4606.7 | learning rate: 5.905E-05 | global batch size:   128 | lm loss: 2.023789E+00 | loss scale: 131072.0 | grad norm: 0.839 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.786 | tokens per gpu per second (tgs): 1778.291 | TFLOPs: 14.31 |
g0314: [2024-08-02 22:46:28,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=3390, skipped=0, lr=[5.922706773333334e-05, 5.922706773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3390 loss: 2.0809 iter time (s): 4.342 samples/sec: 29.477
g0332:  iteration     3390/10000000 | consumed samples:       433920 | consumed tokens:    888668160 | elapsed time per iteration (ms): 4374.9 | learning rate: 5.923E-05 | global batch size:   128 | lm loss: 2.064705E+00 | loss scale: 131072.0 | grad norm: 0.841 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.258 | tokens per gpu per second (tgs): 1872.481 | TFLOPs: 15.07 |
g0314: [2024-08-02 22:47:11,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=3400, skipped=0, lr=[5.9401830400000006e-05, 5.9401830400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3400 loss: 2.0444 iter time (s): 4.264 samples/sec: 30.017
g0332:  iteration     3400/10000000 | consumed samples:       435200 | consumed tokens:    891289600 | elapsed time per iteration (ms): 4296.8 | learning rate: 5.940E-05 | global batch size:   128 | lm loss: 2.034556E+00 | loss scale: 131072.0 | grad norm: 0.821 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.789 | tokens per gpu per second (tgs): 1906.524 | TFLOPs: 15.34 |
g0314: [2024-08-02 22:47:54,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=3410, skipped=0, lr=[5.957659306666667e-05, 5.957659306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3410 loss: 2.0078 iter time (s): 4.253 samples/sec: 30.097
g0332:  iteration     3410/10000000 | consumed samples:       436480 | consumed tokens:    893911040 | elapsed time per iteration (ms): 4285.6 | learning rate: 5.958E-05 | global batch size:   128 | lm loss: 2.043680E+00 | loss scale: 131072.0 | grad norm: 0.813 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.868 | tokens per gpu per second (tgs): 1911.537 | TFLOPs: 15.38 |
g0314: [2024-08-02 22:48:37,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=0, lr=[5.975135573333334e-05, 5.975135573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3420 loss: 2.0376 iter time (s): 4.295 samples/sec: 29.803
g0332:  iteration     3420/10000000 | consumed samples:       437760 | consumed tokens:    896532480 | elapsed time per iteration (ms): 4327.5 | learning rate: 5.975E-05 | global batch size:   128 | lm loss: 2.049236E+00 | loss scale: 131072.0 | grad norm: 0.835 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.579 | tokens per gpu per second (tgs): 1893.030 | TFLOPs: 15.23 |
g0314: [2024-08-02 22:49:20,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=0, lr=[5.9926118400000004e-05, 5.9926118400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3430 loss: 2.0280 iter time (s): 4.209 samples/sec: 30.415
g0332:  iteration     3430/10000000 | consumed samples:       439040 | consumed tokens:    899153920 | elapsed time per iteration (ms): 4241.1 | learning rate: 5.993E-05 | global batch size:   128 | lm loss: 2.014929E+00 | loss scale: 131072.0 | grad norm: 0.815 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.181 | tokens per gpu per second (tgs): 1931.592 | TFLOPs: 15.54 |
g0314: [2024-08-02 22:50:03,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=0, lr=[6.010088106666667e-05, 6.010088106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3440 loss: 1.9881 iter time (s): 4.296 samples/sec: 29.794
g0332:  iteration     3440/10000000 | consumed samples:       440320 | consumed tokens:    901775360 | elapsed time per iteration (ms): 4328.9 | learning rate: 6.010E-05 | global batch size:   128 | lm loss: 2.018742E+00 | loss scale: 131072.0 | grad norm: 0.807 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.569 | tokens per gpu per second (tgs): 1892.414 | TFLOPs: 15.23 |
g0314: [2024-08-02 22:50:46,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=0, lr=[6.027564373333334e-05, 6.027564373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3450 loss: 2.0670 iter time (s): 4.217 samples/sec: 30.350
g0332:  iteration     3450/10000000 | consumed samples:       441600 | consumed tokens:    904396800 | elapsed time per iteration (ms): 4250.5 | learning rate: 6.028E-05 | global batch size:   128 | lm loss: 2.013963E+00 | loss scale: 131072.0 | grad norm: 0.814 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.114 | tokens per gpu per second (tgs): 1927.313 | TFLOPs: 15.51 |
g0314: [2024-08-02 22:51:29,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=0, lr=[6.045040640000001e-05, 6.045040640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3460 loss: 2.0608 iter time (s): 4.324 samples/sec: 29.601
g0332:  iteration     3460/10000000 | consumed samples:       442880 | consumed tokens:    907018240 | elapsed time per iteration (ms): 4356.8 | learning rate: 6.045E-05 | global batch size:   128 | lm loss: 2.003995E+00 | loss scale: 131072.0 | grad norm: 0.816 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.380 | tokens per gpu per second (tgs): 1880.289 | TFLOPs: 15.13 |
g0314: [2024-08-02 22:52:12,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=0, lr=[6.0625169066666676e-05, 6.0625169066666676e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3470 loss: 2.1225 iter time (s): 4.218 samples/sec: 30.343
g0332:  iteration     3470/10000000 | consumed samples:       444160 | consumed tokens:    909639680 | elapsed time per iteration (ms): 4250.9 | learning rate: 6.063E-05 | global batch size:   128 | lm loss: 2.029399E+00 | loss scale: 131072.0 | grad norm: 0.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.111 | tokens per gpu per second (tgs): 1927.123 | TFLOPs: 15.51 |
g0314: [2024-08-02 22:52:55,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=0, lr=[6.079993173333334e-05, 6.079993173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3480 loss: 2.0237 iter time (s): 4.300 samples/sec: 29.769
g0332:  iteration     3480/10000000 | consumed samples:       445440 | consumed tokens:    912261120 | elapsed time per iteration (ms): 4334.3 | learning rate: 6.080E-05 | global batch size:   128 | lm loss: 2.022658E+00 | loss scale: 131072.0 | grad norm: 0.771 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.532 | tokens per gpu per second (tgs): 1890.047 | TFLOPs: 15.21 |
g0314: [2024-08-02 22:53:39,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=0, lr=[6.097469440000001e-05, 6.097469440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3490 loss: 2.1112 iter time (s): 4.329 samples/sec: 29.569
g0332:  iteration     3490/10000000 | consumed samples:       446720 | consumed tokens:    914882560 | elapsed time per iteration (ms): 4361.4 | learning rate: 6.097E-05 | global batch size:   128 | lm loss: 2.021454E+00 | loss scale: 131072.0 | grad norm: 0.802 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.349 | tokens per gpu per second (tgs): 1878.311 | TFLOPs: 15.12 |
g0314: [2024-08-02 22:54:21,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=0, lr=[6.114945706666668e-05, 6.114945706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3500 loss: 1.9951 iter time (s): 4.260 samples/sec: 30.045
g0332:  iteration     3500/10000000 | consumed samples:       448000 | consumed tokens:    917504000 | elapsed time per iteration (ms): 4292.7 | learning rate: 6.115E-05 | global batch size:   128 | lm loss: 2.009694E+00 | loss scale: 131072.0 | grad norm: 0.831 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.818 | tokens per gpu per second (tgs): 1908.341 | TFLOPs: 15.36 |
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0332: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0316: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0320: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0329: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-02 22:54:26,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0314: [2024-08-02 22:55:04,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=3510, skipped=0, lr=[6.132421973333333e-05, 6.132421973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3510 loss: 2.0446 iter time (s): 4.242 samples/sec: 30.176
g0332:  iteration     3510/10000000 | consumed samples:       449280 | consumed tokens:    920125440 | elapsed time per iteration (ms): 4274.5 | learning rate: 6.132E-05 | global batch size:   128 | lm loss: 2.027266E+00 | loss scale: 262144.0 | grad norm: 0.844 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.945 | tokens per gpu per second (tgs): 1916.461 | TFLOPs: 15.42 |
g0314: [2024-08-02 22:55:46,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=3520, skipped=0, lr=[6.14989824e-05, 6.14989824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3520 loss: 2.0592 iter time (s): 4.171 samples/sec: 30.686
g0332:  iteration     3520/10000000 | consumed samples:       450560 | consumed tokens:    922746880 | elapsed time per iteration (ms): 4204.3 | learning rate: 6.150E-05 | global batch size:   128 | lm loss: 2.013291E+00 | loss scale: 262144.0 | grad norm: 0.795 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.445 | tokens per gpu per second (tgs): 1948.501 | TFLOPs: 15.68 |
g0314: [2024-08-02 22:56:29,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=3530, skipped=0, lr=[6.167374506666667e-05, 6.167374506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3530 loss: 1.9696 iter time (s): 4.289 samples/sec: 29.846
g0332:  iteration     3530/10000000 | consumed samples:       451840 | consumed tokens:    925368320 | elapsed time per iteration (ms): 4321.1 | learning rate: 6.167E-05 | global batch size:   128 | lm loss: 1.985860E+00 | loss scale: 262144.0 | grad norm: 0.790 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.622 | tokens per gpu per second (tgs): 1895.809 | TFLOPs: 15.26 |
g0314: [2024-08-02 22:57:13,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=3540, skipped=0, lr=[6.184850773333333e-05, 6.184850773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3540 loss: 2.0605 iter time (s): 4.311 samples/sec: 29.694
g0332:  iteration     3540/10000000 | consumed samples:       453120 | consumed tokens:    927989760 | elapsed time per iteration (ms): 4343.3 | learning rate: 6.185E-05 | global batch size:   128 | lm loss: 2.020235E+00 | loss scale: 262144.0 | grad norm: 0.824 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.471 | tokens per gpu per second (tgs): 1886.139 | TFLOPs: 15.18 |
g0314: [2024-08-02 22:58:00,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=3550, skipped=0, lr=[6.20232704e-05, 6.20232704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3550 loss: 2.0070 iter time (s): 4.662 samples/sec: 27.453
g0332:  iteration     3550/10000000 | consumed samples:       454400 | consumed tokens:    930611200 | elapsed time per iteration (ms): 4695.4 | learning rate: 6.202E-05 | global batch size:   128 | lm loss: 2.002734E+00 | loss scale: 262144.0 | grad norm: 0.856 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.260 | tokens per gpu per second (tgs): 1744.670 | TFLOPs: 14.04 |
g0314: [2024-08-02 22:58:42,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=3560, skipped=0, lr=[6.219803306666666e-05, 6.219803306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3560 loss: 2.1095 iter time (s): 4.223 samples/sec: 30.307
g0332:  iteration     3560/10000000 | consumed samples:       455680 | consumed tokens:    933232640 | elapsed time per iteration (ms): 4255.9 | learning rate: 6.220E-05 | global batch size:   128 | lm loss: 2.035505E+00 | loss scale: 262144.0 | grad norm: 0.797 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.076 | tokens per gpu per second (tgs): 1924.875 | TFLOPs: 15.49 |
g0314: [2024-08-02 22:59:26,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=3570, skipped=0, lr=[6.237279573333333e-05, 6.237279573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3570 loss: 1.9287 iter time (s): 4.361 samples/sec: 29.349
g0332:  iteration     3570/10000000 | consumed samples:       456960 | consumed tokens:    935854080 | elapsed time per iteration (ms): 4393.7 | learning rate: 6.237E-05 | global batch size:   128 | lm loss: 1.988304E+00 | loss scale: 262144.0 | grad norm: 0.863 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.133 | tokens per gpu per second (tgs): 1864.496 | TFLOPs: 15.00 |
g0314: [2024-08-02 23:00:09,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=3580, skipped=0, lr=[6.25475584e-05, 6.25475584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3580 loss: 1.9695 iter time (s): 4.251 samples/sec: 30.108
g0332:  iteration     3580/10000000 | consumed samples:       458240 | consumed tokens:    938475520 | elapsed time per iteration (ms): 4283.8 | learning rate: 6.255E-05 | global batch size:   128 | lm loss: 1.987947E+00 | loss scale: 262144.0 | grad norm: 0.782 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.880 | tokens per gpu per second (tgs): 1912.343 | TFLOPs: 15.39 |
g0314: [2024-08-02 23:00:52,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=3590, skipped=0, lr=[6.272232106666666e-05, 6.272232106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3590 loss: 1.9849 iter time (s): 4.295 samples/sec: 29.801
g0332:  iteration     3590/10000000 | consumed samples:       459520 | consumed tokens:    941096960 | elapsed time per iteration (ms): 4327.6 | learning rate: 6.272E-05 | global batch size:   128 | lm loss: 1.969885E+00 | loss scale: 262144.0 | grad norm: 0.821 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.577 | tokens per gpu per second (tgs): 1892.958 | TFLOPs: 15.23 |
g0314: [2024-08-02 23:01:35,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=3600, skipped=0, lr=[6.289708373333333e-05, 6.289708373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3600 loss: 1.9217 iter time (s): 4.249 samples/sec: 30.126
g0332:  iteration     3600/10000000 | consumed samples:       460800 | consumed tokens:    943718400 | elapsed time per iteration (ms): 4281.1 | learning rate: 6.290E-05 | global batch size:   128 | lm loss: 1.999286E+00 | loss scale: 262144.0 | grad norm: 0.784 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.899 | tokens per gpu per second (tgs): 1913.538 | TFLOPs: 15.40 |
g0314: [2024-08-02 23:02:18,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=3610, skipped=0, lr=[6.30718464e-05, 6.30718464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3610 loss: 2.0120 iter time (s): 4.235 samples/sec: 30.221
g0332:  iteration     3610/10000000 | consumed samples:       462080 | consumed tokens:    946339840 | elapsed time per iteration (ms): 4271.5 | learning rate: 6.307E-05 | global batch size:   128 | lm loss: 2.006424E+00 | loss scale: 262144.0 | grad norm: 0.835 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.966 | tokens per gpu per second (tgs): 1917.828 | TFLOPs: 15.43 |
g0314: [2024-08-02 23:03:01,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=3620, skipped=0, lr=[6.324660906666667e-05, 6.324660906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3620 loss: 2.0728 iter time (s): 4.304 samples/sec: 29.739
g0332:  iteration     3620/10000000 | consumed samples:       463360 | consumed tokens:    948961280 | elapsed time per iteration (ms): 4336.6 | learning rate: 6.325E-05 | global batch size:   128 | lm loss: 2.020143E+00 | loss scale: 262144.0 | grad norm: 0.816 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.516 | tokens per gpu per second (tgs): 1889.045 | TFLOPs: 15.20 |
g0314: [2024-08-02 23:03:45,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=3630, skipped=0, lr=[6.342137173333334e-05, 6.342137173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3630 loss: 1.9848 iter time (s): 4.284 samples/sec: 29.879
g0332:  iteration     3630/10000000 | consumed samples:       464640 | consumed tokens:    951582720 | elapsed time per iteration (ms): 4316.5 | learning rate: 6.342E-05 | global batch size:   128 | lm loss: 2.006822E+00 | loss scale: 262144.0 | grad norm: 0.803 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.654 | tokens per gpu per second (tgs): 1897.840 | TFLOPs: 15.27 |
g0314: [2024-08-02 23:04:27,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=3640, skipped=0, lr=[6.35961344e-05, 6.35961344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3640 loss: 1.9991 iter time (s): 4.186 samples/sec: 30.576
g0332:  iteration     3640/10000000 | consumed samples:       465920 | consumed tokens:    954204160 | elapsed time per iteration (ms): 4218.7 | learning rate: 6.360E-05 | global batch size:   128 | lm loss: 1.993044E+00 | loss scale: 262144.0 | grad norm: 0.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.341 | tokens per gpu per second (tgs): 1941.809 | TFLOPs: 15.63 |
g0314: [2024-08-02 23:05:09,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=3650, skipped=0, lr=[6.377089706666667e-05, 6.377089706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3650 loss: 1.9853 iter time (s): 4.214 samples/sec: 30.372
g0332:  iteration     3650/10000000 | consumed samples:       467200 | consumed tokens:    956825600 | elapsed time per iteration (ms): 4247.4 | learning rate: 6.377E-05 | global batch size:   128 | lm loss: 1.973878E+00 | loss scale: 262144.0 | grad norm: 0.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.136 | tokens per gpu per second (tgs): 1928.704 | TFLOPs: 15.52 |
g0314: [2024-08-02 23:05:51,726] [INFO] [logging.py:96:log_dist] [Rank 0] step=3660, skipped=0, lr=[6.394565973333334e-05, 6.394565973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3660 loss: 1.9842 iter time (s): 4.169 samples/sec: 30.702
g0332:  iteration     3660/10000000 | consumed samples:       468480 | consumed tokens:    959447040 | elapsed time per iteration (ms): 4201.6 | learning rate: 6.395E-05 | global batch size:   128 | lm loss: 2.014912E+00 | loss scale: 262144.0 | grad norm: 0.836 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.464 | tokens per gpu per second (tgs): 1949.727 | TFLOPs: 15.69 |
g0314: [2024-08-02 23:06:34,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=3670, skipped=0, lr=[6.41204224e-05, 6.41204224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3670 loss: 2.0555 iter time (s): 4.266 samples/sec: 30.007
g0332:  iteration     3670/10000000 | consumed samples:       469760 | consumed tokens:    962068480 | elapsed time per iteration (ms): 4298.1 | learning rate: 6.412E-05 | global batch size:   128 | lm loss: 2.020088E+00 | loss scale: 262144.0 | grad norm: 0.779 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.781 | tokens per gpu per second (tgs): 1905.966 | TFLOPs: 15.34 |
g0314: [2024-08-02 23:07:18,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=3680, skipped=0, lr=[6.429518506666667e-05, 6.429518506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3680 loss: 1.9530 iter time (s): 4.335 samples/sec: 29.530
g0332:  iteration     3680/10000000 | consumed samples:       471040 | consumed tokens:    964689920 | elapsed time per iteration (ms): 4375.6 | learning rate: 6.430E-05 | global batch size:   128 | lm loss: 1.986648E+00 | loss scale: 262144.0 | grad norm: 0.787 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.253 | tokens per gpu per second (tgs): 1872.203 | TFLOPs: 15.07 |
g0314: [2024-08-02 23:08:01,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=3690, skipped=0, lr=[6.446994773333334e-05, 6.446994773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3690 loss: 1.9174 iter time (s): 4.261 samples/sec: 30.039
g0332:  iteration     3690/10000000 | consumed samples:       472320 | consumed tokens:    967311360 | elapsed time per iteration (ms): 4293.8 | learning rate: 6.447E-05 | global batch size:   128 | lm loss: 1.985177E+00 | loss scale: 262144.0 | grad norm: 0.734 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.810 | tokens per gpu per second (tgs): 1907.858 | TFLOPs: 15.35 |
g0314: [2024-08-02 23:08:43,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=3700, skipped=0, lr=[6.46447104e-05, 6.46447104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3700 loss: 1.9728 iter time (s): 4.224 samples/sec: 30.303
g0332:  iteration     3700/10000000 | consumed samples:       473600 | consumed tokens:    969932800 | elapsed time per iteration (ms): 4256.6 | learning rate: 6.464E-05 | global batch size:   128 | lm loss: 1.991566E+00 | loss scale: 262144.0 | grad norm: 0.780 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.071 | tokens per gpu per second (tgs): 1924.519 | TFLOPs: 15.49 |
g0314: [2024-08-02 23:09:26,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=3710, skipped=0, lr=[6.481947306666667e-05, 6.481947306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3710 loss: 2.0305 iter time (s): 4.234 samples/sec: 30.233
g0332:  iteration     3710/10000000 | consumed samples:       474880 | consumed tokens:    972554240 | elapsed time per iteration (ms): 4266.6 | learning rate: 6.482E-05 | global batch size:   128 | lm loss: 1.973816E+00 | loss scale: 262144.0 | grad norm: 0.744 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.001 | tokens per gpu per second (tgs): 1920.033 | TFLOPs: 15.45 |
g0314: [2024-08-02 23:10:09,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=3720, skipped=0, lr=[6.499423573333333e-05, 6.499423573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3720 loss: 2.0188 iter time (s): 4.222 samples/sec: 30.315
g0332:  iteration     3720/10000000 | consumed samples:       476160 | consumed tokens:    975175680 | elapsed time per iteration (ms): 4256.0 | learning rate: 6.499E-05 | global batch size:   128 | lm loss: 2.004875E+00 | loss scale: 262144.0 | grad norm: 0.741 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.075 | tokens per gpu per second (tgs): 1924.804 | TFLOPs: 15.49 |
g0314: [2024-08-02 23:10:53,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=3730, skipped=0, lr=[6.51689984e-05, 6.51689984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3730 loss: 2.0154 iter time (s): 4.347 samples/sec: 29.443
g0332:  iteration     3730/10000000 | consumed samples:       477440 | consumed tokens:    977797120 | elapsed time per iteration (ms): 4382.1 | learning rate: 6.517E-05 | global batch size:   128 | lm loss: 1.990820E+00 | loss scale: 262144.0 | grad norm: 0.758 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.210 | tokens per gpu per second (tgs): 1869.435 | TFLOPs: 15.04 |
g0314: [2024-08-02 23:11:37,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=3740, skipped=0, lr=[6.534376106666667e-05, 6.534376106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3740 loss: 1.9474 iter time (s): 4.365 samples/sec: 29.324
g0332:  iteration     3740/10000000 | consumed samples:       478720 | consumed tokens:    980418560 | elapsed time per iteration (ms): 4398.2 | learning rate: 6.534E-05 | global batch size:   128 | lm loss: 1.992694E+00 | loss scale: 262144.0 | grad norm: 0.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.103 | tokens per gpu per second (tgs): 1862.570 | TFLOPs: 14.99 |
g0314: [2024-08-02 23:12:20,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=3750, skipped=0, lr=[6.551852373333333e-05, 6.551852373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3750 loss: 1.9424 iter time (s): 4.353 samples/sec: 29.405
g0332:  iteration     3750/10000000 | consumed samples:       480000 | consumed tokens:    983040000 | elapsed time per iteration (ms): 4385.6 | learning rate: 6.552E-05 | global batch size:   128 | lm loss: 1.953247E+00 | loss scale: 262144.0 | grad norm: 0.755 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.187 | tokens per gpu per second (tgs): 1867.939 | TFLOPs: 15.03 |
g0314: [2024-08-02 23:13:04,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=3760, skipped=0, lr=[6.56932864e-05, 6.56932864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3760 loss: 1.9373 iter time (s): 4.319 samples/sec: 29.638
g0332:  iteration     3760/10000000 | consumed samples:       481280 | consumed tokens:    985661440 | elapsed time per iteration (ms): 4351.5 | learning rate: 6.569E-05 | global batch size:   128 | lm loss: 1.965648E+00 | loss scale: 262144.0 | grad norm: 0.740 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.415 | tokens per gpu per second (tgs): 1882.568 | TFLOPs: 15.15 |
g0314: [2024-08-02 23:13:49,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=3770, skipped=0, lr=[6.586804906666666e-05, 6.586804906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3770 loss: 1.9589 iter time (s): 4.483 samples/sec: 28.555
g0332:  iteration     3770/10000000 | consumed samples:       482560 | consumed tokens:    988282880 | elapsed time per iteration (ms): 4515.3 | learning rate: 6.587E-05 | global batch size:   128 | lm loss: 1.955046E+00 | loss scale: 262144.0 | grad norm: 0.746 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.348 | tokens per gpu per second (tgs): 1814.265 | TFLOPs: 14.60 |
g0314: [2024-08-02 23:14:32,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=3780, skipped=0, lr=[6.604281173333333e-05, 6.604281173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3780 loss: 1.9743 iter time (s): 4.233 samples/sec: 30.239
g0332:  iteration     3780/10000000 | consumed samples:       483840 | consumed tokens:    990904320 | elapsed time per iteration (ms): 4266.1 | learning rate: 6.604E-05 | global batch size:   128 | lm loss: 1.981310E+00 | loss scale: 262144.0 | grad norm: 0.802 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.004 | tokens per gpu per second (tgs): 1920.269 | TFLOPs: 15.45 |
g0314: [2024-08-02 23:15:16,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=3790, skipped=0, lr=[6.62175744e-05, 6.62175744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3790 loss: 1.8933 iter time (s): 4.372 samples/sec: 29.274
g0332:  iteration     3790/10000000 | consumed samples:       485120 | consumed tokens:    993525760 | elapsed time per iteration (ms): 4405.3 | learning rate: 6.622E-05 | global batch size:   128 | lm loss: 1.965437E+00 | loss scale: 262144.0 | grad norm: 0.769 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.056 | tokens per gpu per second (tgs): 1859.569 | TFLOPs: 14.96 |
g0314: [2024-08-02 23:15:59,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=3800, skipped=0, lr=[6.639233706666668e-05, 6.639233706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3800 loss: 1.8904 iter time (s): 4.271 samples/sec: 29.967
g0332:  iteration     3800/10000000 | consumed samples:       486400 | consumed tokens:    996147200 | elapsed time per iteration (ms): 4304.3 | learning rate: 6.639E-05 | global batch size:   128 | lm loss: 1.965907E+00 | loss scale: 262144.0 | grad norm: 0.800 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.738 | tokens per gpu per second (tgs): 1903.202 | TFLOPs: 15.32 |
g0314: [2024-08-02 23:17:52,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=3810, skipped=0, lr=[6.656709973333334e-05, 6.656709973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3810 loss: 1.9414 iter time (s): 11.292 samples/sec: 11.335
g0332:  iteration     3810/10000000 | consumed samples:       487680 | consumed tokens:    998768640 | elapsed time per iteration (ms): 11325.7 | learning rate: 6.657E-05 | global batch size:   128 | lm loss: 1.974991E+00 | loss scale: 262144.0 | grad norm: 0.782 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.302 | tokens per gpu per second (tgs): 723.310 | TFLOPs: 5.82 |
g0314: [2024-08-02 23:19:39,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=3820, skipped=0, lr=[6.674186240000001e-05, 6.674186240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3820 loss: 2.0751 iter time (s): 10.706 samples/sec: 11.956
g0332:  iteration     3820/10000000 | consumed samples:       488960 | consumed tokens:   1001390080 | elapsed time per iteration (ms): 10739.5 | learning rate: 6.674E-05 | global batch size:   128 | lm loss: 1.982166E+00 | loss scale: 262144.0 | grad norm: 0.743 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.919 | tokens per gpu per second (tgs): 762.788 | TFLOPs: 6.14 |
g0314: [2024-08-02 23:20:48,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=3830, skipped=0, lr=[6.691662506666667e-05, 6.691662506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3830 loss: 1.9599 iter time (s): 6.841 samples/sec: 18.710
g0332:  iteration     3830/10000000 | consumed samples:       490240 | consumed tokens:   1004011520 | elapsed time per iteration (ms): 6874.8 | learning rate: 6.692E-05 | global batch size:   128 | lm loss: 1.965440E+00 | loss scale: 262144.0 | grad norm: 0.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.619 | tokens per gpu per second (tgs): 1191.596 | TFLOPs: 9.59 |
g0314: [2024-08-02 23:21:36,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=3840, skipped=0, lr=[6.709138773333334e-05, 6.709138773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3840 loss: 1.9862 iter time (s): 4.720 samples/sec: 27.119
g0332:  iteration     3840/10000000 | consumed samples:       491520 | consumed tokens:   1006632960 | elapsed time per iteration (ms): 4752.7 | learning rate: 6.709E-05 | global batch size:   128 | lm loss: 1.953486E+00 | loss scale: 262144.0 | grad norm: 0.737 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.932 | tokens per gpu per second (tgs): 1723.635 | TFLOPs: 13.87 |
g0314: [2024-08-02 23:22:20,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=3850, skipped=0, lr=[6.72661504e-05, 6.72661504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3850 loss: 2.0393 iter time (s): 4.415 samples/sec: 28.991
g0332:  iteration     3850/10000000 | consumed samples:       492800 | consumed tokens:   1009254400 | elapsed time per iteration (ms): 4447.4 | learning rate: 6.727E-05 | global batch size:   128 | lm loss: 1.971177E+00 | loss scale: 262144.0 | grad norm: 0.737 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.781 | tokens per gpu per second (tgs): 1841.972 | TFLOPs: 14.82 |
g0314: [2024-08-02 23:23:12,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=3860, skipped=0, lr=[6.744091306666667e-05, 6.744091306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3860 loss: 1.9569 iter time (s): 5.193 samples/sec: 24.647
g0332:  iteration     3860/10000000 | consumed samples:       494080 | consumed tokens:   1011875840 | elapsed time per iteration (ms): 5226.7 | learning rate: 6.744E-05 | global batch size:   128 | lm loss: 1.979452E+00 | loss scale: 262144.0 | grad norm: 0.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.490 | tokens per gpu per second (tgs): 1567.337 | TFLOPs: 12.61 |
g0314: [2024-08-02 23:24:32,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=3870, skipped=0, lr=[6.761567573333334e-05, 6.761567573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3870 loss: 1.9900 iter time (s): 7.921 samples/sec: 16.159
g0332:  iteration     3870/10000000 | consumed samples:       495360 | consumed tokens:   1014497280 | elapsed time per iteration (ms): 7955.6 | learning rate: 6.762E-05 | global batch size:   128 | lm loss: 1.942286E+00 | loss scale: 262144.0 | grad norm: 0.712 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.089 | tokens per gpu per second (tgs): 1029.718 | TFLOPs: 8.29 |
g0314: [2024-08-02 23:25:16,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=3880, skipped=0, lr=[6.77904384e-05, 6.77904384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3880 loss: 1.9774 iter time (s): 4.325 samples/sec: 29.596
g0332:  iteration     3880/10000000 | consumed samples:       496640 | consumed tokens:   1017118720 | elapsed time per iteration (ms): 4357.3 | learning rate: 6.779E-05 | global batch size:   128 | lm loss: 1.949416E+00 | loss scale: 262144.0 | grad norm: 0.693 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.376 | tokens per gpu per second (tgs): 1880.057 | TFLOPs: 15.13 |
g0314: [2024-08-02 23:25:57,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=3890, skipped=0, lr=[6.796520106666667e-05, 6.796520106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3890 loss: 2.0327 iter time (s): 4.108 samples/sec: 31.161
g0332:  iteration     3890/10000000 | consumed samples:       497920 | consumed tokens:   1019740160 | elapsed time per iteration (ms): 4140.0 | learning rate: 6.797E-05 | global batch size:   128 | lm loss: 1.968213E+00 | loss scale: 262144.0 | grad norm: 0.759 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.918 | tokens per gpu per second (tgs): 1978.768 | TFLOPs: 15.92 |
g0314: [2024-08-02 23:26:38,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=3900, skipped=0, lr=[6.813996373333334e-05, 6.813996373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3900 loss: 1.9547 iter time (s): 4.031 samples/sec: 31.752
g0332:  iteration     3900/10000000 | consumed samples:       499200 | consumed tokens:   1022361600 | elapsed time per iteration (ms): 4064.0 | learning rate: 6.814E-05 | global batch size:   128 | lm loss: 1.944236E+00 | loss scale: 262144.0 | grad norm: 0.697 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.496 | tokens per gpu per second (tgs): 2015.735 | TFLOPs: 16.22 |
g0314: [2024-08-02 23:27:19,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=3910, skipped=0, lr=[6.83147264e-05, 6.83147264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3910 loss: 1.9860 iter time (s): 4.109 samples/sec: 31.152
g0332:  iteration     3910/10000000 | consumed samples:       500480 | consumed tokens:   1024983040 | elapsed time per iteration (ms): 4141.5 | learning rate: 6.831E-05 | global batch size:   128 | lm loss: 1.971701E+00 | loss scale: 262144.0 | grad norm: 0.700 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.907 | tokens per gpu per second (tgs): 1978.022 | TFLOPs: 15.92 |
g0314: [2024-08-02 23:28:01,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=3920, skipped=0, lr=[6.848948906666667e-05, 6.848948906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3920 loss: 1.8424 iter time (s): 4.204 samples/sec: 30.445
g0332:  iteration     3920/10000000 | consumed samples:       501760 | consumed tokens:   1027604480 | elapsed time per iteration (ms): 4238.5 | learning rate: 6.849E-05 | global batch size:   128 | lm loss: 1.954669E+00 | loss scale: 262144.0 | grad norm: 0.678 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.199 | tokens per gpu per second (tgs): 1932.751 | TFLOPs: 15.55 |
g0314: [2024-08-02 23:28:44,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=3930, skipped=0, lr=[6.866425173333333e-05, 6.866425173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3930 loss: 1.9984 iter time (s): 4.243 samples/sec: 30.167
g0332:  iteration     3930/10000000 | consumed samples:       503040 | consumed tokens:   1030225920 | elapsed time per iteration (ms): 4276.2 | learning rate: 6.866E-05 | global batch size:   128 | lm loss: 1.984991E+00 | loss scale: 262144.0 | grad norm: 0.714 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.933 | tokens per gpu per second (tgs): 1915.715 | TFLOPs: 15.42 |
g0314: [2024-08-02 23:29:26,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=3940, skipped=0, lr=[6.88390144e-05, 6.88390144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3940 loss: 1.9994 iter time (s): 4.157 samples/sec: 30.789
g0332:  iteration     3940/10000000 | consumed samples:       504320 | consumed tokens:   1032847360 | elapsed time per iteration (ms): 4189.7 | learning rate: 6.884E-05 | global batch size:   128 | lm loss: 1.945214E+00 | loss scale: 262144.0 | grad norm: 0.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.551 | tokens per gpu per second (tgs): 1955.269 | TFLOPs: 15.73 |
g0314: [2024-08-02 23:30:10,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=3950, skipped=0, lr=[6.901377706666667e-05, 6.901377706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3950 loss: 1.9371 iter time (s): 4.364 samples/sec: 29.328
g0332:  iteration     3950/10000000 | consumed samples:       505600 | consumed tokens:   1035468800 | elapsed time per iteration (ms): 4403.3 | learning rate: 6.901E-05 | global batch size:   128 | lm loss: 1.961206E+00 | loss scale: 262144.0 | grad norm: 0.686 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.069 | tokens per gpu per second (tgs): 1860.407 | TFLOPs: 14.97 |
g0314: [2024-08-02 23:30:54,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=3960, skipped=0, lr=[6.918853973333333e-05, 6.918853973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3960 loss: 1.9112 iter time (s): 4.367 samples/sec: 29.310
g0332:  iteration     3960/10000000 | consumed samples:       506880 | consumed tokens:   1038090240 | elapsed time per iteration (ms): 4408.5 | learning rate: 6.919E-05 | global batch size:   128 | lm loss: 1.962332E+00 | loss scale: 262144.0 | grad norm: 0.708 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.035 | tokens per gpu per second (tgs): 1858.243 | TFLOPs: 14.95 |
g0314: [2024-08-02 23:31:36,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=3970, skipped=0, lr=[6.93633024e-05, 6.93633024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3970 loss: 1.8965 iter time (s): 4.173 samples/sec: 30.675
g0332:  iteration     3970/10000000 | consumed samples:       508160 | consumed tokens:   1040711680 | elapsed time per iteration (ms): 4206.0 | learning rate: 6.936E-05 | global batch size:   128 | lm loss: 1.963814E+00 | loss scale: 262144.0 | grad norm: 0.679 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.433 | tokens per gpu per second (tgs): 1947.700 | TFLOPs: 15.67 |
g0314: [2024-08-02 23:32:20,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=3980, skipped=0, lr=[6.953806506666668e-05, 6.953806506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3980 loss: 1.9756 iter time (s): 4.314 samples/sec: 29.674
g0332:  iteration     3980/10000000 | consumed samples:       509440 | consumed tokens:   1043333120 | elapsed time per iteration (ms): 4346.4 | learning rate: 6.954E-05 | global batch size:   128 | lm loss: 1.966269E+00 | loss scale: 262144.0 | grad norm: 0.681 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.450 | tokens per gpu per second (tgs): 1884.786 | TFLOPs: 15.17 |
g0314: [2024-08-02 23:33:03,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=3990, skipped=0, lr=[6.971282773333334e-05, 6.971282773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 3990 loss: 1.9390 iter time (s): 4.253 samples/sec: 30.098
g0332:  iteration     3990/10000000 | consumed samples:       510720 | consumed tokens:   1045954560 | elapsed time per iteration (ms): 4285.1 | learning rate: 6.971E-05 | global batch size:   128 | lm loss: 1.987938E+00 | loss scale: 262144.0 | grad norm: 0.686 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.871 | tokens per gpu per second (tgs): 1911.746 | TFLOPs: 15.38 |
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: Grad overflow on iteration 3997
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: Grad overflow on iteration 3997
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: Grad overflow on iteration 3997
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: Grad overflow on iteration 3997
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: Grad overflow on iteration 3997
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: Grad overflow on iteration 3997
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: Grad overflow on iteration 3997
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: Grad overflow on iteration 3997
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: Grad overflow on iteration 3997
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: Grad overflow on iteration 3997
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: Grad overflow on iteration 3997
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0316: Grad overflow on iteration 3997
g0332: Grad overflow on iteration 3997
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: Grad overflow on iteration 3997
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: Grad overflow on iteration 3997
g0318: Grad overflow on iteration 3997
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: Grad overflow on iteration 3997
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: Grad overflow on iteration 3997
g0332: Grad overflow on iteration 3997
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0318: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0316: Grad overflow on iteration 3997
g0325: Grad overflow on iteration 3997
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: Grad overflow on iteration 3997
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0329: Grad overflow on iteration 3997
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0329: Grad overflow on iteration 3997
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0316: Grad overflow on iteration 3997
g0329: Grad overflow on iteration 3997
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: Grad overflow on iteration 3997
g0325: Grad overflow on iteration 3997
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0332: Grad overflow on iteration 3997
g0329: Grad overflow on iteration 3997
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0325: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0314: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:344:_update_scale] 
g0332: Grad overflow on iteration 3997
g0319: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0329: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0316: Grad overflow on iteration 3997
g0332: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0316: [2024-08-02 23:33:37,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
g0314: [2024-08-02 23:33:37,375] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
g0314: [2024-08-02 23:33:45,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=1, lr=[6.988759040000001e-05, 6.988759040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4000 loss: 1.9741 iter time (s): 4.212 samples/sec: 30.391
g0332:  iteration     4000/10000000 | consumed samples:       512000 | consumed tokens:   1048576000 | elapsed time per iteration (ms): 4244.8 | learning rate: 6.989E-05 | global batch size:   128 | lm loss: 1.971094E+00 | loss scale: 131072.0 | grad norm: 0.721 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.155 | tokens per gpu per second (tgs): 1929.911 | TFLOPs: 15.53 |
g0332: ------------------------------------------------------------------------------------------------
g0332:  validation loss at iteration 4000 | lm loss value: 1.961553E+00 | lm loss PPL: 7.110362E+00 | 
g0332: ------------------------------------------------------------------------------------------------
g0314: saving checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: [2024-08-02 23:40:22,955] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
g0332: [2024-08-02 23:40:22,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0332: [2024-08-02 23:40:22,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0332: [2024-08-02 23:40:22,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0314: [2024-08-02 23:40:22,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0314: [2024-08-02 23:40:22,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0314: [2024-08-02 23:40:22,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0319: [2024-08-02 23:40:22,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0319: [2024-08-02 23:40:22,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0319: [2024-08-02 23:40:22,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0318: [2024-08-02 23:40:22,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0318: [2024-08-02 23:40:22,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0318: [2024-08-02 23:40:22,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0316: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0329: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0329: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0329: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0316: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0316: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0320: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0320: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0325: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0325: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0320: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0325: [2024-08-02 23:40:22,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0332: [2024-08-02 23:40:22,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt...
g0318: [2024-08-02 23:40:22,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt...
g0316: [2024-08-02 23:40:22,999] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt...
g0325: [2024-08-02 23:40:22,999] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt...
g0329: [2024-08-02 23:40:22,999] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt...
g0319: [2024-08-02 23:40:23,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt...
g0320: [2024-08-02 23:40:23,005] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt...
g0314: [2024-08-02 23:40:23,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt...
g0319: [2024-08-02 23:40:23,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt.
g0320: [2024-08-02 23:40:23,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt.
g0316: [2024-08-02 23:40:23,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt.
g0314: [2024-08-02 23:40:23,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt.
g0319: [2024-08-02 23:40:23,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt...
g0316: [2024-08-02 23:40:23,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt...
g0314: [2024-08-02 23:40:23,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt...
g0320: [2024-08-02 23:40:23,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt...
g0325: [2024-08-02 23:40:23,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt.
g0318: [2024-08-02 23:40:23,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt.
g0325: [2024-08-02 23:40:23,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt...
g0318: [2024-08-02 23:40:23,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt...
g0332: [2024-08-02 23:40:23,237] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt.
g0332: [2024-08-02 23:40:23,238] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt...
g0332: [2024-08-02 23:40:23,239] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt.
g0329: [2024-08-02 23:40:23,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt.
g0329: [2024-08-02 23:40:23,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt...
g0314: [2024-08-02 23:40:23,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt.
g0332: [2024-08-02 23:40:23,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt...
g0316: [2024-08-02 23:40:23,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt.
g0314: [2024-08-02 23:40:23,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt...
g0316: [2024-08-02 23:40:23,326] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt...
g0325: [2024-08-02 23:40:23,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt.
g0325: [2024-08-02 23:40:23,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt...
g0318: [2024-08-02 23:40:23,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt.
g0318: [2024-08-02 23:40:23,403] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt...
g0329: [2024-08-02 23:40:23,421] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt.
g0316: [2024-08-02 23:40:23,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt.
g0316: [2024-08-02 23:40:23,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt...
g0329: [2024-08-02 23:40:23,455] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt...
g0320: [2024-08-02 23:40:23,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt.
g0314: [2024-08-02 23:40:23,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt.
g0332: [2024-08-02 23:40:23,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt.
g0332: [2024-08-02 23:40:23,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt...
g0320: [2024-08-02 23:40:23,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt...
g0314: [2024-08-02 23:40:23,504] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt...
g0325: [2024-08-02 23:40:23,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt.
g0325: [2024-08-02 23:40:23,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt...
g0320: [2024-08-02 23:40:23,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt.
g0319: [2024-08-02 23:40:23,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt.
g0320: [2024-08-02 23:40:23,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt...
g0319: [2024-08-02 23:40:23,628] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt...
g0314: [2024-08-02 23:40:23,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt.
g0314: [2024-08-02 23:40:23,632] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt
g0314: [2024-08-02 23:40:23,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt...
g0329: [2024-08-02 23:40:23,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt.
g0329: [2024-08-02 23:40:23,697] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt...
g0319: [2024-08-02 23:40:23,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt.
g0319: [2024-08-02 23:40:23,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt...
g0318: [2024-08-02 23:40:24,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt.
g0318: [2024-08-02 23:40:24,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt...
g0332: [2024-08-02 23:40:25,477] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt.
g0332: [2024-08-02 23:40:25,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0316: [2024-08-02 23:40:25,686] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt.
g0316: [2024-08-02 23:40:25,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0325: [2024-08-02 23:40:25,955] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt.
g0325: [2024-08-02 23:40:25,955] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0320: [2024-08-02 23:40:26,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt.
g0320: [2024-08-02 23:40:26,086] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0329: [2024-08-02 23:40:26,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt.
g0329: [2024-08-02 23:40:26,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0319: [2024-08-02 23:40:26,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt.
g0319: [2024-08-02 23:40:26,534] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0318: [2024-08-02 23:40:26,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt.
g0318: [2024-08-02 23:40:26,921] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0314: [2024-08-02 23:40:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt.
g0314: [2024-08-02 23:40:27,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0314:   successfully saved checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: Checkpoint Save GB: 22.521, GB/Sec: 5.5, Latency(second): 4.098
g0332: (min, max) time across ranks (ms):
g0332:     save-checkpoint ................................: (4097.88, 4098.22)
g0314: [2024-08-02 23:41:09,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=4010, skipped=1, lr=[7.006235306666668e-05, 7.006235306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4010 loss: 2.0073 iter time (s): 4.247 samples/sec: 30.141
g0332:  iteration     4010/10000000 | consumed samples:       513280 | consumed tokens:   1051197440 | elapsed time per iteration (ms): 44425.8 | learning rate: 7.006E-05 | global batch size:   128 | lm loss: 1.959536E+00 | loss scale: 131072.0 | grad norm: 0.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.881 | tokens per gpu per second (tgs): 184.397 | TFLOPs: 1.48 |
g0314: [2024-08-02 23:41:53,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=4020, skipped=1, lr=[7.023711573333334e-05, 7.023711573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4020 loss: 1.9331 iter time (s): 4.294 samples/sec: 29.811
g0332:  iteration     4020/10000000 | consumed samples:       514560 | consumed tokens:   1053818880 | elapsed time per iteration (ms): 4327.0 | learning rate: 7.024E-05 | global batch size:   128 | lm loss: 1.958736E+00 | loss scale: 131072.0 | grad norm: 0.699 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.582 | tokens per gpu per second (tgs): 1893.244 | TFLOPs: 15.24 |
g0314: [2024-08-02 23:42:35,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=4030, skipped=1, lr=[7.041187840000001e-05, 7.041187840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4030 loss: 1.9028 iter time (s): 4.206 samples/sec: 30.433
g0332:  iteration     4030/10000000 | consumed samples:       515840 | consumed tokens:   1056440320 | elapsed time per iteration (ms): 4238.3 | learning rate: 7.041E-05 | global batch size:   128 | lm loss: 1.925637E+00 | loss scale: 131072.0 | grad norm: 0.665 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.201 | tokens per gpu per second (tgs): 1932.837 | TFLOPs: 15.55 |
g0314: [2024-08-02 23:43:17,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=4040, skipped=1, lr=[7.058664106666667e-05, 7.058664106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4040 loss: 1.9749 iter time (s): 4.189 samples/sec: 30.556
g0332:  iteration     4040/10000000 | consumed samples:       517120 | consumed tokens:   1059061760 | elapsed time per iteration (ms): 4221.6 | learning rate: 7.059E-05 | global batch size:   128 | lm loss: 1.944323E+00 | loss scale: 131072.0 | grad norm: 0.684 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.321 | tokens per gpu per second (tgs): 1940.514 | TFLOPs: 15.62 |
g0314: [2024-08-02 23:44:14,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=4050, skipped=1, lr=[7.076140373333334e-05, 7.076140373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4050 loss: 1.9084 iter time (s): 5.689 samples/sec: 22.498
g0332:  iteration     4050/10000000 | consumed samples:       518400 | consumed tokens:   1061683200 | elapsed time per iteration (ms): 5722.0 | learning rate: 7.076E-05 | global batch size:   128 | lm loss: 1.952726E+00 | loss scale: 131072.0 | grad norm: 0.727 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.370 | tokens per gpu per second (tgs): 1431.671 | TFLOPs: 11.52 |
g0314: [2024-08-02 23:45:26,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=4060, skipped=1, lr=[7.093616640000001e-05, 7.093616640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4060 loss: 1.9471 iter time (s): 7.113 samples/sec: 17.995
g0332:  iteration     4060/10000000 | consumed samples:       519680 | consumed tokens:   1064304640 | elapsed time per iteration (ms): 7146.1 | learning rate: 7.094E-05 | global batch size:   128 | lm loss: 1.940967E+00 | loss scale: 131072.0 | grad norm: 0.690 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.912 | tokens per gpu per second (tgs): 1146.366 | TFLOPs: 9.22 |
g0314: [2024-08-02 23:46:45,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=4070, skipped=1, lr=[7.111092906666667e-05, 7.111092906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4070 loss: 1.8779 iter time (s): 7.842 samples/sec: 16.323
g0332:  iteration     4070/10000000 | consumed samples:       520960 | consumed tokens:   1066926080 | elapsed time per iteration (ms): 7878.9 | learning rate: 7.111E-05 | global batch size:   128 | lm loss: 1.941128E+00 | loss scale: 131072.0 | grad norm: 0.653 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.246 | tokens per gpu per second (tgs): 1039.733 | TFLOPs: 8.37 |
g0314: [2024-08-02 23:47:54,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=4080, skipped=1, lr=[7.128569173333334e-05, 7.128569173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4080 loss: 1.8917 iter time (s): 6.884 samples/sec: 18.594
g0332:  iteration     4080/10000000 | consumed samples:       522240 | consumed tokens:   1069547520 | elapsed time per iteration (ms): 6917.6 | learning rate: 7.129E-05 | global batch size:   128 | lm loss: 1.937239E+00 | loss scale: 131072.0 | grad norm: 0.657 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.503 | tokens per gpu per second (tgs): 1184.223 | TFLOPs: 9.53 |
g0314: [2024-08-02 23:49:08,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=4090, skipped=1, lr=[7.14604544e-05, 7.14604544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4090 loss: 1.9887 iter time (s): 7.416 samples/sec: 17.260
g0332:  iteration     4090/10000000 | consumed samples:       523520 | consumed tokens:   1072168960 | elapsed time per iteration (ms): 7449.3 | learning rate: 7.146E-05 | global batch size:   128 | lm loss: 1.984386E+00 | loss scale: 131072.0 | grad norm: 0.674 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.183 | tokens per gpu per second (tgs): 1099.699 | TFLOPs: 8.85 |
g0314: [2024-08-02 23:50:17,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=4100, skipped=1, lr=[7.163521706666667e-05, 7.163521706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4100 loss: 1.9390 iter time (s): 6.813 samples/sec: 18.789
g0332:  iteration     4100/10000000 | consumed samples:       524800 | consumed tokens:   1074790400 | elapsed time per iteration (ms): 6846.7 | learning rate: 7.164E-05 | global batch size:   128 | lm loss: 1.948766E+00 | loss scale: 131072.0 | grad norm: 0.687 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.695 | tokens per gpu per second (tgs): 1196.492 | TFLOPs: 9.63 |
g0314: [2024-08-02 23:51:24,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=4110, skipped=1, lr=[7.180997973333334e-05, 7.180997973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4110 loss: 1.9574 iter time (s): 6.665 samples/sec: 19.204
g0332:  iteration     4110/10000000 | consumed samples:       526080 | consumed tokens:   1077411840 | elapsed time per iteration (ms): 6699.7 | learning rate: 7.181E-05 | global batch size:   128 | lm loss: 1.957114E+00 | loss scale: 131072.0 | grad norm: 0.682 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.105 | tokens per gpu per second (tgs): 1222.735 | TFLOPs: 9.84 |
g0314: [2024-08-02 23:52:30,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=4120, skipped=1, lr=[7.19847424e-05, 7.19847424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4120 loss: 1.9102 iter time (s): 6.633 samples/sec: 19.299
g0332:  iteration     4120/10000000 | consumed samples:       527360 | consumed tokens:   1080033280 | elapsed time per iteration (ms): 6666.2 | learning rate: 7.198E-05 | global batch size:   128 | lm loss: 1.927549E+00 | loss scale: 131072.0 | grad norm: 0.652 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.201 | tokens per gpu per second (tgs): 1228.885 | TFLOPs: 9.89 |
g0314: [2024-08-02 23:53:39,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=4130, skipped=1, lr=[7.215950506666667e-05, 7.215950506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4130 loss: 1.9294 iter time (s): 6.838 samples/sec: 18.719
g0332:  iteration     4130/10000000 | consumed samples:       528640 | consumed tokens:   1082654720 | elapsed time per iteration (ms): 6870.8 | learning rate: 7.216E-05 | global batch size:   128 | lm loss: 1.922978E+00 | loss scale: 131072.0 | grad norm: 0.675 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.629 | tokens per gpu per second (tgs): 1192.288 | TFLOPs: 9.59 |
g0314: [2024-08-02 23:54:39,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=4140, skipped=1, lr=[7.233426773333334e-05, 7.233426773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4140 loss: 1.9327 iter time (s): 5.959 samples/sec: 21.478
g0332:  iteration     4140/10000000 | consumed samples:       529920 | consumed tokens:   1085276160 | elapsed time per iteration (ms): 5992.8 | learning rate: 7.233E-05 | global batch size:   128 | lm loss: 1.940011E+00 | loss scale: 131072.0 | grad norm: 0.714 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.359 | tokens per gpu per second (tgs): 1366.978 | TFLOPs: 11.00 |
g0314: [2024-08-02 23:55:24,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=4150, skipped=1, lr=[7.25090304e-05, 7.25090304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4150 loss: 1.9143 iter time (s): 4.500 samples/sec: 28.446
g0332:  iteration     4150/10000000 | consumed samples:       531200 | consumed tokens:   1087897600 | elapsed time per iteration (ms): 4532.9 | learning rate: 7.251E-05 | global batch size:   128 | lm loss: 1.918528E+00 | loss scale: 131072.0 | grad norm: 0.672 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.238 | tokens per gpu per second (tgs): 1807.228 | TFLOPs: 14.54 |
g0314: [2024-08-02 23:56:13,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=4160, skipped=1, lr=[7.268379306666668e-05, 7.268379306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4160 loss: 1.9471 iter time (s): 4.814 samples/sec: 26.591
g0332:  iteration     4160/10000000 | consumed samples:       532480 | consumed tokens:   1090519040 | elapsed time per iteration (ms): 4846.2 | learning rate: 7.268E-05 | global batch size:   128 | lm loss: 1.933576E+00 | loss scale: 131072.0 | grad norm: 0.658 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.413 | tokens per gpu per second (tgs): 1690.408 | TFLOPs: 13.60 |
g0314: [2024-08-02 23:56:57,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=4170, skipped=1, lr=[7.285855573333333e-05, 7.285855573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4170 loss: 1.9902 iter time (s): 4.348 samples/sec: 29.436
g0332:  iteration     4170/10000000 | consumed samples:       533760 | consumed tokens:   1093140480 | elapsed time per iteration (ms): 4381.6 | learning rate: 7.286E-05 | global batch size:   128 | lm loss: 1.948704E+00 | loss scale: 131072.0 | grad norm: 0.678 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.213 | tokens per gpu per second (tgs): 1869.629 | TFLOPs: 15.05 |
g0314: [2024-08-02 23:57:45,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=4180, skipped=1, lr=[7.30333184e-05, 7.30333184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4180 loss: 1.9591 iter time (s): 4.757 samples/sec: 26.908
g0332:  iteration     4180/10000000 | consumed samples:       535040 | consumed tokens:   1095761920 | elapsed time per iteration (ms): 4791.9 | learning rate: 7.303E-05 | global batch size:   128 | lm loss: 1.921666E+00 | loss scale: 131072.0 | grad norm: 0.654 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.712 | tokens per gpu per second (tgs): 1709.537 | TFLOPs: 13.76 |
g0314: [2024-08-02 23:58:29,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=4190, skipped=1, lr=[7.320808106666667e-05, 7.320808106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4190 loss: 1.8795 iter time (s): 4.450 samples/sec: 28.767
g0332:  iteration     4190/10000000 | consumed samples:       536320 | consumed tokens:   1098383360 | elapsed time per iteration (ms): 4482.6 | learning rate: 7.321E-05 | global batch size:   128 | lm loss: 1.920549E+00 | loss scale: 131072.0 | grad norm: 0.686 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.555 | tokens per gpu per second (tgs): 1827.502 | TFLOPs: 14.71 |
g0314: [2024-08-02 23:59:17,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=4200, skipped=1, lr=[7.338284373333333e-05, 7.338284373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4200 loss: 1.9036 iter time (s): 4.765 samples/sec: 26.865
g0332:  iteration     4200/10000000 | consumed samples:       537600 | consumed tokens:   1101004800 | elapsed time per iteration (ms): 4797.5 | learning rate: 7.338E-05 | global batch size:   128 | lm loss: 1.934212E+00 | loss scale: 131072.0 | grad norm: 0.625 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.681 | tokens per gpu per second (tgs): 1707.571 | TFLOPs: 13.74 |
g0314: [2024-08-03 00:00:01,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=4210, skipped=1, lr=[7.35576064e-05, 7.35576064e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4210 loss: 1.8334 iter time (s): 4.328 samples/sec: 29.575
g0332:  iteration     4210/10000000 | consumed samples:       538880 | consumed tokens:   1103626240 | elapsed time per iteration (ms): 4360.5 | learning rate: 7.356E-05 | global batch size:   128 | lm loss: 1.932089E+00 | loss scale: 131072.0 | grad norm: 0.623 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.354 | tokens per gpu per second (tgs): 1878.685 | TFLOPs: 15.12 |
g0314: [2024-08-03 00:00:45,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=4220, skipped=1, lr=[7.373236906666666e-05, 7.373236906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4220 loss: 1.9417 iter time (s): 4.414 samples/sec: 28.999
g0332:  iteration     4220/10000000 | consumed samples:       540160 | consumed tokens:   1106247680 | elapsed time per iteration (ms): 4446.8 | learning rate: 7.373E-05 | global batch size:   128 | lm loss: 1.929961E+00 | loss scale: 131072.0 | grad norm: 0.660 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.785 | tokens per gpu per second (tgs): 1842.239 | TFLOPs: 14.82 |
g0314: [2024-08-03 00:01:34,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=4230, skipped=1, lr=[7.390713173333333e-05, 7.390713173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4230 loss: 1.8582 iter time (s): 4.805 samples/sec: 26.637
g0332:  iteration     4230/10000000 | consumed samples:       541440 | consumed tokens:   1108869120 | elapsed time per iteration (ms): 4837.9 | learning rate: 7.391E-05 | global batch size:   128 | lm loss: 1.912187E+00 | loss scale: 131072.0 | grad norm: 0.630 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.458 | tokens per gpu per second (tgs): 1693.302 | TFLOPs: 13.63 |
g0314: [2024-08-03 00:02:18,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=4240, skipped=1, lr=[7.40818944e-05, 7.40818944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4240 loss: 1.9013 iter time (s): 4.338 samples/sec: 29.503
g0332:  iteration     4240/10000000 | consumed samples:       542720 | consumed tokens:   1111490560 | elapsed time per iteration (ms): 4371.7 | learning rate: 7.408E-05 | global batch size:   128 | lm loss: 1.933106E+00 | loss scale: 131072.0 | grad norm: 0.651 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.279 | tokens per gpu per second (tgs): 1873.887 | TFLOPs: 15.08 |
g0314: [2024-08-03 00:03:02,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=4250, skipped=1, lr=[7.425665706666666e-05, 7.425665706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4250 loss: 1.8709 iter time (s): 4.413 samples/sec: 29.007
g0332:  iteration     4250/10000000 | consumed samples:       544000 | consumed tokens:   1114112000 | elapsed time per iteration (ms): 4446.1 | learning rate: 7.426E-05 | global batch size:   128 | lm loss: 1.909772E+00 | loss scale: 131072.0 | grad norm: 0.616 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.789 | tokens per gpu per second (tgs): 1842.494 | TFLOPs: 14.83 |
g0314: [2024-08-03 00:03:44,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=4260, skipped=1, lr=[7.443141973333333e-05, 7.443141973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4260 loss: 1.9993 iter time (s): 4.163 samples/sec: 30.745
g0332:  iteration     4260/10000000 | consumed samples:       545280 | consumed tokens:   1116733440 | elapsed time per iteration (ms): 4196.7 | learning rate: 7.443E-05 | global batch size:   128 | lm loss: 1.913629E+00 | loss scale: 131072.0 | grad norm: 0.621 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.500 | tokens per gpu per second (tgs): 1952.025 | TFLOPs: 15.71 |
g0314: [2024-08-03 00:04:26,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=4270, skipped=1, lr=[7.46061824e-05, 7.46061824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4270 loss: 1.9417 iter time (s): 4.121 samples/sec: 31.057
g0332:  iteration     4270/10000000 | consumed samples:       546560 | consumed tokens:   1119354880 | elapsed time per iteration (ms): 4154.9 | learning rate: 7.461E-05 | global batch size:   128 | lm loss: 1.917853E+00 | loss scale: 131072.0 | grad norm: 0.635 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.807 | tokens per gpu per second (tgs): 1971.640 | TFLOPs: 15.87 |
g0314: [2024-08-03 00:05:08,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=4280, skipped=1, lr=[7.478094506666666e-05, 7.478094506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4280 loss: 1.8690 iter time (s): 4.198 samples/sec: 30.490
g0332:  iteration     4280/10000000 | consumed samples:       547840 | consumed tokens:   1121976320 | elapsed time per iteration (ms): 4230.6 | learning rate: 7.478E-05 | global batch size:   128 | lm loss: 1.907034E+00 | loss scale: 131072.0 | grad norm: 0.614 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.256 | tokens per gpu per second (tgs): 1936.382 | TFLOPs: 15.58 |
g0314: [2024-08-03 00:05:49,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=4290, skipped=1, lr=[7.495570773333334e-05, 7.495570773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4290 loss: 1.8689 iter time (s): 4.114 samples/sec: 31.110
g0332:  iteration     4290/10000000 | consumed samples:       549120 | consumed tokens:   1124597760 | elapsed time per iteration (ms): 4147.1 | learning rate: 7.496E-05 | global batch size:   128 | lm loss: 1.900229E+00 | loss scale: 131072.0 | grad norm: 0.614 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.865 | tokens per gpu per second (tgs): 1975.370 | TFLOPs: 15.90 |
g0314: [2024-08-03 00:06:31,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=4300, skipped=1, lr=[7.51304704e-05, 7.51304704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4300 loss: 1.9682 iter time (s): 4.099 samples/sec: 31.224
g0332:  iteration     4300/10000000 | consumed samples:       550400 | consumed tokens:   1127219200 | elapsed time per iteration (ms): 4132.5 | learning rate: 7.513E-05 | global batch size:   128 | lm loss: 1.939418E+00 | loss scale: 131072.0 | grad norm: 0.625 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.974 | tokens per gpu per second (tgs): 1982.351 | TFLOPs: 15.95 |
g0314: [2024-08-03 00:07:13,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=4310, skipped=1, lr=[7.530523306666667e-05, 7.530523306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4310 loss: 1.9626 iter time (s): 4.175 samples/sec: 30.657
g0332:  iteration     4310/10000000 | consumed samples:       551680 | consumed tokens:   1129840640 | elapsed time per iteration (ms): 4207.7 | learning rate: 7.531E-05 | global batch size:   128 | lm loss: 1.934660E+00 | loss scale: 131072.0 | grad norm: 0.619 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.421 | tokens per gpu per second (tgs): 1946.927 | TFLOPs: 15.67 |
g0314: [2024-08-03 00:07:55,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=4320, skipped=1, lr=[7.547999573333334e-05, 7.547999573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4320 loss: 1.9231 iter time (s): 4.153 samples/sec: 30.819
g0332:  iteration     4320/10000000 | consumed samples:       552960 | consumed tokens:   1132462080 | elapsed time per iteration (ms): 4187.8 | learning rate: 7.548E-05 | global batch size:   128 | lm loss: 1.919865E+00 | loss scale: 131072.0 | grad norm: 0.635 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.565 | tokens per gpu per second (tgs): 1956.161 | TFLOPs: 15.74 |
g0314: [2024-08-03 00:08:37,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=4330, skipped=1, lr=[7.56547584e-05, 7.56547584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4330 loss: 1.8551 iter time (s): 4.160 samples/sec: 30.766
g0332:  iteration     4330/10000000 | consumed samples:       554240 | consumed tokens:   1135083520 | elapsed time per iteration (ms): 4193.0 | learning rate: 7.565E-05 | global batch size:   128 | lm loss: 1.901604E+00 | loss scale: 131072.0 | grad norm: 0.656 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.527 | tokens per gpu per second (tgs): 1953.748 | TFLOPs: 15.72 |
g0314: [2024-08-03 00:09:22,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=4340, skipped=1, lr=[7.582952106666667e-05, 7.582952106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4340 loss: 1.9544 iter time (s): 4.526 samples/sec: 28.283
g0332:  iteration     4340/10000000 | consumed samples:       555520 | consumed tokens:   1137704960 | elapsed time per iteration (ms): 4558.5 | learning rate: 7.583E-05 | global batch size:   128 | lm loss: 1.928452E+00 | loss scale: 131072.0 | grad norm: 0.632 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.080 | tokens per gpu per second (tgs): 1797.100 | TFLOPs: 14.46 |
g0314: [2024-08-03 00:10:04,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=4350, skipped=1, lr=[7.600428373333334e-05, 7.600428373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4350 loss: 1.9925 iter time (s): 4.174 samples/sec: 30.666
g0332:  iteration     4350/10000000 | consumed samples:       556800 | consumed tokens:   1140326400 | elapsed time per iteration (ms): 4206.4 | learning rate: 7.600E-05 | global batch size:   128 | lm loss: 1.911261E+00 | loss scale: 131072.0 | grad norm: 0.623 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.430 | tokens per gpu per second (tgs): 1947.504 | TFLOPs: 15.67 |
g0314: [2024-08-03 00:10:46,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=4360, skipped=1, lr=[7.61790464e-05, 7.61790464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4360 loss: 1.9046 iter time (s): 4.149 samples/sec: 30.853
g0332:  iteration     4360/10000000 | consumed samples:       558080 | consumed tokens:   1142947840 | elapsed time per iteration (ms): 4181.6 | learning rate: 7.618E-05 | global batch size:   128 | lm loss: 1.918972E+00 | loss scale: 131072.0 | grad norm: 0.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.610 | tokens per gpu per second (tgs): 1959.059 | TFLOPs: 15.76 |
g0314: [2024-08-03 00:11:28,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=4370, skipped=1, lr=[7.635380906666667e-05, 7.635380906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4370 loss: 1.8839 iter time (s): 4.191 samples/sec: 30.542
g0332:  iteration     4370/10000000 | consumed samples:       559360 | consumed tokens:   1145569280 | elapsed time per iteration (ms): 4223.5 | learning rate: 7.635E-05 | global batch size:   128 | lm loss: 1.897748E+00 | loss scale: 131072.0 | grad norm: 0.617 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.616 | TFLOPs: 15.61 |
g0314: [2024-08-03 00:12:12,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=4380, skipped=1, lr=[7.652857173333333e-05, 7.652857173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4380 loss: 1.9242 iter time (s): 4.302 samples/sec: 29.754
g0332:  iteration     4380/10000000 | consumed samples:       560640 | consumed tokens:   1148190720 | elapsed time per iteration (ms): 4335.2 | learning rate: 7.653E-05 | global batch size:   128 | lm loss: 1.910672E+00 | loss scale: 131072.0 | grad norm: 0.613 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.526 | tokens per gpu per second (tgs): 1889.645 | TFLOPs: 15.21 |
g0314: [2024-08-03 00:12:53,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=4390, skipped=1, lr=[7.67033344e-05, 7.67033344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4390 loss: 1.9459 iter time (s): 4.158 samples/sec: 30.782
g0332:  iteration     4390/10000000 | consumed samples:       561920 | consumed tokens:   1150812160 | elapsed time per iteration (ms): 4190.6 | learning rate: 7.670E-05 | global batch size:   128 | lm loss: 1.907557E+00 | loss scale: 131072.0 | grad norm: 0.639 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.545 | tokens per gpu per second (tgs): 1954.872 | TFLOPs: 15.73 |
g0314: [2024-08-03 00:13:35,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=4400, skipped=1, lr=[7.687809706666667e-05, 7.687809706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4400 loss: 1.9058 iter time (s): 4.144 samples/sec: 30.884
g0332:  iteration     4400/10000000 | consumed samples:       563200 | consumed tokens:   1153433600 | elapsed time per iteration (ms): 4176.9 | learning rate: 7.688E-05 | global batch size:   128 | lm loss: 1.902666E+00 | loss scale: 131072.0 | grad norm: 0.608 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.645 | tokens per gpu per second (tgs): 1961.280 | TFLOPs: 15.78 |
g0314: [2024-08-03 00:14:19,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=4410, skipped=1, lr=[7.705285973333333e-05, 7.705285973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4410 loss: 1.8802 iter time (s): 4.355 samples/sec: 29.391
g0332:  iteration     4410/10000000 | consumed samples:       564480 | consumed tokens:   1156055040 | elapsed time per iteration (ms): 4387.7 | learning rate: 7.705E-05 | global batch size:   128 | lm loss: 1.919016E+00 | loss scale: 131072.0 | grad norm: 0.593 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.172 | tokens per gpu per second (tgs): 1867.020 | TFLOPs: 15.02 |
g0314: [2024-08-03 00:15:02,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=4420, skipped=1, lr=[7.72276224e-05, 7.72276224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4420 loss: 1.7960 iter time (s): 4.304 samples/sec: 29.738
g0332:  iteration     4420/10000000 | consumed samples:       565760 | consumed tokens:   1158676480 | elapsed time per iteration (ms): 4337.0 | learning rate: 7.723E-05 | global batch size:   128 | lm loss: 1.898720E+00 | loss scale: 131072.0 | grad norm: 0.608 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.513 | tokens per gpu per second (tgs): 1888.852 | TFLOPs: 15.20 |
g0314: [2024-08-03 00:15:43,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=4430, skipped=1, lr=[7.740238506666667e-05, 7.740238506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4430 loss: 2.0005 iter time (s): 4.046 samples/sec: 31.636
g0332:  iteration     4430/10000000 | consumed samples:       567040 | consumed tokens:   1161297920 | elapsed time per iteration (ms): 4078.7 | learning rate: 7.740E-05 | global batch size:   128 | lm loss: 1.925942E+00 | loss scale: 131072.0 | grad norm: 0.623 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.382 | tokens per gpu per second (tgs): 2008.470 | TFLOPs: 16.16 |
g0314: [2024-08-03 00:16:24,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=4440, skipped=1, lr=[7.757714773333333e-05, 7.757714773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4440 loss: 2.0053 iter time (s): 4.017 samples/sec: 31.863
g0332:  iteration     4440/10000000 | consumed samples:       568320 | consumed tokens:   1163919360 | elapsed time per iteration (ms): 4050.0 | learning rate: 7.758E-05 | global batch size:   128 | lm loss: 1.905339E+00 | loss scale: 131072.0 | grad norm: 0.596 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.605 | tokens per gpu per second (tgs): 2022.704 | TFLOPs: 16.28 |
g0314: [2024-08-03 00:17:05,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=4450, skipped=1, lr=[7.77519104e-05, 7.77519104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4450 loss: 1.9182 iter time (s): 4.047 samples/sec: 31.630
g0332:  iteration     4450/10000000 | consumed samples:       569600 | consumed tokens:   1166540800 | elapsed time per iteration (ms): 4079.6 | learning rate: 7.775E-05 | global batch size:   128 | lm loss: 1.913114E+00 | loss scale: 131072.0 | grad norm: 0.622 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.376 | tokens per gpu per second (tgs): 2008.043 | TFLOPs: 16.16 |
g0314: [2024-08-03 00:17:45,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=4460, skipped=1, lr=[7.792667306666666e-05, 7.792667306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4460 loss: 1.9154 iter time (s): 4.058 samples/sec: 31.542
g0332:  iteration     4460/10000000 | consumed samples:       570880 | consumed tokens:   1169162240 | elapsed time per iteration (ms): 4090.4 | learning rate: 7.793E-05 | global batch size:   128 | lm loss: 1.905531E+00 | loss scale: 131072.0 | grad norm: 0.584 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.293 | tokens per gpu per second (tgs): 2002.728 | TFLOPs: 16.12 |
g0314: [2024-08-03 00:18:28,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=4470, skipped=1, lr=[7.810143573333334e-05, 7.810143573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4470 loss: 1.9081 iter time (s): 4.184 samples/sec: 30.591
g0332:  iteration     4470/10000000 | consumed samples:       572160 | consumed tokens:   1171783680 | elapsed time per iteration (ms): 4216.6 | learning rate: 7.810E-05 | global batch size:   128 | lm loss: 1.917137E+00 | loss scale: 131072.0 | grad norm: 0.590 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.356 | tokens per gpu per second (tgs): 1942.797 | TFLOPs: 15.63 |
g0314: [2024-08-03 00:19:09,982] [INFO] [logging.py:96:log_dist] [Rank 0] step=4480, skipped=1, lr=[7.827619840000001e-05, 7.827619840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4480 loss: 1.9252 iter time (s): 4.151 samples/sec: 30.834
g0332:  iteration     4480/10000000 | consumed samples:       573440 | consumed tokens:   1174405120 | elapsed time per iteration (ms): 4183.9 | learning rate: 7.828E-05 | global batch size:   128 | lm loss: 1.916203E+00 | loss scale: 131072.0 | grad norm: 0.608 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.594 | tokens per gpu per second (tgs): 1957.993 | TFLOPs: 15.76 |
g0314: [2024-08-03 00:19:52,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=4490, skipped=1, lr=[7.845096106666667e-05, 7.845096106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4490 loss: 1.9281 iter time (s): 4.168 samples/sec: 30.707
g0332:  iteration     4490/10000000 | consumed samples:       574720 | consumed tokens:   1177026560 | elapsed time per iteration (ms): 4202.0 | learning rate: 7.845E-05 | global batch size:   128 | lm loss: 1.907179E+00 | loss scale: 131072.0 | grad norm: 0.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.461 | tokens per gpu per second (tgs): 1949.528 | TFLOPs: 15.69 |
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0316: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0329: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0319: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0314: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0318: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0320: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0325: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0332: [2024-08-03 00:20:28,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0314: [2024-08-03 00:20:33,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=4500, skipped=1, lr=[7.862572373333334e-05, 7.862572373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4500 loss: 1.8682 iter time (s): 4.068 samples/sec: 31.467
g0332:  iteration     4500/10000000 | consumed samples:       576000 | consumed tokens:   1179648000 | elapsed time per iteration (ms): 4100.5 | learning rate: 7.863E-05 | global batch size:   128 | lm loss: 1.883772E+00 | loss scale: 262144.0 | grad norm: 0.641 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.216 | tokens per gpu per second (tgs): 1997.805 | TFLOPs: 16.08 |
g0314: [2024-08-03 00:21:14,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=4510, skipped=1, lr=[7.880048640000001e-05, 7.880048640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4510 loss: 1.9164 iter time (s): 4.137 samples/sec: 30.943
g0332:  iteration     4510/10000000 | consumed samples:       577280 | consumed tokens:   1182269440 | elapsed time per iteration (ms): 4169.1 | learning rate: 7.880E-05 | global batch size:   128 | lm loss: 1.912446E+00 | loss scale: 262144.0 | grad norm: 0.602 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.702 | tokens per gpu per second (tgs): 1964.923 | TFLOPs: 15.81 |
g0314: [2024-08-03 00:21:55,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=4520, skipped=1, lr=[7.897524906666667e-05, 7.897524906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4520 loss: 1.9270 iter time (s): 4.060 samples/sec: 31.524
g0332:  iteration     4520/10000000 | consumed samples:       578560 | consumed tokens:   1184890880 | elapsed time per iteration (ms): 4093.5 | learning rate: 7.898E-05 | global batch size:   128 | lm loss: 1.896466E+00 | loss scale: 262144.0 | grad norm: 0.611 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.269 | tokens per gpu per second (tgs): 2001.202 | TFLOPs: 16.10 |
g0314: [2024-08-03 00:22:38,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=4530, skipped=1, lr=[7.915001173333334e-05, 7.915001173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4530 loss: 1.8628 iter time (s): 4.258 samples/sec: 30.064
g0332:  iteration     4530/10000000 | consumed samples:       579840 | consumed tokens:   1187512320 | elapsed time per iteration (ms): 4290.5 | learning rate: 7.915E-05 | global batch size:   128 | lm loss: 1.907707E+00 | loss scale: 262144.0 | grad norm: 0.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.833 | tokens per gpu per second (tgs): 1909.343 | TFLOPs: 15.36 |
g0314: [2024-08-03 00:23:21,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=4540, skipped=1, lr=[7.93247744e-05, 7.93247744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4540 loss: 1.8965 iter time (s): 4.237 samples/sec: 30.212
g0332:  iteration     4540/10000000 | consumed samples:       581120 | consumed tokens:   1190133760 | elapsed time per iteration (ms): 4269.8 | learning rate: 7.932E-05 | global batch size:   128 | lm loss: 1.876557E+00 | loss scale: 262144.0 | grad norm: 0.585 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.978 | tokens per gpu per second (tgs): 1918.585 | TFLOPs: 15.44 |
g0314: [2024-08-03 00:24:04,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=4550, skipped=1, lr=[7.949953706666667e-05, 7.949953706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4550 loss: 1.9035 iter time (s): 4.245 samples/sec: 30.152
g0332:  iteration     4550/10000000 | consumed samples:       582400 | consumed tokens:   1192755200 | elapsed time per iteration (ms): 4278.1 | learning rate: 7.950E-05 | global batch size:   128 | lm loss: 1.905260E+00 | loss scale: 262144.0 | grad norm: 0.586 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.920 | tokens per gpu per second (tgs): 1914.854 | TFLOPs: 15.41 |
g0314: [2024-08-03 00:24:46,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=4560, skipped=1, lr=[7.967429973333334e-05, 7.967429973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4560 loss: 1.9258 iter time (s): 4.196 samples/sec: 30.505
g0332:  iteration     4560/10000000 | consumed samples:       583680 | consumed tokens:   1195376640 | elapsed time per iteration (ms): 4229.4 | learning rate: 7.967E-05 | global batch size:   128 | lm loss: 1.909686E+00 | loss scale: 262144.0 | grad norm: 0.608 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.264 | tokens per gpu per second (tgs): 1936.905 | TFLOPs: 15.59 |
g0314: [2024-08-03 00:25:29,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=4570, skipped=1, lr=[7.98490624e-05, 7.98490624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4570 loss: 1.8395 iter time (s): 4.244 samples/sec: 30.162
g0332:  iteration     4570/10000000 | consumed samples:       584960 | consumed tokens:   1197998080 | elapsed time per iteration (ms): 4276.2 | learning rate: 7.985E-05 | global batch size:   128 | lm loss: 1.878127E+00 | loss scale: 262144.0 | grad norm: 0.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.933 | tokens per gpu per second (tgs): 1915.734 | TFLOPs: 15.42 |
g0314: [2024-08-03 00:26:11,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=4580, skipped=1, lr=[8.002382506666667e-05, 8.002382506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4580 loss: 1.9421 iter time (s): 4.175 samples/sec: 30.661
g0332:  iteration     4580/10000000 | consumed samples:       586240 | consumed tokens:   1200619520 | elapsed time per iteration (ms): 4207.1 | learning rate: 8.002E-05 | global batch size:   128 | lm loss: 1.921260E+00 | loss scale: 262144.0 | grad norm: 0.582 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.424 | tokens per gpu per second (tgs): 1947.164 | TFLOPs: 15.67 |
g0314: [2024-08-03 00:26:53,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=4590, skipped=1, lr=[8.019858773333334e-05, 8.019858773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4590 loss: 1.8599 iter time (s): 4.251 samples/sec: 30.112
g0332:  iteration     4590/10000000 | consumed samples:       587520 | consumed tokens:   1203240960 | elapsed time per iteration (ms): 4283.3 | learning rate: 8.020E-05 | global batch size:   128 | lm loss: 1.924483E+00 | loss scale: 262144.0 | grad norm: 0.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.883 | tokens per gpu per second (tgs): 1912.535 | TFLOPs: 15.39 |
g0314: [2024-08-03 00:27:36,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=4600, skipped=1, lr=[8.03733504e-05, 8.03733504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4600 loss: 1.8337 iter time (s): 4.189 samples/sec: 30.554
g0332:  iteration     4600/10000000 | consumed samples:       588800 | consumed tokens:   1205862400 | elapsed time per iteration (ms): 4221.9 | learning rate: 8.037E-05 | global batch size:   128 | lm loss: 1.883876E+00 | loss scale: 262144.0 | grad norm: 0.621 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.318 | tokens per gpu per second (tgs): 1940.378 | TFLOPs: 15.61 |
g0314: [2024-08-03 00:28:41,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=4610, skipped=1, lr=[8.054811306666667e-05, 8.054811306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4610 loss: 1.9037 iter time (s): 6.448 samples/sec: 19.851
g0332:  iteration     4610/10000000 | consumed samples:       590080 | consumed tokens:   1208483840 | elapsed time per iteration (ms): 6480.7 | learning rate: 8.055E-05 | global batch size:   128 | lm loss: 1.900147E+00 | loss scale: 262144.0 | grad norm: 0.620 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.751 | tokens per gpu per second (tgs): 1264.057 | TFLOPs: 10.17 |
g0314: [2024-08-03 00:29:35,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=4620, skipped=1, lr=[8.072287573333333e-05, 8.072287573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4620 loss: 1.8895 iter time (s): 5.418 samples/sec: 23.627
g0332:  iteration     4620/10000000 | consumed samples:       591360 | consumed tokens:   1211105280 | elapsed time per iteration (ms): 5450.6 | learning rate: 8.072E-05 | global batch size:   128 | lm loss: 1.888894E+00 | loss scale: 262144.0 | grad norm: 0.595 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.484 | tokens per gpu per second (tgs): 1502.957 | TFLOPs: 12.09 |
g0314: [2024-08-03 00:30:25,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=4630, skipped=1, lr=[8.08976384e-05, 8.08976384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4630 loss: 1.9112 iter time (s): 4.940 samples/sec: 25.911
g0332:  iteration     4630/10000000 | consumed samples:       592640 | consumed tokens:   1213726720 | elapsed time per iteration (ms): 4973.0 | learning rate: 8.090E-05 | global batch size:   128 | lm loss: 1.877930E+00 | loss scale: 262144.0 | grad norm: 0.576 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.739 | tokens per gpu per second (tgs): 1647.289 | TFLOPs: 13.26 |
g0314: [2024-08-03 00:31:07,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=4640, skipped=1, lr=[8.107240106666668e-05, 8.107240106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4640 loss: 1.8410 iter time (s): 4.145 samples/sec: 30.881
g0332:  iteration     4640/10000000 | consumed samples:       593920 | consumed tokens:   1216348160 | elapsed time per iteration (ms): 4178.2 | learning rate: 8.107E-05 | global batch size:   128 | lm loss: 1.896489E+00 | loss scale: 262144.0 | grad norm: 0.557 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.635 | tokens per gpu per second (tgs): 1960.659 | TFLOPs: 15.78 |
g0314: [2024-08-03 00:31:48,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=4650, skipped=1, lr=[8.124716373333335e-05, 8.124716373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4650 loss: 1.8520 iter time (s): 4.162 samples/sec: 30.753
g0332:  iteration     4650/10000000 | consumed samples:       595200 | consumed tokens:   1218969600 | elapsed time per iteration (ms): 4196.2 | learning rate: 8.125E-05 | global batch size:   128 | lm loss: 1.882210E+00 | loss scale: 262144.0 | grad norm: 0.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.504 | tokens per gpu per second (tgs): 1952.236 | TFLOPs: 15.71 |
g0314: [2024-08-03 00:32:30,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=4660, skipped=1, lr=[8.142192640000001e-05, 8.142192640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4660 loss: 1.8708 iter time (s): 4.164 samples/sec: 30.737
g0332:  iteration     4660/10000000 | consumed samples:       596480 | consumed tokens:   1221591040 | elapsed time per iteration (ms): 4196.7 | learning rate: 8.142E-05 | global batch size:   128 | lm loss: 1.874105E+00 | loss scale: 262144.0 | grad norm: 0.572 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.500 | tokens per gpu per second (tgs): 1951.993 | TFLOPs: 15.71 |
g0314: [2024-08-03 00:33:12,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=4670, skipped=1, lr=[8.159668906666668e-05, 8.159668906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4670 loss: 1.7980 iter time (s): 4.156 samples/sec: 30.796
g0332:  iteration     4670/10000000 | consumed samples:       597760 | consumed tokens:   1224212480 | elapsed time per iteration (ms): 4190.7 | learning rate: 8.160E-05 | global batch size:   128 | lm loss: 1.888829E+00 | loss scale: 262144.0 | grad norm: 0.563 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.544 | tokens per gpu per second (tgs): 1954.826 | TFLOPs: 15.73 |
g0314: [2024-08-03 00:34:34,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=4680, skipped=1, lr=[8.177145173333334e-05, 8.177145173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4680 loss: 1.9944 iter time (s): 8.111 samples/sec: 15.781
g0332:  iteration     4680/10000000 | consumed samples:       599040 | consumed tokens:   1226833920 | elapsed time per iteration (ms): 8143.8 | learning rate: 8.177E-05 | global batch size:   128 | lm loss: 1.885748E+00 | loss scale: 262144.0 | grad norm: 0.580 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.717 | tokens per gpu per second (tgs): 1005.913 | TFLOPs: 8.09 |
g0314: [2024-08-03 00:35:31,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=4690, skipped=1, lr=[8.194621440000001e-05, 8.194621440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4690 loss: 1.8174 iter time (s): 5.690 samples/sec: 22.496
g0332:  iteration     4690/10000000 | consumed samples:       600320 | consumed tokens:   1229455360 | elapsed time per iteration (ms): 5733.1 | learning rate: 8.195E-05 | global batch size:   128 | lm loss: 1.872912E+00 | loss scale: 262144.0 | grad norm: 0.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.326 | tokens per gpu per second (tgs): 1428.889 | TFLOPs: 11.50 |
g0314: [2024-08-03 00:36:15,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=4700, skipped=1, lr=[8.212097706666668e-05, 8.212097706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4700 loss: 1.8925 iter time (s): 4.307 samples/sec: 29.719
g0332:  iteration     4700/10000000 | consumed samples:       601600 | consumed tokens:   1232076800 | elapsed time per iteration (ms): 4340.7 | learning rate: 8.212E-05 | global batch size:   128 | lm loss: 1.882775E+00 | loss scale: 262144.0 | grad norm: 0.576 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.489 | tokens per gpu per second (tgs): 1887.272 | TFLOPs: 15.19 |
g0314: [2024-08-03 00:36:57,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=4710, skipped=1, lr=[8.229573973333334e-05, 8.229573973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4710 loss: 1.8419 iter time (s): 4.175 samples/sec: 30.657
g0332:  iteration     4710/10000000 | consumed samples:       602880 | consumed tokens:   1234698240 | elapsed time per iteration (ms): 4208.9 | learning rate: 8.230E-05 | global batch size:   128 | lm loss: 1.906413E+00 | loss scale: 262144.0 | grad norm: 0.558 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.412 | tokens per gpu per second (tgs): 1946.360 | TFLOPs: 15.66 |
g0314: [2024-08-03 00:37:38,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=4720, skipped=1, lr=[8.247050240000001e-05, 8.247050240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4720 loss: 1.8954 iter time (s): 4.056 samples/sec: 31.559
g0332:  iteration     4720/10000000 | consumed samples:       604160 | consumed tokens:   1237319680 | elapsed time per iteration (ms): 4088.9 | learning rate: 8.247E-05 | global batch size:   128 | lm loss: 1.879099E+00 | loss scale: 262144.0 | grad norm: 0.564 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.304 | tokens per gpu per second (tgs): 2003.476 | TFLOPs: 16.12 |
g0314: [2024-08-03 00:38:19,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=4730, skipped=1, lr=[8.264526506666667e-05, 8.264526506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4730 loss: 1.9058 iter time (s): 4.141 samples/sec: 30.910
g0332:  iteration     4730/10000000 | consumed samples:       605440 | consumed tokens:   1239941120 | elapsed time per iteration (ms): 4173.8 | learning rate: 8.265E-05 | global batch size:   128 | lm loss: 1.877944E+00 | loss scale: 262144.0 | grad norm: 0.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.668 | tokens per gpu per second (tgs): 1962.737 | TFLOPs: 15.79 |
g0314: [2024-08-03 00:39:01,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=4740, skipped=1, lr=[8.282002773333334e-05, 8.282002773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4740 loss: 1.8224 iter time (s): 4.145 samples/sec: 30.879
g0332:  iteration     4740/10000000 | consumed samples:       606720 | consumed tokens:   1242562560 | elapsed time per iteration (ms): 4178.2 | learning rate: 8.282E-05 | global batch size:   128 | lm loss: 1.869315E+00 | loss scale: 262144.0 | grad norm: 0.536 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.635 | tokens per gpu per second (tgs): 1960.635 | TFLOPs: 15.78 |
g0314: [2024-08-03 00:39:44,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=4750, skipped=1, lr=[8.29947904e-05, 8.29947904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4750 loss: 1.9090 iter time (s): 4.225 samples/sec: 30.295
g0332:  iteration     4750/10000000 | consumed samples:       608000 | consumed tokens:   1245184000 | elapsed time per iteration (ms): 4257.5 | learning rate: 8.299E-05 | global batch size:   128 | lm loss: 1.867915E+00 | loss scale: 262144.0 | grad norm: 0.590 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.065 | tokens per gpu per second (tgs): 1924.138 | TFLOPs: 15.48 |
g0314: [2024-08-03 00:40:25,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=4760, skipped=1, lr=[8.316955306666667e-05, 8.316955306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4760 loss: 1.8196 iter time (s): 4.147 samples/sec: 30.864
g0332:  iteration     4760/10000000 | consumed samples:       609280 | consumed tokens:   1247805440 | elapsed time per iteration (ms): 4180.1 | learning rate: 8.317E-05 | global batch size:   128 | lm loss: 1.882911E+00 | loss scale: 262144.0 | grad norm: 0.582 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.621 | tokens per gpu per second (tgs): 1959.749 | TFLOPs: 15.77 |
g0314: [2024-08-03 00:41:07,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=4770, skipped=1, lr=[8.334431573333334e-05, 8.334431573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4770 loss: 1.8967 iter time (s): 4.147 samples/sec: 30.869
g0332:  iteration     4770/10000000 | consumed samples:       610560 | consumed tokens:   1250426880 | elapsed time per iteration (ms): 4178.8 | learning rate: 8.334E-05 | global batch size:   128 | lm loss: 1.890131E+00 | loss scale: 262144.0 | grad norm: 0.565 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.630 | tokens per gpu per second (tgs): 1960.350 | TFLOPs: 15.78 |
g0314: [2024-08-03 00:41:52,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=4780, skipped=1, lr=[8.35190784e-05, 8.35190784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4780 loss: 1.8890 iter time (s): 4.455 samples/sec: 28.731
g0332:  iteration     4780/10000000 | consumed samples:       611840 | consumed tokens:   1253048320 | elapsed time per iteration (ms): 4487.5 | learning rate: 8.352E-05 | global batch size:   128 | lm loss: 1.885712E+00 | loss scale: 262144.0 | grad norm: 0.580 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.523 | tokens per gpu per second (tgs): 1825.497 | TFLOPs: 14.69 |
g0314: [2024-08-03 00:42:37,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=4790, skipped=1, lr=[8.369384106666667e-05, 8.369384106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4790 loss: 1.9023 iter time (s): 4.501 samples/sec: 28.438
g0332:  iteration     4790/10000000 | consumed samples:       613120 | consumed tokens:   1255669760 | elapsed time per iteration (ms): 4537.3 | learning rate: 8.369E-05 | global batch size:   128 | lm loss: 1.894936E+00 | loss scale: 262144.0 | grad norm: 0.572 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.210 | tokens per gpu per second (tgs): 1805.469 | TFLOPs: 14.53 |
g0314: [2024-08-03 00:43:19,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=4800, skipped=1, lr=[8.386860373333334e-05, 8.386860373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4800 loss: 1.8802 iter time (s): 4.142 samples/sec: 30.906
g0332:  iteration     4800/10000000 | consumed samples:       614400 | consumed tokens:   1258291200 | elapsed time per iteration (ms): 4174.6 | learning rate: 8.387E-05 | global batch size:   128 | lm loss: 1.885104E+00 | loss scale: 262144.0 | grad norm: 0.576 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.662 | tokens per gpu per second (tgs): 1962.357 | TFLOPs: 15.79 |
g0314: [2024-08-03 00:44:02,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=4810, skipped=1, lr=[8.40433664e-05, 8.40433664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4810 loss: 1.8810 iter time (s): 4.204 samples/sec: 30.449
g0332:  iteration     4810/10000000 | consumed samples:       615680 | consumed tokens:   1260912640 | elapsed time per iteration (ms): 4236.6 | learning rate: 8.404E-05 | global batch size:   128 | lm loss: 1.882751E+00 | loss scale: 262144.0 | grad norm: 0.567 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.213 | tokens per gpu per second (tgs): 1933.617 | TFLOPs: 15.56 |
g0314: [2024-08-03 00:44:44,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=4820, skipped=1, lr=[8.421812906666668e-05, 8.421812906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4820 loss: 1.8640 iter time (s): 4.193 samples/sec: 30.528
g0332:  iteration     4820/10000000 | consumed samples:       616960 | consumed tokens:   1263534080 | elapsed time per iteration (ms): 4225.1 | learning rate: 8.422E-05 | global batch size:   128 | lm loss: 1.894929E+00 | loss scale: 262144.0 | grad norm: 0.638 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.295 | tokens per gpu per second (tgs): 1938.879 | TFLOPs: 15.60 |
g0314: [2024-08-03 00:45:27,117] [INFO] [logging.py:96:log_dist] [Rank 0] step=4830, skipped=1, lr=[8.439289173333333e-05, 8.439289173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4830 loss: 1.8518 iter time (s): 4.246 samples/sec: 30.144
g0332:  iteration     4830/10000000 | consumed samples:       618240 | consumed tokens:   1266155520 | elapsed time per iteration (ms): 4278.9 | learning rate: 8.439E-05 | global batch size:   128 | lm loss: 1.892832E+00 | loss scale: 262144.0 | grad norm: 0.530 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.914 | tokens per gpu per second (tgs): 1914.496 | TFLOPs: 15.41 |
g0314: [2024-08-03 00:46:09,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=4840, skipped=1, lr=[8.45676544e-05, 8.45676544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4840 loss: 1.8323 iter time (s): 4.166 samples/sec: 30.726
g0332:  iteration     4840/10000000 | consumed samples:       619520 | consumed tokens:   1268776960 | elapsed time per iteration (ms): 4198.6 | learning rate: 8.457E-05 | global batch size:   128 | lm loss: 1.873970E+00 | loss scale: 262144.0 | grad norm: 0.557 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.486 | tokens per gpu per second (tgs): 1951.117 | TFLOPs: 15.70 |
g0314: [2024-08-03 00:46:50,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=4850, skipped=1, lr=[8.474241706666667e-05, 8.474241706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4850 loss: 1.8817 iter time (s): 4.153 samples/sec: 30.823
g0332:  iteration     4850/10000000 | consumed samples:       620800 | consumed tokens:   1271398400 | elapsed time per iteration (ms): 4185.4 | learning rate: 8.474E-05 | global batch size:   128 | lm loss: 1.867960E+00 | loss scale: 262144.0 | grad norm: 0.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.583 | tokens per gpu per second (tgs): 1957.297 | TFLOPs: 15.75 |
g0314: [2024-08-03 00:47:37,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=4860, skipped=1, lr=[8.491717973333333e-05, 8.491717973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4860 loss: 1.8431 iter time (s): 4.646 samples/sec: 27.551
g0332:  iteration     4860/10000000 | consumed samples:       622080 | consumed tokens:   1274019840 | elapsed time per iteration (ms): 4678.3 | learning rate: 8.492E-05 | global batch size:   128 | lm loss: 1.868570E+00 | loss scale: 262144.0 | grad norm: 0.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.360 | tokens per gpu per second (tgs): 1751.066 | TFLOPs: 14.09 |
g0314: [2024-08-03 00:48:19,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=4870, skipped=1, lr=[8.50919424e-05, 8.50919424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4870 loss: 1.8555 iter time (s): 4.143 samples/sec: 30.896
g0332:  iteration     4870/10000000 | consumed samples:       623360 | consumed tokens:   1276641280 | elapsed time per iteration (ms): 4175.3 | learning rate: 8.509E-05 | global batch size:   128 | lm loss: 1.881657E+00 | loss scale: 262144.0 | grad norm: 0.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.657 | tokens per gpu per second (tgs): 1962.031 | TFLOPs: 15.79 |
g0314: [2024-08-03 00:49:01,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=4880, skipped=1, lr=[8.526670506666666e-05, 8.526670506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4880 loss: 1.8219 iter time (s): 4.125 samples/sec: 31.028
g0332:  iteration     4880/10000000 | consumed samples:       624640 | consumed tokens:   1279262720 | elapsed time per iteration (ms): 4158.3 | learning rate: 8.527E-05 | global batch size:   128 | lm loss: 1.861074E+00 | loss scale: 262144.0 | grad norm: 0.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.782 | tokens per gpu per second (tgs): 1970.021 | TFLOPs: 15.85 |
g0314: [2024-08-03 00:49:43,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=4890, skipped=1, lr=[8.544146773333333e-05, 8.544146773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4890 loss: 1.9169 iter time (s): 4.223 samples/sec: 30.310
g0332:  iteration     4890/10000000 | consumed samples:       625920 | consumed tokens:   1281884160 | elapsed time per iteration (ms): 4255.5 | learning rate: 8.544E-05 | global batch size:   128 | lm loss: 1.867473E+00 | loss scale: 262144.0 | grad norm: 0.578 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.079 | tokens per gpu per second (tgs): 1925.050 | TFLOPs: 15.49 |
g0314: [2024-08-03 00:50:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=4900, skipped=1, lr=[8.56162304e-05, 8.56162304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4900 loss: 1.8713 iter time (s): 4.231 samples/sec: 30.252
g0332:  iteration     4900/10000000 | consumed samples:       627200 | consumed tokens:   1284505600 | elapsed time per iteration (ms): 4263.4 | learning rate: 8.562E-05 | global batch size:   128 | lm loss: 1.883072E+00 | loss scale: 262144.0 | grad norm: 0.544 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.023 | tokens per gpu per second (tgs): 1921.466 | TFLOPs: 15.46 |
g0314: [2024-08-03 00:51:06,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=4910, skipped=1, lr=[8.579099306666666e-05, 8.579099306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4910 loss: 1.8160 iter time (s): 3.972 samples/sec: 32.228
g0332:  iteration     4910/10000000 | consumed samples:       628480 | consumed tokens:   1287127040 | elapsed time per iteration (ms): 4006.5 | learning rate: 8.579E-05 | global batch size:   128 | lm loss: 1.876681E+00 | loss scale: 262144.0 | grad norm: 0.541 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.948 | tokens per gpu per second (tgs): 2044.676 | TFLOPs: 16.45 |
g0314: [2024-08-03 00:51:47,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=4920, skipped=1, lr=[8.596575573333333e-05, 8.596575573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4920 loss: 1.8344 iter time (s): 4.103 samples/sec: 31.195
g0332:  iteration     4920/10000000 | consumed samples:       629760 | consumed tokens:   1289748480 | elapsed time per iteration (ms): 4135.6 | learning rate: 8.597E-05 | global batch size:   128 | lm loss: 1.862196E+00 | loss scale: 262144.0 | grad norm: 0.544 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.951 | tokens per gpu per second (tgs): 1980.871 | TFLOPs: 15.94 |
g0314: [2024-08-03 00:52:31,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=4930, skipped=1, lr=[8.61405184e-05, 8.61405184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4930 loss: 1.8838 iter time (s): 4.329 samples/sec: 29.565
g0332:  iteration     4930/10000000 | consumed samples:       631040 | consumed tokens:   1292369920 | elapsed time per iteration (ms): 4362.2 | learning rate: 8.614E-05 | global batch size:   128 | lm loss: 1.878621E+00 | loss scale: 262144.0 | grad norm: 0.531 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.343 | tokens per gpu per second (tgs): 1877.965 | TFLOPs: 15.11 |
g0314: [2024-08-03 00:53:23,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=4940, skipped=1, lr=[8.631528106666666e-05, 8.631528106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4940 loss: 1.9262 iter time (s): 5.171 samples/sec: 24.755
g0332:  iteration     4940/10000000 | consumed samples:       632320 | consumed tokens:   1294991360 | elapsed time per iteration (ms): 5202.9 | learning rate: 8.632E-05 | global batch size:   128 | lm loss: 1.862213E+00 | loss scale: 262144.0 | grad norm: 0.570 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.602 | tokens per gpu per second (tgs): 1574.503 | TFLOPs: 12.67 |
g0314: [2024-08-03 00:54:18,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=4950, skipped=1, lr=[8.649004373333334e-05, 8.649004373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4950 loss: 1.8087 iter time (s): 5.464 samples/sec: 23.427
g0332:  iteration     4950/10000000 | consumed samples:       633600 | consumed tokens:   1297612800 | elapsed time per iteration (ms): 5496.1 | learning rate: 8.649E-05 | global batch size:   128 | lm loss: 1.860735E+00 | loss scale: 262144.0 | grad norm: 0.537 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.289 | tokens per gpu per second (tgs): 1490.524 | TFLOPs: 11.99 |
g0314: [2024-08-03 00:55:08,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=4960, skipped=1, lr=[8.666480640000001e-05, 8.666480640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4960 loss: 1.8576 iter time (s): 5.008 samples/sec: 25.559
g0332:  iteration     4960/10000000 | consumed samples:       634880 | consumed tokens:   1300234240 | elapsed time per iteration (ms): 5041.0 | learning rate: 8.666E-05 | global batch size:   128 | lm loss: 1.854631E+00 | loss scale: 262144.0 | grad norm: 0.595 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.392 | tokens per gpu per second (tgs): 1625.059 | TFLOPs: 13.08 |
g0314: [2024-08-03 00:55:52,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=4970, skipped=1, lr=[8.683956906666667e-05, 8.683956906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4970 loss: 1.8907 iter time (s): 4.325 samples/sec: 29.592
g0332:  iteration     4970/10000000 | consumed samples:       636160 | consumed tokens:   1302855680 | elapsed time per iteration (ms): 4357.7 | learning rate: 8.684E-05 | global batch size:   128 | lm loss: 1.878328E+00 | loss scale: 262144.0 | grad norm: 0.515 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.373 | tokens per gpu per second (tgs): 1879.883 | TFLOPs: 15.13 |
g0314: [2024-08-03 00:56:35,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=4980, skipped=1, lr=[8.701433173333334e-05, 8.701433173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4980 loss: 1.8189 iter time (s): 4.261 samples/sec: 30.038
g0332:  iteration     4980/10000000 | consumed samples:       637440 | consumed tokens:   1305477120 | elapsed time per iteration (ms): 4294.3 | learning rate: 8.701E-05 | global batch size:   128 | lm loss: 1.851542E+00 | loss scale: 262144.0 | grad norm: 0.535 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.807 | tokens per gpu per second (tgs): 1907.641 | TFLOPs: 15.35 |
g0314: [2024-08-03 00:57:17,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=4990, skipped=1, lr=[8.71890944e-05, 8.71890944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 4990 loss: 1.8820 iter time (s): 4.197 samples/sec: 30.496
g0332:  iteration     4990/10000000 | consumed samples:       638720 | consumed tokens:   1308098560 | elapsed time per iteration (ms): 4229.6 | learning rate: 8.719E-05 | global batch size:   128 | lm loss: 1.859944E+00 | loss scale: 262144.0 | grad norm: 0.539 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.263 | tokens per gpu per second (tgs): 1936.816 | TFLOPs: 15.59 |
g0314: [2024-08-03 00:57:54,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 00:57:54,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0314: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 00:57:54,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 00:57:54,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 00:57:54,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0314: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0314: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 00:57:54,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 00:57:54,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 00:57:54,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 00:57:54,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0314: [2024-08-03 00:57:59,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=1, lr=[8.736385706666667e-05, 8.736385706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5000 loss: 1.8867 iter time (s): 4.129 samples/sec: 31.003
g0332:  iteration     5000/10000000 | consumed samples:       640000 | consumed tokens:   1310720000 | elapsed time per iteration (ms): 4161.1 | learning rate: 8.736E-05 | global batch size:   128 | lm loss: 1.858082E+00 | loss scale: 524288.0 | grad norm: 0.562 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.761 | tokens per gpu per second (tgs): 1968.715 | TFLOPs: 15.84 |
g0332: ------------------------------------------------------------------------------------------------
g0332:  validation loss at iteration 5000 | lm loss value: 1.869343E+00 | lm loss PPL: 6.484038E+00 | 
g0332: ------------------------------------------------------------------------------------------------
g0314: saving checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: [2024-08-03 01:04:29,539] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!
g0314: [2024-08-03 01:04:29,545] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0332: [2024-08-03 01:04:29,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0332: [2024-08-03 01:04:29,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0314: [2024-08-03 01:04:29,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0332: [2024-08-03 01:04:29,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0314: [2024-08-03 01:04:29,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0316: [2024-08-03 01:04:29,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0316: [2024-08-03 01:04:29,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0316: [2024-08-03 01:04:29,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0318: [2024-08-03 01:04:29,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0329: [2024-08-03 01:04:29,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0329: [2024-08-03 01:04:29,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0329: [2024-08-03 01:04:29,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0318: [2024-08-03 01:04:29,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0318: [2024-08-03 01:04:29,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0325: [2024-08-03 01:04:29,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0325: [2024-08-03 01:04:29,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0325: [2024-08-03 01:04:29,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0320: [2024-08-03 01:04:29,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0320: [2024-08-03 01:04:29,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0320: [2024-08-03 01:04:29,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0319: [2024-08-03 01:04:29,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0319: [2024-08-03 01:04:29,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0319: [2024-08-03 01:04:29,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0332: [2024-08-03 01:04:29,561] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt...
g0316: [2024-08-03 01:04:29,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt...
g0318: [2024-08-03 01:04:29,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt...
g0329: [2024-08-03 01:04:29,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt...
g0325: [2024-08-03 01:04:29,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt...
g0319: [2024-08-03 01:04:29,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt...
g0320: [2024-08-03 01:04:29,588] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt...
g0314: [2024-08-03 01:04:29,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt...
g0316: [2024-08-03 01:04:29,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt.
g0329: [2024-08-03 01:04:29,693] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt.
g0320: [2024-08-03 01:04:29,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt.
g0332: [2024-08-03 01:04:29,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt.
g0332: [2024-08-03 01:04:29,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt...
g0332: [2024-08-03 01:04:29,722] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt.
g0319: [2024-08-03 01:04:29,723] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt.
g0316: [2024-08-03 01:04:29,723] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt...
g0318: [2024-08-03 01:04:29,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt.
g0329: [2024-08-03 01:04:29,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt...
g0320: [2024-08-03 01:04:29,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt...
g0325: [2024-08-03 01:04:29,751] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt.
g0319: [2024-08-03 01:04:29,759] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt...
g0332: [2024-08-03 01:04:29,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt...
g0318: [2024-08-03 01:04:29,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt...
g0325: [2024-08-03 01:04:29,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt...
g0316: [2024-08-03 01:04:29,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt.
g0314: [2024-08-03 01:04:29,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt.
g0319: [2024-08-03 01:04:29,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt.
g0318: [2024-08-03 01:04:29,883] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt.
g0314: [2024-08-03 01:04:29,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt...
g0320: [2024-08-03 01:04:29,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt.
g0316: [2024-08-03 01:04:29,895] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt...
g0319: [2024-08-03 01:04:29,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt...
g0325: [2024-08-03 01:04:29,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt.
g0318: [2024-08-03 01:04:29,914] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt...
g0320: [2024-08-03 01:04:29,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt...
g0325: [2024-08-03 01:04:29,940] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt...
g0329: [2024-08-03 01:04:29,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt.
g0332: [2024-08-03 01:04:29,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt.
g0332: [2024-08-03 01:04:29,978] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt...
g0329: [2024-08-03 01:04:29,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt...
g0320: [2024-08-03 01:04:30,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt.
g0320: [2024-08-03 01:04:30,051] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt...
g0316: [2024-08-03 01:04:30,051] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt.
g0316: [2024-08-03 01:04:30,053] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt...
g0314: [2024-08-03 01:04:30,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt.
g0319: [2024-08-03 01:04:30,077] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt.
g0319: [2024-08-03 01:04:30,079] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt...
g0314: [2024-08-03 01:04:30,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt...
g0329: [2024-08-03 01:04:30,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt.
g0329: [2024-08-03 01:04:30,099] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt...
g0318: [2024-08-03 01:04:30,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt.
g0318: [2024-08-03 01:04:30,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt...
g0314: [2024-08-03 01:04:30,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt.
g0314: [2024-08-03 01:04:30,208] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt...
g0314: [2024-08-03 01:04:30,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt.
g0314: [2024-08-03 01:04:30,298] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt
g0314: [2024-08-03 01:04:30,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt...
g0325: [2024-08-03 01:04:30,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt.
g0325: [2024-08-03 01:04:30,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt...
g0332: [2024-08-03 01:04:31,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt.
g0332: [2024-08-03 01:04:31,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0316: [2024-08-03 01:04:32,355] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt.
g0316: [2024-08-03 01:04:32,355] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0320: [2024-08-03 01:04:32,393] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt.
g0320: [2024-08-03 01:04:32,394] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0319: [2024-08-03 01:04:32,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt.
g0319: [2024-08-03 01:04:32,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0318: [2024-08-03 01:04:32,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt.
g0318: [2024-08-03 01:04:32,715] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0325: [2024-08-03 01:04:32,764] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt.
g0325: [2024-08-03 01:04:32,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0329: [2024-08-03 01:04:33,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt.
g0329: [2024-08-03 01:04:33,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0314: [2024-08-03 01:04:33,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt.
g0314: [2024-08-03 01:04:33,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0314:   successfully saved checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: Checkpoint Save GB: 22.521, GB/Sec: 5.15, Latency(second): 4.369
g0332: (min, max) time across ranks (ms):
g0332:     save-checkpoint ................................: (4368.03, 4369.28)
g0314: [2024-08-03 01:05:15,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=5010, skipped=1, lr=[8.753861973333334e-05, 8.753861973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5010 loss: 1.7970 iter time (s): 4.114 samples/sec: 31.111
g0332:  iteration     5010/10000000 | consumed samples:       641280 | consumed tokens:   1313341440 | elapsed time per iteration (ms): 43613.5 | learning rate: 8.754E-05 | global batch size:   128 | lm loss: 1.868691E+00 | loss scale: 524288.0 | grad norm: 0.567 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.935 | tokens per gpu per second (tgs): 187.832 | TFLOPs: 1.51 |
g0314: [2024-08-03 01:05:56,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=5020, skipped=1, lr=[8.77133824e-05, 8.77133824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5020 loss: 1.8000 iter time (s): 4.127 samples/sec: 31.012
g0332:  iteration     5020/10000000 | consumed samples:       642560 | consumed tokens:   1315962880 | elapsed time per iteration (ms): 4159.9 | learning rate: 8.771E-05 | global batch size:   128 | lm loss: 1.828506E+00 | loss scale: 524288.0 | grad norm: 0.502 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.770 | tokens per gpu per second (tgs): 1969.272 | TFLOPs: 15.85 |
g0314: [2024-08-03 01:06:38,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=5030, skipped=1, lr=[8.788814506666667e-05, 8.788814506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5030 loss: 1.8660 iter time (s): 4.152 samples/sec: 30.827
g0332:  iteration     5030/10000000 | consumed samples:       643840 | consumed tokens:   1318584320 | elapsed time per iteration (ms): 4184.5 | learning rate: 8.789E-05 | global batch size:   128 | lm loss: 1.849708E+00 | loss scale: 524288.0 | grad norm: 0.525 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.589 | tokens per gpu per second (tgs): 1957.686 | TFLOPs: 15.75 |
g0314: [2024-08-03 01:07:23,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=5040, skipped=1, lr=[8.806290773333334e-05, 8.806290773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5040 loss: 1.9187 iter time (s): 4.429 samples/sec: 28.899
g0332:  iteration     5040/10000000 | consumed samples:       645120 | consumed tokens:   1321205760 | elapsed time per iteration (ms): 4462.1 | learning rate: 8.806E-05 | global batch size:   128 | lm loss: 1.866072E+00 | loss scale: 524288.0 | grad norm: 0.548 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.686 | tokens per gpu per second (tgs): 1835.923 | TFLOPs: 14.77 |
g0314: [2024-08-03 01:08:05,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=5050, skipped=1, lr=[8.82376704e-05, 8.82376704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5050 loss: 1.9120 iter time (s): 4.225 samples/sec: 30.294
g0332:  iteration     5050/10000000 | consumed samples:       646400 | consumed tokens:   1323827200 | elapsed time per iteration (ms): 4258.5 | learning rate: 8.824E-05 | global batch size:   128 | lm loss: 1.859015E+00 | loss scale: 524288.0 | grad norm: 0.520 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.058 | tokens per gpu per second (tgs): 1923.682 | TFLOPs: 15.48 |
g0314: [2024-08-03 01:08:48,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=5060, skipped=1, lr=[8.841243306666667e-05, 8.841243306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5060 loss: 1.8526 iter time (s): 4.191 samples/sec: 30.543
g0332:  iteration     5060/10000000 | consumed samples:       647680 | consumed tokens:   1326448640 | elapsed time per iteration (ms): 4223.7 | learning rate: 8.841E-05 | global batch size:   128 | lm loss: 1.866502E+00 | loss scale: 524288.0 | grad norm: 0.514 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.305 | tokens per gpu per second (tgs): 1939.530 | TFLOPs: 15.61 |
g0314: [2024-08-03 01:09:30,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=5070, skipped=1, lr=[8.858719573333333e-05, 8.858719573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5070 loss: 1.8590 iter time (s): 4.245 samples/sec: 30.151
g0332:  iteration     5070/10000000 | consumed samples:       648960 | consumed tokens:   1329070080 | elapsed time per iteration (ms): 4279.2 | learning rate: 8.859E-05 | global batch size:   128 | lm loss: 1.886374E+00 | loss scale: 524288.0 | grad norm: 0.508 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.912 | tokens per gpu per second (tgs): 1914.366 | TFLOPs: 15.41 |
g0314: [2024-08-03 01:10:12,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=5080, skipped=1, lr=[8.87619584e-05, 8.87619584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5080 loss: 1.8150 iter time (s): 4.164 samples/sec: 30.738
g0332:  iteration     5080/10000000 | consumed samples:       650240 | consumed tokens:   1331691520 | elapsed time per iteration (ms): 4197.4 | learning rate: 8.876E-05 | global batch size:   128 | lm loss: 1.865064E+00 | loss scale: 524288.0 | grad norm: 0.495 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.495 | tokens per gpu per second (tgs): 1951.702 | TFLOPs: 15.71 |
g0314: [2024-08-03 01:10:54,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=5090, skipped=1, lr=[8.893672106666667e-05, 8.893672106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5090 loss: 1.8768 iter time (s): 4.167 samples/sec: 30.716
g0332:  iteration     5090/10000000 | consumed samples:       651520 | consumed tokens:   1334312960 | elapsed time per iteration (ms): 4200.0 | learning rate: 8.894E-05 | global batch size:   128 | lm loss: 1.844530E+00 | loss scale: 524288.0 | grad norm: 0.504 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.476 | tokens per gpu per second (tgs): 1950.460 | TFLOPs: 15.70 |
g0314: [2024-08-03 01:11:37,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=5100, skipped=1, lr=[8.911148373333333e-05, 8.911148373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5100 loss: 1.8541 iter time (s): 4.266 samples/sec: 30.004
g0332:  iteration     5100/10000000 | consumed samples:       652800 | consumed tokens:   1336934400 | elapsed time per iteration (ms): 4299.3 | learning rate: 8.911E-05 | global batch size:   128 | lm loss: 1.850424E+00 | loss scale: 524288.0 | grad norm: 0.528 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.772 | tokens per gpu per second (tgs): 1905.427 | TFLOPs: 15.33 |
g0314: [2024-08-03 01:12:20,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=5110, skipped=1, lr=[8.92862464e-05, 8.92862464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5110 loss: 1.8436 iter time (s): 4.184 samples/sec: 30.591
g0332:  iteration     5110/10000000 | consumed samples:       654080 | consumed tokens:   1339555840 | elapsed time per iteration (ms): 4216.8 | learning rate: 8.929E-05 | global batch size:   128 | lm loss: 1.833424E+00 | loss scale: 524288.0 | grad norm: 0.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.355 | tokens per gpu per second (tgs): 1942.726 | TFLOPs: 15.63 |
g0314: [2024-08-03 01:13:03,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=5120, skipped=1, lr=[8.946100906666666e-05, 8.946100906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5120 loss: 1.7501 iter time (s): 4.337 samples/sec: 29.511
g0332:  iteration     5120/10000000 | consumed samples:       655360 | consumed tokens:   1342177280 | elapsed time per iteration (ms): 4372.5 | learning rate: 8.946E-05 | global batch size:   128 | lm loss: 1.845860E+00 | loss scale: 524288.0 | grad norm: 0.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.274 | tokens per gpu per second (tgs): 1873.535 | TFLOPs: 15.08 |
g0314: [2024-08-03 01:13:46,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=5130, skipped=1, lr=[8.963577173333334e-05, 8.963577173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5130 loss: 1.8579 iter time (s): 4.220 samples/sec: 30.334
g0332:  iteration     5130/10000000 | consumed samples:       656640 | consumed tokens:   1344798720 | elapsed time per iteration (ms): 4252.4 | learning rate: 8.964E-05 | global batch size:   128 | lm loss: 1.861651E+00 | loss scale: 524288.0 | grad norm: 0.499 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.101 | tokens per gpu per second (tgs): 1926.457 | TFLOPs: 15.50 |
g0314: [2024-08-03 01:14:44,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=5140, skipped=1, lr=[8.981053440000001e-05, 8.981053440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5140 loss: 1.7950 iter time (s): 5.828 samples/sec: 21.964
g0332:  iteration     5140/10000000 | consumed samples:       657920 | consumed tokens:   1347420160 | elapsed time per iteration (ms): 5860.3 | learning rate: 8.981E-05 | global batch size:   128 | lm loss: 1.852732E+00 | loss scale: 524288.0 | grad norm: 0.517 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.842 | tokens per gpu per second (tgs): 1397.888 | TFLOPs: 11.25 |
g0314: [2024-08-03 01:15:41,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=5150, skipped=1, lr=[8.998529706666668e-05, 8.998529706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5150 loss: 1.8998 iter time (s): 5.618 samples/sec: 22.783
g0332:  iteration     5150/10000000 | consumed samples:       659200 | consumed tokens:   1350041600 | elapsed time per iteration (ms): 5651.3 | learning rate: 8.999E-05 | global batch size:   128 | lm loss: 1.876201E+00 | loss scale: 524288.0 | grad norm: 0.499 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.650 | tokens per gpu per second (tgs): 1449.591 | TFLOPs: 11.67 |
g0314: [2024-08-03 01:16:40,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=5160, skipped=1, lr=[9.016005973333334e-05, 9.016005973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5160 loss: 1.9044 iter time (s): 5.823 samples/sec: 21.984
g0332:  iteration     5160/10000000 | consumed samples:       660480 | consumed tokens:   1352663040 | elapsed time per iteration (ms): 5855.2 | learning rate: 9.016E-05 | global batch size:   128 | lm loss: 1.878659E+00 | loss scale: 524288.0 | grad norm: 0.523 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.861 | tokens per gpu per second (tgs): 1399.108 | TFLOPs: 11.26 |
g0314: [2024-08-03 01:17:35,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=5170, skipped=1, lr=[9.033482240000001e-05, 9.033482240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5170 loss: 1.8330 iter time (s): 5.556 samples/sec: 23.040
g0332:  iteration     5170/10000000 | consumed samples:       661760 | consumed tokens:   1355284480 | elapsed time per iteration (ms): 5588.4 | learning rate: 9.033E-05 | global batch size:   128 | lm loss: 1.829591E+00 | loss scale: 524288.0 | grad norm: 0.491 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.905 | tokens per gpu per second (tgs): 1465.905 | TFLOPs: 11.80 |
g0314: [2024-08-03 01:18:18,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=5180, skipped=1, lr=[9.050958506666667e-05, 9.050958506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5180 loss: 1.8354 iter time (s): 4.201 samples/sec: 30.466
g0332:  iteration     5180/10000000 | consumed samples:       663040 | consumed tokens:   1357905920 | elapsed time per iteration (ms): 4234.3 | learning rate: 9.051E-05 | global batch size:   128 | lm loss: 1.874180E+00 | loss scale: 524288.0 | grad norm: 0.500 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.230 | tokens per gpu per second (tgs): 1934.689 | TFLOPs: 15.57 |
g0314: [2024-08-03 01:19:00,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=5190, skipped=1, lr=[9.068434773333334e-05, 9.068434773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5190 loss: 1.8439 iter time (s): 4.221 samples/sec: 30.326
g0332:  iteration     5190/10000000 | consumed samples:       664320 | consumed tokens:   1360527360 | elapsed time per iteration (ms): 4253.5 | learning rate: 9.068E-05 | global batch size:   128 | lm loss: 1.836484E+00 | loss scale: 524288.0 | grad norm: 0.505 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.093 | tokens per gpu per second (tgs): 1925.931 | TFLOPs: 15.50 |
g0314: [2024-08-03 01:19:43,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=5200, skipped=1, lr=[9.08591104e-05, 9.08591104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5200 loss: 1.8684 iter time (s): 4.214 samples/sec: 30.373
g0332:  iteration     5200/10000000 | consumed samples:       665600 | consumed tokens:   1363148800 | elapsed time per iteration (ms): 4247.1 | learning rate: 9.086E-05 | global batch size:   128 | lm loss: 1.864622E+00 | loss scale: 524288.0 | grad norm: 0.504 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.138 | tokens per gpu per second (tgs): 1928.849 | TFLOPs: 15.52 |
g0314: [2024-08-03 01:20:26,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=5210, skipped=1, lr=[9.103387306666667e-05, 9.103387306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5210 loss: 1.8358 iter time (s): 4.298 samples/sec: 29.781
g0332:  iteration     5210/10000000 | consumed samples:       666880 | consumed tokens:   1365770240 | elapsed time per iteration (ms): 4330.8 | learning rate: 9.103E-05 | global batch size:   128 | lm loss: 1.847093E+00 | loss scale: 524288.0 | grad norm: 0.495 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.556 | tokens per gpu per second (tgs): 1891.580 | TFLOPs: 15.22 |
g0314: [2024-08-03 01:21:08,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=5220, skipped=1, lr=[9.120863573333334e-05, 9.120863573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5220 loss: 1.8745 iter time (s): 4.173 samples/sec: 30.671
g0332:  iteration     5220/10000000 | consumed samples:       668160 | consumed tokens:   1368391680 | elapsed time per iteration (ms): 4206.0 | learning rate: 9.121E-05 | global batch size:   128 | lm loss: 1.863543E+00 | loss scale: 524288.0 | grad norm: 0.508 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.433 | tokens per gpu per second (tgs): 1947.709 | TFLOPs: 15.67 |
g0314: [2024-08-03 01:21:51,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=5230, skipped=1, lr=[9.13833984e-05, 9.13833984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5230 loss: 1.8881 iter time (s): 4.240 samples/sec: 30.187
g0332:  iteration     5230/10000000 | consumed samples:       669440 | consumed tokens:   1371013120 | elapsed time per iteration (ms): 4272.8 | learning rate: 9.138E-05 | global batch size:   128 | lm loss: 1.846284E+00 | loss scale: 524288.0 | grad norm: 0.505 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.957 | tokens per gpu per second (tgs): 1917.242 | TFLOPs: 15.43 |
g0314: [2024-08-03 01:22:33,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=5240, skipped=1, lr=[9.155816106666667e-05, 9.155816106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5240 loss: 1.8225 iter time (s): 4.205 samples/sec: 30.440
g0332:  iteration     5240/10000000 | consumed samples:       670720 | consumed tokens:   1373634560 | elapsed time per iteration (ms): 4238.1 | learning rate: 9.156E-05 | global batch size:   128 | lm loss: 1.841197E+00 | loss scale: 524288.0 | grad norm: 0.477 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.202 | tokens per gpu per second (tgs): 1932.934 | TFLOPs: 15.55 |
g0314: [2024-08-03 01:23:17,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=5250, skipped=1, lr=[9.173292373333334e-05, 9.173292373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5250 loss: 1.8217 iter time (s): 4.353 samples/sec: 29.404
g0332:  iteration     5250/10000000 | consumed samples:       672000 | consumed tokens:   1376256000 | elapsed time per iteration (ms): 4385.9 | learning rate: 9.173E-05 | global batch size:   128 | lm loss: 1.840666E+00 | loss scale: 524288.0 | grad norm: 0.506 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.184 | tokens per gpu per second (tgs): 1867.784 | TFLOPs: 15.03 |
g0314: [2024-08-03 01:24:00,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=5260, skipped=1, lr=[9.19076864e-05, 9.19076864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5260 loss: 1.8198 iter time (s): 4.295 samples/sec: 29.801
g0332:  iteration     5260/10000000 | consumed samples:       673280 | consumed tokens:   1378877440 | elapsed time per iteration (ms): 4327.7 | learning rate: 9.191E-05 | global batch size:   128 | lm loss: 1.854033E+00 | loss scale: 524288.0 | grad norm: 0.523 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.577 | tokens per gpu per second (tgs): 1892.944 | TFLOPs: 15.23 |
g0314: [2024-08-03 01:24:42,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=5270, skipped=1, lr=[9.208244906666667e-05, 9.208244906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5270 loss: 1.8381 iter time (s): 4.148 samples/sec: 30.856
g0332:  iteration     5270/10000000 | consumed samples:       674560 | consumed tokens:   1381498880 | elapsed time per iteration (ms): 4181.0 | learning rate: 9.208E-05 | global batch size:   128 | lm loss: 1.846001E+00 | loss scale: 524288.0 | grad norm: 0.490 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.615 | tokens per gpu per second (tgs): 1959.343 | TFLOPs: 15.77 |
g0314: [2024-08-03 01:25:25,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=5280, skipped=1, lr=[9.225721173333333e-05, 9.225721173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5280 loss: 1.8286 iter time (s): 4.219 samples/sec: 30.335
g0332:  iteration     5280/10000000 | consumed samples:       675840 | consumed tokens:   1384120320 | elapsed time per iteration (ms): 4252.4 | learning rate: 9.226E-05 | global batch size:   128 | lm loss: 1.894894E+00 | loss scale: 524288.0 | grad norm: 0.497 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.101 | tokens per gpu per second (tgs): 1926.452 | TFLOPs: 15.50 |
g0314: [2024-08-03 01:26:07,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=5290, skipped=1, lr=[9.24319744e-05, 9.24319744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5290 loss: 1.9688 iter time (s): 4.160 samples/sec: 30.767
g0332:  iteration     5290/10000000 | consumed samples:       677120 | consumed tokens:   1386741760 | elapsed time per iteration (ms): 4193.2 | learning rate: 9.243E-05 | global batch size:   128 | lm loss: 1.868141E+00 | loss scale: 524288.0 | grad norm: 0.500 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.525 | tokens per gpu per second (tgs): 1953.617 | TFLOPs: 15.72 |
g0314: [2024-08-03 01:26:50,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=5300, skipped=1, lr=[9.260673706666667e-05, 9.260673706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5300 loss: 1.8150 iter time (s): 4.321 samples/sec: 29.620
g0332:  iteration     5300/10000000 | consumed samples:       678400 | consumed tokens:   1389363200 | elapsed time per iteration (ms): 4355.5 | learning rate: 9.261E-05 | global batch size:   128 | lm loss: 1.856758E+00 | loss scale: 524288.0 | grad norm: 0.492 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.388 | tokens per gpu per second (tgs): 1880.839 | TFLOPs: 15.14 |
g0314: [2024-08-03 01:27:32,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=5310, skipped=1, lr=[9.278149973333335e-05, 9.278149973333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5310 loss: 1.8181 iter time (s): 4.190 samples/sec: 30.548
g0332:  iteration     5310/10000000 | consumed samples:       679680 | consumed tokens:   1391984640 | elapsed time per iteration (ms): 4222.5 | learning rate: 9.278E-05 | global batch size:   128 | lm loss: 1.865990E+00 | loss scale: 524288.0 | grad norm: 0.481 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.314 | tokens per gpu per second (tgs): 1940.104 | TFLOPs: 15.61 |
g0314: [2024-08-03 01:28:16,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=5320, skipped=1, lr=[9.295626240000001e-05, 9.295626240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5320 loss: 1.8397 iter time (s): 4.322 samples/sec: 29.618
g0332:  iteration     5320/10000000 | consumed samples:       680960 | consumed tokens:   1394606080 | elapsed time per iteration (ms): 4356.0 | learning rate: 9.296E-05 | global batch size:   128 | lm loss: 1.854466E+00 | loss scale: 524288.0 | grad norm: 0.489 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.385 | tokens per gpu per second (tgs): 1880.635 | TFLOPs: 15.13 |
g0314: [2024-08-03 01:29:02,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=5330, skipped=1, lr=[9.313102506666668e-05, 9.313102506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5330 loss: 1.8192 iter time (s): 4.601 samples/sec: 27.823
g0332:  iteration     5330/10000000 | consumed samples:       682240 | consumed tokens:   1397227520 | elapsed time per iteration (ms): 4633.1 | learning rate: 9.313E-05 | global batch size:   128 | lm loss: 1.850995E+00 | loss scale: 524288.0 | grad norm: 0.497 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.627 | tokens per gpu per second (tgs): 1768.141 | TFLOPs: 14.23 |
g0314: [2024-08-03 01:29:45,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=5340, skipped=1, lr=[9.330578773333334e-05, 9.330578773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5340 loss: 1.7975 iter time (s): 4.191 samples/sec: 30.543
g0332:  iteration     5340/10000000 | consumed samples:       683520 | consumed tokens:   1399848960 | elapsed time per iteration (ms): 4223.6 | learning rate: 9.331E-05 | global batch size:   128 | lm loss: 1.817116E+00 | loss scale: 524288.0 | grad norm: 0.487 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.600 | TFLOPs: 15.61 |
g0314: [2024-08-03 01:30:27,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=5350, skipped=1, lr=[9.348055040000001e-05, 9.348055040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5350 loss: 1.7304 iter time (s): 4.195 samples/sec: 30.515
g0332:  iteration     5350/10000000 | consumed samples:       684800 | consumed tokens:   1402470400 | elapsed time per iteration (ms): 4227.2 | learning rate: 9.348E-05 | global batch size:   128 | lm loss: 1.829752E+00 | loss scale: 524288.0 | grad norm: 0.488 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.280 | tokens per gpu per second (tgs): 1937.928 | TFLOPs: 15.59 |
g0314: [2024-08-03 01:31:13,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=5360, skipped=1, lr=[9.365531306666668e-05, 9.365531306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5360 loss: 1.7772 iter time (s): 4.566 samples/sec: 28.035
g0332:  iteration     5360/10000000 | consumed samples:       686080 | consumed tokens:   1405091840 | elapsed time per iteration (ms): 4598.4 | learning rate: 9.366E-05 | global batch size:   128 | lm loss: 1.833325E+00 | loss scale: 524288.0 | grad norm: 0.486 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.836 | tokens per gpu per second (tgs): 1781.505 | TFLOPs: 14.34 |
g0314: [2024-08-03 01:31:59,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=5370, skipped=1, lr=[9.383007573333334e-05, 9.383007573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5370 loss: 1.8286 iter time (s): 4.581 samples/sec: 27.944
g0332:  iteration     5370/10000000 | consumed samples:       687360 | consumed tokens:   1407713280 | elapsed time per iteration (ms): 4613.5 | learning rate: 9.383E-05 | global batch size:   128 | lm loss: 1.851933E+00 | loss scale: 524288.0 | grad norm: 0.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.745 | tokens per gpu per second (tgs): 1775.649 | TFLOPs: 14.29 |
g0314: [2024-08-03 01:32:42,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=5380, skipped=1, lr=[9.400483840000001e-05, 9.400483840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5380 loss: 1.8156 iter time (s): 4.307 samples/sec: 29.718
g0332:  iteration     5380/10000000 | consumed samples:       688640 | consumed tokens:   1410334720 | elapsed time per iteration (ms): 4339.9 | learning rate: 9.400E-05 | global batch size:   128 | lm loss: 1.840393E+00 | loss scale: 524288.0 | grad norm: 0.480 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.494 | tokens per gpu per second (tgs): 1887.604 | TFLOPs: 15.19 |
g0314: [2024-08-03 01:33:25,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=5390, skipped=1, lr=[9.417960106666667e-05, 9.417960106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5390 loss: 1.9339 iter time (s): 4.225 samples/sec: 30.297
g0332:  iteration     5390/10000000 | consumed samples:       689920 | consumed tokens:   1412956160 | elapsed time per iteration (ms): 4257.6 | learning rate: 9.418E-05 | global batch size:   128 | lm loss: 1.849812E+00 | loss scale: 524288.0 | grad norm: 0.490 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.064 | tokens per gpu per second (tgs): 1924.073 | TFLOPs: 15.48 |
g0314: [2024-08-03 01:34:07,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=5400, skipped=1, lr=[9.435436373333334e-05, 9.435436373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5400 loss: 1.8798 iter time (s): 4.168 samples/sec: 30.713
g0332:  iteration     5400/10000000 | consumed samples:       691200 | consumed tokens:   1415577600 | elapsed time per iteration (ms): 4200.3 | learning rate: 9.435E-05 | global batch size:   128 | lm loss: 1.855641E+00 | loss scale: 524288.0 | grad norm: 0.479 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.474 | tokens per gpu per second (tgs): 1950.324 | TFLOPs: 15.69 |
g0314: [2024-08-03 01:34:49,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=5410, skipped=1, lr=[9.452912640000001e-05, 9.452912640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5410 loss: 1.8524 iter time (s): 4.220 samples/sec: 30.334
g0332:  iteration     5410/10000000 | consumed samples:       692480 | consumed tokens:   1418199040 | elapsed time per iteration (ms): 4252.5 | learning rate: 9.453E-05 | global batch size:   128 | lm loss: 1.818440E+00 | loss scale: 524288.0 | grad norm: 0.469 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.100 | tokens per gpu per second (tgs): 1926.398 | TFLOPs: 15.50 |
g0316: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: Grad overflow on iteration 5414
g0318: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0316: Grad overflow on iteration 5414
g0318: Grad overflow on iteration 5414
g0329: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0316: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0329: Grad overflow on iteration 5414
g0325: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: Grad overflow on iteration 5414
g0316: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: Grad overflow on iteration 5414
g0316: Grad overflow on iteration 5414
g0316: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0318: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0318: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0320: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0329: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0316: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: Grad overflow on iteration 5414
g0325: Grad overflow on iteration 5414
g0314: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0319: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0316: Grad overflow on iteration 5414
g0325: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: Grad overflow on iteration 5414
g0316: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0314: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0316: Grad overflow on iteration 5414
g0314: Grad overflow on iteration 5414
g0316: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0320: Grad overflow on iteration 5414
g0325: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0325: Grad overflow on iteration 5414
g0319: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: Grad overflow on iteration 5414
g0319: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0318: Grad overflow on iteration 5414
g0319: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0319: Grad overflow on iteration 5414
g0318: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0325: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: Grad overflow on iteration 5414
g0319: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: Grad overflow on iteration 5414
g0314: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0320: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0320: Grad overflow on iteration 5414
g0329: Grad overflow on iteration 5414
g0325: Grad overflow on iteration 5414
g0320: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0329: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0319: Grad overflow on iteration 5414
g0329: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0329: Grad overflow on iteration 5414
g0332: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0329: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0320: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0319: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0332: Grad overflow on iteration 5414
g0332: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0332: Grad overflow on iteration 5414
g0319: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0325: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0332: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0329: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0329: Grad overflow on iteration 5414
g0325: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: Grad overflow on iteration 5414
g0332: Grad overflow on iteration 5414
g0329: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0332: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0329: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: [2024-08-03 01:35:10,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0318: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: Grad overflow on iteration 5414
g0314: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0332: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0332: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0332: Grad overflow on iteration 5414
g0316: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0332: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0332: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: [2024-08-03 01:35:10,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0314: [2024-08-03 01:35:10,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0314: [2024-08-03 01:35:31,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=5420, skipped=2, lr=[9.470388906666667e-05, 9.470388906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5420 loss: 1.8080 iter time (s): 4.133 samples/sec: 30.974
g0332:  iteration     5420/10000000 | consumed samples:       693760 | consumed tokens:   1420820480 | elapsed time per iteration (ms): 4165.3 | learning rate: 9.470E-05 | global batch size:   128 | lm loss: 1.833775E+00 | loss scale: 262144.0 | grad norm: 0.488 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.730 | tokens per gpu per second (tgs): 1966.746 | TFLOPs: 15.83 |
g0314: [2024-08-03 01:36:14,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=5430, skipped=2, lr=[9.487865173333334e-05, 9.487865173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5430 loss: 1.8246 iter time (s): 4.219 samples/sec: 30.342
g0332:  iteration     5430/10000000 | consumed samples:       695040 | consumed tokens:   1423441920 | elapsed time per iteration (ms): 4250.9 | learning rate: 9.488E-05 | global batch size:   128 | lm loss: 1.844001E+00 | loss scale: 262144.0 | grad norm: 0.484 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.111 | tokens per gpu per second (tgs): 1927.116 | TFLOPs: 15.51 |
g0314: [2024-08-03 01:37:00,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=5440, skipped=2, lr=[9.50534144e-05, 9.50534144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5440 loss: 1.8000 iter time (s): 4.616 samples/sec: 27.730
g0332:  iteration     5440/10000000 | consumed samples:       696320 | consumed tokens:   1426063360 | elapsed time per iteration (ms): 4648.7 | learning rate: 9.505E-05 | global batch size:   128 | lm loss: 1.845746E+00 | loss scale: 262144.0 | grad norm: 0.466 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.535 | tokens per gpu per second (tgs): 1762.231 | TFLOPs: 14.18 |
g0314: [2024-08-03 01:37:56,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=5450, skipped=2, lr=[9.522817706666667e-05, 9.522817706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5450 loss: 1.9060 iter time (s): 5.593 samples/sec: 22.886
g0332:  iteration     5450/10000000 | consumed samples:       697600 | consumed tokens:   1428684800 | elapsed time per iteration (ms): 5630.6 | learning rate: 9.523E-05 | global batch size:   128 | lm loss: 1.830918E+00 | loss scale: 262144.0 | grad norm: 0.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.733 | tokens per gpu per second (tgs): 1454.903 | TFLOPs: 11.71 |
g0314: [2024-08-03 01:38:54,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=5460, skipped=2, lr=[9.540293973333334e-05, 9.540293973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5460 loss: 1.8203 iter time (s): 5.745 samples/sec: 22.281
g0332:  iteration     5460/10000000 | consumed samples:       698880 | consumed tokens:   1431306240 | elapsed time per iteration (ms): 5777.9 | learning rate: 9.540E-05 | global batch size:   128 | lm loss: 1.848674E+00 | loss scale: 262144.0 | grad norm: 0.484 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.154 | tokens per gpu per second (tgs): 1417.825 | TFLOPs: 11.41 |
g0314: [2024-08-03 01:39:48,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=5470, skipped=2, lr=[9.55777024e-05, 9.55777024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5470 loss: 1.7396 iter time (s): 5.377 samples/sec: 23.805
g0332:  iteration     5470/10000000 | consumed samples:       700160 | consumed tokens:   1433927680 | elapsed time per iteration (ms): 5410.4 | learning rate: 9.558E-05 | global batch size:   128 | lm loss: 1.818146E+00 | loss scale: 262144.0 | grad norm: 0.473 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.658 | tokens per gpu per second (tgs): 1514.134 | TFLOPs: 12.18 |
g0314: [2024-08-03 01:40:46,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=5480, skipped=2, lr=[9.575246506666667e-05, 9.575246506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5480 loss: 1.8824 iter time (s): 5.727 samples/sec: 22.352
g0332:  iteration     5480/10000000 | consumed samples:       701440 | consumed tokens:   1436549120 | elapsed time per iteration (ms): 5766.7 | learning rate: 9.575E-05 | global batch size:   128 | lm loss: 1.823524E+00 | loss scale: 262144.0 | grad norm: 0.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.196 | tokens per gpu per second (tgs): 1420.570 | TFLOPs: 11.43 |
g0314: [2024-08-03 01:41:47,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=5490, skipped=2, lr=[9.592722773333335e-05, 9.592722773333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5490 loss: 1.8101 iter time (s): 6.038 samples/sec: 21.199
g0332:  iteration     5490/10000000 | consumed samples:       702720 | consumed tokens:   1439170560 | elapsed time per iteration (ms): 6071.2 | learning rate: 9.593E-05 | global batch size:   128 | lm loss: 1.838111E+00 | loss scale: 262144.0 | grad norm: 0.463 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.083 | tokens per gpu per second (tgs): 1349.326 | TFLOPs: 10.86 |
g0314: [2024-08-03 01:42:29,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=5500, skipped=2, lr=[9.610199040000002e-05, 9.610199040000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5500 loss: 1.8014 iter time (s): 4.248 samples/sec: 30.129
g0332:  iteration     5500/10000000 | consumed samples:       704000 | consumed tokens:   1441792000 | elapsed time per iteration (ms): 4280.9 | learning rate: 9.610E-05 | global batch size:   128 | lm loss: 1.844642E+00 | loss scale: 262144.0 | grad norm: 0.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.900 | tokens per gpu per second (tgs): 1913.628 | TFLOPs: 15.40 |
g0314: [2024-08-03 01:43:11,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=5510, skipped=2, lr=[9.627675306666668e-05, 9.627675306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5510 loss: 1.8824 iter time (s): 4.141 samples/sec: 30.913
g0332:  iteration     5510/10000000 | consumed samples:       705280 | consumed tokens:   1444413440 | elapsed time per iteration (ms): 4173.5 | learning rate: 9.628E-05 | global batch size:   128 | lm loss: 1.814312E+00 | loss scale: 262144.0 | grad norm: 0.466 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.670 | tokens per gpu per second (tgs): 1962.848 | TFLOPs: 15.80 |
g0314: [2024-08-03 01:43:54,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=5520, skipped=2, lr=[9.645151573333335e-05, 9.645151573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5520 loss: 1.8377 iter time (s): 4.274 samples/sec: 29.945
g0332:  iteration     5520/10000000 | consumed samples:       706560 | consumed tokens:   1447034880 | elapsed time per iteration (ms): 4307.0 | learning rate: 9.645E-05 | global batch size:   128 | lm loss: 1.841023E+00 | loss scale: 262144.0 | grad norm: 0.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.719 | tokens per gpu per second (tgs): 1902.036 | TFLOPs: 15.31 |
g0314: [2024-08-03 01:44:38,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=5530, skipped=2, lr=[9.662627840000001e-05, 9.662627840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5530 loss: 1.7891 iter time (s): 4.293 samples/sec: 29.813
g0332:  iteration     5530/10000000 | consumed samples:       707840 | consumed tokens:   1449656320 | elapsed time per iteration (ms): 4326.1 | learning rate: 9.663E-05 | global batch size:   128 | lm loss: 1.828947E+00 | loss scale: 262144.0 | grad norm: 0.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.588 | tokens per gpu per second (tgs): 1893.603 | TFLOPs: 15.24 |
g0314: [2024-08-03 01:45:20,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=5540, skipped=2, lr=[9.680104106666668e-05, 9.680104106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5540 loss: 1.8078 iter time (s): 4.194 samples/sec: 30.521
g0332:  iteration     5540/10000000 | consumed samples:       709120 | consumed tokens:   1452277760 | elapsed time per iteration (ms): 4226.5 | learning rate: 9.680E-05 | global batch size:   128 | lm loss: 1.820255E+00 | loss scale: 262144.0 | grad norm: 0.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.285 | tokens per gpu per second (tgs): 1938.260 | TFLOPs: 15.60 |
g0314: [2024-08-03 01:46:02,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=5550, skipped=2, lr=[9.697580373333335e-05, 9.697580373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5550 loss: 1.7953 iter time (s): 4.204 samples/sec: 30.449
g0332:  iteration     5550/10000000 | consumed samples:       710400 | consumed tokens:   1454899200 | elapsed time per iteration (ms): 4236.5 | learning rate: 9.698E-05 | global batch size:   128 | lm loss: 1.805587E+00 | loss scale: 262144.0 | grad norm: 0.465 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.214 | tokens per gpu per second (tgs): 1933.680 | TFLOPs: 15.56 |
g0314: [2024-08-03 01:46:44,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=5560, skipped=2, lr=[9.715056640000001e-05, 9.715056640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5560 loss: 1.8514 iter time (s): 4.111 samples/sec: 31.137
g0332:  iteration     5560/10000000 | consumed samples:       711680 | consumed tokens:   1457520640 | elapsed time per iteration (ms): 4143.4 | learning rate: 9.715E-05 | global batch size:   128 | lm loss: 1.837411E+00 | loss scale: 262144.0 | grad norm: 0.457 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.892 | tokens per gpu per second (tgs): 1977.117 | TFLOPs: 15.91 |
g0314: [2024-08-03 01:47:40,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=5570, skipped=2, lr=[9.732532906666668e-05, 9.732532906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5570 loss: 1.8573 iter time (s): 5.592 samples/sec: 22.891
g0332:  iteration     5570/10000000 | consumed samples:       712960 | consumed tokens:   1460142080 | elapsed time per iteration (ms): 5633.0 | learning rate: 9.733E-05 | global batch size:   128 | lm loss: 1.827505E+00 | loss scale: 262144.0 | grad norm: 0.467 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.723 | tokens per gpu per second (tgs): 1454.288 | TFLOPs: 11.70 |
g0314: [2024-08-03 01:48:42,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=5580, skipped=2, lr=[9.750009173333334e-05, 9.750009173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5580 loss: 1.7671 iter time (s): 6.159 samples/sec: 20.784
g0332:  iteration     5580/10000000 | consumed samples:       714240 | consumed tokens:   1462763520 | elapsed time per iteration (ms): 6191.5 | learning rate: 9.750E-05 | global batch size:   128 | lm loss: 1.817878E+00 | loss scale: 262144.0 | grad norm: 0.465 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.673 | tokens per gpu per second (tgs): 1323.098 | TFLOPs: 10.65 |
g0314: [2024-08-03 01:49:47,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=5590, skipped=2, lr=[9.767485440000001e-05, 9.767485440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5590 loss: 1.8725 iter time (s): 6.443 samples/sec: 19.865
g0332:  iteration     5590/10000000 | consumed samples:       715520 | consumed tokens:   1465384960 | elapsed time per iteration (ms): 6476.1 | learning rate: 9.767E-05 | global batch size:   128 | lm loss: 1.824273E+00 | loss scale: 262144.0 | grad norm: 0.463 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.765 | tokens per gpu per second (tgs): 1264.958 | TFLOPs: 10.18 |
g0314: [2024-08-03 01:50:29,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=5600, skipped=2, lr=[9.784961706666668e-05, 9.784961706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5600 loss: 1.8715 iter time (s): 4.246 samples/sec: 30.143
g0332:  iteration     5600/10000000 | consumed samples:       716800 | consumed tokens:   1468006400 | elapsed time per iteration (ms): 4279.0 | learning rate: 9.785E-05 | global batch size:   128 | lm loss: 1.820855E+00 | loss scale: 262144.0 | grad norm: 1.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.913 | tokens per gpu per second (tgs): 1914.458 | TFLOPs: 15.41 |
g0314: [2024-08-03 01:51:12,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=5610, skipped=2, lr=[9.802437973333334e-05, 9.802437973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5610 loss: 1.7969 iter time (s): 4.273 samples/sec: 29.954
g0332:  iteration     5610/10000000 | consumed samples:       718080 | consumed tokens:   1470627840 | elapsed time per iteration (ms): 4306.0 | learning rate: 9.802E-05 | global batch size:   128 | lm loss: 1.836479E+00 | loss scale: 262144.0 | grad norm: 0.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.726 | tokens per gpu per second (tgs): 1902.477 | TFLOPs: 15.31 |
g0314: [2024-08-03 01:51:56,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=5620, skipped=2, lr=[9.819914240000001e-05, 9.819914240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5620 loss: 1.7716 iter time (s): 4.311 samples/sec: 29.690
g0332:  iteration     5620/10000000 | consumed samples:       719360 | consumed tokens:   1473249280 | elapsed time per iteration (ms): 4345.4 | learning rate: 9.820E-05 | global batch size:   128 | lm loss: 1.818123E+00 | loss scale: 262144.0 | grad norm: 0.462 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.456 | tokens per gpu per second (tgs): 1885.215 | TFLOPs: 15.17 |
g0314: [2024-08-03 01:52:41,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=5630, skipped=2, lr=[9.837390506666667e-05, 9.837390506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5630 loss: 1.8117 iter time (s): 4.457 samples/sec: 28.721
g0332:  iteration     5630/10000000 | consumed samples:       720640 | consumed tokens:   1475870720 | elapsed time per iteration (ms): 4490.5 | learning rate: 9.837E-05 | global batch size:   128 | lm loss: 1.823443E+00 | loss scale: 262144.0 | grad norm: 0.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.505 | tokens per gpu per second (tgs): 1824.295 | TFLOPs: 14.68 |
g0314: [2024-08-03 01:53:22,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=5640, skipped=2, lr=[9.854866773333334e-05, 9.854866773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5640 loss: 1.8134 iter time (s): 4.132 samples/sec: 30.981
g0332:  iteration     5640/10000000 | consumed samples:       721920 | consumed tokens:   1478492160 | elapsed time per iteration (ms): 4164.8 | learning rate: 9.855E-05 | global batch size:   128 | lm loss: 1.835870E+00 | loss scale: 262144.0 | grad norm: 0.470 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.734 | tokens per gpu per second (tgs): 1966.955 | TFLOPs: 15.83 |
g0314: [2024-08-03 01:54:05,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=5650, skipped=2, lr=[9.87234304e-05, 9.87234304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5650 loss: 1.8441 iter time (s): 4.206 samples/sec: 30.430
g0332:  iteration     5650/10000000 | consumed samples:       723200 | consumed tokens:   1481113600 | elapsed time per iteration (ms): 4239.0 | learning rate: 9.872E-05 | global batch size:   128 | lm loss: 1.862081E+00 | loss scale: 262144.0 | grad norm: 0.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.196 | tokens per gpu per second (tgs): 1932.552 | TFLOPs: 15.55 |
g0314: [2024-08-03 01:54:48,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=5660, skipped=2, lr=[9.889819306666667e-05, 9.889819306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5660 loss: 1.8949 iter time (s): 4.269 samples/sec: 29.984
g0332:  iteration     5660/10000000 | consumed samples:       724480 | consumed tokens:   1483735040 | elapsed time per iteration (ms): 4301.9 | learning rate: 9.890E-05 | global batch size:   128 | lm loss: 1.834730E+00 | loss scale: 262144.0 | grad norm: 0.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.754 | tokens per gpu per second (tgs): 1904.276 | TFLOPs: 15.32 |
g0314: [2024-08-03 01:55:31,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=5670, skipped=2, lr=[9.907295573333335e-05, 9.907295573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5670 loss: 1.8698 iter time (s): 4.296 samples/sec: 29.792
g0332:  iteration     5670/10000000 | consumed samples:       725760 | consumed tokens:   1486356480 | elapsed time per iteration (ms): 4329.1 | learning rate: 9.907E-05 | global batch size:   128 | lm loss: 1.811628E+00 | loss scale: 262144.0 | grad norm: 0.493 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.568 | tokens per gpu per second (tgs): 1892.320 | TFLOPs: 15.23 |
g0314: [2024-08-03 01:56:14,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=5680, skipped=2, lr=[9.924771840000002e-05, 9.924771840000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5680 loss: 1.8118 iter time (s): 4.256 samples/sec: 30.078
g0332:  iteration     5680/10000000 | consumed samples:       727040 | consumed tokens:   1488977920 | elapsed time per iteration (ms): 4289.3 | learning rate: 9.925E-05 | global batch size:   128 | lm loss: 1.824681E+00 | loss scale: 262144.0 | grad norm: 0.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.842 | tokens per gpu per second (tgs): 1909.869 | TFLOPs: 15.37 |
g0314: [2024-08-03 01:56:56,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=5690, skipped=2, lr=[9.942248106666668e-05, 9.942248106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5690 loss: 1.7804 iter time (s): 4.210 samples/sec: 30.407
g0332:  iteration     5690/10000000 | consumed samples:       728320 | consumed tokens:   1491599360 | elapsed time per iteration (ms): 4242.1 | learning rate: 9.942E-05 | global batch size:   128 | lm loss: 1.812428E+00 | loss scale: 262144.0 | grad norm: 0.483 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.174 | tokens per gpu per second (tgs): 1931.118 | TFLOPs: 15.54 |
g0314: [2024-08-03 01:57:40,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=5700, skipped=2, lr=[9.959724373333335e-05, 9.959724373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5700 loss: 1.8424 iter time (s): 4.275 samples/sec: 29.939
g0332:  iteration     5700/10000000 | consumed samples:       729600 | consumed tokens:   1494220800 | elapsed time per iteration (ms): 4308.2 | learning rate: 9.960E-05 | global batch size:   128 | lm loss: 1.810229E+00 | loss scale: 262144.0 | grad norm: 0.451 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.711 | tokens per gpu per second (tgs): 1901.492 | TFLOPs: 15.30 |
g0314: [2024-08-03 01:58:22,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=5710, skipped=2, lr=[9.977200640000002e-05, 9.977200640000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5710 loss: 1.7948 iter time (s): 4.233 samples/sec: 30.239
g0332:  iteration     5710/10000000 | consumed samples:       730880 | consumed tokens:   1496842240 | elapsed time per iteration (ms): 4265.5 | learning rate: 9.977E-05 | global batch size:   128 | lm loss: 1.806263E+00 | loss scale: 262144.0 | grad norm: 0.463 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.009 | tokens per gpu per second (tgs): 1920.545 | TFLOPs: 15.45 |
g0314: [2024-08-03 01:59:05,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=5720, skipped=2, lr=[9.994676906666668e-05, 9.994676906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5720 loss: 1.8405 iter time (s): 4.206 samples/sec: 30.429
g0332:  iteration     5720/10000000 | consumed samples:       732160 | consumed tokens:   1499463680 | elapsed time per iteration (ms): 4239.5 | learning rate: 9.995E-05 | global batch size:   128 | lm loss: 1.824869E+00 | loss scale: 262144.0 | grad norm: 0.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.192 | tokens per gpu per second (tgs): 1932.306 | TFLOPs: 15.55 |
g0314: [2024-08-03 01:59:46,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=5730, skipped=2, lr=[0.00010012153173333335, 0.00010012153173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5730 loss: 1.7775 iter time (s): 4.142 samples/sec: 30.902
g0332:  iteration     5730/10000000 | consumed samples:       733440 | consumed tokens:   1502085120 | elapsed time per iteration (ms): 4175.2 | learning rate: 1.001E-04 | global batch size:   128 | lm loss: 1.810048E+00 | loss scale: 262144.0 | grad norm: 0.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.657 | tokens per gpu per second (tgs): 1962.056 | TFLOPs: 15.79 |
g0314: [2024-08-03 02:00:29,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=5740, skipped=2, lr=[0.00010029629440000001, 0.00010029629440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5740 loss: 1.8201 iter time (s): 4.211 samples/sec: 30.396
g0332:  iteration     5740/10000000 | consumed samples:       734720 | consumed tokens:   1504706560 | elapsed time per iteration (ms): 4243.8 | learning rate: 1.003E-04 | global batch size:   128 | lm loss: 1.825776E+00 | loss scale: 262144.0 | grad norm: 0.443 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.162 | tokens per gpu per second (tgs): 1930.351 | TFLOPs: 15.53 |
g0314: [2024-08-03 02:01:10,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=5750, skipped=2, lr=[0.00010047105706666668, 0.00010047105706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5750 loss: 1.7419 iter time (s): 4.137 samples/sec: 30.942
g0332:  iteration     5750/10000000 | consumed samples:       736000 | consumed tokens:   1507328000 | elapsed time per iteration (ms): 4169.7 | learning rate: 1.005E-04 | global batch size:   128 | lm loss: 1.826596E+00 | loss scale: 262144.0 | grad norm: 0.455 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.698 | tokens per gpu per second (tgs): 1964.649 | TFLOPs: 15.81 |
g0314: [2024-08-03 02:01:55,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=5760, skipped=2, lr=[0.00010064581973333335, 0.00010064581973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5760 loss: 1.7648 iter time (s): 4.375 samples/sec: 29.255
g0332:  iteration     5760/10000000 | consumed samples:       737280 | consumed tokens:   1509949440 | elapsed time per iteration (ms): 4408.3 | learning rate: 1.006E-04 | global batch size:   128 | lm loss: 1.823832E+00 | loss scale: 262144.0 | grad norm: 0.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.036 | tokens per gpu per second (tgs): 1858.294 | TFLOPs: 14.95 |
g0314: [2024-08-03 02:02:38,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=5770, skipped=2, lr=[0.00010082058240000001, 0.00010082058240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5770 loss: 1.7952 iter time (s): 4.305 samples/sec: 29.732
g0332:  iteration     5770/10000000 | consumed samples:       738560 | consumed tokens:   1512570880 | elapsed time per iteration (ms): 4337.9 | learning rate: 1.008E-04 | global batch size:   128 | lm loss: 1.799900E+00 | loss scale: 262144.0 | grad norm: 0.441 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.508 | tokens per gpu per second (tgs): 1888.486 | TFLOPs: 15.20 |
g0314: [2024-08-03 02:04:00,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=5780, skipped=2, lr=[0.00010099534506666668, 0.00010099534506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5780 loss: 1.7832 iter time (s): 8.204 samples/sec: 15.603
g0332:  iteration     5780/10000000 | consumed samples:       739840 | consumed tokens:   1515192320 | elapsed time per iteration (ms): 8238.4 | learning rate: 1.010E-04 | global batch size:   128 | lm loss: 1.814583E+00 | loss scale: 262144.0 | grad norm: 0.463 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.537 | tokens per gpu per second (tgs): 994.366 | TFLOPs: 8.00 |
g0314: [2024-08-03 02:05:54,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=5790, skipped=2, lr=[0.00010117010773333334, 0.00010117010773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5790 loss: 1.8202 iter time (s): 11.342 samples/sec: 11.286
g0332:  iteration     5790/10000000 | consumed samples:       741120 | consumed tokens:   1517813760 | elapsed time per iteration (ms): 11376.1 | learning rate: 1.012E-04 | global batch size:   128 | lm loss: 1.794900E+00 | loss scale: 262144.0 | grad norm: 0.443 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.252 | tokens per gpu per second (tgs): 720.106 | TFLOPs: 5.79 |
g0314: [2024-08-03 02:06:55,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=5800, skipped=2, lr=[0.00010134487040000001, 0.00010134487040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5800 loss: 1.8667 iter time (s): 6.052 samples/sec: 21.150
g0332:  iteration     5800/10000000 | consumed samples:       742400 | consumed tokens:   1520435200 | elapsed time per iteration (ms): 6085.2 | learning rate: 1.013E-04 | global batch size:   128 | lm loss: 1.806717E+00 | loss scale: 262144.0 | grad norm: 0.449 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.035 | tokens per gpu per second (tgs): 1346.227 | TFLOPs: 10.83 |
g0314: [2024-08-03 02:07:48,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=5810, skipped=2, lr=[0.00010151963306666668, 0.00010151963306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5810 loss: 1.8615 iter time (s): 5.236 samples/sec: 24.444
g0332:  iteration     5810/10000000 | consumed samples:       743680 | consumed tokens:   1523056640 | elapsed time per iteration (ms): 5270.8 | learning rate: 1.015E-04 | global batch size:   128 | lm loss: 1.825722E+00 | loss scale: 262144.0 | grad norm: 0.443 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.285 | tokens per gpu per second (tgs): 1554.214 | TFLOPs: 12.51 |
g0314: [2024-08-03 02:08:32,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=5820, skipped=2, lr=[0.00010169439573333333, 0.00010169439573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5820 loss: 1.7889 iter time (s): 4.398 samples/sec: 29.104
g0332:  iteration     5820/10000000 | consumed samples:       744960 | consumed tokens:   1525678080 | elapsed time per iteration (ms): 4431.2 | learning rate: 1.017E-04 | global batch size:   128 | lm loss: 1.817010E+00 | loss scale: 262144.0 | grad norm: 0.439 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.886 | tokens per gpu per second (tgs): 1848.712 | TFLOPs: 14.88 |
g0314: [2024-08-03 02:09:13,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=5830, skipped=2, lr=[0.0001018691584, 0.0001018691584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5830 loss: 1.7829 iter time (s): 4.121 samples/sec: 31.063
g0332:  iteration     5830/10000000 | consumed samples:       746240 | consumed tokens:   1528299520 | elapsed time per iteration (ms): 4153.9 | learning rate: 1.019E-04 | global batch size:   128 | lm loss: 1.798693E+00 | loss scale: 262144.0 | grad norm: 0.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.815 | tokens per gpu per second (tgs): 1972.130 | TFLOPs: 15.87 |
g0314: [2024-08-03 02:09:57,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=5840, skipped=2, lr=[0.00010204392106666666, 0.00010204392106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5840 loss: 1.8453 iter time (s): 4.326 samples/sec: 29.591
g0332:  iteration     5840/10000000 | consumed samples:       747520 | consumed tokens:   1530920960 | elapsed time per iteration (ms): 4358.6 | learning rate: 1.020E-04 | global batch size:   128 | lm loss: 1.811844E+00 | loss scale: 262144.0 | grad norm: 0.457 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.367 | tokens per gpu per second (tgs): 1879.491 | TFLOPs: 15.12 |
g0314: [2024-08-03 02:10:41,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=5850, skipped=2, lr=[0.00010221868373333333, 0.00010221868373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5850 loss: 1.7693 iter time (s): 4.365 samples/sec: 29.322
g0332:  iteration     5850/10000000 | consumed samples:       748800 | consumed tokens:   1533542400 | elapsed time per iteration (ms): 4399.3 | learning rate: 1.022E-04 | global batch size:   128 | lm loss: 1.792604E+00 | loss scale: 262144.0 | grad norm: 0.444 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.095 | tokens per gpu per second (tgs): 1862.107 | TFLOPs: 14.98 |
g0314: [2024-08-03 02:11:32,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=5860, skipped=2, lr=[0.0001023934464, 0.0001023934464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5860 loss: 1.8295 iter time (s): 5.079 samples/sec: 25.200
g0332:  iteration     5860/10000000 | consumed samples:       750080 | consumed tokens:   1536163840 | elapsed time per iteration (ms): 5111.9 | learning rate: 1.024E-04 | global batch size:   128 | lm loss: 1.817600E+00 | loss scale: 262144.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.039 | tokens per gpu per second (tgs): 1602.521 | TFLOPs: 12.90 |
g0314: [2024-08-03 02:12:24,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=5870, skipped=2, lr=[0.00010256820906666666, 0.00010256820906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5870 loss: 1.7964 iter time (s): 5.154 samples/sec: 24.836
g0332:  iteration     5870/10000000 | consumed samples:       751360 | consumed tokens:   1538785280 | elapsed time per iteration (ms): 5186.4 | learning rate: 1.026E-04 | global batch size:   128 | lm loss: 1.805637E+00 | loss scale: 262144.0 | grad norm: 0.455 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.680 | tokens per gpu per second (tgs): 1579.524 | TFLOPs: 12.71 |
g0314: [2024-08-03 02:13:14,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=5880, skipped=2, lr=[0.00010274297173333333, 0.00010274297173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5880 loss: 1.6729 iter time (s): 5.005 samples/sec: 25.572
g0332:  iteration     5880/10000000 | consumed samples:       752640 | consumed tokens:   1541406720 | elapsed time per iteration (ms): 5038.5 | learning rate: 1.027E-04 | global batch size:   128 | lm loss: 1.803478E+00 | loss scale: 262144.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.404 | tokens per gpu per second (tgs): 1625.877 | TFLOPs: 13.08 |
g0314: [2024-08-03 02:15:27,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=5890, skipped=2, lr=[0.00010291773439999999, 0.00010291773439999999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5890 loss: 1.8446 iter time (s): 13.267 samples/sec: 9.648
g0332:  iteration     5890/10000000 | consumed samples:       753920 | consumed tokens:   1544028160 | elapsed time per iteration (ms): 13300.3 | learning rate: 1.029E-04 | global batch size:   128 | lm loss: 1.832409E+00 | loss scale: 262144.0 | grad norm: 0.434 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.624 | tokens per gpu per second (tgs): 615.925 | TFLOPs: 4.96 |
g0314: [2024-08-03 02:17:26,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=5900, skipped=2, lr=[0.00010309249706666666, 0.00010309249706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5900 loss: 1.7892 iter time (s): 11.868 samples/sec: 10.785
g0332:  iteration     5900/10000000 | consumed samples:       755200 | consumed tokens:   1546649600 | elapsed time per iteration (ms): 11900.8 | learning rate: 1.031E-04 | global batch size:   128 | lm loss: 1.811065E+00 | loss scale: 262144.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.756 | tokens per gpu per second (tgs): 688.359 | TFLOPs: 5.54 |
g0314: [2024-08-03 02:18:44,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=5910, skipped=2, lr=[0.00010326725973333332, 0.00010326725973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5910 loss: 1.7921 iter time (s): 7.760 samples/sec: 16.494
g0332:  iteration     5910/10000000 | consumed samples:       756480 | consumed tokens:   1549271040 | elapsed time per iteration (ms): 7795.8 | learning rate: 1.033E-04 | global batch size:   128 | lm loss: 1.787847E+00 | loss scale: 262144.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.419 | tokens per gpu per second (tgs): 1050.819 | TFLOPs: 8.46 |
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0314: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0316: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0329: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0318: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0325: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0332: [2024-08-03 02:19:23,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0314: [2024-08-03 02:19:23,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0319: [2024-08-03 02:19:23,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0314: [2024-08-03 02:19:45,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=5920, skipped=2, lr=[0.0001034420224, 0.0001034420224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5920 loss: 1.7835 iter time (s): 5.977 samples/sec: 21.416
g0332:  iteration     5920/10000000 | consumed samples:       757760 | consumed tokens:   1551892480 | elapsed time per iteration (ms): 6010.4 | learning rate: 1.034E-04 | global batch size:   128 | lm loss: 1.789982E+00 | loss scale: 524288.0 | grad norm: 0.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.296 | tokens per gpu per second (tgs): 1362.964 | TFLOPs: 10.97 |
g0314: [2024-08-03 02:20:31,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=5930, skipped=2, lr=[0.00010361678506666667, 0.00010361678506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5930 loss: 1.7864 iter time (s): 4.588 samples/sec: 27.898
g0332:  iteration     5930/10000000 | consumed samples:       759040 | consumed tokens:   1554513920 | elapsed time per iteration (ms): 4621.4 | learning rate: 1.036E-04 | global batch size:   128 | lm loss: 1.792164E+00 | loss scale: 524288.0 | grad norm: 0.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.697 | tokens per gpu per second (tgs): 1772.626 | TFLOPs: 14.26 |
g0314: [2024-08-03 02:21:14,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=5940, skipped=2, lr=[0.00010379154773333334, 0.00010379154773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5940 loss: 1.7727 iter time (s): 4.343 samples/sec: 29.472
g0332:  iteration     5940/10000000 | consumed samples:       760320 | consumed tokens:   1557135360 | elapsed time per iteration (ms): 4376.3 | learning rate: 1.038E-04 | global batch size:   128 | lm loss: 1.811643E+00 | loss scale: 524288.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.248 | tokens per gpu per second (tgs): 1871.901 | TFLOPs: 15.06 |
g0314: [2024-08-03 02:21:56,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=5950, skipped=2, lr=[0.0001039663104, 0.0001039663104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5950 loss: 1.7453 iter time (s): 4.079 samples/sec: 31.380
g0332:  iteration     5950/10000000 | consumed samples:       761600 | consumed tokens:   1559756800 | elapsed time per iteration (ms): 4111.4 | learning rate: 1.040E-04 | global batch size:   128 | lm loss: 1.796277E+00 | loss scale: 524288.0 | grad norm: 0.435 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.133 | tokens per gpu per second (tgs): 1992.498 | TFLOPs: 16.03 |
g0314: [2024-08-03 02:22:38,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=5960, skipped=2, lr=[0.00010414107306666667, 0.00010414107306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5960 loss: 1.8119 iter time (s): 4.189 samples/sec: 30.559
g0332:  iteration     5960/10000000 | consumed samples:       762880 | consumed tokens:   1562378240 | elapsed time per iteration (ms): 4221.2 | learning rate: 1.041E-04 | global batch size:   128 | lm loss: 1.805769E+00 | loss scale: 524288.0 | grad norm: 0.469 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.323 | tokens per gpu per second (tgs): 1940.689 | TFLOPs: 15.62 |
g0314: [2024-08-03 02:23:20,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=5970, skipped=2, lr=[0.00010431583573333333, 0.00010431583573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5970 loss: 1.8341 iter time (s): 4.160 samples/sec: 30.769
g0332:  iteration     5970/10000000 | consumed samples:       764160 | consumed tokens:   1564999680 | elapsed time per iteration (ms): 4192.4 | learning rate: 1.043E-04 | global batch size:   128 | lm loss: 1.817437E+00 | loss scale: 524288.0 | grad norm: 0.425 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.532 | tokens per gpu per second (tgs): 1954.020 | TFLOPs: 15.72 |
g0314: [2024-08-03 02:24:05,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=5980, skipped=2, lr=[0.0001044905984, 0.0001044905984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5980 loss: 1.8123 iter time (s): 4.492 samples/sec: 28.496
g0332:  iteration     5980/10000000 | consumed samples:       765440 | consumed tokens:   1567621120 | elapsed time per iteration (ms): 4524.8 | learning rate: 1.045E-04 | global batch size:   128 | lm loss: 1.802289E+00 | loss scale: 524288.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.288 | tokens per gpu per second (tgs): 1810.458 | TFLOPs: 14.57 |
g0314: [2024-08-03 02:24:47,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=5990, skipped=2, lr=[0.00010466536106666667, 0.00010466536106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 5990 loss: 1.7473 iter time (s): 4.163 samples/sec: 30.749
g0332:  iteration     5990/10000000 | consumed samples:       766720 | consumed tokens:   1570242560 | elapsed time per iteration (ms): 4195.2 | learning rate: 1.047E-04 | global batch size:   128 | lm loss: 1.817576E+00 | loss scale: 524288.0 | grad norm: 0.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.511 | tokens per gpu per second (tgs): 1952.724 | TFLOPs: 15.71 |
g0314: [2024-08-03 02:25:29,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=2, lr=[0.00010484012373333333, 0.00010484012373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6000 loss: 1.7838 iter time (s): 4.151 samples/sec: 30.837
g0332:  iteration     6000/10000000 | consumed samples:       768000 | consumed tokens:   1572864000 | elapsed time per iteration (ms): 4183.4 | learning rate: 1.048E-04 | global batch size:   128 | lm loss: 1.785030E+00 | loss scale: 524288.0 | grad norm: 0.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.597 | tokens per gpu per second (tgs): 1958.205 | TFLOPs: 15.76 |
g0332: ------------------------------------------------------------------------------------------------
g0332:  validation loss at iteration 6000 | lm loss value: 1.803598E+00 | lm loss PPL: 6.071452E+00 | 
g0332: ------------------------------------------------------------------------------------------------
g0314: saving checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: [2024-08-03 02:32:00,459] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
g0332: [2024-08-03 02:32:00,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0332: [2024-08-03 02:32:00,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0332: [2024-08-03 02:32:00,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0314: [2024-08-03 02:32:00,466] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0314: [2024-08-03 02:32:00,466] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0314: [2024-08-03 02:32:00,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0325: [2024-08-03 02:32:00,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0319: [2024-08-03 02:32:00,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0319: [2024-08-03 02:32:00,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0319: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0325: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0325: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0329: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0329: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0329: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0316: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0316: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0316: [2024-08-03 02:32:00,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0320: [2024-08-03 02:32:00,469] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0320: [2024-08-03 02:32:00,469] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0320: [2024-08-03 02:32:00,469] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0318: [2024-08-03 02:32:00,470] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0318: [2024-08-03 02:32:00,470] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0318: [2024-08-03 02:32:00,470] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0332: [2024-08-03 02:32:00,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt...
g0316: [2024-08-03 02:32:00,500] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt...
g0325: [2024-08-03 02:32:00,500] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt...
g0319: [2024-08-03 02:32:00,500] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt...
g0329: [2024-08-03 02:32:00,501] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt...
g0318: [2024-08-03 02:32:00,503] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt...
g0320: [2024-08-03 02:32:00,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt...
g0314: [2024-08-03 02:32:00,520] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt...
g0318: [2024-08-03 02:32:00,620] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt.
g0319: [2024-08-03 02:32:00,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt.
g0329: [2024-08-03 02:32:00,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt.
g0325: [2024-08-03 02:32:00,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt.
g0332: [2024-08-03 02:32:00,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt.
g0332: [2024-08-03 02:32:00,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt...
g0332: [2024-08-03 02:32:00,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt.
g0320: [2024-08-03 02:32:00,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt.
g0318: [2024-08-03 02:32:00,656] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt...
g0319: [2024-08-03 02:32:00,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt...
g0316: [2024-08-03 02:32:00,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt.
g0329: [2024-08-03 02:32:00,671] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt...
g0325: [2024-08-03 02:32:00,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt...
g0320: [2024-08-03 02:32:00,682] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt...
g0332: [2024-08-03 02:32:00,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt...
g0316: [2024-08-03 02:32:00,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt...
g0325: [2024-08-03 02:32:00,791] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt.
g0329: [2024-08-03 02:32:00,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt.
g0319: [2024-08-03 02:32:00,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt.
g0325: [2024-08-03 02:32:00,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt...
g0320: [2024-08-03 02:32:00,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt.
g0319: [2024-08-03 02:32:00,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt...
g0329: [2024-08-03 02:32:00,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt...
g0316: [2024-08-03 02:32:00,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt.
g0318: [2024-08-03 02:32:00,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt.
g0320: [2024-08-03 02:32:00,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt...
g0316: [2024-08-03 02:32:00,879] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt...
g0318: [2024-08-03 02:32:00,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt...
g0314: [2024-08-03 02:32:00,940] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt.
g0314: [2024-08-03 02:32:00,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt...
g0320: [2024-08-03 02:32:00,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt.
g0320: [2024-08-03 02:32:00,992] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt...
g0329: [2024-08-03 02:32:01,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt.
g0329: [2024-08-03 02:32:01,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt...
g0319: [2024-08-03 02:32:01,028] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt.
g0319: [2024-08-03 02:32:01,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt...
g0318: [2024-08-03 02:32:01,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt.
g0318: [2024-08-03 02:32:01,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt...
g0314: [2024-08-03 02:32:01,089] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt.
g0314: [2024-08-03 02:32:01,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt...
g0316: [2024-08-03 02:32:01,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt.
g0316: [2024-08-03 02:32:01,132] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt...
g0314: [2024-08-03 02:32:01,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt.
g0314: [2024-08-03 02:32:01,244] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt...
g0314: [2024-08-03 02:32:01,357] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt.
g0314: [2024-08-03 02:32:01,358] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt
g0314: [2024-08-03 02:32:01,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt...
g0325: [2024-08-03 02:32:01,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt.
g0325: [2024-08-03 02:32:01,520] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt...
g0332: [2024-08-03 02:32:01,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt.
g0332: [2024-08-03 02:32:01,812] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt...
g0319: [2024-08-03 02:32:03,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt.
g0319: [2024-08-03 02:32:03,344] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0329: [2024-08-03 02:32:03,371] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt.
g0329: [2024-08-03 02:32:03,372] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0318: [2024-08-03 02:32:03,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt.
g0318: [2024-08-03 02:32:03,431] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0316: [2024-08-03 02:32:03,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt.
g0316: [2024-08-03 02:32:03,448] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0332: [2024-08-03 02:32:03,708] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt.
g0332: [2024-08-03 02:32:03,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0325: [2024-08-03 02:32:03,832] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt.
g0325: [2024-08-03 02:32:03,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0320: [2024-08-03 02:32:04,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt.
g0320: [2024-08-03 02:32:04,586] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0314: [2024-08-03 02:32:04,845] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt.
g0314: [2024-08-03 02:32:04,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0314:   successfully saved checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000001_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0314: Checkpoint Save GB: 22.521, GB/Sec: 5.11, Latency(second): 4.407
g0332: (min, max) time across ranks (ms):
g0332:     save-checkpoint ................................: (4407.48, 4408.02)
g0314: [2024-08-03 02:32:47,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=6010, skipped=2, lr=[0.0001050148864, 0.0001050148864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6010 loss: 1.8030 iter time (s): 4.190 samples/sec: 30.551
g0332:  iteration     6010/10000000 | consumed samples:       769280 | consumed tokens:   1575485440 | elapsed time per iteration (ms): 43777.0 | learning rate: 1.050E-04 | global batch size:   128 | lm loss: 1.780984E+00 | loss scale: 524288.0 | grad norm: 0.417 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.924 | tokens per gpu per second (tgs): 187.130 | TFLOPs: 1.51 |
g0314: [2024-08-03 02:33:29,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=6020, skipped=2, lr=[0.00010518964906666666, 0.00010518964906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6020 loss: 1.8186 iter time (s): 4.223 samples/sec: 30.310
g0332:  iteration     6020/10000000 | consumed samples:       770560 | consumed tokens:   1578106880 | elapsed time per iteration (ms): 4255.7 | learning rate: 1.052E-04 | global batch size:   128 | lm loss: 1.813907E+00 | loss scale: 524288.0 | grad norm: 0.422 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.077 | tokens per gpu per second (tgs): 1924.926 | TFLOPs: 15.49 |
g0314: [2024-08-03 02:34:11,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=6030, skipped=2, lr=[0.00010536441173333333, 0.00010536441173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6030 loss: 1.8002 iter time (s): 4.148 samples/sec: 30.859
g0332:  iteration     6030/10000000 | consumed samples:       771840 | consumed tokens:   1580728320 | elapsed time per iteration (ms): 4180.7 | learning rate: 1.054E-04 | global batch size:   128 | lm loss: 1.800536E+00 | loss scale: 524288.0 | grad norm: 0.424 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.617 | tokens per gpu per second (tgs): 1959.503 | TFLOPs: 15.77 |
g0314: [2024-08-03 02:34:53,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=6040, skipped=2, lr=[0.0001055391744, 0.0001055391744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6040 loss: 1.7914 iter time (s): 4.168 samples/sec: 30.714
g0332:  iteration     6040/10000000 | consumed samples:       773120 | consumed tokens:   1583349760 | elapsed time per iteration (ms): 4200.3 | learning rate: 1.055E-04 | global batch size:   128 | lm loss: 1.813494E+00 | loss scale: 524288.0 | grad norm: 0.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.474 | tokens per gpu per second (tgs): 1950.326 | TFLOPs: 15.69 |
g0314: [2024-08-03 02:35:35,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=6050, skipped=2, lr=[0.00010571393706666666, 0.00010571393706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6050 loss: 1.8096 iter time (s): 4.203 samples/sec: 30.455
g0332:  iteration     6050/10000000 | consumed samples:       774400 | consumed tokens:   1585971200 | elapsed time per iteration (ms): 4235.8 | learning rate: 1.057E-04 | global batch size:   128 | lm loss: 1.774487E+00 | loss scale: 524288.0 | grad norm: 0.786 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.219 | tokens per gpu per second (tgs): 1933.997 | TFLOPs: 15.56 |
g0314: [2024-08-03 02:36:19,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=6060, skipped=2, lr=[0.00010588869973333333, 0.00010588869973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6060 loss: 1.7345 iter time (s): 4.299 samples/sec: 29.775
g0332:  iteration     6060/10000000 | consumed samples:       775680 | consumed tokens:   1588592640 | elapsed time per iteration (ms): 4331.7 | learning rate: 1.059E-04 | global batch size:   128 | lm loss: 1.793884E+00 | loss scale: 524288.0 | grad norm: 0.427 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.550 | tokens per gpu per second (tgs): 1891.196 | TFLOPs: 15.22 |
g0314: [2024-08-03 02:37:03,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=6070, skipped=2, lr=[0.0001060634624, 0.0001060634624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6070 loss: 1.7536 iter time (s): 4.388 samples/sec: 29.168
g0332:  iteration     6070/10000000 | consumed samples:       776960 | consumed tokens:   1591214080 | elapsed time per iteration (ms): 4420.9 | learning rate: 1.061E-04 | global batch size:   128 | lm loss: 1.786945E+00 | loss scale: 524288.0 | grad norm: 0.429 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.954 | tokens per gpu per second (tgs): 1853.031 | TFLOPs: 14.91 |
g0314: [2024-08-03 02:37:45,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=6080, skipped=2, lr=[0.00010623822506666666, 0.00010623822506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6080 loss: 1.7740 iter time (s): 4.198 samples/sec: 30.487
g0332:  iteration     6080/10000000 | consumed samples:       778240 | consumed tokens:   1593835520 | elapsed time per iteration (ms): 4230.9 | learning rate: 1.062E-04 | global batch size:   128 | lm loss: 1.815017E+00 | loss scale: 524288.0 | grad norm: 0.420 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.253 | tokens per gpu per second (tgs): 1936.208 | TFLOPs: 15.58 |
g0314: [2024-08-03 02:38:28,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=6090, skipped=2, lr=[0.00010641298773333333, 0.00010641298773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6090 loss: 1.8492 iter time (s): 4.299 samples/sec: 29.775
g0332:  iteration     6090/10000000 | consumed samples:       779520 | consumed tokens:   1596456960 | elapsed time per iteration (ms): 4331.5 | learning rate: 1.064E-04 | global batch size:   128 | lm loss: 1.802117E+00 | loss scale: 524288.0 | grad norm: 0.430 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.551 | tokens per gpu per second (tgs): 1891.247 | TFLOPs: 15.22 |
g0314: [2024-08-03 02:39:11,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=6100, skipped=2, lr=[0.0001065877504, 0.0001065877504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6100 loss: 1.8026 iter time (s): 4.188 samples/sec: 30.560
g0332:  iteration     6100/10000000 | consumed samples:       780800 | consumed tokens:   1599078400 | elapsed time per iteration (ms): 4220.9 | learning rate: 1.066E-04 | global batch size:   128 | lm loss: 1.813666E+00 | loss scale: 524288.0 | grad norm: 0.429 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.325 | tokens per gpu per second (tgs): 1940.820 | TFLOPs: 15.62 |
g0314: [2024-08-03 02:39:52,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=6110, skipped=2, lr=[0.00010676251306666667, 0.00010676251306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6110 loss: 1.8075 iter time (s): 4.148 samples/sec: 30.859
g0332:  iteration     6110/10000000 | consumed samples:       782080 | consumed tokens:   1601699840 | elapsed time per iteration (ms): 4180.7 | learning rate: 1.068E-04 | global batch size:   128 | lm loss: 1.803312E+00 | loss scale: 524288.0 | grad norm: 0.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.617 | tokens per gpu per second (tgs): 1959.458 | TFLOPs: 15.77 |
g0314: [2024-08-03 02:40:34,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=6120, skipped=2, lr=[0.00010693727573333334, 0.00010693727573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6120 loss: 1.8206 iter time (s): 4.163 samples/sec: 30.750
g0332:  iteration     6120/10000000 | consumed samples:       783360 | consumed tokens:   1604321280 | elapsed time per iteration (ms): 4196.1 | learning rate: 1.069E-04 | global batch size:   128 | lm loss: 1.765412E+00 | loss scale: 524288.0 | grad norm: 0.419 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.505 | tokens per gpu per second (tgs): 1952.311 | TFLOPs: 15.71 |
g0314: [2024-08-03 02:41:17,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=6130, skipped=2, lr=[0.0001071120384, 0.0001071120384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6130 loss: 1.8559 iter time (s): 4.197 samples/sec: 30.495
g0332:  iteration     6130/10000000 | consumed samples:       784640 | consumed tokens:   1606942720 | elapsed time per iteration (ms): 4231.2 | learning rate: 1.071E-04 | global batch size:   128 | lm loss: 1.800543E+00 | loss scale: 524288.0 | grad norm: 0.437 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.251 | tokens per gpu per second (tgs): 1936.085 | TFLOPs: 15.58 |
g0314: [2024-08-03 02:42:00,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=6140, skipped=2, lr=[0.00010728680106666667, 0.00010728680106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6140 loss: 1.8145 iter time (s): 4.259 samples/sec: 30.051
g0332:  iteration     6140/10000000 | consumed samples:       785920 | consumed tokens:   1609564160 | elapsed time per iteration (ms): 4292.2 | learning rate: 1.073E-04 | global batch size:   128 | lm loss: 1.804969E+00 | loss scale: 524288.0 | grad norm: 0.427 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.822 | tokens per gpu per second (tgs): 1908.576 | TFLOPs: 15.36 |
g0314: [2024-08-03 02:42:43,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=6150, skipped=2, lr=[0.00010746156373333334, 0.00010746156373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6150 loss: 1.8709 iter time (s): 4.277 samples/sec: 29.926
g0332:  iteration     6150/10000000 | consumed samples:       787200 | consumed tokens:   1612185600 | elapsed time per iteration (ms): 4310.5 | learning rate: 1.075E-04 | global batch size:   128 | lm loss: 1.803573E+00 | loss scale: 524288.0 | grad norm: 0.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.695 | tokens per gpu per second (tgs): 1900.464 | TFLOPs: 15.29 |
g0314: [2024-08-03 02:43:26,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=6160, skipped=2, lr=[0.0001076363264, 0.0001076363264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6160 loss: 1.8222 iter time (s): 4.267 samples/sec: 29.995
g0332:  iteration     6160/10000000 | consumed samples:       788480 | consumed tokens:   1614807040 | elapsed time per iteration (ms): 4302.2 | learning rate: 1.076E-04 | global batch size:   128 | lm loss: 1.784853E+00 | loss scale: 524288.0 | grad norm: 0.427 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.752 | tokens per gpu per second (tgs): 1904.144 | TFLOPs: 15.32 |
g0314: [2024-08-03 02:44:08,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=6170, skipped=2, lr=[0.00010781108906666667, 0.00010781108906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6170 loss: 1.7985 iter time (s): 4.222 samples/sec: 30.316
g0332:  iteration     6170/10000000 | consumed samples:       789760 | consumed tokens:   1617428480 | elapsed time per iteration (ms): 4255.8 | learning rate: 1.078E-04 | global batch size:   128 | lm loss: 1.783243E+00 | loss scale: 524288.0 | grad norm: 0.442 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.077 | tokens per gpu per second (tgs): 1924.911 | TFLOPs: 15.49 |
g0314: [2024-08-03 02:44:51,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=6180, skipped=2, lr=[0.00010798585173333333, 0.00010798585173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6180 loss: 1.7607 iter time (s): 4.241 samples/sec: 30.184
g0332:  iteration     6180/10000000 | consumed samples:       791040 | consumed tokens:   1620049920 | elapsed time per iteration (ms): 4273.3 | learning rate: 1.080E-04 | global batch size:   128 | lm loss: 1.793786E+00 | loss scale: 524288.0 | grad norm: 0.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.954 | tokens per gpu per second (tgs): 1917.035 | TFLOPs: 15.43 |
g0314: [2024-08-03 02:45:35,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=6190, skipped=2, lr=[0.0001081606144, 0.0001081606144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6190 loss: 1.7951 iter time (s): 4.319 samples/sec: 29.640
g0332:  iteration     6190/10000000 | consumed samples:       792320 | consumed tokens:   1622671360 | elapsed time per iteration (ms): 4364.3 | learning rate: 1.082E-04 | global batch size:   128 | lm loss: 1.785833E+00 | loss scale: 524288.0 | grad norm: 0.422 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.329 | tokens per gpu per second (tgs): 1877.042 | TFLOPs: 15.10 |
g0314: [2024-08-03 02:46:17,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=6200, skipped=2, lr=[0.00010833537706666667, 0.00010833537706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6200 loss: 1.7473 iter time (s): 4.233 samples/sec: 30.236
g0332:  iteration     6200/10000000 | consumed samples:       793600 | consumed tokens:   1625292800 | elapsed time per iteration (ms): 4266.3 | learning rate: 1.083E-04 | global batch size:   128 | lm loss: 1.785819E+00 | loss scale: 524288.0 | grad norm: 0.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.003 | tokens per gpu per second (tgs): 1920.165 | TFLOPs: 15.45 |
g0314: [2024-08-03 02:47:00,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=6210, skipped=2, lr=[0.00010851013973333333, 0.00010851013973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6210 loss: 1.8028 iter time (s): 4.266 samples/sec: 30.007
g0332:  iteration     6210/10000000 | consumed samples:       794880 | consumed tokens:   1627914240 | elapsed time per iteration (ms): 4298.2 | learning rate: 1.085E-04 | global batch size:   128 | lm loss: 1.806065E+00 | loss scale: 524288.0 | grad norm: 0.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.780 | tokens per gpu per second (tgs): 1905.928 | TFLOPs: 15.34 |
g0314: [2024-08-03 02:47:43,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=6220, skipped=2, lr=[0.0001086849024, 0.0001086849024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6220 loss: 1.7684 iter time (s): 4.278 samples/sec: 29.919
g0332:  iteration     6220/10000000 | consumed samples:       796160 | consumed tokens:   1630535680 | elapsed time per iteration (ms): 4311.1 | learning rate: 1.087E-04 | global batch size:   128 | lm loss: 1.821615E+00 | loss scale: 524288.0 | grad norm: 0.408 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.691 | tokens per gpu per second (tgs): 1900.218 | TFLOPs: 15.29 |
g0314: [2024-08-03 02:48:28,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=6230, skipped=2, lr=[0.00010885966506666666, 0.00010885966506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6230 loss: 1.8242 iter time (s): 4.457 samples/sec: 28.717
g0332:  iteration     6230/10000000 | consumed samples:       797440 | consumed tokens:   1633157120 | elapsed time per iteration (ms): 4490.3 | learning rate: 1.089E-04 | global batch size:   128 | lm loss: 1.794771E+00 | loss scale: 524288.0 | grad norm: 0.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.506 | tokens per gpu per second (tgs): 1824.374 | TFLOPs: 14.68 |
g0314: [2024-08-03 02:49:13,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=6240, skipped=2, lr=[0.00010903442773333333, 0.00010903442773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6240 loss: 1.8435 iter time (s): 4.396 samples/sec: 29.119
g0332:  iteration     6240/10000000 | consumed samples:       798720 | consumed tokens:   1635778560 | elapsed time per iteration (ms): 4429.4 | learning rate: 1.090E-04 | global batch size:   128 | lm loss: 1.783675E+00 | loss scale: 524288.0 | grad norm: 0.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.898 | tokens per gpu per second (tgs): 1849.481 | TFLOPs: 14.88 |
g0314: [2024-08-03 02:49:55,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=6250, skipped=2, lr=[0.0001092091904, 0.0001092091904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6250 loss: 1.8142 iter time (s): 4.200 samples/sec: 30.475
g0332:  iteration     6250/10000000 | consumed samples:       800000 | consumed tokens:   1638400000 | elapsed time per iteration (ms): 4233.0 | learning rate: 1.092E-04 | global batch size:   128 | lm loss: 1.788340E+00 | loss scale: 524288.0 | grad norm: 0.405 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.239 | tokens per gpu per second (tgs): 1935.271 | TFLOPs: 15.57 |
g0314: [2024-08-03 02:50:37,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=6260, skipped=2, lr=[0.00010938395306666666, 0.00010938395306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6260 loss: 1.7786 iter time (s): 4.218 samples/sec: 30.349
g0332:  iteration     6260/10000000 | consumed samples:       801280 | consumed tokens:   1641021440 | elapsed time per iteration (ms): 4250.2 | learning rate: 1.094E-04 | global batch size:   128 | lm loss: 1.777859E+00 | loss scale: 524288.0 | grad norm: 0.419 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.116 | tokens per gpu per second (tgs): 1927.425 | TFLOPs: 15.51 |
g0314: [2024-08-03 02:51:20,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=6270, skipped=2, lr=[0.00010955871573333333, 0.00010955871573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6270 loss: 1.7697 iter time (s): 4.234 samples/sec: 30.230
g0332:  iteration     6270/10000000 | consumed samples:       802560 | consumed tokens:   1643642880 | elapsed time per iteration (ms): 4267.3 | learning rate: 1.096E-04 | global batch size:   128 | lm loss: 1.784652E+00 | loss scale: 524288.0 | grad norm: 0.403 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.995 | tokens per gpu per second (tgs): 1919.708 | TFLOPs: 15.45 |
g0314: [2024-08-03 02:52:03,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=6280, skipped=2, lr=[0.00010973347840000001, 0.00010973347840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6280 loss: 1.7705 iter time (s): 4.212 samples/sec: 30.389
g0332:  iteration     6280/10000000 | consumed samples:       803840 | consumed tokens:   1646264320 | elapsed time per iteration (ms): 4244.7 | learning rate: 1.097E-04 | global batch size:   128 | lm loss: 1.789200E+00 | loss scale: 524288.0 | grad norm: 0.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.155 | tokens per gpu per second (tgs): 1929.942 | TFLOPs: 15.53 |
g0314: [2024-08-03 02:52:45,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=6290, skipped=2, lr=[0.00010990824106666667, 0.00010990824106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6290 loss: 1.7755 iter time (s): 4.218 samples/sec: 30.345
g0332:  iteration     6290/10000000 | consumed samples:       805120 | consumed tokens:   1648885760 | elapsed time per iteration (ms): 4257.2 | learning rate: 1.099E-04 | global batch size:   128 | lm loss: 1.758622E+00 | loss scale: 524288.0 | grad norm: 0.427 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.067 | tokens per gpu per second (tgs): 1924.269 | TFLOPs: 15.48 |
g0314: [2024-08-03 02:53:28,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=6300, skipped=2, lr=[0.00011008300373333334, 0.00011008300373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6300 loss: 1.7861 iter time (s): 4.291 samples/sec: 29.830
g0332:  iteration     6300/10000000 | consumed samples:       806400 | consumed tokens:   1651507200 | elapsed time per iteration (ms): 4323.7 | learning rate: 1.101E-04 | global batch size:   128 | lm loss: 1.772542E+00 | loss scale: 524288.0 | grad norm: 0.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.604 | tokens per gpu per second (tgs): 1894.680 | TFLOPs: 15.25 |
g0314: [2024-08-03 02:54:11,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=6310, skipped=2, lr=[0.00011025776640000001, 0.00011025776640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6310 loss: 1.7398 iter time (s): 4.264 samples/sec: 30.018
g0332:  iteration     6310/10000000 | consumed samples:       807680 | consumed tokens:   1654128640 | elapsed time per iteration (ms): 4296.7 | learning rate: 1.103E-04 | global batch size:   128 | lm loss: 1.764702E+00 | loss scale: 524288.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.790 | tokens per gpu per second (tgs): 1906.573 | TFLOPs: 15.34 |
g0314: [2024-08-03 02:54:54,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=6320, skipped=2, lr=[0.00011043252906666667, 0.00011043252906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6320 loss: 1.7161 iter time (s): 4.265 samples/sec: 30.008
g0332:  iteration     6320/10000000 | consumed samples:       808960 | consumed tokens:   1656750080 | elapsed time per iteration (ms): 4298.4 | learning rate: 1.104E-04 | global batch size:   128 | lm loss: 1.760883E+00 | loss scale: 524288.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.779 | tokens per gpu per second (tgs): 1905.827 | TFLOPs: 15.34 |
g0314: [2024-08-03 02:55:37,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=6330, skipped=2, lr=[0.00011060729173333334, 0.00011060729173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6330 loss: 1.8081 iter time (s): 4.211 samples/sec: 30.394
g0332:  iteration     6330/10000000 | consumed samples:       810240 | consumed tokens:   1659371520 | elapsed time per iteration (ms): 4244.7 | learning rate: 1.106E-04 | global batch size:   128 | lm loss: 1.787504E+00 | loss scale: 524288.0 | grad norm: 0.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.155 | tokens per gpu per second (tgs): 1929.942 | TFLOPs: 15.53 |
g0314: [2024-08-03 02:56:19,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=6340, skipped=2, lr=[0.0001107820544, 0.0001107820544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6340 loss: 1.8091 iter time (s): 4.148 samples/sec: 30.856
g0332:  iteration     6340/10000000 | consumed samples:       811520 | consumed tokens:   1661992960 | elapsed time per iteration (ms): 4182.6 | learning rate: 1.108E-04 | global batch size:   128 | lm loss: 1.784010E+00 | loss scale: 524288.0 | grad norm: 0.405 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.603 | tokens per gpu per second (tgs): 1958.570 | TFLOPs: 15.76 |
g0314: [2024-08-03 02:57:01,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=6350, skipped=2, lr=[0.00011095681706666667, 0.00011095681706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6350 loss: 1.8578 iter time (s): 4.232 samples/sec: 30.244
g0332:  iteration     6350/10000000 | consumed samples:       812800 | consumed tokens:   1664614400 | elapsed time per iteration (ms): 4264.9 | learning rate: 1.110E-04 | global batch size:   128 | lm loss: 1.782920E+00 | loss scale: 524288.0 | grad norm: 0.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.012 | tokens per gpu per second (tgs): 1920.800 | TFLOPs: 15.46 |
g0314: [2024-08-03 02:57:45,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=6360, skipped=2, lr=[0.00011113157973333334, 0.00011113157973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6360 loss: 1.8267 iter time (s): 4.305 samples/sec: 29.733
g0332:  iteration     6360/10000000 | consumed samples:       814080 | consumed tokens:   1667235840 | elapsed time per iteration (ms): 4337.5 | learning rate: 1.111E-04 | global batch size:   128 | lm loss: 1.780938E+00 | loss scale: 524288.0 | grad norm: 0.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.510 | tokens per gpu per second (tgs): 1888.626 | TFLOPs: 15.20 |
g0314: [2024-08-03 02:58:27,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=6370, skipped=2, lr=[0.0001113063424, 0.0001113063424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6370 loss: 1.7464 iter time (s): 4.238 samples/sec: 30.202
g0332:  iteration     6370/10000000 | consumed samples:       815360 | consumed tokens:   1669857280 | elapsed time per iteration (ms): 4270.7 | learning rate: 1.113E-04 | global batch size:   128 | lm loss: 1.786769E+00 | loss scale: 524288.0 | grad norm: 0.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.972 | tokens per gpu per second (tgs): 1918.192 | TFLOPs: 15.44 |
g0314: [2024-08-03 02:59:10,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=6380, skipped=2, lr=[0.00011148110506666667, 0.00011148110506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6380 loss: 1.7250 iter time (s): 4.274 samples/sec: 29.950
g0332:  iteration     6380/10000000 | consumed samples:       816640 | consumed tokens:   1672478720 | elapsed time per iteration (ms): 4307.0 | learning rate: 1.115E-04 | global batch size:   128 | lm loss: 1.768427E+00 | loss scale: 524288.0 | grad norm: 0.404 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.719 | tokens per gpu per second (tgs): 1902.011 | TFLOPs: 15.31 |
g0314: [2024-08-03 02:59:53,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=6390, skipped=2, lr=[0.00011165586773333334, 0.00011165586773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6390 loss: 1.7231 iter time (s): 4.258 samples/sec: 30.062
g0332:  iteration     6390/10000000 | consumed samples:       817920 | consumed tokens:   1675100160 | elapsed time per iteration (ms): 4290.7 | learning rate: 1.117E-04 | global batch size:   128 | lm loss: 1.794097E+00 | loss scale: 524288.0 | grad norm: 0.390 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.832 | tokens per gpu per second (tgs): 1909.238 | TFLOPs: 15.36 |
g0314: [2024-08-03 03:00:36,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=6400, skipped=2, lr=[0.0001118306304, 0.0001118306304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6400 loss: 1.7560 iter time (s): 4.235 samples/sec: 30.228
g0332:  iteration     6400/10000000 | consumed samples:       819200 | consumed tokens:   1677721600 | elapsed time per iteration (ms): 4267.6 | learning rate: 1.118E-04 | global batch size:   128 | lm loss: 1.782649E+00 | loss scale: 524288.0 | grad norm: 0.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.994 | tokens per gpu per second (tgs): 1919.600 | TFLOPs: 15.45 |
g0314: [2024-08-03 03:01:21,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=6410, skipped=2, lr=[0.00011200539306666667, 0.00011200539306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6410 loss: 1.7702 iter time (s): 4.413 samples/sec: 29.008
g0332:  iteration     6410/10000000 | consumed samples:       820480 | consumed tokens:   1680343040 | elapsed time per iteration (ms): 4447.7 | learning rate: 1.120E-04 | global batch size:   128 | lm loss: 1.784487E+00 | loss scale: 524288.0 | grad norm: 0.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.779 | tokens per gpu per second (tgs): 1841.837 | TFLOPs: 14.82 |
g0319: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0319: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0319: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0316: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0316: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0320: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0319: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0320: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0329: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0320: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0320: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0329: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0329: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0318: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0332: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0318: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0318: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0332: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0332: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0325: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0325: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0332: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0314: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0314: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0314: [2024-08-03 03:01:52,237] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0314: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0319: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0325: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0318: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0329: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0332: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0320: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0316: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0318: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0325: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0314: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0319: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0332: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0329: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0320: [2024-08-03 03:01:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0314: [2024-08-03 03:02:16,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=6420, skipped=2, lr=[0.00011218015573333333, 0.00011218015573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6420 loss: 1.8021 iter time (s): 5.502 samples/sec: 23.264
g0332:  iteration     6420/10000000 | consumed samples:       821760 | consumed tokens:   1682964480 | elapsed time per iteration (ms): 5535.1 | learning rate: 1.122E-04 | global batch size:   128 | lm loss: 1.783263E+00 | loss scale: 1048576.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.125 | tokens per gpu per second (tgs): 1480.014 | TFLOPs: 11.91 |
g0314: [2024-08-03 03:03:00,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=6430, skipped=2, lr=[0.0001123549184, 0.0001123549184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6430 loss: 1.7254 iter time (s): 4.388 samples/sec: 29.168
g0332:  iteration     6430/10000000 | consumed samples:       823040 | consumed tokens:   1685585920 | elapsed time per iteration (ms): 4420.9 | learning rate: 1.124E-04 | global batch size:   128 | lm loss: 1.782715E+00 | loss scale: 1048576.0 | grad norm: 0.395 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.953 | tokens per gpu per second (tgs): 1853.014 | TFLOPs: 14.91 |
g0314: [2024-08-03 03:03:44,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=6440, skipped=2, lr=[0.00011252968106666667, 0.00011252968106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6440 loss: 1.7413 iter time (s): 4.312 samples/sec: 29.684
g0332:  iteration     6440/10000000 | consumed samples:       824320 | consumed tokens:   1688207360 | elapsed time per iteration (ms): 4344.7 | learning rate: 1.125E-04 | global batch size:   128 | lm loss: 1.782713E+00 | loss scale: 1048576.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.461 | tokens per gpu per second (tgs): 1885.498 | TFLOPs: 15.17 |
g0314: [2024-08-03 03:04:31,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=6450, skipped=2, lr=[0.00011270444373333333, 0.00011270444373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6450 loss: 1.7679 iter time (s): 4.753 samples/sec: 26.928
g0332:  iteration     6450/10000000 | consumed samples:       825600 | consumed tokens:   1690828800 | elapsed time per iteration (ms): 4786.7 | learning rate: 1.127E-04 | global batch size:   128 | lm loss: 1.764688E+00 | loss scale: 1048576.0 | grad norm: 0.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.741 | tokens per gpu per second (tgs): 1711.422 | TFLOPs: 13.77 |
g0314: [2024-08-03 03:05:15,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=6460, skipped=2, lr=[0.00011287920640000001, 0.00011287920640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6460 loss: 1.7901 iter time (s): 4.313 samples/sec: 29.678
g0332:  iteration     6460/10000000 | consumed samples:       826880 | consumed tokens:   1693450240 | elapsed time per iteration (ms): 4346.6 | learning rate: 1.129E-04 | global batch size:   128 | lm loss: 1.780733E+00 | loss scale: 1048576.0 | grad norm: 0.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.449 | tokens per gpu per second (tgs): 1884.712 | TFLOPs: 15.17 |
g0314: [2024-08-03 03:05:58,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=6470, skipped=2, lr=[0.00011305396906666668, 0.00011305396906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6470 loss: 1.7706 iter time (s): 4.323 samples/sec: 29.608
g0332:  iteration     6470/10000000 | consumed samples:       828160 | consumed tokens:   1696071680 | elapsed time per iteration (ms): 4355.5 | learning rate: 1.131E-04 | global batch size:   128 | lm loss: 1.780033E+00 | loss scale: 1048576.0 | grad norm: 0.382 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.388 | tokens per gpu per second (tgs): 1880.854 | TFLOPs: 15.14 |
g0314: [2024-08-03 03:06:42,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=6480, skipped=2, lr=[0.00011322873173333334, 0.00011322873173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6480 loss: 1.7321 iter time (s): 4.301 samples/sec: 29.763
g0332:  iteration     6480/10000000 | consumed samples:       829440 | consumed tokens:   1698693120 | elapsed time per iteration (ms): 4333.2 | learning rate: 1.132E-04 | global batch size:   128 | lm loss: 1.770199E+00 | loss scale: 1048576.0 | grad norm: 0.403 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.539 | tokens per gpu per second (tgs): 1890.519 | TFLOPs: 15.21 |
g0314: [2024-08-03 03:07:25,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=6490, skipped=2, lr=[0.00011340349440000001, 0.00011340349440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6490 loss: 1.8266 iter time (s): 4.285 samples/sec: 29.874
g0332:  iteration     6490/10000000 | consumed samples:       830720 | consumed tokens:   1701314560 | elapsed time per iteration (ms): 4317.3 | learning rate: 1.134E-04 | global batch size:   128 | lm loss: 1.769739E+00 | loss scale: 1048576.0 | grad norm: 0.392 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.648 | tokens per gpu per second (tgs): 1897.501 | TFLOPs: 15.27 |
g0314: [2024-08-03 03:08:07,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=6500, skipped=2, lr=[0.00011357825706666668, 0.00011357825706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6500 loss: 1.8011 iter time (s): 4.197 samples/sec: 30.497
g0332:  iteration     6500/10000000 | consumed samples:       832000 | consumed tokens:   1703936000 | elapsed time per iteration (ms): 4229.6 | learning rate: 1.136E-04 | global batch size:   128 | lm loss: 1.781426E+00 | loss scale: 1048576.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.263 | tokens per gpu per second (tgs): 1936.839 | TFLOPs: 15.59 |
g0314: [2024-08-03 03:08:52,117] [INFO] [logging.py:96:log_dist] [Rank 0] step=6510, skipped=2, lr=[0.00011375301973333334, 0.00011375301973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6510 loss: 1.6981 iter time (s): 4.409 samples/sec: 29.029
g0332:  iteration     6510/10000000 | consumed samples:       833280 | consumed tokens:   1706557440 | elapsed time per iteration (ms): 4441.9 | learning rate: 1.138E-04 | global batch size:   128 | lm loss: 1.779288E+00 | loss scale: 1048576.0 | grad norm: 0.380 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.817 | tokens per gpu per second (tgs): 1844.269 | TFLOPs: 14.84 |
g0314: [2024-08-03 03:09:37,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=6520, skipped=2, lr=[0.00011392778240000001, 0.00011392778240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6520 loss: 1.8215 iter time (s): 4.491 samples/sec: 28.500
g0332:  iteration     6520/10000000 | consumed samples:       834560 | consumed tokens:   1709178880 | elapsed time per iteration (ms): 4524.3 | learning rate: 1.139E-04 | global batch size:   128 | lm loss: 1.785105E+00 | loss scale: 1048576.0 | grad norm: 0.386 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.291 | tokens per gpu per second (tgs): 1810.649 | TFLOPs: 14.57 |
g0314: [2024-08-03 03:10:20,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=6530, skipped=2, lr=[0.00011410254506666667, 0.00011410254506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6530 loss: 1.8397 iter time (s): 4.265 samples/sec: 30.013
g0332:  iteration     6530/10000000 | consumed samples:       835840 | consumed tokens:   1711800320 | elapsed time per iteration (ms): 4297.3 | learning rate: 1.141E-04 | global batch size:   128 | lm loss: 1.772331E+00 | loss scale: 1048576.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.786 | tokens per gpu per second (tgs): 1906.303 | TFLOPs: 15.34 |
g0314: [2024-08-03 03:11:02,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=6540, skipped=2, lr=[0.00011427730773333334, 0.00011427730773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6540 loss: 1.8109 iter time (s): 4.203 samples/sec: 30.457
g0332:  iteration     6540/10000000 | consumed samples:       837120 | consumed tokens:   1714421760 | elapsed time per iteration (ms): 4235.5 | learning rate: 1.143E-04 | global batch size:   128 | lm loss: 1.792407E+00 | loss scale: 1048576.0 | grad norm: 0.398 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.221 | tokens per gpu per second (tgs): 1934.118 | TFLOPs: 15.56 |
g0314: [2024-08-03 03:11:45,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=6550, skipped=2, lr=[0.0001144520704, 0.0001144520704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6550 loss: 1.7814 iter time (s): 4.272 samples/sec: 29.960
g0332:  iteration     6550/10000000 | consumed samples:       838400 | consumed tokens:   1717043200 | elapsed time per iteration (ms): 4305.6 | learning rate: 1.145E-04 | global batch size:   128 | lm loss: 1.792494E+00 | loss scale: 1048576.0 | grad norm: 0.387 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.729 | tokens per gpu per second (tgs): 1902.646 | TFLOPs: 15.31 |
g0314: [2024-08-03 03:12:28,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=6560, skipped=2, lr=[0.00011462683306666667, 0.00011462683306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6560 loss: 1.8368 iter time (s): 4.223 samples/sec: 30.309
g0332:  iteration     6560/10000000 | consumed samples:       839680 | consumed tokens:   1719664640 | elapsed time per iteration (ms): 4255.6 | learning rate: 1.146E-04 | global batch size:   128 | lm loss: 1.761847E+00 | loss scale: 1048576.0 | grad norm: 0.408 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.078 | tokens per gpu per second (tgs): 1924.984 | TFLOPs: 15.49 |
g0314: [2024-08-03 03:13:11,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=6570, skipped=2, lr=[0.00011480159573333334, 0.00011480159573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6570 loss: 1.7455 iter time (s): 4.262 samples/sec: 30.035
g0332:  iteration     6570/10000000 | consumed samples:       840960 | consumed tokens:   1722286080 | elapsed time per iteration (ms): 4296.3 | learning rate: 1.148E-04 | global batch size:   128 | lm loss: 1.745007E+00 | loss scale: 1048576.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.793 | tokens per gpu per second (tgs): 1906.762 | TFLOPs: 15.34 |
g0314: [2024-08-03 03:13:54,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=6580, skipped=2, lr=[0.0001149763584, 0.0001149763584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6580 loss: 1.7851 iter time (s): 4.309 samples/sec: 29.706
g0332:  iteration     6580/10000000 | consumed samples:       842240 | consumed tokens:   1724907520 | elapsed time per iteration (ms): 4341.6 | learning rate: 1.150E-04 | global batch size:   128 | lm loss: 1.775289E+00 | loss scale: 1048576.0 | grad norm: 0.404 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.482 | tokens per gpu per second (tgs): 1886.876 | TFLOPs: 15.18 |
g0314: [2024-08-03 03:14:37,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=6590, skipped=2, lr=[0.00011515112106666667, 0.00011515112106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6590 loss: 1.7672 iter time (s): 4.226 samples/sec: 30.289
g0332:  iteration     6590/10000000 | consumed samples:       843520 | consumed tokens:   1727528960 | elapsed time per iteration (ms): 4258.6 | learning rate: 1.152E-04 | global batch size:   128 | lm loss: 1.771565E+00 | loss scale: 1048576.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.057 | tokens per gpu per second (tgs): 1923.619 | TFLOPs: 15.48 |
g0314: [2024-08-03 03:15:19,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=6600, skipped=2, lr=[0.00011532588373333334, 0.00011532588373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6600 loss: 1.7797 iter time (s): 4.218 samples/sec: 30.348
g0332:  iteration     6600/10000000 | consumed samples:       844800 | consumed tokens:   1730150400 | elapsed time per iteration (ms): 4250.9 | learning rate: 1.153E-04 | global batch size:   128 | lm loss: 1.756850E+00 | loss scale: 1048576.0 | grad norm: 0.441 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.111 | tokens per gpu per second (tgs): 1927.101 | TFLOPs: 15.51 |
g0314: [2024-08-03 03:16:03,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=6610, skipped=2, lr=[0.0001155006464, 0.0001155006464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6610 loss: 1.7217 iter time (s): 4.356 samples/sec: 29.382
g0332:  iteration     6610/10000000 | consumed samples:       846080 | consumed tokens:   1732771840 | elapsed time per iteration (ms): 4389.0 | learning rate: 1.155E-04 | global batch size:   128 | lm loss: 1.759061E+00 | loss scale: 1048576.0 | grad norm: 0.396 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.164 | tokens per gpu per second (tgs): 1866.476 | TFLOPs: 15.02 |
g0314: [2024-08-03 03:16:46,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=6620, skipped=2, lr=[0.00011567540906666667, 0.00011567540906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6620 loss: 1.7716 iter time (s): 4.265 samples/sec: 30.011
g0332:  iteration     6620/10000000 | consumed samples:       847360 | consumed tokens:   1735393280 | elapsed time per iteration (ms): 4298.1 | learning rate: 1.157E-04 | global batch size:   128 | lm loss: 1.752872E+00 | loss scale: 1048576.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.781 | tokens per gpu per second (tgs): 1905.973 | TFLOPs: 15.34 |
g0314: [2024-08-03 03:17:29,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=6630, skipped=2, lr=[0.00011585017173333333, 0.00011585017173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6630 loss: 1.7554 iter time (s): 4.240 samples/sec: 30.188
g0332:  iteration     6630/10000000 | consumed samples:       848640 | consumed tokens:   1738014720 | elapsed time per iteration (ms): 4272.5 | learning rate: 1.159E-04 | global batch size:   128 | lm loss: 1.768106E+00 | loss scale: 1048576.0 | grad norm: 0.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.959 | tokens per gpu per second (tgs): 1917.385 | TFLOPs: 15.43 |
g0314: [2024-08-03 03:18:11,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=6640, skipped=2, lr=[0.00011602493440000001, 0.00011602493440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6640 loss: 1.7828 iter time (s): 4.207 samples/sec: 30.428
g0332:  iteration     6640/10000000 | consumed samples:       849920 | consumed tokens:   1740636160 | elapsed time per iteration (ms): 4240.0 | learning rate: 1.160E-04 | global batch size:   128 | lm loss: 1.785020E+00 | loss scale: 1048576.0 | grad norm: 0.382 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.189 | tokens per gpu per second (tgs): 1932.091 | TFLOPs: 15.55 |
g0314: [2024-08-03 03:18:54,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=6650, skipped=2, lr=[0.00011619969706666668, 0.00011619969706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6650 loss: 1.7483 iter time (s): 4.268 samples/sec: 29.991
g0332:  iteration     6650/10000000 | consumed samples:       851200 | consumed tokens:   1743257600 | elapsed time per iteration (ms): 4300.6 | learning rate: 1.162E-04 | global batch size:   128 | lm loss: 1.770679E+00 | loss scale: 1048576.0 | grad norm: 0.387 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.763 | tokens per gpu per second (tgs): 1904.859 | TFLOPs: 15.33 |
g0314: [2024-08-03 03:19:36,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=6660, skipped=2, lr=[0.00011637445973333335, 0.00011637445973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6660 loss: 1.7701 iter time (s): 4.182 samples/sec: 30.608
g0332:  iteration     6660/10000000 | consumed samples:       852480 | consumed tokens:   1745879040 | elapsed time per iteration (ms): 4214.7 | learning rate: 1.164E-04 | global batch size:   128 | lm loss: 1.747783E+00 | loss scale: 1048576.0 | grad norm: 0.380 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.370 | tokens per gpu per second (tgs): 1943.654 | TFLOPs: 15.64 |
g0314: [2024-08-03 03:20:19,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=6670, skipped=2, lr=[0.00011654922240000001, 0.00011654922240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6670 loss: 1.7360 iter time (s): 4.258 samples/sec: 30.063
g0332:  iteration     6670/10000000 | consumed samples:       853760 | consumed tokens:   1748500480 | elapsed time per iteration (ms): 4290.4 | learning rate: 1.165E-04 | global batch size:   128 | lm loss: 1.755794E+00 | loss scale: 1048576.0 | grad norm: 0.377 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.834 | tokens per gpu per second (tgs): 1909.378 | TFLOPs: 15.37 |
g0314: [2024-08-03 03:21:02,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=6680, skipped=2, lr=[0.00011672398506666668, 0.00011672398506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6680 loss: 1.8317 iter time (s): 4.223 samples/sec: 30.307
g0332:  iteration     6680/10000000 | consumed samples:       855040 | consumed tokens:   1751121920 | elapsed time per iteration (ms): 4256.3 | learning rate: 1.167E-04 | global batch size:   128 | lm loss: 1.750776E+00 | loss scale: 1048576.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.073 | tokens per gpu per second (tgs): 1924.673 | TFLOPs: 15.49 |
g0314: [2024-08-03 03:21:44,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=6690, skipped=2, lr=[0.00011689874773333334, 0.00011689874773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6690 loss: 1.7855 iter time (s): 4.187 samples/sec: 30.572
g0332:  iteration     6690/10000000 | consumed samples:       856320 | consumed tokens:   1753743360 | elapsed time per iteration (ms): 4219.4 | learning rate: 1.169E-04 | global batch size:   128 | lm loss: 1.774506E+00 | loss scale: 1048576.0 | grad norm: 0.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.336 | tokens per gpu per second (tgs): 1941.499 | TFLOPs: 15.62 |
g0314: [2024-08-03 03:22:27,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=6700, skipped=2, lr=[0.00011707351040000001, 0.00011707351040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6700 loss: 1.7751 iter time (s): 4.271 samples/sec: 29.966
g0332:  iteration     6700/10000000 | consumed samples:       857600 | consumed tokens:   1756364800 | elapsed time per iteration (ms): 4304.5 | learning rate: 1.171E-04 | global batch size:   128 | lm loss: 1.788858E+00 | loss scale: 1048576.0 | grad norm: 0.392 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.737 | tokens per gpu per second (tgs): 1903.137 | TFLOPs: 15.31 |
g0314: [2024-08-03 03:23:09,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=6710, skipped=2, lr=[0.00011724827306666668, 0.00011724827306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6710 loss: 1.7686 iter time (s): 4.198 samples/sec: 30.488
g0332:  iteration     6710/10000000 | consumed samples:       858880 | consumed tokens:   1758986240 | elapsed time per iteration (ms): 4233.2 | learning rate: 1.172E-04 | global batch size:   128 | lm loss: 1.772788E+00 | loss scale: 1048576.0 | grad norm: 0.375 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.237 | tokens per gpu per second (tgs): 1935.197 | TFLOPs: 15.57 |
g0314: [2024-08-03 03:23:52,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=6720, skipped=2, lr=[0.00011742303573333334, 0.00011742303573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6720 loss: 1.8141 iter time (s): 4.220 samples/sec: 30.329
g0332:  iteration     6720/10000000 | consumed samples:       860160 | consumed tokens:   1761607680 | elapsed time per iteration (ms): 4253.9 | learning rate: 1.174E-04 | global batch size:   128 | lm loss: 1.769875E+00 | loss scale: 1048576.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.090 | tokens per gpu per second (tgs): 1925.766 | TFLOPs: 15.50 |
g0314: [2024-08-03 03:24:35,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=6730, skipped=2, lr=[0.00011759779840000001, 0.00011759779840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6730 loss: 1.8018 iter time (s): 4.260 samples/sec: 30.045
g0332:  iteration     6730/10000000 | consumed samples:       861440 | consumed tokens:   1764229120 | elapsed time per iteration (ms): 4292.8 | learning rate: 1.176E-04 | global batch size:   128 | lm loss: 1.753269E+00 | loss scale: 1048576.0 | grad norm: 0.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.817 | tokens per gpu per second (tgs): 1908.310 | TFLOPs: 15.36 |
g0314: [2024-08-03 03:25:17,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=6740, skipped=2, lr=[0.00011777256106666667, 0.00011777256106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6740 loss: 1.7397 iter time (s): 4.177 samples/sec: 30.642
g0332:  iteration     6740/10000000 | consumed samples:       862720 | consumed tokens:   1766850560 | elapsed time per iteration (ms): 4210.0 | learning rate: 1.178E-04 | global batch size:   128 | lm loss: 1.760430E+00 | loss scale: 1048576.0 | grad norm: 0.375 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.404 | tokens per gpu per second (tgs): 1945.859 | TFLOPs: 15.66 |
g0314: [2024-08-03 03:26:00,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=6750, skipped=2, lr=[0.00011794732373333334, 0.00011794732373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6750 loss: 1.7842 iter time (s): 4.309 samples/sec: 29.705
g0332:  iteration     6750/10000000 | consumed samples:       864000 | consumed tokens:   1769472000 | elapsed time per iteration (ms): 4342.5 | learning rate: 1.179E-04 | global batch size:   128 | lm loss: 1.769974E+00 | loss scale: 1048576.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.476 | tokens per gpu per second (tgs): 1886.464 | TFLOPs: 15.18 |
g0314: [2024-08-03 03:26:44,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=6760, skipped=2, lr=[0.00011812208640000001, 0.00011812208640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0314: steps: 6760 loss: 1.8469 iter time (s): 4.289 samples/sec: 29.844
g0332:  iteration     6760/10000000 | consumed samples:       865280 | consumed tokens:   1772093440 | elapsed time per iteration (ms): 4321.5 | learning rate: 1.181E-04 | global batch size:   128 | lm loss: 1.796640E+00 | loss scale: 1048576.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.619 | tokens per gpu per second (tgs): 1895.632 | TFLOPs: 15.25 |
