
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0171
    HostName g0171
    Port 2222
    StrictHostKeyChecking no

Host g0172
    HostName g0172
    Port 2222
    StrictHostKeyChecking no

Host g0173
    HostName g0173
    Port 2222
    StrictHostKeyChecking no

Host g0174
    HostName g0174
    Port 2222
    StrictHostKeyChecking no

Host g0175
    HostName g0175
    Port 2222
    StrictHostKeyChecking no

Host g0176
    HostName g0176
    Port 2222
    StrictHostKeyChecking no

Host g0177
    HostName g0177
    Port 2222
    StrictHostKeyChecking no

Host g0178
    HostName g0178
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42830323
g0171 slots=4
g0172 slots=4
g0173 slots=4
g0174 slots=4
g0175 slots=4
g0176 slots=4
g0177 slots=4
g0178 slots=4

[2024-08-12 12:17:42,374] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-12 12:17:48,141] [INFO] [runner.py:463:main] Using IP address of 10.1.6.1 for node g0171
[2024-08-12 12:17:48,143] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0171,g0172,g0173,g0174,g0175,g0176,g0177,g0178
[2024-08-12 12:17:48,143] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0171,g0172,g0173,g0174,g0175,g0176,g0177,g0178 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDE3MSI6IFswLCAxLCAyLCAzXSwgImcwMTcyIjogWzAsIDEsIDIsIDNdLCAiZzAxNzMiOiBbMCwgMSwgMiwgM10sICJnMDE3NCI6IFswLCAxLCAyLCAzXSwgImcwMTc1IjogWzAsIDEsIDIsIDNdLCAiZzAxNzYiOiBbMCwgMSwgMiwgM10sICJnMDE3NyI6IFswLCAxLCAyLCAzXSwgImcwMTc4IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.6.1 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '11010048000' --train-samples '5376000' --lr '2.0e-4' --min-lr '1.0e-6' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/1_0.latin_wikipedia_poly_000000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True' --wandb_tag 'other_gpu'
g0171: [2024-08-12 12:17:51,607] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0171: [2024-08-12 12:17:53,898] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0171: [2024-08-12 12:17:53,898] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0171: [2024-08-12 12:17:53,898] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0171: [2024-08-12 12:17:53,898] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0171: [2024-08-12 12:17:53,898] [INFO] [launch.py:163:main] dist_world_size=32
g0171: [2024-08-12 12:17:53,898] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0171: [2024-08-12 12:17:57,087] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0171: [2024-08-12 12:17:57,087] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0171: [2024-08-12 12:17:57,137] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0171: [2024-08-12 12:17:57,161] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0172: [2024-08-12 12:17:59,753] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0176: [2024-08-12 12:17:59,801] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0178: [2024-08-12 12:17:59,810] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0173: [2024-08-12 12:17:59,817] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0177: [2024-08-12 12:18:00,005] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0174: [2024-08-12 12:18:00,116] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0175: [2024-08-12 12:18:00,228] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0171: --------------------------------------------------
g0171: DeepSpeed C++/CUDA extension op report
g0171: --------------------------------------------------
g0171: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0171:       runtime if needed. Op compatibility means that your system
g0171:       meet the required dependencies to JIT install the op.
g0171: --------------------------------------------------
g0171: JIT compiled ops requires ninja
g0171: --------------------------------------------------
g0171: DeepSpeed C++/CUDA extension op report
g0171: --------------------------------------------------
g0171: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0171:       runtime if needed. Op compatibility means that your system
g0171:       meet the required dependencies to JIT install the op.
g0171: --------------------------------------------------
g0171: JIT compiled ops requires ninja
g0171: --------------------------------------------------
g0171: DeepSpeed C++/CUDA extension op report
g0171: --------------------------------------------------
g0171: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0171:       runtime if needed. Op compatibility means that your system
g0171:       meet the required dependencies to JIT install the op.
g0171: --------------------------------------------------
g0171: JIT compiled ops requires ninja
g0171: --------------------------------------------------
g0171: DeepSpeed C++/CUDA extension op report
g0171: --------------------------------------------------
g0171: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0171:       runtime if needed. Op compatibility means that your system
g0171:       meet the required dependencies to JIT install the op.
g0171: --------------------------------------------------
g0171: JIT compiled ops requires ninja
g0171: ninjaninja ninja ..................ninja..................    [92m[OKAY][0m..................[92m[OKAY][0m..................
g0171:  
g0171:  [92m[OKAY][0m--------------------------------------------------[92m[OKAY][0m--------------------------------------------------
g0171: 
g0171: 
g0171: 
g0171: --------------------------------------------------op name
g0171: -------------------------------------------------- op name
g0171: op name................  op name ................................ installed   ................installedinstalled..    installed....compatible   ..compatible
g0171:  compatible
g0171: compatible----------------------------------------------------------------------------------------------------
g0171: 
g0171: 
g0171: 
g0171: ----------------------------------------------------------------------------------------------------
g0171: 
g0171: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0171: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0171: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0171: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0171: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0171: fused_lamb async_io............. [92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0171: ...... [92m[OKAY][0m
g0171: fused_adamfused_lion  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0171: 
g0171: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0171: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0171: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0171: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0171: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: async_io async_io...............  [92m[YES][0m...............  ......[92m[YES][0m  ...... [92m[OKAY][0m[92m[OKAY][0m
g0171: 
g0171: fused_adam fused_adam.............  [92m[YES][0m.............  ......[92m[YES][0m [92m[OKAY][0m 
g0171: ...... cpu_adam[92m[OKAY][0m 
g0171: ............... [92m[YES][0m ...... cpu_adam[92m[OKAY][0m 
g0171: ............... cpu_adagrad [92m[YES][0m............  ......[92m[YES][0m  ......[92m[OKAY][0m [92m[OKAY][0m
g0171: 
g0171: cpu_lioncpu_adagrad ...............  ............[92m[YES][0m  ......[92m[YES][0m [92m[OKAY][0m 
g0171: ...... [92m[OKAY][0m
g0171: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_lion
g0171:  ...............evoformer_attn  .........[92m[YES][0m  [93m[NO][0m...... .......  [92m[OKAY][0m[93m[NO][0m
g0171: 
g0171: fused_lamb ............. [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0171: 
g0171: evoformer_attn ......... fused_lion [93m[NO][0m.............  .......[92m[YES][0m  ......[93m[NO][0m [92m[OKAY][0m
g0171: 
g0171: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0171: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0171: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0171: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0171: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: cutlass_opscutlass_ops ............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0171: [92m[OKAY][0m
g0171: quantizerquantizer  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0171: 
g0171: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0171: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0171: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0171: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0171: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0171: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0171: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0171: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0171: sparse_attn ............ ragged_ops[93m[NO][0m  ....................  [93m[NO][0m[92m[YES][0m
g0171:  ...... [92m[OKAY][0m
g0171: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0171: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0171: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0171: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0171: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0171: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0171: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0171: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0171: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0171: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0171: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0171: --------------------------------------------------
g0171: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0171: --------------------------------------------------
g0171: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0171: --------------------------------------------------
g0171: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0171: --------------------------------------------------
g0171: DeepSpeed general environment info:
g0171: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0171: torch version .................... 2.0.1+cu118
g0171: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0171: deepspeed info ................... 0.12.4, unknown, unknown
g0171: torch cuda version ............... 11.8
g0171: torch hip version ................ None
g0171: nvcc version ..................... 11.8
g0171: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0171: shared memory (/dev/shm) size .... 188.13 GB
g0171: DeepSpeed general environment info:
g0171: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0171: torch version .................... 2.0.1+cu118
g0171: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0171: deepspeed info ................... 0.12.4, unknown, unknown
g0171: torch cuda version ............... 11.8
g0171: torch hip version ................ None
g0171: nvcc version ..................... 11.8
g0171: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0171: shared memory (/dev/shm) size .... 188.13 GB
g0171: DeepSpeed general environment info:
g0171: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0171: torch version .................... 2.0.1+cu118
g0171: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0171: deepspeed info ................... 0.12.4, unknown, unknown
g0171: torch cuda version ............... 11.8
g0171: torch hip version ................ None
g0171: nvcc version ..................... 11.8
g0171: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0171: shared memory (/dev/shm) size .... 188.13 GB
g0171: DeepSpeed general environment info:
g0171: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0171: torch version .................... 2.0.1+cu118
g0171: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0171: deepspeed info ................... 0.12.4, unknown, unknown
g0171: torch cuda version ............... 11.8
g0171: torch hip version ................ None
g0171: nvcc version ..................... 11.8
g0171: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0171: shared memory (/dev/shm) size .... 188.13 GB
g0171: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0171: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0171: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0171: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0171: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0171: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0171: using torch.float32 for parameters ...
g0171: ------------------------ arguments ------------------------
g0171:   accumulate_allreduce_grads_in_fp32 .............. False
g0171:   adam_beta1 ...................................... 0.9
g0171:   adam_beta2 ...................................... 0.95
g0171:   adam_eps ........................................ 1e-08
g0171:   add_bias_linear ................................. False
g0171:   add_position_embedding .......................... False
g0171:   adlr_autoresume ................................. False
g0171:   adlr_autoresume_interval ........................ 1000
g0171:   aml_data_download_path .......................... None
g0171:   apply_layernorm_1p .............................. False
g0171:   apply_query_key_layer_scaling ................... False
g0171:   apply_residual_connection_post_layernorm ........ False
g0171:   async_tensor_model_parallel_allreduce ........... False
g0171:   attention_dropout ............................... 0.0
g0171:   attention_softmax_in_fp32 ....................... False
g0171:   barrier_with_L1_time ............................ True
g0171:   bert_binary_head ................................ True
g0171:   bert_embedder_type .............................. megatron
g0171:   bert_load ....................................... None
g0171:   bf16 ............................................ False
g0171:   bias_dropout_fusion ............................. True
g0171:   bias_gelu_fusion ................................ False
g0171:   biencoder_projection_dim ........................ 0
g0171:   biencoder_shared_query_context_model ............ False
g0171:   block_data_path ................................. None
g0171:   checkpoint_activations .......................... False
g0171:   checkpoint_in_cpu ............................... False
g0171:   checkpoint_num_layers ........................... 1
g0171:   classes_fraction ................................ 1.0
g0171:   clip_grad ....................................... 1.0
g0171:   compression_training ............................ False
g0171:   consumed_train_samples .......................... 0
g0171:   consumed_train_tokens ........................... 0
g0171:   consumed_valid_samples .......................... 0
g0171:   contigious_checkpointing ........................ False
g0171:   cpu_optimizer ................................... False
g0171:   cpu_torch_adam .................................. False
g0171:   create_moe_param_group .......................... False
g0171:   curriculum_learning_legacy ...................... False
g0171:   data_cache_path ................................. None
g0171:   data_efficiency_curriculum_learning ............. False
g0171:   data_impl ....................................... mmap
g0171:   data_parallel_random_init ....................... False
g0171:   data_parallel_size .............................. 4
g0171:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document']
g0171:   data_per_class_fraction ......................... 1.0
g0171:   data_sharding ................................... True
g0171:   dataloader_type ................................. single
g0171:   DDP_impl ........................................ local
g0171:   decoder_num_layers .............................. None
g0171:   decoder_seq_length .............................. None
g0171:   deepscale ....................................... False
g0171:   deepscale_config ................................ None
g0171:   deepspeed ....................................... True
g0171:   deepspeed_activation_checkpointing .............. False
g0171:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0171:   deepspeed_mpi ................................... False
g0171:   dino_bottleneck_size ............................ 256
g0171:   dino_freeze_last_layer .......................... 1
g0171:   dino_head_hidden_size ........................... 2048
g0171:   dino_local_crops_number ......................... 10
g0171:   dino_local_img_size ............................. 96
g0171:   dino_norm_last_layer ............................ False
g0171:   dino_teacher_temp ............................... 0.07
g0171:   dino_warmup_teacher_temp ........................ 0.04
g0171:   dino_warmup_teacher_temp_epochs ................. 30
g0171:   distribute_checkpointed_activations ............. False
g0171:   distribute_saved_activations .................... False
g0171:   distributed_backend ............................. nccl
g0171:   distributed_timeout_minutes ..................... 10
g0171:   ds_fused_adam ................................... False
g0171:   ds_inference .................................... False
g0171:   ds_pipeline_enabled ............................. True
g0171:   ds_sequence_parallel_size ....................... 1
g0171:   embedding_path .................................. None
g0171:   embedding_weights_in_fp32 ....................... False
g0171:   empty_unused_memory_level ....................... 0
g0171:   enable_expert_tensor_parallelism ................ False
g0171:   encoder_num_layers .............................. 22
g0171:   encoder_seq_length .............................. 2048
g0171:   end_weight_decay ................................ 0.1
g0171:   eod_mask_loss ................................... False
g0171:   eval_interval ................................... 1000
g0171:   eval_iters ...................................... 100
g0171:   evidence_data_path .............................. None
g0171:   exit_duration_in_mins ........................... 30000000
g0171:   exit_interval ................................... None
g0171:   exit_on_missing_checkpoint ...................... False
g0171:   exit_signal_handler ............................. False
g0171:   expert_interval ................................. 2
g0171:   ffn_hidden_size ................................. 5632
g0171:   finetune ........................................ False
g0171:   force_ds_sequence_parallel ...................... False
g0171:   fp16 ............................................ False
g0171:   fp16_lm_cross_entropy ........................... False
g0171:   fp32_residual_connection ........................ False
g0171:   fp8_amax_compute_algo ........................... most_recent
g0171:   fp8_amax_history_len ............................ 1
g0171:   fp8_e4m3 ........................................ False
g0171:   fp8_hybrid ...................................... False
g0171:   fp8_interval .................................... 1
g0171:   fp8_margin ...................................... 0
g0171:   fp8_wgrad ....................................... True
g0171:   global_batch_size ............................... 128
g0171:   gradient_accumulation_fusion .................... True
g0171:   head_lr_mult .................................... 1.0
g0171:   hidden_dropout .................................. 0.0
g0171:   hidden_size ..................................... 2048
g0171:   hidden_size_teacher ............................. None
g0171:   hysteresis ...................................... 2
g0171:   ict_head_size ................................... None
g0171:   ict_load ........................................ None
g0171:   img_h ........................................... 224
g0171:   img_w ........................................... 224
g0171:   indexer_batch_size .............................. 128
g0171:   indexer_log_interval ............................ 1000
g0171:   inference ....................................... False
g0171:   inference_batch_times_seqlen_threshold .......... 512
g0171:   init_method_std ................................. 0.013
g0171:   init_method_xavier_uniform ...................... False
g0171:   initial_loss_scale .............................. 4294967296
g0171:   iter_per_epoch .................................. 1250
g0171:   kd .............................................. False
g0171:   kd_alpha_ce ..................................... 1
g0171:   kd_beta_ce ...................................... 1
g0171:   kd_temp ......................................... 1.0
g0171:   kv_channels ..................................... 128
g0171:   layernorm_epsilon ............................... 1e-05
g0171:   lazy_mpu_init ................................... None
g0171:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1
g0171:   load_teacher .................................... None
g0171:   local_rank ...................................... 0
g0171:   log_batch_size_to_tensorboard ................... True
g0171:   log_interval .................................... 10
g0171:   log_learning_rate_to_tensorboard ................ True
g0171:   log_loss_scale_to_tensorboard ................... True
g0171:   log_memory_to_tensorboard ....................... False
g0171:   log_num_zeros_in_grad ........................... False
g0171:   log_optimizer_states_to_tensorboard ............. True
g0171:   log_params_norm ................................. False
g0171:   log_timers_to_tensorboard ....................... True
g0171:   log_validation_ppl_to_tensorboard ............... True
g0171:   log_world_size_to_tensorboard ................... False
g0171:   loss_scale ...................................... None
g0171:   loss_scale_window ............................... 1000
g0171:   lr .............................................. 0.0002
g0171:   lr_decay_iters .................................. None
g0171:   lr_decay_samples ................................ None
g0171:   lr_decay_style .................................. cosine
g0171:   lr_decay_tokens ................................. 300000000000
g0171:   lr_warmup_fraction .............................. None
g0171:   lr_warmup_iters ................................. 0
g0171:   lr_warmup_samples ............................... 0
g0171:   lr_warmup_tokens ................................ 3000000000
g0171:   make_vocab_size_divisible_by .................... 128
g0171:   mask_factor ..................................... 1.0
g0171:   mask_prob ....................................... 0.15
g0171:   mask_type ....................................... random
g0171:   masked_softmax_fusion ........................... True
g0171:   max_position_embeddings ......................... 2048
g0171:   max_tokens_to_oom ............................... 12000
g0171:   mem_efficient_ln ................................ True
g0171:   memory_centric_tiled_linear ..................... False
g0171:   merge_file ...................................... None
g0171:   micro_batch_size ................................ 1
g0171:   min_loss_scale .................................. 1.0
g0171:   min_lr .......................................... 1e-06
g0171:   mlp_type ........................................ standard
g0171:   mmap_warmup ..................................... False
g0171:   moe_eval_capacity_factor ........................ 1.0
g0171:   moe_expert_parallel_size ........................ 1
g0171:   moe_loss_coeff .................................. 0.1
g0171:   moe_min_capacity ................................ 4
g0171:   moe_token_dropping .............................. True
g0171:   moe_train_capacity_factor ....................... 1.0
g0171:   mos ............................................. False
g0171:   no_load_lr_state ................................ False
g0171:   no_load_optim ................................... None
g0171:   no_load_rng ..................................... None
g0171:   no_persist_layer_norm ........................... False
g0171:   no_pipeline_parallel ............................ False
g0171:   no_save_optim ................................... None
g0171:   no_save_rng ..................................... None
g0171:   normalization ................................... rmsnorm
g0171:   num_attention_heads ............................. 16
g0171:   num_attention_heads_teacher ..................... None
g0171:   num_channels .................................... 3
g0171:   num_classes ..................................... 1000
g0171:   num_experts ..................................... [1]
g0171:   num_experts_switch .............................. None
g0171:   num_experts_teacher ............................. [1]
g0171:   num_key_value_heads ............................. 4
g0171:   num_layers ...................................... 22
g0171:   num_layers_per_virtual_pipeline_stage ........... None
g0171:   num_layers_teacher .............................. None
g0171:   num_workers ..................................... 0
g0171:   onnx_safe ....................................... None
g0171:   openai_gelu ..................................... False
g0171:   optimizer ....................................... adam
g0171:   output_bert_embeddings .......................... False
g0171:   overlap_p2p_comm ................................ False
g0171:   override_opt_param_scheduler .................... True
g0171:   params_dtype .................................... torch.float32
g0171:   partition_activations ........................... False
g0171:   patch_dim ....................................... 16
g0171:   perform_initialization .......................... True
g0171:   pipeline_model_parallel_size .................... 8
g0171:   pipeline_model_parallel_split_rank .............. None
g0171:   profile_backward ................................ False
g0171:   query_in_block_prob ............................. 0.1
g0171:   rampup_batch_size ............................... None
g0171:   random_ltd ...................................... False
g0171:   rank ............................................ 0
g0171:   recompute_granularity ........................... None
g0171:   recompute_method ................................ None
g0171:   recompute_num_layers ............................ 1
g0171:   remote_device ................................... none
g0171:   repeated_dataloader ............................. False
g0171:   reset_attention_mask ............................ False
g0171:   reset_iteration ................................. False
g0171:   reset_position_ids .............................. False
g0171:   retriever_report_topk_accuracies ................ []
g0171:   retriever_score_scaling ......................... False
g0171:   retriever_seq_length ............................ 256
g0171:   retro_add_retriever ............................. False
g0171:   retro_cyclic_train_iters ........................ None
g0171:   retro_encoder_attention_dropout ................. 0.1
g0171:   retro_encoder_hidden_dropout .................... 0.1
g0171:   retro_encoder_layers ............................ 2
g0171:   retro_num_neighbors ............................. 2
g0171:   retro_num_retrieved_chunks ...................... 2
g0171:   retro_return_doc_ids ............................ False
g0171:   retro_workdir ................................... None
g0171:   return_data_index ............................... False
g0171:   rotary_percent .................................. 1.0
g0171:   sample_rate ..................................... 1.0
g0171:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1
g0171:   save_interval ................................... 1000
g0171:   scatter_gather_tensors_in_pipeline .............. True
g0171:   scattered_embeddings ............................ False
g0171:   seed ............................................ 1234
g0171:   seq_length ...................................... 2048
g0171:   sequence_parallel ............................... False
g0171:   sgd_momentum .................................... 0.9
g0171:   short_seq_prob .................................. 0.1
g0171:   skip_train ...................................... False
g0171:   split ........................................... 949,50,1
g0171:   split_transformers .............................. False
g0171:   squared_relu .................................... False
g0171:   standalone_embedding_stage ...................... False
g0171:   start_weight_decay .............................. 0.1
g0171:   swiglu .......................................... True
g0171:   swin_backbone_type .............................. tiny
g0171:   synchronize_each_layer .......................... False
g0171:   tensor_model_parallel_size ...................... 1
g0171:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/1_0.latin_wikipedia_poly_000000_1234_True
g0171:   tensorboard_log_interval ........................ 1
g0171:   tensorboard_queue_size .......................... 1
g0171:   test_data_path .................................. None
g0171:   tf32 ............................................ False
g0171:   tile_factor ..................................... 1
g0171:   timing_log_level ................................ 0
g0171:   timing_log_option ............................... minmax
g0171:   titles_data_path ................................ None
g0171:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
g0171:   tokenizer_type .................................. SentencePieceTokenizer
g0171:   topk ............................................ 1
g0171:   train_data_exact_num_epochs ..................... None
g0171:   train_data_path ................................. None
g0171:   train_desc_path ................................. None
g0171:   train_doc_idx_path .............................. None
g0171:   train_idx_path .................................. None
g0171:   train_iters ..................................... None
g0171:   train_sample_idx_path ........................... None
g0171:   train_samples ................................... 5376000
g0171:   train_shuffle_idx_path .......................... None
g0171:   train_tokens .................................... 11010048000
g0171:   transformer_impl ................................ local
g0171:   transformer_pipeline_model_parallel_size ........ 8
g0171:   universal_checkpoint ............................ False
g0171:   untie_embeddings_and_output_weights ............. True
g0171:   use_checkpoint_args ............................. False
g0171:   use_checkpoint_opt_param_scheduler .............. False
g0171:   use_contiguous_buffers_in_local_ddp ............. True
g0171:   use_cpu_initialization .......................... None
g0171:   use_dataset_only ................................ False
g0171:   use_distributed_optimizer ....................... False
g0171:   use_flash_attn .................................. False
g0171:   use_flash_attn_triton ........................... False
g0171:   use_flash_attn_v1 ............................... False
g0171:   use_flash_attn_v2 ............................... False
g0171:   use_one_sent_docs ............................... False
g0171:   use_pin_memory .................................. False
g0171:   use_ring_exchange_p2p ........................... False
g0171:   use_rotary_position_embeddings .................. True
g0171:   use_tutel ....................................... False
g0171:   use_wandb ....................................... True
g0171:   valid_data_path ................................. None
g0171:   variable_seq_lengths ............................ False
g0171:   virtual_pipeline_model_parallel_size ............ None
g0171:   vision_backbone_type ............................ vit
g0171:   vision_pretraining .............................. False
g0171:   vision_pretraining_type ......................... classify
g0171:   vocab_extra_ids ................................. 0
g0171:   vocab_file ...................................... None
g0171:   vocab_size ...................................... None
g0171:   wandb_entity .................................... yohei-kobashi
g0171:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True
g0171:   wandb_project ................................... encrypted_data_LLM
g0171:   wandb_tag ....................................... other_gpu
g0171:   weight_decay .................................... 0.1
g0171:   weight_decay_incr_style ......................... constant
g0171:   world_size ...................................... 32
g0171:   zero_allgather_bucket_size ...................... 0.0
g0171:   zero_contigious_gradients ....................... False
g0171:   zero_reduce_bucket_size ......................... 0.0
g0171:   zero_reduce_scatter ............................. False
g0171:   zero_stage ...................................... 0
g0171: -------------------- end of arguments ---------------------
g0171: setting number of micro-batches to constant 32
g0171: > building SentencePieceTokenizer tokenizer ...
g0171: [2024-08-12 12:18:02,092] [INFO] [comm.py:637:init_distributed] cdb=None
g0171:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0171: > initializing torch distributed ...
g0171: [2024-08-12 12:18:02,092] [INFO] [comm.py:637:init_distributed] cdb=None
g0171: [2024-08-12 12:18:02,092] [INFO] [comm.py:637:init_distributed] cdb=None
g0171: [2024-08-12 12:18:02,092] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0171: [2024-08-12 12:18:02,092] [INFO] [comm.py:637:init_distributed] cdb=None
g0171: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [2024-08-12 12:18:03,741] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0176: [2024-08-12 12:18:03,741] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0176: [2024-08-12 12:18:03,741] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0176: [2024-08-12 12:18:03,741] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0176: [2024-08-12 12:18:03,742] [INFO] [launch.py:163:main] dist_world_size=32
g0176: [2024-08-12 12:18:03,742] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0172: [2024-08-12 12:18:03,746] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0172: [2024-08-12 12:18:03,746] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0172: [2024-08-12 12:18:03,746] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0172: [2024-08-12 12:18:03,746] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0172: [2024-08-12 12:18:03,746] [INFO] [launch.py:163:main] dist_world_size=32
g0172: [2024-08-12 12:18:03,746] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0173: [2024-08-12 12:18:03,956] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0173: [2024-08-12 12:18:03,956] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0173: [2024-08-12 12:18:03,956] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0173: [2024-08-12 12:18:03,956] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0173: [2024-08-12 12:18:03,957] [INFO] [launch.py:163:main] dist_world_size=32
g0173: [2024-08-12 12:18:03,957] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0178: [2024-08-12 12:18:03,967] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0178: [2024-08-12 12:18:03,968] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0178: [2024-08-12 12:18:03,968] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0178: [2024-08-12 12:18:03,968] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0178: [2024-08-12 12:18:03,968] [INFO] [launch.py:163:main] dist_world_size=32
g0178: [2024-08-12 12:18:03,968] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0177: [2024-08-12 12:18:04,114] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0177: [2024-08-12 12:18:04,115] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0177: [2024-08-12 12:18:04,115] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0177: [2024-08-12 12:18:04,115] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0177: [2024-08-12 12:18:04,115] [INFO] [launch.py:163:main] dist_world_size=32
g0177: [2024-08-12 12:18:04,115] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0174: [2024-08-12 12:18:04,170] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0174: [2024-08-12 12:18:04,171] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0174: [2024-08-12 12:18:04,171] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0174: [2024-08-12 12:18:04,171] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0174: [2024-08-12 12:18:04,171] [INFO] [launch.py:163:main] dist_world_size=32
g0174: [2024-08-12 12:18:04,171] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0175: [2024-08-12 12:18:04,397] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0175: [2024-08-12 12:18:04,397] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0171': [0, 1, 2, 3], 'g0172': [0, 1, 2, 3], 'g0173': [0, 1, 2, 3], 'g0174': [0, 1, 2, 3], 'g0175': [0, 1, 2, 3], 'g0176': [0, 1, 2, 3], 'g0177': [0, 1, 2, 3], 'g0178': [0, 1, 2, 3]}
g0175: [2024-08-12 12:18:04,397] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0175: [2024-08-12 12:18:04,397] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0171': [0, 1, 2, 3], 'g0172': [4, 5, 6, 7], 'g0173': [8, 9, 10, 11], 'g0174': [12, 13, 14, 15], 'g0175': [16, 17, 18, 19], 'g0176': [20, 21, 22, 23], 'g0177': [24, 25, 26, 27], 'g0178': [28, 29, 30, 31]})
g0175: [2024-08-12 12:18:04,397] [INFO] [launch.py:163:main] dist_world_size=32
g0175: [2024-08-12 12:18:04,397] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0176: [2024-08-12 12:18:06,843] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0176: [2024-08-12 12:18:06,844] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0176: [2024-08-12 12:18:06,844] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0172: [2024-08-12 12:18:06,856] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0172: [2024-08-12 12:18:06,856] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0172: [2024-08-12 12:18:06,856] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0172: [2024-08-12 12:18:07,004] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0178: [2024-08-12 12:18:07,051] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0178: [2024-08-12 12:18:07,051] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0176: [2024-08-12 12:18:07,058] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0178: [2024-08-12 12:18:07,104] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0173: [2024-08-12 12:18:07,119] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0173: [2024-08-12 12:18:07,119] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0178: [2024-08-12 12:18:07,153] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0173: [2024-08-12 12:18:07,200] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0173: [2024-08-12 12:18:07,202] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0177: [2024-08-12 12:18:07,221] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0177: [2024-08-12 12:18:07,221] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0177: [2024-08-12 12:18:07,231] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0174: [2024-08-12 12:18:07,289] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0174: [2024-08-12 12:18:07,371] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0174: [2024-08-12 12:18:07,385] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0174: [2024-08-12 12:18:07,396] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0177: [2024-08-12 12:18:07,483] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0175: [2024-08-12 12:18:07,493] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0175: [2024-08-12 12:18:07,493] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0175: [2024-08-12 12:18:07,576] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0175: [2024-08-12 12:18:07,610] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0176: ----------------------------------------------------------------------------------------------------
g0176: 
g0176: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0176: 
g0176: ----------------------------------------------------------------------------------------------------
g0176: 
g0176: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0176:       runtime if needed. Op compatibility means that your system
g0176:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0176:       runtime if needed. Op compatibility means that your system
g0176:       meet the required dependencies to JIT install the op.
g0176: 
g0176: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0176: 
g0176: 
g0176: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0176: DeepSpeed C++/CUDA extension op report
g0176: 
g0176: --------------------------------------------------
g0176: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0176:       runtime if needed. Op compatibility means that your system
g0176:       meet the required dependencies to JIT install the op.
g0176: --------------------------------------------------
g0176: JIT compiled ops requires ninja
g0176: --------------------------------------------------
g0176: DeepSpeed C++/CUDA extension op report
g0176: --------------------------------------------------
g0176: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0176:       runtime if needed. Op compatibility means that your system
g0176:       meet the required dependencies to JIT install the op.
g0176: --------------------------------------------------
g0176: JIT compiled ops requires ninja
g0176: ninjaninjaninja ninja  ......................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m..................
g0176: 
g0176: 
g0176:  [92m[OKAY][0m------------------------------------------------------------------------------------------------------------------------------------------------------
g0176: 
g0176: 
g0176: 
g0176: --------------------------------------------------op nameop nameop name
g0176:    ................................op name................    installedinstalled................installed    ....installed..    compatiblecompatible..compatible
g0176: 
g0176:  
g0176: compatible------------------------------------------------------------------------------------------------------------------------------------------------------
g0176: 
g0176: 
g0176: 
g0176: --------------------------------------------------
g0172: --------------------------------------------------
g0172: DeepSpeed C++/CUDA extension op report
g0172: --------------------------------------------------
g0172: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0172:       runtime if needed. Op compatibility means that your system
g0172:       meet the required dependencies to JIT install the op.
g0172: --------------------------------------------------
g0172: JIT compiled ops requires ninja
g0172: ----------------------------------------------------------------------------------------------------
g0172: 
g0172: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0172: 
g0172: ----------------------------------------------------------------------------------------------------
g0172: 
g0172: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0172:       runtime if needed. Op compatibility means that your system
g0172:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0172:       runtime if needed. Op compatibility means that your system
g0172:       meet the required dependencies to JIT install the op.
g0172: 
g0172: ----------------------------------------------------------------------------------------------------
g0172: 
g0172: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0172: 
g0172: --------------------------------------------------
g0172: DeepSpeed C++/CUDA extension op report
g0172: --------------------------------------------------
g0172: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0172:       runtime if needed. Op compatibility means that your system
g0172:       meet the required dependencies to JIT install the op.
g0172: --------------------------------------------------
g0172: JIT compiled ops requires ninja
g0172: ninjaninja ..................ninja ninja .................. [92m[OKAY][0m  ..................
g0172: ..................[92m[OKAY][0m  
g0172: --------------------------------------------------[92m[OKAY][0m[92m[OKAY][0m
g0172: 
g0172: 
g0172: --------------------------------------------------op name
g0172: ---------------------------------------------------------------------------------------------------- 
g0172: ................op name
g0172:   op nameinstalledop name................   ..................................    installedcompatibleinstalledinstalled 
g0172:   ......--------------------------------------------------   
g0172: compatiblecompatiblecompatible
g0172: 
g0172: 
g0172: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0172: 
g0172: 
g0178: ----------------------------------------------------------------------------------------------------
g0178: 
g0178: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0178: 
g0178: ----------------------------------------------------------------------------------------------------
g0178: 
g0178: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0178:       runtime if needed. Op compatibility means that your system
g0178:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0178:       runtime if needed. Op compatibility means that your system
g0178:       meet the required dependencies to JIT install the op.
g0178: 
g0178: ----------------------------------------------------------------------------------------------------
g0178: 
g0178: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0178: 
g0178: --------------------------------------------------
g0178: DeepSpeed C++/CUDA extension op report
g0178: --------------------------------------------------
g0178: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0178:       runtime if needed. Op compatibility means that your system
g0178:       meet the required dependencies to JIT install the op.
g0178: --------------------------------------------------
g0178: JIT compiled ops requires ninja
g0178: --------------------------------------------------
g0178: DeepSpeed C++/CUDA extension op report
g0178: --------------------------------------------------
g0178: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0178:       runtime if needed. Op compatibility means that your system
g0178:       meet the required dependencies to JIT install the op.
g0178: --------------------------------------------------
g0178: JIT compiled ops requires ninja
g0178: ninjaninjaninjaninja   .................................... ..................  .................. [92m[OKAY][0m[92m[OKAY][0m [92m[OKAY][0m
g0178: 
g0178: [92m[OKAY][0m
g0178: 
g0178: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0178: 
g0178: --------------------------------------------------
g0178: op name
g0178: op name op name ................ op name................ ................  installed ................installed installed  .. installed.. ..  compatible ..compatible
g0178: 
g0178:  compatible--------------------------------------------------
g0178: --------------------------------------------------compatible
g0178: 
g0178: --------------------------------------------------
g0178: 
g0178: --------------------------------------------------
g0176: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0176: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0176: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0176: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0176: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0176: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0176: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: async_iocpu_adam  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0176: 
g0176: cpu_adagrad ............async_io fused_adam[92m[YES][0m   ..................................   [92m[YES][0m[92m[YES][0m[92m[OKAY][0m  
g0176: ............  cpu_lion[92m[OKAY][0m[92m[OKAY][0m 
g0176: 
g0176: ............... [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0176: fused_adam[92m[YES][0m  ...................  [92m[OKAY][0m[92m[YES][0m
g0176:  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0176: cpu_adagrad...... evoformer_attn ............ [92m[OKAY][0m .........
g0176: [92m[YES][0m  [93m[NO][0m......  cpu_adam.......[92m[OKAY][0m 
g0176:  ...............[93m[NO][0m 
g0176: [92m[YES][0mcpu_lion  ..................... fused_lamb [92m[OKAY][0m [92m[YES][0m
g0176: .............  ......[92m[YES][0m cpu_adagrad [92m[OKAY][0m ......
g0176: ............  [92m[OKAY][0m[92m[YES][0m
g0176:  ...... [92m[OKAY][0m
g0176: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0176: cpu_lion evoformer_attnfused_lion...............   ......................[92m[YES][0m   [93m[NO][0m[92m[YES][0m......   .............  [92m[OKAY][0m[93m[NO][0m[92m[OKAY][0m
g0176: 
g0176: 
g0176: fused_lamb ............. [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0176:  [92m[OKAY][0mevoformer_attn
g0176:  ......... [93m[NO][0m ....... [93m[NO][0m
g0176: fused_lion .............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0176:  [92m[OKAY][0m
g0176: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0172: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0172: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0172: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0172: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0172: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0172: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0172: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: async_iocpu_adam  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m
g0172: [92m[OKAY][0m
g0172: cpu_adagrad ............async_iofused_adam   [92m[YES][0m............. ............... ...... [92m[YES][0m [92m[YES][0m [92m[OKAY][0m ......
g0172: ......  [92m[OKAY][0m[92m[OKAY][0m
g0172: cpu_lion
g0172:  ............... cpu_adam[92m[YES][0m  .....................fused_adam   [92m[YES][0m[92m[OKAY][0m............. 
g0172:  ......[92m[YES][0m  [92m[OKAY][0m......
g0172:  [92m[OKAY][0m
g0172: cpu_adagrad [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH............
g0172:  cpu_adam[92m[YES][0m evoformer_attn ............... ...... ......... [92m[YES][0m [92m[OKAY][0m [93m[NO][0m
g0172: ......  .......[92m[OKAY][0mcpu_lion 
g0172:  [93m[NO][0m...............
g0172:  cpu_adagrad[92m[YES][0m  fused_lamb..................   .............[92m[YES][0m [92m[OKAY][0m [92m[YES][0m
g0172: ......  ......[92m[OKAY][0m 
g0172: [92m[OKAY][0m
g0172: cpu_lion [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0172:  [92m[YES][0m evoformer_attn...... fused_lion ......... [92m[OKAY][0m .............
g0172: [93m[NO][0m  [92m[YES][0m.......  ......[93m[NO][0m 
g0172: [92m[OKAY][0m
g0172: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHfused_lamb 
g0172: ............. evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0172: ....... [93m[NO][0m
g0172: fused_lamb .............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0172:  [92m[OKAY][0m
g0172: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0176: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0176: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0176: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0176: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0172: inference_core_opsinference_core_ops .....  .....[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0172: [92m[OKAY][0m
g0172: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0176: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0172: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: cutlass_ops ............ cutlass_ops[92m[YES][0m  ..................  [92m[OKAY][0m[92m[YES][0m
g0172:  ...... [92m[OKAY][0m
g0172: quantizer .............. quantizer[92m[YES][0m  ....................  [92m[YES][0m[92m[OKAY][0m 
g0172: ...... [92m[OKAY][0m
g0172: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0172: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0176: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0176: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0176: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0176: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0176: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0176: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0176: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0176: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0176: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0176: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0176: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0176: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0172: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0176: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0176: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0176: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0176: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0176: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0172: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0172: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0172: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0172: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0172: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0172: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0172: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0172: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0172: ragged_ops ............. [92m[YES][0m[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0172: ...... [92m[OKAY][0msparse_attn
g0172:  ............ [93m[NO][0mrandom_ltd  ....................  [93m[NO][0m[92m[YES][0m
g0172:  ...... [92m[OKAY][0m
g0172: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0172: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0172: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0176: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0176: --------------------------------------------------
g0176: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0176: --------------------------------------------------
g0176: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0176: --------------------------------------------------
g0176: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0176: --------------------------------------------------
g0172: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0172: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0172: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0172: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0172: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0172: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0172: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0172: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0172: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0176: DeepSpeed general environment info:
g0176: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0176: torch version .................... 2.0.1+cu118
g0176: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0176: deepspeed info ................... 0.12.4, unknown, unknown
g0176: torch cuda version ............... 11.8
g0176: torch hip version ................ None
g0176: nvcc version ..................... 11.8
g0176: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0176: shared memory (/dev/shm) size .... 188.13 GB
g0176: DeepSpeed general environment info:
g0176: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0176: torch version .................... 2.0.1+cu118
g0176: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0176: deepspeed info ................... 0.12.4, unknown, unknown
g0176: torch cuda version ............... 11.8
g0176: torch hip version ................ None
g0176: nvcc version ..................... 11.8
g0176: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0176: shared memory (/dev/shm) size .... 188.13 GB
g0176: DeepSpeed general environment info:
g0176: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0176: torch version .................... 2.0.1+cu118
g0176: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0176: deepspeed info ................... 0.12.4, unknown, unknown
g0176: torch cuda version ............... 11.8
g0176: torch hip version ................ None
g0176: nvcc version ..................... 11.8
g0176: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0176: shared memory (/dev/shm) size .... 188.13 GB
g0172: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0172: --------------------------------------------------
g0176: DeepSpeed general environment info:
g0176: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0176: torch version .................... 2.0.1+cu118
g0176: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0176: deepspeed info ................... 0.12.4, unknown, unknown
g0176: torch cuda version ............... 11.8
g0176: torch hip version ................ None
g0176: nvcc version ..................... 11.8
g0176: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0176: shared memory (/dev/shm) size .... 188.13 GB
g0172: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0172: --------------------------------------------------
g0172: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0172: --------------------------------------------------
g0172: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0172: --------------------------------------------------
g0172: DeepSpeed general environment info:
g0172: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0172: torch version .................... 2.0.1+cu118
g0172: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0172: deepspeed info ................... 0.12.4, unknown, unknown
g0172: torch cuda version ............... 11.8
g0172: torch hip version ................ None
g0172: nvcc version ..................... 11.8
g0172: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0172: shared memory (/dev/shm) size .... 188.13 GB
g0172: DeepSpeed general environment info:
g0172: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0172: torch version .................... 2.0.1+cu118
g0172: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0172: deepspeed info ................... 0.12.4, unknown, unknown
g0172: torch cuda version ............... 11.8
g0172: torch hip version ................ None
g0172: nvcc version ..................... 11.8
g0172: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0172: shared memory (/dev/shm) size .... 188.13 GB
g0172: DeepSpeed general environment info:
g0172: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0172: torch version .................... 2.0.1+cu118
g0172: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0172: deepspeed info ................... 0.12.4, unknown, unknown
g0172: torch cuda version ............... 11.8
g0172: torch hip version ................ None
g0172: nvcc version ..................... 11.8
g0172: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0172: shared memory (/dev/shm) size .... 188.13 GB
g0172: DeepSpeed general environment info:
g0172: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0172: torch version .................... 2.0.1+cu118
g0172: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0172: deepspeed info ................... 0.12.4, unknown, unknown
g0172: torch cuda version ............... 11.8
g0172: torch hip version ................ None
g0172: nvcc version ..................... 11.8
g0172: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0172: shared memory (/dev/shm) size .... 188.13 GB
g0173: --------------------------------------------------
g0173: DeepSpeed C++/CUDA extension op report
g0173: --------------------------------------------------
g0173: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0173:       runtime if needed. Op compatibility means that your system
g0173:       meet the required dependencies to JIT install the op.
g0173: --------------------------------------------------
g0173: JIT compiled ops requires ninja
g0173: --------------------------------------------------
g0173: DeepSpeed C++/CUDA extension op report
g0173: --------------------------------------------------
g0173: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0173:       runtime if needed. Op compatibility means that your system
g0173:       meet the required dependencies to JIT install the op.
g0173: ----------------------------------------------------------------------------------------------------
g0173: 
g0173: JIT compiled ops requires ninja
g0173: DeepSpeed C++/CUDA extension op report
g0173: --------------------------------------------------
g0173: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0173:       runtime if needed. Op compatibility means that your system
g0173:       meet the required dependencies to JIT install the op.
g0173: --------------------------------------------------
g0173: JIT compiled ops requires ninja
g0173: --------------------------------------------------
g0173: DeepSpeed C++/CUDA extension op report
g0173: --------------------------------------------------
g0173: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0173:       runtime if needed. Op compatibility means that your system
g0173:       meet the required dependencies to JIT install the op.
g0173: --------------------------------------------------
g0173: JIT compiled ops requires ninja
g0173: ninjaninja ninja..................   ..................[92m[OKAY][0m.................. 
g0173:  [92m[OKAY][0m[92m[OKAY][0m--------------------------------------------------ninja
g0173: 
g0173: 
g0173:  op name--------------------------------------------------..................-------------------------------------------------- 
g0173:  
g0173: ................[92m[OKAY][0m op name
g0173: op nameinstalled   ................--------------------------------------------------.................. 
g0173:   installedinstalledcompatible op name 
g0173: .. ..  ................--------------------------------------------------compatiblecompatible 
g0173: 
g0173: 
g0173: installed ----------------------------------------------------------------------------------------------------..
g0173: 
g0173:  compatible
g0173: --------------------------------------------------
g0172: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0172: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0172: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0172: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0176: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0176: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0176: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0176: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0177: --------------------------------------------------
g0177: DeepSpeed C++/CUDA extension op report
g0177: --------------------------------------------------
g0177: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0177:       runtime if needed. Op compatibility means that your system
g0177:       meet the required dependencies to JIT install the op.
g0177: --------------------------------------------------
g0177: JIT compiled ops requires ninja
g0177: --------------------------------------------------
g0177: DeepSpeed C++/CUDA extension op report
g0177: --------------------------------------------------
g0177: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0177:       runtime if needed. Op compatibility means that your system
g0177:       meet the required dependencies to JIT install the op.
g0177: --------------------------------------------------
g0177: JIT compiled ops requires ninja
g0177: --------------------------------------------------
g0177: DeepSpeed C++/CUDA extension op report
g0177: --------------------------------------------------
g0177: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0177:       runtime if needed. Op compatibility means that your system
g0177:       meet the required dependencies to JIT install the op.
g0177: --------------------------------------------------
g0177: JIT compiled ops requires ninja
g0177: --------------------------------------------------
g0177: DeepSpeed C++/CUDA extension op report
g0177: --------------------------------------------------
g0177: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0177:       runtime if needed. Op compatibility means that your system
g0177:       meet the required dependencies to JIT install the op.
g0177: --------------------------------------------------
g0177: JIT compiled ops requires ninja
g0177: ninjaninjaninja ninja  .................................... ..................  .................. [92m[OKAY][0m[92m[OKAY][0m [92m[OKAY][0m
g0177: 
g0177: [92m[OKAY][0m
g0177: 
g0177: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0177: 
g0177: 
g0177: --------------------------------------------------op name
g0177: op nameop name   op name................................................    ................installedinstalledinstalled    installed......    ..compatiblecompatiblecompatible 
g0177: 
g0177: 
g0177: compatible----------------------------------------------------------------------------------------------------
g0177: --------------------------------------------------
g0177: 
g0177: 
g0177: --------------------------------------------------
g0176: [2024-08-12 12:18:11,527] [INFO] [comm.py:637:init_distributed] cdb=None
g0176: [2024-08-12 12:18:11,530] [INFO] [comm.py:637:init_distributed] cdb=None
g0176: [2024-08-12 12:18:11,531] [INFO] [comm.py:637:init_distributed] cdb=None
g0172: [2024-08-12 12:18:11,532] [INFO] [comm.py:637:init_distributed] cdb=None
g0172: [2024-08-12 12:18:11,532] [INFO] [comm.py:637:init_distributed] cdb=None
g0172: [2024-08-12 12:18:11,533] [INFO] [comm.py:637:init_distributed] cdb=None
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [2024-08-12 12:18:11,535] [INFO] [comm.py:637:init_distributed] cdb=None
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [2024-08-12 12:18:11,540] [INFO] [comm.py:637:init_distributed] cdb=None
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0172: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0176: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0173: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0173: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0173: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0173: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0173: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0173: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: async_iocpu_adam  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0173: 
g0173: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0173:  ............. cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0173: ...... [92m[OKAY][0m
g0173: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0173: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_adagrad
g0173:  ............ evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0173: ....... async_io[93m[NO][0mcpu_lion
g0173:   ..............................  fused_lamb[92m[YES][0m[92m[YES][0m   .........................   [92m[YES][0m[92m[OKAY][0m[92m[OKAY][0m 
g0173: 
g0173: ...... [92m[OKAY][0m
g0173: fused_adam ............. [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH fused_lion
g0173: ......  .............evoformer_attn[92m[OKAY][0m  
g0173: [92m[YES][0m.........  ......[93m[NO][0m cpu_adam[92m[OKAY][0m  
g0173: ......................  [93m[NO][0m[92m[YES][0m
g0173:  ...... [92m[OKAY][0mfused_lamb
g0173:  ............. [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0173:  ...... [92m[OKAY][0m
g0173: cpu_lion ...............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0173:  [92m[OKAY][0m
g0173: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0173: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0173: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0178: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0178: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0178: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0178: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0178: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0178: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: async_iocpu_adam  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0178: 
g0178: cpu_adagrad ............ [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0178:  ...... cpu_lion[92m[OKAY][0m 
g0178: ............... [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0178: [92m[YES][0m ...... [92m[OKAY][0m
g0178: cpu_adagrad[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0178: ............async_io evoformer_attn[92m[YES][0m   ..............................   [93m[NO][0m[92m[YES][0m[92m[OKAY][0m  
g0178: .............  [93m[NO][0m[92m[OKAY][0mcpu_lion
g0178:  
g0178: ............... fused_lamb[92m[YES][0m  ...................  fused_adam[92m[YES][0m[92m[OKAY][0m  
g0178: ...................  [92m[YES][0m[92m[OKAY][0m 
g0178: ...... [92m[OKAY][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0178: 
g0178: evoformer_attncpu_adam  .........fused_lion...............   [93m[NO][0m.............[92m[YES][0m   .......[92m[YES][0m......   [93m[NO][0m......[92m[OKAY][0m
g0178:  
g0178: [92m[OKAY][0m
g0178: fused_lamb cpu_adagrad.............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0178: [92m[OKAY][0m
g0178: cpu_lion ............... [92m[YES][0mfused_lion  ...................  [92m[OKAY][0m
g0178: [92m[YES][0m ...... [92m[OKAY][0m
g0178: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0178: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0178: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0173: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0173: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0173: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0178: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0178: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0178: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0178: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0173: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0mcutlass_ops
g0178:  ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0173: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0173: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0173: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0173: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatibleragged_ops
g0173:  .............sparse_attn  [92m[YES][0m............  ......[93m[NO][0m  [92m[OKAY][0m.......
g0173:  [93m[NO][0m
g0173: random_ltd ............. [92m[YES][0m ...... ragged_ops[92m[OKAY][0m
g0173:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: random_ltd [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0.............
g0173:  [92m[YES][0m ...... [92m[OKAY][0m
g0173: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0173: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0173: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0173: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0173: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0178: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0178: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0178: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0178: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0178: 
g0178: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0178: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0178: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0178: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0178: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0178: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0178: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0178: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0178: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0173: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0173: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0173: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0173: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0173: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0173: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0178: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0173: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0173: --------------------------------------------------
g0178: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0178: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0178: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0178: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0173: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0173: --------------------------------------------------
g0173: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0173: --------------------------------------------------
g0173: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0173: --------------------------------------------------
g0178: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0178: --------------------------------------------------
g0173: DeepSpeed general environment info:
g0173: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0173: torch version .................... 2.0.1+cu118
g0173: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0173: deepspeed info ................... 0.12.4, unknown, unknown
g0173: torch cuda version ............... 11.8
g0173: torch hip version ................ None
g0173: nvcc version ..................... 11.8
g0173: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0173: shared memory (/dev/shm) size .... 188.13 GB
g0178: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0178: --------------------------------------------------
g0178: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0178: --------------------------------------------------
g0178: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0178: --------------------------------------------------
g0173: DeepSpeed general environment info:
g0173: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0173: torch version .................... 2.0.1+cu118
g0173: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0173: deepspeed info ................... 0.12.4, unknown, unknown
g0173: torch cuda version ............... 11.8
g0173: torch hip version ................ None
g0173: nvcc version ..................... 11.8
g0173: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0173: shared memory (/dev/shm) size .... 188.13 GB
g0173: DeepSpeed general environment info:
g0173: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0173: torch version .................... 2.0.1+cu118
g0173: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0173: deepspeed info ................... 0.12.4, unknown, unknown
g0173: torch cuda version ............... 11.8
g0173: torch hip version ................ None
g0173: nvcc version ..................... 11.8
g0173: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0173: shared memory (/dev/shm) size .... 188.13 GB
g0173: DeepSpeed general environment info:
g0173: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0173: torch version .................... 2.0.1+cu118
g0173: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0173: deepspeed info ................... 0.12.4, unknown, unknown
g0173: torch cuda version ............... 11.8
g0173: torch hip version ................ None
g0173: nvcc version ..................... 11.8
g0173: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0173: shared memory (/dev/shm) size .... 188.13 GB
g0178: DeepSpeed general environment info:
g0178: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0178: torch version .................... 2.0.1+cu118
g0178: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0178: deepspeed info ................... 0.12.4, unknown, unknown
g0178: torch cuda version ............... 11.8
g0178: torch hip version ................ None
g0178: nvcc version ..................... 11.8
g0178: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0178: shared memory (/dev/shm) size .... 188.13 GB
g0178: DeepSpeed general environment info:
g0178: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0178: torch version .................... 2.0.1+cu118
g0178: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0178: deepspeed info ................... 0.12.4, unknown, unknown
g0178: torch cuda version ............... 11.8
g0178: torch hip version ................ None
g0178: nvcc version ..................... 11.8
g0178: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0178: shared memory (/dev/shm) size .... 188.13 GB
g0178: DeepSpeed general environment info:
g0178: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0178: torch version .................... 2.0.1+cu118
g0178: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0178: deepspeed info ................... 0.12.4, unknown, unknown
g0178: torch cuda version ............... 11.8
g0178: torch hip version ................ None
g0178: nvcc version ..................... 11.8
g0178: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0178: shared memory (/dev/shm) size .... 188.13 GB
g0178: DeepSpeed general environment info:
g0178: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0178: torch version .................... 2.0.1+cu118
g0178: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0178: deepspeed info ................... 0.12.4, unknown, unknown
g0178: torch cuda version ............... 11.8
g0178: torch hip version ................ None
g0178: nvcc version ..................... 11.8
g0178: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0178: shared memory (/dev/shm) size .... 188.13 GB
g0174: ----------------------------------------------------------------------------------------------------
g0174: 
g0174: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report--------------------------------------------------
g0174: 
g0174: --------------------------------------------------
g0174: --------------------------------------------------
g0174: 
g0174: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0174:       runtime if needed. Op compatibility means that your system
g0174:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0174:       runtime if needed. Op compatibility means that your system
g0174:       meet the required dependencies to JIT install the op.
g0174: 
g0174: 
g0174: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0174: 
g0174: 
g0174: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0174:       runtime if needed. Op compatibility means that your system
g0174:       meet the required dependencies to JIT install the op.JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0174: 
g0174: 
g0174: --------------------------------------------------
g0174: JIT compiled ops requires ninja
g0174: --------------------------------------------------
g0174: DeepSpeed C++/CUDA extension op report
g0174: --------------------------------------------------
g0174: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0174:       runtime if needed. Op compatibility means that your system
g0174:       meet the required dependencies to JIT install the op.
g0174: --------------------------------------------------
g0174: JIT compiled ops requires ninja
g0173: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0173: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0173: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0173: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0174: ninjaninjaninja ninja  ......................................................    [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0174:  
g0174: 
g0174: [92m[OKAY][0m--------------------------------------------------
g0174: ----------------------------------------------------------------------------------------------------
g0174: 
g0174: 
g0174: --------------------------------------------------op name
g0174: op nameop name   ................op name................................    installed................installedinstalled    ..installed....    compatiblecompatible
g0174: ..compatible
g0174:  
g0174: --------------------------------------------------compatible--------------------------------------------------
g0174: --------------------------------------------------
g0174: 
g0174: 
g0174: --------------------------------------------------
g0178: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0178: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0178: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0178: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0177: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0177: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0177: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0177: async_io ............... [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0177: ...... [92m[OKAY][0mevoformer_attn 
g0177: ......... [93m[NO][0m ....... [93m[NO][0mfused_adam
g0177:  .............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0177:  [92m[OKAY][0m
g0177: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0177: fused_lion ............. cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0177: ...... [92m[OKAY][0m
g0177: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0177: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0177: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0177: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: fused_lion ............. [92m[YES][0m ...... async_io[92m[OKAY][0m
g0177:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0177: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: cpu_adamasync_io ...............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0177: [92m[OKAY][0m
g0177: cpu_adagrad ............ [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0177: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0177:  [92m[YES][0m ......cpu_adam  [92m[OKAY][0m...............
g0177:  [92m[YES][0m ...... [92m[OKAY][0m
g0177: cpu_adagrad ............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0177: [92m[YES][0m ......evoformer_attn  [92m[OKAY][0m.........
g0177:  [93m[NO][0m cpu_lion.......  ...............[93m[NO][0m 
g0177: [92m[YES][0m ......fused_lamb  [92m[OKAY][0m.............
g0177:  [92m[YES][0m ...... [92m[OKAY][0m
g0177: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0177: evoformer_attn .........fused_lion  [93m[NO][0m.............  .......[92m[YES][0m  [93m[NO][0m......
g0177:  [92m[OKAY][0m
g0177: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0173: [2024-08-12 12:18:11,727] [INFO] [comm.py:637:init_distributed] cdb=None
g0173: [2024-08-12 12:18:11,727] [INFO] [comm.py:637:init_distributed] cdb=None
g0173: [2024-08-12 12:18:11,729] [INFO] [comm.py:637:init_distributed] cdb=None
g0173: [2024-08-12 12:18:11,729] [INFO] [comm.py:637:init_distributed] cdb=None
g0178: [2024-08-12 12:18:11,730] [INFO] [comm.py:637:init_distributed] cdb=None
g0178: [2024-08-12 12:18:11,731] [INFO] [comm.py:637:init_distributed] cdb=None
g0178: [2024-08-12 12:18:11,731] [INFO] [comm.py:637:init_distributed] cdb=None
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0177: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0177: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0177: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0177: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0177: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0177: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0177: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0177: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0173: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0177: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0177: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0177: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0177: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0177: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0177: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0177: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0177: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0177: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0177: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0177: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0177: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0177: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0177: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0177: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0177: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0177: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0177: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0177: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0177: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0177: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0177: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0177: --------------------------------------------------
g0177: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0177: --------------------------------------------------
g0177: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0177: --------------------------------------------------
g0177: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0177: --------------------------------------------------
g0177: DeepSpeed general environment info:
g0177: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0177: torch version .................... 2.0.1+cu118
g0177: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0177: deepspeed info ................... 0.12.4, unknown, unknown
g0177: torch cuda version ............... 11.8
g0177: torch hip version ................ None
g0177: nvcc version ..................... 11.8
g0177: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0177: shared memory (/dev/shm) size .... 188.13 GB
g0177: DeepSpeed general environment info:
g0177: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0177: torch version .................... 2.0.1+cu118
g0177: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0177: deepspeed info ................... 0.12.4, unknown, unknown
g0177: torch cuda version ............... 11.8
g0177: torch hip version ................ None
g0177: nvcc version ..................... 11.8
g0177: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0177: shared memory (/dev/shm) size .... 188.13 GB
g0177: DeepSpeed general environment info:
g0177: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0177: torch version .................... 2.0.1+cu118
g0177: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0177: deepspeed info ................... 0.12.4, unknown, unknown
g0177: torch cuda version ............... 11.8
g0177: torch hip version ................ None
g0177: nvcc version ..................... 11.8
g0177: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0177: shared memory (/dev/shm) size .... 188.13 GB
g0177: DeepSpeed general environment info:
g0177: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0177: torch version .................... 2.0.1+cu118
g0177: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0177: deepspeed info ................... 0.12.4, unknown, unknown
g0177: torch cuda version ............... 11.8
g0177: torch hip version ................ None
g0177: nvcc version ..................... 11.8
g0177: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0177: shared memory (/dev/shm) size .... 188.13 GB
g0177: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0177: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0177: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0177: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0177: [2024-08-12 12:18:11,896] [INFO] [comm.py:637:init_distributed] cdb=None
g0177: [2024-08-12 12:18:11,897] [INFO] [comm.py:637:init_distributed] cdb=None
g0177: [2024-08-12 12:18:11,898] [INFO] [comm.py:637:init_distributed] cdb=None
g0177: [2024-08-12 12:18:11,898] [INFO] [comm.py:637:init_distributed] cdb=None
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0174: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0174: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0174: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0174: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0174: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0174: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0174: fused_adam ............. [92m[YES][0m async_io......  [92m[OKAY][0m...............
g0174:  [92m[YES][0m ...... [92m[OKAY][0m
g0174: cpu_adam ............... [92m[YES][0m fused_adam...... .............  [92m[OKAY][0m[92m[YES][0m 
g0174: ...... [92m[OKAY][0m
g0174: cpu_adagrad cpu_adam............  ...............[92m[YES][0m [92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0174: 
g0174: cpu_adagrad ............ cpu_lion[92m[YES][0masync_io  ...... ............... ...............[92m[OKAY][0m  
g0174: [92m[YES][0m[92m[YES][0m  cpu_lion...... .....................   [92m[OKAY][0m[92m[YES][0m[92m[OKAY][0m 
g0174: ......
g0174:  [92m[OKAY][0m
g0174: fused_adam ............. [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0174: [92m[OKAY][0m
g0174: 
g0174: evoformer_attn .........evoformer_attn cpu_adam [93m[NO][0m  ...............................   [93m[NO][0m[92m[YES][0m[93m[NO][0m 
g0174:  ...... .......[92m[OKAY][0mfused_lamb 
g0174:  [93m[NO][0m............. 
g0174: cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0mfused_lamb 
g0174:  ...... .............[92m[OKAY][0m 
g0174: [92m[YES][0m cpu_lion...... fused_lion...............   .............[92m[OKAY][0m[92m[YES][0m  [92m[YES][0m
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: ...... ......  [92m[OKAY][0m[92m[OKAY][0m
g0174: 
g0174: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHfused_lion
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174:  .............evoformer_attn  .........[92m[YES][0m  [93m[NO][0m .............  [93m[NO][0m[92m[OKAY][0m
g0174: 
g0174: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0177: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0174: inference_core_ops ..... [92m[YES][0m ...... inference_core_ops[92m[OKAY][0m 
g0174: ..... [92m[YES][0m ...... [92m[OKAY][0m
g0174: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0174: cutlass_ops ............ cutlass_ops[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0174: ...... [92m[OKAY][0mquantizer
g0174:  .............. quantizer[92m[YES][0m  ....................  [92m[YES][0m[92m[OKAY][0m 
g0174: ...... [92m[OKAY][0m
g0174: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0174: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0174: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0174: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0174: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0174: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0174: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: random_ltd ............. [92m[YES][0m[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0174: ...... [92m[OKAY][0msparse_attn
g0174:  ............ [93m[NO][0m ....... [93m[NO][0m
g0174: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0174: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0174: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0174: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0174: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0174: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0174: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0174: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0174: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0174: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0174: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0174: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0174: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0174: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0174: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0174: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0174: --------------------------------------------------
g0174: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0174: --------------------------------------------------
g0174: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0174: --------------------------------------------------
g0174: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0174: --------------------------------------------------
g0174: DeepSpeed general environment info:
g0174: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0174: torch version .................... 2.0.1+cu118
g0174: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0174: deepspeed info ................... 0.12.4, unknown, unknown
g0174: torch cuda version ............... 11.8
g0174: torch hip version ................ None
g0174: nvcc version ..................... 11.8
g0174: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0174: shared memory (/dev/shm) size .... 188.13 GB
g0174: DeepSpeed general environment info:
g0174: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0174: torch version .................... 2.0.1+cu118
g0174: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0174: deepspeed info ................... 0.12.4, unknown, unknown
g0174: torch cuda version ............... 11.8
g0174: torch hip version ................ None
g0174: nvcc version ..................... 11.8
g0174: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0174: shared memory (/dev/shm) size .... 188.13 GB
g0174: DeepSpeed general environment info:
g0174: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0174: torch version .................... 2.0.1+cu118
g0174: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0174: deepspeed info ................... 0.12.4, unknown, unknown
g0174: torch cuda version ............... 11.8
g0174: torch hip version ................ None
g0174: nvcc version ..................... 11.8
g0174: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0174: shared memory (/dev/shm) size .... 188.13 GB
g0174: DeepSpeed general environment info:
g0174: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0174: torch version .................... 2.0.1+cu118
g0174: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0174: deepspeed info ................... 0.12.4, unknown, unknown
g0174: torch cuda version ............... 11.8
g0174: torch hip version ................ None
g0174: nvcc version ..................... 11.8
g0174: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0174: shared memory (/dev/shm) size .... 188.13 GB
g0175: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0175: 
g0175: 
g0175: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0175: 
g0175: 
g0175: ----------------------------------------------------------------------------------------------------
g0175: --------------------------------------------------
g0175: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0175:       runtime if needed. Op compatibility means that your system
g0175:       meet the required dependencies to JIT install the op.
g0175: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0175:       runtime if needed. Op compatibility means that your system
g0175:       meet the required dependencies to JIT install the op.
g0175: 
g0175: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0175:       runtime if needed. Op compatibility means that your system
g0175:       meet the required dependencies to JIT install the op.----------------------------------------------------------------------------------------------------
g0175: 
g0175: 
g0175: JIT compiled ops requires ninjaJIT compiled ops requires ninja--------------------------------------------------
g0175: 
g0175: 
g0175: JIT compiled ops requires ninja
g0175: --------------------------------------------------
g0175: DeepSpeed C++/CUDA extension op report
g0175: --------------------------------------------------
g0175: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0175:       runtime if needed. Op compatibility means that your system
g0175:       meet the required dependencies to JIT install the op.
g0175: --------------------------------------------------
g0175: JIT compiled ops requires ninja
g0175: ninjaninjaninjaninja   .................. ....................................   [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0175: 
g0175: 
g0175:  ------------------------------------------------------------------------------------------------------------------------------------------------------[92m[OKAY][0m
g0175: 
g0175: 
g0175: 
g0175: op nameop nameop name   ................................................--------------------------------------------------   
g0175: installedinstalledinstalled   ..op name....    compatiblecompatible................
g0175: 
g0175: compatible 
g0175: ----------------------------------------------------------------------------------------------------installed
g0175: 
g0175: -------------------------------------------------- 
g0175: .. compatible
g0175: --------------------------------------------------
g0174: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0174: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0174: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0174: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0175: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0175: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0175: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0175: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0175: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0175: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0175: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: async_iocpu_adam  ............... ...............[92m[YES][0m  ...... [92m[YES][0m[92m[OKAY][0m
g0175:  ...... cpu_adagrad[92m[OKAY][0m ............
g0175:  [92m[YES][0m ...... [92m[OKAY][0m
g0175: fused_adamcpu_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0175: 
g0175: cpu_adam ...............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0175: [92m[YES][0m ......evoformer_attn  .........[92m[OKAY][0masync_io 
g0175: [93m[NO][0m  ......................  cpu_adagrad[93m[NO][0m[92m[YES][0m 
g0175:  ..................  fused_lamb[92m[OKAY][0m[92m[YES][0m .............
g0175:   ......[92m[YES][0m  ......[92m[OKAY][0m fused_adam[92m[OKAY][0m
g0175:  
g0175: ............. [92m[YES][0m ......cpu_lion [92m[OKAY][0m 
g0175: ...............fused_lion  cpu_adam[92m[YES][0m.............   ...............[92m[YES][0m ...... [92m[YES][0m ......  [92m[OKAY][0m......[92m[OKAY][0m
g0175:  
g0175: [92m[OKAY][0m
g0175: cpu_adagrad ............ [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH ......
g0175:  [92m[OKAY][0m
g0175: evoformer_attn .........cpu_lion  [93m[NO][0m...............  .......[92m[YES][0m  ......[93m[NO][0m 
g0175: [92m[OKAY][0m
g0175: fused_lamb ............. [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0175: ......evoformer_attn  .........[92m[OKAY][0m 
g0175: [93m[NO][0m ....... [93m[NO][0m
g0175: fused_lamb ............. [92m[YES][0mfused_lion ......  .............[92m[OKAY][0m
g0175:  [92m[YES][0m ...... [92m[OKAY][0m
g0175: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0175:  [92m[OKAY][0m
g0175: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0175: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0175: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0175: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0175: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0175: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0174: [2024-08-12 12:18:12,090] [INFO] [comm.py:637:init_distributed] cdb=None
g0174: [2024-08-12 12:18:12,091] [INFO] [comm.py:637:init_distributed] cdb=None
g0174: [2024-08-12 12:18:12,091] [INFO] [comm.py:637:init_distributed] cdb=None
g0174: [2024-08-12 12:18:12,092] [INFO] [comm.py:637:init_distributed] cdb=None
g0175: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0175: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0175: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0175: sparse_attn ............ragged_ops [93m[NO][0m  ....................  [93m[NO][0m[92m[YES][0m
g0175:  ...... [92m[OKAY][0m
g0175: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0175: 
g0175: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0175: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0175: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0175: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0175: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0175: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0175: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0175: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0175: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0175: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0175: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0175: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0175: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0175: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0175: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0175: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0174: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0175: --------------------------------------------------
g0175: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0175: --------------------------------------------------
g0175: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0175: --------------------------------------------------
g0175: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0175: --------------------------------------------------
g0175: DeepSpeed general environment info:
g0175: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0175: torch version .................... 2.0.1+cu118
g0175: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0175: deepspeed info ................... 0.12.4, unknown, unknown
g0175: torch cuda version ............... 11.8
g0175: torch hip version ................ None
g0175: nvcc version ..................... 11.8
g0175: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0175: shared memory (/dev/shm) size .... 188.13 GB
g0175: DeepSpeed general environment info:
g0175: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0175: torch version .................... 2.0.1+cu118
g0175: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0175: deepspeed info ................... 0.12.4, unknown, unknown
g0175: torch cuda version ............... 11.8
g0175: torch hip version ................ None
g0175: nvcc version ..................... 11.8
g0175: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0175: shared memory (/dev/shm) size .... 188.13 GB
g0175: DeepSpeed general environment info:
g0175: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0175: torch version .................... 2.0.1+cu118
g0175: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0175: deepspeed info ................... 0.12.4, unknown, unknown
g0175: torch cuda version ............... 11.8
g0175: torch hip version ................ None
g0175: nvcc version ..................... 11.8
g0175: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0175: shared memory (/dev/shm) size .... 188.13 GB
g0175: DeepSpeed general environment info:
g0175: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0175: torch version .................... 2.0.1+cu118
g0175: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0175: deepspeed info ................... 0.12.4, unknown, unknown
g0175: torch cuda version ............... 11.8
g0175: torch hip version ................ None
g0175: nvcc version ..................... 11.8
g0175: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0175: shared memory (/dev/shm) size .... 188.13 GB
g0175: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0175: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0175: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0175: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0178: > setting tensorboard ...
g0178: [2024-08-12 12:18:12,156] [INFO] [comm.py:637:init_distributed] cdb=None
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0178: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [2024-08-12 12:18:12,242] [INFO] [comm.py:637:init_distributed] cdb=None
g0175: [2024-08-12 12:18:12,242] [INFO] [comm.py:637:init_distributed] cdb=None
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [2024-08-12 12:18:12,250] [INFO] [comm.py:637:init_distributed] cdb=None
g0175: [2024-08-12 12:18:12,253] [INFO] [comm.py:637:init_distributed] cdb=None
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0175: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0171-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0171: > initialized tensor model parallel with size 1
g0171: > initialized pipeline model parallel with size 8
g0171: > setting random seeds to 1234 ...
g0171: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0171: > compiling dataset index builder ...
g0171: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0171: make: Nothing to be done for 'default'.
g0171: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0171: >>> done with dataset index builder. Compilation time: 0.086 seconds
g0171: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0171: > compiling and loading fused kernels ...
g0171: Detected CUDA files, patching ldflags
g0171: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0171: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0171: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0171: ninja: no work to do.
g0171: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0171: Detected CUDA files, patching ldflags
g0171: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0171: Building extension module scaled_masked_softmax_cuda...
g0171: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0171: ninja: no work to do.
g0171: Loading extension module scaled_masked_softmax_cuda...
g0171: Detected CUDA files, patching ldflags
g0171: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0171: Building extension module scaled_softmax_cuda...
g0171: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0171: ninja: no work to do.
g0171: Loading extension module scaled_softmax_cuda...
g0171: >>> done with compiling and loading fused kernels. Compilation time: 7.218 seconds
g0171: time to initialize megatron (seconds): 22.360
g0171: [after megatron is initialized] datetime: 2024-08-12 12:18:22 
g0175: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0178: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0172: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0171: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0173: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0174: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0177: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0176: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0172: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0172: wandb:  $ pip install wandb --upgrade
g0172: wandb: Tracking run with wandb version 0.17.5
g0172: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-xz3rvbit
g0172: wandb: Run `wandb offline` to turn off syncing.
g0172: wandb: Syncing run g0172.abci.local
g0172: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0172: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/xz3rvbit
g0173: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0173: wandb:  $ pip install wandb --upgrade
g0173: wandb: Tracking run with wandb version 0.17.5
g0173: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-snk9utt6
g0173: wandb: Run `wandb offline` to turn off syncing.
g0173: wandb: Syncing run g0173.abci.local
g0173: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0173: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/snk9utt6
g0171: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0171: wandb:  $ pip install wandb --upgrade
g0171: wandb: Tracking run with wandb version 0.17.5
g0171: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-b7gz70q4
g0171: wandb: Run `wandb offline` to turn off syncing.
g0171: wandb: Syncing run g0171.abci.local
g0171: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0171: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/b7gz70q4
g0178: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0178: wandb:  $ pip install wandb --upgrade
g0178: wandb: Tracking run with wandb version 0.17.5
g0178: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-sbfbx2sl
g0178: wandb: Run `wandb offline` to turn off syncing.
g0178: wandb: Syncing run g0178.abci.local
g0178: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0178: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/sbfbx2sl
g0175: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0175: wandb:  $ pip install wandb --upgrade
g0175: wandb: Tracking run with wandb version 0.17.5
g0175: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-fmfpgdoi
g0175: wandb: Run `wandb offline` to turn off syncing.
g0174: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0174: wandb:  $ pip install wandb --upgrade
g0174: wandb: Tracking run with wandb version 0.17.5
g0174: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-op7invpl
g0174: wandb: Run `wandb offline` to turn off syncing.
g0175: wandb: Syncing run g0175.abci.local
g0175: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0175: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/fmfpgdoi
g0177: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0177: wandb:  $ pip install wandb --upgrade
g0177: wandb: Tracking run with wandb version 0.17.5
g0177: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-8l4mkl8h
g0177: wandb: Run `wandb offline` to turn off syncing.
g0174: wandb: Syncing run g0174.abci.local
g0174: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0174: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/op7invpl
g0177: wandb: Syncing run g0177.abci.local
g0177: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0177: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/8l4mkl8h
g0176: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0176: wandb:  $ pip install wandb --upgrade
g0176: wandb: Tracking run with wandb version 0.17.5
g0176: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_121823-pkpt9tlv
g0176: wandb: Run `wandb offline` to turn off syncing.
g0176: wandb: Syncing run g0176.abci.local
g0176: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0176: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/pkpt9tlv
g0171: building GPT model ...
g0171: [2024-08-12 12:18:24,718] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0171: [2024-08-12 12:18:24,719] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0171: [2024-08-12 12:18:24,719] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 21.48 GB, percent = 5.7%
g0171: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0171: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0171: [2024-08-12 12:18:25,233] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0171: stage=0 layers=5
g0171:      0: _to_float16
g0171:      1: EmbeddingPipe
g0171:      2: ParallelTransformerLayerPipe
g0171:      3: ParallelTransformerLayerPipe
g0171:      4: ParallelTransformerLayerPipe
g0171: stage=1 layers=3
g0171:      5: ParallelTransformerLayerPipe
g0171:      6: ParallelTransformerLayerPipe
g0171:      7: ParallelTransformerLayerPipe
g0171: stage=2 layers=3
g0171:      8: ParallelTransformerLayerPipe
g0171:      9: ParallelTransformerLayerPipe
g0171:     10: ParallelTransformerLayerPipe
g0171: stage=3 layers=3
g0171:     11: ParallelTransformerLayerPipe
g0171:     12: ParallelTransformerLayerPipe
g0171:     13: ParallelTransformerLayerPipe
g0171: stage=4 layers=3
g0171:     14: ParallelTransformerLayerPipe
g0171:     15: ParallelTransformerLayerPipe
g0171:     16: ParallelTransformerLayerPipe
g0171: stage=5 layers=3
g0171:     17: ParallelTransformerLayerPipe
g0171:     18: ParallelTransformerLayerPipe
g0171:     19: ParallelTransformerLayerPipe
g0171: stage=6 layers=3
g0171:     20: ParallelTransformerLayerPipe
g0171:     21: ParallelTransformerLayerPipe
g0171:     22: ParallelTransformerLayerPipe
g0171: stage=7 layers=3
g0171:     23: ParallelTransformerLayerPipe
g0171:     24: MixedFusedRMSNorm
g0171:     25: LMHeadPipe
g0171:   loss: CrossEntropy
g0178:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0174:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0176:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0175:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0177:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0172:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0173:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0171: [2024-08-12 12:18:25,733] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0171: [2024-08-12 12:18:25,734] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0171: [2024-08-12 12:18:25,734] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 21.54 GB, percent = 5.7%
g0171:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0171: setting training iterations to 42000
g0171: > learning rate decay style: cosine
g0171: DeepSpeed is enabled.
g0171: [2024-08-12 12:18:25,736] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0174: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0174: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0174: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0174: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0178: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0178: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0178: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0178: [2024-08-12 12:18:25,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0173: [2024-08-12 12:18:25,878] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0173: [2024-08-12 12:18:25,878] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0173: [2024-08-12 12:18:25,879] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0173: [2024-08-12 12:18:25,879] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0172: [2024-08-12 12:18:25,884] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0172: [2024-08-12 12:18:25,884] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0172: [2024-08-12 12:18:25,884] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0172: [2024-08-12 12:18:25,884] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0175: [2024-08-12 12:18:25,887] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0175: [2024-08-12 12:18:25,887] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0177: [2024-08-12 12:18:25,887] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0177: [2024-08-12 12:18:25,887] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0177: [2024-08-12 12:18:25,887] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0175: [2024-08-12 12:18:25,888] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0177: [2024-08-12 12:18:25,888] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0175: [2024-08-12 12:18:25,888] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0176: [2024-08-12 12:18:25,896] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0176: [2024-08-12 12:18:25,896] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0176: [2024-08-12 12:18:25,896] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0176: [2024-08-12 12:18:25,896] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0171: [2024-08-12 12:18:25,948] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0171: [2024-08-12 12:18:25,949] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0171: [2024-08-12 12:18:25,950] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0171: [2024-08-12 12:18:25,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0171: [2024-08-12 12:18:25,950] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0171: [2024-08-12 12:18:25,984] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0171: [2024-08-12 12:18:25,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0171: [2024-08-12 12:18:25,985] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0171: [2024-08-12 12:18:25,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f4e742c6250>
g0171: [2024-08-12 12:18:25,985] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0171: [2024-08-12 12:18:25,985] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0171: [2024-08-12 12:18:25,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: [2024-08-12 12:18:25,985] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0171: [2024-08-12 12:18:25,986] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0171:     "partition_activations": false, 
g0171:     "contiguous_memory_optimization": false, 
g0171:     "cpu_checkpointing": false, 
g0171:     "number_checkpoints": null, 
g0171:     "synchronize_checkpoint_boundary": false, 
g0171:     "profile": false
g0171: }
g0171: [2024-08-12 12:18:25,986] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0171: [2024-08-12 12:18:25,986] [INFO] [config.py:983:print]   amp_enabled .................. False
g0171: [2024-08-12 12:18:25,986] [INFO] [config.py:983:print]   amp_params ................... False
g0171: [2024-08-12 12:18:25,986] [INFO] [config.py:983:print]   autotuning_config ............ {
g0171:     "enabled": false, 
g0171:     "start_step": null, 
g0171:     "end_step": null, 
g0171:     "metric_path": null, 
g0171:     "arg_mappings": null, 
g0171:     "metric": "throughput", 
g0171:     "model_info": null, 
g0171:     "results_dir": "autotuning_results", 
g0171:     "exps_dir": "autotuning_exps", 
g0171:     "overwrite": true, 
g0171:     "fast": true, 
g0171:     "start_profile_step": 3, 
g0171:     "end_profile_step": 5, 
g0171:     "tuner_type": "gridsearch", 
g0171:     "tuner_early_stopping": 5, 
g0171:     "tuner_num_trials": 50, 
g0171:     "model_info_path": null, 
g0171:     "mp_size": 1, 
g0171:     "max_train_batch_size": null, 
g0171:     "min_train_batch_size": 1, 
g0171:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0171:     "min_train_micro_batch_size_per_gpu": 1, 
g0171:     "num_tuning_micro_batch_sizes": 3
g0171: }
g0171: [2024-08-12 12:18:25,986] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0171: [2024-08-12 12:18:25,987] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0171: [2024-08-12 12:18:25,987] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0171: [2024-08-12 12:18:25,987] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0171: [2024-08-12 12:18:25,987] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4e3846b910>
g0171: [2024-08-12 12:18:25,987] [INFO] [config.py:983:print]   communication_data_type ...... None
g0171: [2024-08-12 12:18:25,987] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0171: [2024-08-12 12:18:25,987] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   disable_allgather ............ False
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   dump_state ................... False
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0171: [2024-08-12 12:18:25,988] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0171: [2024-08-12 12:18:25,989] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0171:     "enabled": false, 
g0171:     "recompute_fwd_factor": 0.0, 
g0171:     "profile_step": 1, 
g0171:     "module_depth": -1, 
g0171:     "top_modules": 1, 
g0171:     "detailed": true, 
g0171:     "output_file": null
g0171: }
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   global_rank .................. 0
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0171: [2024-08-12 12:18:25,990] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   loss_scale ................... 0
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0171: [2024-08-12 12:18:25,991] [INFO] [config.py:983:print]   nebula_config ................ {
g0171:     "enabled": false, 
g0171:     "persistent_storage_path": null, 
g0171:     "persistent_time_interval": 100, 
g0171:     "num_of_version_in_retention": 2, 
g0171:     "enable_nebula_load": true, 
g0171:     "load_path": null
g0171: }
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   optimizer_name ............... None
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   optimizer_params ............. None
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   pld_enabled .................. False
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   pld_params ................... False
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0171: [2024-08-12 12:18:25,992] [INFO] [config.py:983:print]   scheduler_name ............... None
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   scheduler_params ............. None
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   sparse_attention ............. None
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0171: [2024-08-12 12:18:25,993] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   world_size ................... 4
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   zero_enabled ................. False
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0171: [2024-08-12 12:18:25,994] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0171: [2024-08-12 12:18:25,995] [INFO] [config.py:969:print_user_config]   json = {
g0171:     "train_batch_size": 128, 
g0171:     "train_micro_batch_size_per_gpu": 1, 
g0171:     "steps_per_print": 10, 
g0171:     "zero_optimization": {
g0171:         "stage": 0
g0171:     }, 
g0171:     "gradient_clipping": 1.0, 
g0171:     "prescale_gradients": true, 
g0171:     "fp16": {
g0171:         "enabled": true, 
g0171:         "loss_scale": 0, 
g0171:         "loss_scale_window": 500, 
g0171:         "hysteresis": 2, 
g0171:         "min_loss_scale": 1, 
g0171:         "initial_scale_power": 11
g0171:     }, 
g0171:     "wall_clock_breakdown": false
g0171: }
g0171: [2024-08-12 12:18:25,995] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0171: [2024-08-12 12:18:25,995] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0171: [2024-08-12 12:18:26,725] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0175: [2024-08-12 12:18:26,725] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0173: [2024-08-12 12:18:26,725] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0177: [2024-08-12 12:18:26,725] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0176: [2024-08-12 12:18:26,726] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0172: [2024-08-12 12:18:26,726] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0178: [2024-08-12 12:18:26,726] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0174: [2024-08-12 12:18:26,726] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0171: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0171: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0175: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0175: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0173: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0173: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0177: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0177: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0177: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0176: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0176: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0176: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0177: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0176: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0171: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0178: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0171: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0178: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0175: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0178: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0178: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0171: WARNING: could not find the metadata file /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1 
g0174: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0172: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0173: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0174: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0174: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0172: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0172: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0172: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0171:     will not load any checkpoints and will start from random
g0174: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0175: [2024-08-12 12:18:27,427] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0173: [2024-08-12 12:18:27,428] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/1/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0178: (min, max) time across ranks (ms):
g0178:     load-checkpoint ................................: (1.33, 2.30)
g0171: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-12 12:18:27 
g0171: > building train, validation, and test datasets ...
g0171:  > datasets target sizes (minimum size):
g0171:     train:      5376000
g0171:     validation: 550400
g0171:     test:       12800
g0171: > building train, validation, and test datasets for GPT ...
g0171: Single data path provided for train, valid & test
g0171:  > building dataset index ...
g0171:     reading sizes...
g0171:     reading pointers...
g0171:     reading document index...
g0171:     creating numpy buffer of mmap...
g0171:     creating memory view of numpy buffer...
g0171:  > finished creating indexed dataset in 0.037627 seconds
g0171:     number of documents: 2237032
g0171:  > dataset split:
g0171:     train:
g0171:      document indices in [0, 2122943) total of 2122943 documents
g0171:     validation:
g0171:      document indices in [2122943, 2234795) total of 111852 documents
g0171:     test:
g0171:      document indices in [2234795, 2237032) total of 2237 documents
g0171:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0171:  > only one epoch required, setting separate_last_epoch to False
g0171:  > elasped time to build and save doc-idx mapping (seconds): 0.081588
g0171:     using:
g0171:      number of documents:       2122943
g0171:      number of epochs:          1
g0171:      sequence length:           2048
g0171:      total number of samples:   10749554
g0171:  > elasped time to build and save sample-idx mapping (seconds): 0.348435
g0171:  > building shuffle index with split [0, 10749554) and [10749554, 10749554) ...
g0171:  > elasped time to build and save shuffle-idx mapping (seconds): 0.375246
g0171:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/fb3fe40f44a3cc15ef149c34ab427e6d_doc_idx.npy
g0171:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/fb3fe40f44a3cc15ef149c34ab427e6d_sample_idx.npy
g0171:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/fb3fe40f44a3cc15ef149c34ab427e6d_shuffle_idx.npy
g0171:     loaded indexed file in 0.011 seconds
g0171:     total number of samples: 10749555
g0171:     total number of epochs: 1
g0171:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0171:  > only one epoch required, setting separate_last_epoch to False
g0171:  > elasped time to build and save doc-idx mapping (seconds): 0.005509
g0171:     using:
g0171:      number of documents:       111852
g0171:      number of epochs:          1
g0171:      sequence length:           2048
g0171:      total number of samples:   563715
g0171:  > elasped time to build and save sample-idx mapping (seconds): 0.012828
g0171:  > building shuffle index with split [0, 563715) and [563715, 563715) ...
g0171:  > elasped time to build and save shuffle-idx mapping (seconds): 0.015508
g0171:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/291a2b6a26ebfc1abee58211a7ac891b_doc_idx.npy
g0171:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/291a2b6a26ebfc1abee58211a7ac891b_sample_idx.npy
g0171:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/291a2b6a26ebfc1abee58211a7ac891b_shuffle_idx.npy
g0171:     loaded indexed file in 0.007 seconds
g0171:     total number of samples: 563716
g0171:     total number of epochs: 1
g0171:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_doc_idx.npy
g0171:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_sample_idx.npy
g0171:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_shuffle_idx.npy
g0171:     loaded indexed file in 0.054 seconds
g0171:     total number of samples: 14468
g0171:     total number of epochs: 2
g0171: > finished creating GPT datasets ...
g0171: [after dataloaders are built] datetime: 2024-08-12 12:18:30 
g0171: done with setup ...
g0171: training ...
g0178: (min, max) time across ranks (ms):
g0178:     model-and-optimizer-setup ......................: (2881.69, 2887.31)
g0178:     train/valid/test-data-iterators-setup ..........: (2861.96, 2872.64)
g0171: [before the start of training step] datetime: 2024-08-12 12:18:30 
g0171: [2024-08-12 12:19:25,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.5728640000000002e-07, 1.5728640000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 10 loss: 10.5452 iter time (s): 5.456 samples/sec: 23.461
g0178:  iteration       10/   42000 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 5492.1 | learning rate: 1.573E-07 | global batch size:   128 | lm loss: 1.055340E+01 | loss scale: 2048.0 | grad norm: 7.831 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.306 | tokens per gpu per second (tgs): 1491.610 | TFLOPs: 12.00 |
g0178: [Rank 28] (after 10 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0177: [Rank 24] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 4662.0 | max reserved: 4662.0
g0176: [Rank 20] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5410.0 | max reserved: 5410.0
g0175: [Rank 16] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6304.0 | max reserved: 6304.0
g0172: [Rank 4] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8986.0 | max reserved: 8986.0
g0171: [Rank 0] (after 10 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11650.0 | max reserved: 11650.0
g0173: [Rank 8] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8092.0 | max reserved: 8092.0
g0174: [Rank 12] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7198.0 | max reserved: 7198.0
g0171: [2024-08-12 12:20:07,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[3.3204906666666666e-07, 3.3204906666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 20 loss: 10.4319 iter time (s): 4.072 samples/sec: 31.438
g0178:  iteration       20/   42000 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 4241.2 | learning rate: 3.320E-07 | global batch size:   128 | lm loss: 1.049183E+01 | loss scale: 2048.0 | grad norm: 8.587 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.180 | tokens per gpu per second (tgs): 1931.510 | TFLOPs: 15.54 |
g0171: [2024-08-12 12:20:48,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[5.068117333333334e-07, 5.068117333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 30 loss: 10.1846 iter time (s): 4.003 samples/sec: 31.979
g0178:  iteration       30/   42000 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 4035.4 | learning rate: 5.068E-07 | global batch size:   128 | lm loss: 1.031337E+01 | loss scale: 2048.0 | grad norm: 11.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.719 | tokens per gpu per second (tgs): 2030.015 | TFLOPs: 16.34 |
g0171: [2024-08-12 12:21:29,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[6.815744000000001e-07, 6.815744000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 40 loss: 9.7851 iter time (s): 4.076 samples/sec: 31.407
g0178:  iteration       40/   42000 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 4108.2 | learning rate: 6.816E-07 | global batch size:   128 | lm loss: 9.960386E+00 | loss scale: 2048.0 | grad norm: 7.431 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.157 | tokens per gpu per second (tgs): 1994.080 | TFLOPs: 16.05 |
g0171: [2024-08-12 12:22:10,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[8.563370666666667e-07, 8.563370666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 50 loss: 9.4983 iter time (s): 4.085 samples/sec: 31.333
g0178:  iteration       50/   42000 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 4118.1 | learning rate: 8.563E-07 | global batch size:   128 | lm loss: 9.609592E+00 | loss scale: 2048.0 | grad norm: 3.943 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.082 | tokens per gpu per second (tgs): 1989.269 | TFLOPs: 16.01 |
g0171: [2024-08-12 12:22:50,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0310997333333332e-06, 1.0310997333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 60 loss: 9.3292 iter time (s): 3.986 samples/sec: 32.116
g0178:  iteration       60/   42000 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 4018.5 | learning rate: 1.031E-06 | global batch size:   128 | lm loss: 9.393959E+00 | loss scale: 2048.0 | grad norm: 2.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.853 | tokens per gpu per second (tgs): 2038.580 | TFLOPs: 16.40 |
g0171: [2024-08-12 12:23:31,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.2058624000000002e-06, 1.2058624000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 70 loss: 9.2416 iter time (s): 4.059 samples/sec: 31.538
g0178:  iteration       70/   42000 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 4091.0 | learning rate: 1.206E-06 | global batch size:   128 | lm loss: 9.271092E+00 | loss scale: 2048.0 | grad norm: 2.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.288 | tokens per gpu per second (tgs): 2002.421 | TFLOPs: 16.11 |
g0171: [2024-08-12 12:24:14,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.3806250666666669e-06, 1.3806250666666669e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0171: steps: 80 loss: 9.1528 iter time (s): 4.281 samples/sec: 29.900
g0178:  iteration       80/   42000 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 4314.5 | learning rate: 1.381E-06 | global batch size:   128 | lm loss: 9.186479E+00 | loss scale: 2048.0 | grad norm: 1.999 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.667 | tokens per gpu per second (tgs): 1898.697 | TFLOPs: 15.28 |
