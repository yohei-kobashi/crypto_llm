
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0184
    HostName g0184
    Port 2222
    StrictHostKeyChecking no

Host g0185
    HostName g0185
    Port 2222
    StrictHostKeyChecking no

Host g0187
    HostName g0187
    Port 2222
    StrictHostKeyChecking no

Host g0188
    HostName g0188
    Port 2222
    StrictHostKeyChecking no

Host g0194
    HostName g0194
    Port 2222
    StrictHostKeyChecking no

Host g0195
    HostName g0195
    Port 2222
    StrictHostKeyChecking no

Host g0197
    HostName g0197
    Port 2222
    StrictHostKeyChecking no

Host g0198
    HostName g0198
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42825011
g0184 slots=4
g0185 slots=4
g0187 slots=4
g0188 slots=4
g0194 slots=4
g0195 slots=4
g0197 slots=4
g0198 slots=4

[2024-08-10 08:03:49,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-10 08:03:55,721] [INFO] [runner.py:463:main] Using IP address of 10.1.6.14 for node g0184
[2024-08-10 08:03:55,723] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0184,g0185,g0187,g0188,g0194,g0195,g0197,g0198
[2024-08-10 08:03:55,723] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0184,g0185,g0187,g0188,g0194,g0195,g0197,g0198 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDE4NCI6IFswLCAxLCAyLCAzXSwgImcwMTg1IjogWzAsIDEsIDIsIDNdLCAiZzAxODciOiBbMCwgMSwgMiwgM10sICJnMDE4OCI6IFswLCAxLCAyLCAzXSwgImcwMTk0IjogWzAsIDEsIDIsIDNdLCAiZzAxOTUiOiBbMCwgMSwgMiwgM10sICJnMDE5NyI6IFswLCAxLCAyLCAzXSwgImcwMTk4IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.6.14 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True' --wandb_tag 'other_gpu'
g0184: [2024-08-10 08:03:59,250] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-10 08:04:01,514] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0184: [2024-08-10 08:04:01,514] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0184: [2024-08-10 08:04:01,514] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0184: [2024-08-10 08:04:01,514] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0184: [2024-08-10 08:04:01,514] [INFO] [launch.py:163:main] dist_world_size=32
g0184: [2024-08-10 08:04:01,514] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0184: [2024-08-10 08:04:04,647] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-10 08:04:04,648] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-10 08:04:04,707] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-10 08:04:04,708] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-10 08:04:06,946] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-10 08:04:06,953] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-10 08:04:07,016] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-10 08:04:07,071] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-10 08:04:07,113] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-10 08:04:07,139] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-10 08:04:07,279] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: --------------------------------------------------
g0184: DeepSpeed C++/CUDA extension op report
g0184: ----------------------------------------------------------------------------------------------------
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: 
g0184: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0184: 
g0184: JIT compiled ops requires ninja--------------------------------------------------
g0184: 
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: --------------------------------------------------
g0184: JIT compiled ops requires ninja
g0184: --------------------------------------------------
g0184: DeepSpeed C++/CUDA extension op report
g0184: --------------------------------------------------
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: --------------------------------------------------
g0184: JIT compiled ops requires ninja
g0184: --------------------------------------------------
g0184: DeepSpeed C++/CUDA extension op report
g0184: --------------------------------------------------
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: --------------------------------------------------
g0184: JIT compiled ops requires ninja
g0184: ninjaninjaninja  ninja....................................    ....................................[92m[OKAY][0m[92m[OKAY][0m  
g0184: 
g0184: [92m[OKAY][0m[92m[OKAY][0m
g0184: 
g0184: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0184: 
g0184: --------------------------------------------------
g0184: op name
g0184: op nameop name   op name................................................    ................installedinstalledinstalled    installed......    compatible..compatiblecompatible
g0184:  
g0184: 
g0184: compatible----------------------------------------------------------------------------------------------------
g0184: --------------------------------------------------
g0184: 
g0184: 
g0184: --------------------------------------------------
g0184: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_adam async_io.............  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0184:  [92m[OKAY][0m
g0184: cpu_adam ............... [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0184: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0184: ............ [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0184: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0184:  [92m[YES][0m cpu_adagrad......  ............[92m[OKAY][0m 
g0184: [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_lion
g0184:  ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0184: ....... [93m[NO][0m
g0184: fused_lamb .............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0184: [92m[YES][0m ......evoformer_attn  [92m[OKAY][0m.........
g0184:  [93m[NO][0m ....... [93m[NO][0m
g0184: fused_lamb fused_lion.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0184: [92m[OKAY][0m
g0184: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0184: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0184: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0184: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0184: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0184: 
g0184: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0184: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ quantizer[92m[YES][0m  ....................  [92m[YES][0m[92m[OKAY][0m 
g0184: ...... [92m[OKAY][0m
g0184: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: ragged_device_opsragged_device_ops  ............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0184: 
g0184: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0184: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0184: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0184: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0184: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0184: using torch.float32 for parameters ...
g0184: ------------------------ arguments ------------------------
g0184:   accumulate_allreduce_grads_in_fp32 .............. False
g0184:   adam_beta1 ...................................... 0.9
g0184:   adam_beta2 ...................................... 0.95
g0184:   adam_eps ........................................ 1e-08
g0184:   add_bias_linear ................................. False
g0184:   add_position_embedding .......................... False
g0184:   adlr_autoresume ................................. False
g0184:   adlr_autoresume_interval ........................ 1000
g0184:   aml_data_download_path .......................... None
g0184:   apply_layernorm_1p .............................. False
g0184:   apply_query_key_layer_scaling ................... False
g0184:   apply_residual_connection_post_layernorm ........ False
g0184:   async_tensor_model_parallel_allreduce ........... False
g0184:   attention_dropout ............................... 0.0
g0184:   attention_softmax_in_fp32 ....................... False
g0184:   barrier_with_L1_time ............................ True
g0184:   bert_binary_head ................................ True
g0184:   bert_embedder_type .............................. megatron
g0184:   bert_load ....................................... None
g0184:   bf16 ............................................ False
g0184:   bias_dropout_fusion ............................. True
g0184:   bias_gelu_fusion ................................ False
g0184:   biencoder_projection_dim ........................ 0
g0184:   biencoder_shared_query_context_model ............ False
g0184:   block_data_path ................................. None
g0184:   checkpoint_activations .......................... False
g0184:   checkpoint_in_cpu ............................... False
g0184:   checkpoint_num_layers ........................... 1
g0184:   classes_fraction ................................ 1.0
g0184:   clip_grad ....................................... 1.0
g0184:   compression_training ............................ False
g0184:   consumed_train_samples .......................... 0
g0184:   consumed_train_tokens ........................... 0
g0184:   consumed_valid_samples .......................... 0
g0184:   contigious_checkpointing ........................ False
g0184:   cpu_optimizer ................................... False
g0184:   cpu_torch_adam .................................. False
g0184:   create_moe_param_group .......................... False
g0184:   curriculum_learning_legacy ...................... False
g0184:   data_cache_path ................................. None
g0184:   data_efficiency_curriculum_learning ............. False
g0184:   data_impl ....................................... mmap
g0184:   data_parallel_random_init ....................... False
g0184:   data_parallel_size .............................. 4
g0184:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document']
g0184:   data_per_class_fraction ......................... 1.0
g0184:   data_sharding ................................... True
g0184:   dataloader_type ................................. single
g0184:   DDP_impl ........................................ local
g0184:   decoder_num_layers .............................. None
g0184:   decoder_seq_length .............................. None
g0184:   deepscale ....................................... False
g0184:   deepscale_config ................................ None
g0184:   deepspeed ....................................... True
g0184:   deepspeed_activation_checkpointing .............. False
g0184:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0184:   deepspeed_mpi ................................... False
g0184:   dino_bottleneck_size ............................ 256
g0184:   dino_freeze_last_layer .......................... 1
g0184:   dino_head_hidden_size ........................... 2048
g0184:   dino_local_crops_number ......................... 10
g0184:   dino_local_img_size ............................. 96
g0184:   dino_norm_last_layer ............................ False
g0184:   dino_teacher_temp ............................... 0.07
g0184:   dino_warmup_teacher_temp ........................ 0.04
g0184:   dino_warmup_teacher_temp_epochs ................. 30
g0184:   distribute_checkpointed_activations ............. False
g0184:   distribute_saved_activations .................... False
g0184:   distributed_backend ............................. nccl
g0184:   distributed_timeout_minutes ..................... 10
g0184:   ds_fused_adam ................................... False
g0184:   ds_inference .................................... False
g0184:   ds_pipeline_enabled ............................. True
g0184:   ds_sequence_parallel_size ....................... 1
g0184:   embedding_path .................................. None
g0184:   embedding_weights_in_fp32 ....................... False
g0184:   empty_unused_memory_level ....................... 0
g0184:   enable_expert_tensor_parallelism ................ False
g0184:   encoder_num_layers .............................. 22
g0184:   encoder_seq_length .............................. 2048
g0184:   end_weight_decay ................................ 0.1
g0184:   eod_mask_loss ................................... False
g0184:   eval_interval ................................... 1000
g0184:   eval_iters ...................................... 100
g0184:   evidence_data_path .............................. None
g0184:   exit_duration_in_mins ........................... 30000000
g0184:   exit_interval ................................... None
g0184:   exit_on_missing_checkpoint ...................... False
g0184:   exit_signal_handler ............................. False
g0184:   expert_interval ................................. 2
g0184:   ffn_hidden_size ................................. 5632
g0184:   finetune ........................................ False
g0184:   force_ds_sequence_parallel ...................... False
g0184:   fp16 ............................................ False
g0184:   fp16_lm_cross_entropy ........................... False
g0184:   fp32_residual_connection ........................ False
g0184:   fp8_amax_compute_algo ........................... most_recent
g0184:   fp8_amax_history_len ............................ 1
g0184:   fp8_e4m3 ........................................ False
g0184:   fp8_hybrid ...................................... False
g0184:   fp8_interval .................................... 1
g0184:   fp8_margin ...................................... 0
g0184:   fp8_wgrad ....................................... True
g0184:   global_batch_size ............................... 128
g0184:   gradient_accumulation_fusion .................... True
g0184:   head_lr_mult .................................... 1.0
g0184:   hidden_dropout .................................. 0.0
g0184:   hidden_size ..................................... 2048
g0184:   hidden_size_teacher ............................. None
g0184:   hysteresis ...................................... 2
g0184:   ict_head_size ................................... None
g0184:   ict_load ........................................ None
g0184:   img_h ........................................... 224
g0184:   img_w ........................................... 224
g0184:   indexer_batch_size .............................. 128
g0184:   indexer_log_interval ............................ 1000
g0184:   inference ....................................... False
g0184:   inference_batch_times_seqlen_threshold .......... 512
g0184:   init_method_std ................................. 0.013
g0184:   init_method_xavier_uniform ...................... False
g0184:   initial_loss_scale .............................. 4294967296
g0184:   iter_per_epoch .................................. 1250
g0184:   kd .............................................. False
g0184:   kd_alpha_ce ..................................... 1
g0184:   kd_beta_ce ...................................... 1
g0184:   kd_temp ......................................... 1.0
g0184:   kv_channels ..................................... 128
g0184:   layernorm_epsilon ............................... 1e-05
g0184:   lazy_mpu_init ................................... None
g0184:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184:   load_teacher .................................... None
g0184:   local_rank ...................................... 0
g0184:   log_batch_size_to_tensorboard ................... True
g0184:   log_interval .................................... 10
g0184:   log_learning_rate_to_tensorboard ................ True
g0184:   log_loss_scale_to_tensorboard ................... True
g0184:   log_memory_to_tensorboard ....................... False
g0184:   log_num_zeros_in_grad ........................... False
g0184:   log_optimizer_states_to_tensorboard ............. True
g0184:   log_params_norm ................................. False
g0184:   log_timers_to_tensorboard ....................... True
g0184:   log_validation_ppl_to_tensorboard ............... True
g0184:   log_world_size_to_tensorboard ................... False
g0184:   loss_scale ...................................... None
g0184:   loss_scale_window ............................... 1000
g0184:   lr .............................................. 0.0002
g0184:   lr_decay_iters .................................. None
g0184:   lr_decay_samples ................................ None
g0184:   lr_decay_style .................................. cosine
g0184:   lr_decay_tokens ................................. 300000000000
g0184:   lr_warmup_fraction .............................. None
g0184:   lr_warmup_iters ................................. 0
g0184:   lr_warmup_samples ............................... 0
g0184:   lr_warmup_tokens ................................ 3000000000
g0184:   make_vocab_size_divisible_by .................... 128
g0184:   mask_factor ..................................... 1.0
g0184:   mask_prob ....................................... 0.15
g0184:   mask_type ....................................... random
g0184:   masked_softmax_fusion ........................... True
g0184:   max_position_embeddings ......................... 2048
g0184:   max_tokens_to_oom ............................... 12000
g0184:   mem_efficient_ln ................................ True
g0184:   memory_centric_tiled_linear ..................... False
g0184:   merge_file ...................................... None
g0184:   micro_batch_size ................................ 1
g0184:   min_loss_scale .................................. 1.0
g0184:   min_lr .......................................... 1e-05
g0184:   mlp_type ........................................ standard
g0184:   mmap_warmup ..................................... False
g0184:   moe_eval_capacity_factor ........................ 1.0
g0184:   moe_expert_parallel_size ........................ 1
g0184:   moe_loss_coeff .................................. 0.1
g0184:   moe_min_capacity ................................ 4
g0184:   moe_token_dropping .............................. True
g0184:   moe_train_capacity_factor ....................... 1.0
g0184:   mos ............................................. False
g0184:   no_load_lr_state ................................ False
g0184:   no_load_optim ................................... None
g0184:   no_load_rng ..................................... None
g0184:   no_persist_layer_norm ........................... False
g0184:   no_pipeline_parallel ............................ False
g0184:   no_save_optim ................................... None
g0184:   no_save_rng ..................................... None
g0184:   normalization ................................... rmsnorm
g0184:   num_attention_heads ............................. 16
g0184:   num_attention_heads_teacher ..................... None
g0184:   num_channels .................................... 3
g0184:   num_classes ..................................... 1000
g0184:   num_experts ..................................... [1]
g0184:   num_experts_switch .............................. None
g0184:   num_experts_teacher ............................. [1]
g0184:   num_key_value_heads ............................. 4
g0184:   num_layers ...................................... 22
g0184:   num_layers_per_virtual_pipeline_stage ........... None
g0184:   num_layers_teacher .............................. None
g0184:   num_workers ..................................... 0
g0184:   onnx_safe ....................................... None
g0184:   openai_gelu ..................................... False
g0184:   optimizer ....................................... adam
g0184:   output_bert_embeddings .......................... False
g0184:   overlap_p2p_comm ................................ False
g0184:   override_opt_param_scheduler .................... True
g0184:   params_dtype .................................... torch.float32
g0184:   partition_activations ........................... False
g0184:   patch_dim ....................................... 16
g0184:   perform_initialization .......................... True
g0184:   pipeline_model_parallel_size .................... 8
g0184:   pipeline_model_parallel_split_rank .............. None
g0184:   profile_backward ................................ False
g0184:   query_in_block_prob ............................. 0.1
g0184:   rampup_batch_size ............................... None
g0184:   random_ltd ...................................... False
g0184:   rank ............................................ 0
g0184:   recompute_granularity ........................... None
g0184:   recompute_method ................................ None
g0184:   recompute_num_layers ............................ 1
g0184:   remote_device ................................... none
g0184:   repeated_dataloader ............................. False
g0184:   reset_attention_mask ............................ False
g0184:   reset_iteration ................................. False
g0184:   reset_position_ids .............................. False
g0184:   retriever_report_topk_accuracies ................ []
g0184:   retriever_score_scaling ......................... False
g0184:   retriever_seq_length ............................ 256
g0184:   retro_add_retriever ............................. False
g0184:   retro_cyclic_train_iters ........................ None
g0184:   retro_encoder_attention_dropout ................. 0.1
g0184:   retro_encoder_hidden_dropout .................... 0.1
g0184:   retro_encoder_layers ............................ 2
g0184:   retro_num_neighbors ............................. 2
g0184:   retro_num_retrieved_chunks ...................... 2
g0184:   retro_return_doc_ids ............................ False
g0184:   retro_workdir ................................... None
g0184:   return_data_index ............................... False
g0184:   rotary_percent .................................. 1.0
g0184:   sample_rate ..................................... 1.0
g0184:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184:   save_interval ................................... 1000
g0184:   scatter_gather_tensors_in_pipeline .............. True
g0184:   scattered_embeddings ............................ False
g0184:   seed ............................................ 1234
g0184:   seq_length ...................................... 2048
g0184:   sequence_parallel ............................... False
g0184:   sgd_momentum .................................... 0.9
g0184:   short_seq_prob .................................. 0.1
g0184:   skip_train ...................................... False
g0184:   split ........................................... 949,50,1
g0184:   split_transformers .............................. False
g0184:   squared_relu .................................... False
g0184:   standalone_embedding_stage ...................... False
g0184:   start_weight_decay .............................. 0.1
g0184:   swiglu .......................................... True
g0184:   swin_backbone_type .............................. tiny
g0184:   synchronize_each_layer .......................... False
g0184:   tensor_model_parallel_size ...................... 1
g0184:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True
g0184:   tensorboard_log_interval ........................ 1
g0184:   tensorboard_queue_size .......................... 1
g0184:   test_data_path .................................. None
g0184:   tf32 ............................................ False
g0184:   tile_factor ..................................... 1
g0184:   timing_log_level ................................ 0
g0184:   timing_log_option ............................... minmax
g0184:   titles_data_path ................................ None
g0184:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
g0184:   tokenizer_type .................................. SentencePieceTokenizer
g0184:   topk ............................................ 1
g0184:   train_data_exact_num_epochs ..................... 1
g0184:   train_data_path ................................. None
g0184:   train_desc_path ................................. None
g0184:   train_doc_idx_path .............................. None
g0184:   train_idx_path .................................. None
g0184:   train_iters ..................................... None
g0184:   train_sample_idx_path ........................... None
g0184:   train_samples ................................... 1280000000
g0184:   train_shuffle_idx_path .......................... None
g0184:   train_tokens .................................... 2621440000000
g0184:   transformer_impl ................................ local
g0184:   transformer_pipeline_model_parallel_size ........ 8
g0184:   universal_checkpoint ............................ False
g0184:   untie_embeddings_and_output_weights ............. True
g0184:   use_checkpoint_args ............................. False
g0184:   use_checkpoint_opt_param_scheduler .............. False
g0184:   use_contiguous_buffers_in_local_ddp ............. True
g0184:   use_cpu_initialization .......................... None
g0184:   use_dataset_only ................................ False
g0184:   use_distributed_optimizer ....................... False
g0184:   use_flash_attn .................................. False
g0184:   use_flash_attn_triton ........................... False
g0184:   use_flash_attn_v1 ............................... False
g0184:   use_flash_attn_v2 ............................... False
g0184:   use_one_sent_docs ............................... False
g0184:   use_pin_memory .................................. False
g0184:   use_ring_exchange_p2p ........................... False
g0184:   use_rotary_position_embeddings .................. True
g0184:   use_tutel ....................................... False
g0184:   use_wandb ....................................... True
g0184:   valid_data_path ................................. None
g0184:   variable_seq_lengths ............................ False
g0184:   virtual_pipeline_model_parallel_size ............ None
g0184:   vision_backbone_type ............................ vit
g0184:   vision_pretraining .............................. False
g0184:   vision_pretraining_type ......................... classify
g0184:   vocab_extra_ids ................................. 0
g0184:   vocab_file ...................................... None
g0184:   vocab_size ...................................... None
g0184:   wandb_entity .................................... yohei-kobashi
g0184:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True
g0184:   wandb_project ................................... encrypted_data_LLM
g0184:   wandb_tag ....................................... other_gpu
g0184:   weight_decay .................................... 0.1
g0184:   weight_decay_incr_style ......................... constant
g0184:   world_size ...................................... 32
g0184:   zero_allgather_bucket_size ...................... 0.0
g0184:   zero_contigious_gradients ....................... False
g0184:   zero_reduce_bucket_size ......................... 0.0
g0184:   zero_reduce_scatter ............................. False
g0184:   zero_stage ...................................... 0
g0184: -------------------- end of arguments ---------------------
g0184: setting number of micro-batches to constant 32
g0184: > building SentencePieceTokenizer tokenizer ...
g0184: [2024-08-10 08:04:09,525] [INFO] [comm.py:637:init_distributed] cdb=None
g0184: [2024-08-10 08:04:09,527] [INFO] [comm.py:637:init_distributed] cdb=None
g0184:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0184: > initializing torch distributed ...
g0184: [2024-08-10 08:04:09,528] [INFO] [comm.py:637:init_distributed] cdb=None
g0184: [2024-08-10 08:04:09,528] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0184: [2024-08-10 08:04:09,529] [INFO] [comm.py:637:init_distributed] cdb=None
g0184: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [2024-08-10 08:04:10,857] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0198: [2024-08-10 08:04:10,857] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0198: [2024-08-10 08:04:10,857] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0198: [2024-08-10 08:04:10,857] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0198: [2024-08-10 08:04:10,857] [INFO] [launch.py:163:main] dist_world_size=32
g0198: [2024-08-10 08:04:10,857] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0195: [2024-08-10 08:04:10,901] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0195: [2024-08-10 08:04:10,901] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0195: [2024-08-10 08:04:10,901] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0195: [2024-08-10 08:04:10,901] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0195: [2024-08-10 08:04:10,901] [INFO] [launch.py:163:main] dist_world_size=32
g0195: [2024-08-10 08:04:10,901] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0194: [2024-08-10 08:04:11,009] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0194: [2024-08-10 08:04:11,009] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0194: [2024-08-10 08:04:11,009] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0194: [2024-08-10 08:04:11,009] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0194: [2024-08-10 08:04:11,009] [INFO] [launch.py:163:main] dist_world_size=32
g0194: [2024-08-10 08:04:11,009] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0187: [2024-08-10 08:04:11,022] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0187: [2024-08-10 08:04:11,022] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0187: [2024-08-10 08:04:11,022] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0187: [2024-08-10 08:04:11,022] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0187: [2024-08-10 08:04:11,022] [INFO] [launch.py:163:main] dist_world_size=32
g0187: [2024-08-10 08:04:11,022] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0185: [2024-08-10 08:04:11,024] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0185: [2024-08-10 08:04:11,024] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0185: [2024-08-10 08:04:11,024] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0185: [2024-08-10 08:04:11,024] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0185: [2024-08-10 08:04:11,024] [INFO] [launch.py:163:main] dist_world_size=32
g0185: [2024-08-10 08:04:11,024] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0197: [2024-08-10 08:04:11,133] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0197: [2024-08-10 08:04:11,134] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0197: [2024-08-10 08:04:11,134] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0197: [2024-08-10 08:04:11,134] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0197: [2024-08-10 08:04:11,134] [INFO] [launch.py:163:main] dist_world_size=32
g0197: [2024-08-10 08:04:11,134] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0188: [2024-08-10 08:04:11,436] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0188: [2024-08-10 08:04:11,436] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0188: [2024-08-10 08:04:11,436] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0188: [2024-08-10 08:04:11,436] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0188: [2024-08-10 08:04:11,436] [INFO] [launch.py:163:main] dist_world_size=32
g0188: [2024-08-10 08:04:11,436] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0198: [2024-08-10 08:04:14,041] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-10 08:04:14,062] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-10 08:04:14,062] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-10 08:04:14,067] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-10 08:04:14,067] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-10 08:04:14,067] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-10 08:04:14,067] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-10 08:04:14,067] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-10 08:04:14,114] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-10 08:04:14,114] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-10 08:04:14,126] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-10 08:04:14,134] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-10 08:04:14,135] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-10 08:04:14,155] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-10 08:04:14,159] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-10 08:04:14,178] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-10 08:04:14,197] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-10 08:04:14,198] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-10 08:04:14,208] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-10 08:04:14,208] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-10 08:04:14,213] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-10 08:04:14,262] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-10 08:04:14,264] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-10 08:04:14,355] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-10 08:04:14,562] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-10 08:04:14,562] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-10 08:04:14,573] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-10 08:04:14,837] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: ----------------------------------------------------------------------------------------------------
g0187: 
g0187: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0187: 
g0187: ----------------------------------------------------------------------------------------------------
g0187: 
g0187: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.
g0187: 
g0187: ----------------------------------------------------------------------------------------------------
g0187: 
g0187: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0187: 
g0187: --------------------------------------------------
g0187: DeepSpeed C++/CUDA extension op report
g0187: --------------------------------------------------
g0187: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.
g0187: --------------------------------------------------
g0187: JIT compiled ops requires ninja
g0187: --------------------------------------------------
g0187: DeepSpeed C++/CUDA extension op report
g0187: --------------------------------------------------
g0187: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.
g0187: --------------------------------------------------
g0187: JIT compiled ops requires ninja
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0187: ninjaninjaninja  ninja.................. ..................   ..................[92m[OKAY][0m..................[92m[OKAY][0m 
g0187:  
g0187: [92m[OKAY][0m[92m[OKAY][0m
g0187: --------------------------------------------------
g0187: --------------------------------------------------
g0187: 
g0187: ----------------------------------------------------------------------------------------------------
g0187: 
g0187: op nameop name op nameop name ................  ................ ................................ installed  installed installedinstalled ..  ......    compatiblecompatiblecompatiblecompatible
g0187: 
g0187: 
g0187: 
g0187: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0187: 
g0187: 
g0187: 
g0194: ninjaninjaninja  .................. .................. ..................ninja [92m[OKAY][0m [92m[OKAY][0m
g0194: [92m[OKAY][0m 
g0194: 
g0194: ..................---------------------------------------------------------------------------------------------------- 
g0194: --------------------------------------------------
g0194: [92m[OKAY][0m
g0194: op name
g0194: op name op name -------------------------------------------------- ................................
g0194: ................   installedop nameinstalledinstalled    ......................    compatibleinstalledcompatiblecompatible
g0194:  
g0194: 
g0194: ..------------------------------------------------------------------------------------------------------------------------------------------------------ 
g0194: 
g0194: 
g0194: compatible
g0194: --------------------------------------------------
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0195: ninjaninja ninja ..................ninja..................   [92m[OKAY][0m.................. [92m[OKAY][0m
g0195:  ..................
g0195: [92m[OKAY][0m --------------------------------------------------
g0195: [92m[OKAY][0m--------------------------------------------------
g0195: 
g0195: 
g0195: --------------------------------------------------op name
g0195: -------------------------------------------------- op name
g0195: op name................  op name ................................ installed  ................ installedinstalled ..  installed .... compatible  ..
g0195: compatible compatible
g0195: --------------------------------------------------compatible
g0195: 
g0195: --------------------------------------------------
g0195: --------------------------------------------------
g0195: 
g0195: --------------------------------------------------
g0198: --------------------------------------------------
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: --------------------------------------------------
g0198: JIT compiled ops requires ninja
g0198: --------------------------------------------------
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: --------------------------------------------------
g0198: JIT compiled ops requires ninja
g0198: --------------------------------------------------
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: --------------------------------------------------
g0198: JIT compiled ops requires ninja
g0198: --------------------------------------------------
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: --------------------------------------------------
g0198: JIT compiled ops requires ninja
g0185: --------------------------------------------------
g0185: DeepSpeed C++/CUDA extension op report
g0185: --------------------------------------------------
g0185: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.
g0185: --------------------------------------------------
g0185: JIT compiled ops requires ninja
g0185: --------------------------------------------------
g0185: DeepSpeed C++/CUDA extension op report
g0185: --------------------------------------------------
g0185: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.
g0185: --------------------------------------------------
g0185: JIT compiled ops requires ninja--------------------------------------------------
g0185: 
g0185: DeepSpeed C++/CUDA extension op report
g0185: --------------------------------------------------
g0185: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.
g0185: --------------------------------------------------
g0185: JIT compiled ops requires ninja
g0185: --------------------------------------------------
g0185: DeepSpeed C++/CUDA extension op report
g0185: --------------------------------------------------
g0185: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.
g0185: --------------------------------------------------
g0185: JIT compiled ops requires ninja
g0197: --------------------------------------------------
g0197: DeepSpeed C++/CUDA extension op report
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.
g0197: --------------------------------------------------
g0197: JIT compiled ops requires ninja
g0197: --------------------------------------------------
g0197: DeepSpeed C++/CUDA extension op report
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.
g0197: --------------------------------------------------
g0197: --------------------------------------------------JIT compiled ops requires ninja
g0197: 
g0197: DeepSpeed C++/CUDA extension op report
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.
g0197: --------------------------------------------------
g0197: JIT compiled ops requires ninja
g0197: --------------------------------------------------
g0197: DeepSpeed C++/CUDA extension op report
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.
g0197: --------------------------------------------------
g0197: JIT compiled ops requires ninja
g0198: ninjaninja ..................  ..................[92m[OKAY][0m 
g0198: [92m[OKAY][0m
g0198: ----------------------------------------------------------------------------------------------------
g0198: 
g0198: op nameop name  ................................  installedinstalled  ....ninja  compatible compatible
g0198: ..................
g0198:  ----------------------------------------------------------------------------------------------------[92m[OKAY][0m
g0198: 
g0198: 
g0198: --------------------------------------------------
g0198: ninjaop name  ..................................  installed[92m[OKAY][0m 
g0198: .. compatible--------------------------------------------------
g0198: 
g0198: --------------------------------------------------op name
g0198:  ................ installed .. compatible
g0198: --------------------------------------------------
g0185: ninjaninja ninja ....................................   [92m[OKAY][0m..................[92m[OKAY][0m
g0185:  ninja
g0185: [92m[OKAY][0m-------------------------------------------------- 
g0185: --------------------------------------------------
g0185: ..................
g0185:  --------------------------------------------------op nameop name
g0185: [92m[OKAY][0m  
g0185: ................................op name   --------------------------------------------------installedinstalled................
g0185:    ....op nameinstalled    compatiblecompatible..................
g0185:  
g0185:  installed--------------------------------------------------compatible --------------------------------------------------
g0185: 
g0185: ..
g0185:  --------------------------------------------------compatible
g0185: 
g0185: --------------------------------------------------
g0197: ninjaninjaninja ninja  ......................................................    ..................[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m 
g0197: 
g0197: 
g0197: [92m[OKAY][0m
g0197: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0197: 
g0197: 
g0197: --------------------------------------------------op name
g0197: op nameop name   op name................................................    ................installedinstalledinstalled    installed......    ..compatiblecompatiblecompatible 
g0197: 
g0197: 
g0197: compatible--------------------------------------------------
g0197: --------------------------------------------------
g0197: --------------------------------------------------
g0197: --------------------------------------------------
g0197: 
g0187: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0187: async_io fused_adam...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m [92m[OKAY][0m
g0187: 
g0187: cpu_adam ...............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0187:  [92m[OKAY][0m
g0187: cpu_adagrad ............ cpu_adam[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0187: ...... [92m[OKAY][0m
g0187: cpu_lion ............... cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0187: ...... [92m[OKAY][0m
g0187: cpu_lion [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0187:  [92m[YES][0m evoformer_attn......  .........[92m[OKAY][0m 
g0187: [93m[NO][0m ....... [93m[NO][0m
g0187: fused_lamb[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0187: ............. evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0187: ....... [93m[NO][0m
g0187: fused_lamb async_io.............fused_lion   [92m[YES][0m............................   ......[92m[YES][0m[92m[YES][0m   ......[92m[OKAY][0m...... 
g0187:  [92m[OKAY][0m[92m[OKAY][0m
g0187: 
g0187: fused_lion ............. [92m[YES][0m fused_adam......  .............[92m[OKAY][0m
g0187:  [92m[YES][0m ...... [92m[OKAY][0m
g0187: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0187: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: cpu_lion ...............async_io [92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0187:  ...... [92m[OKAY][0m
g0187: fused_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0187: ............. [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0187:  ....... [93m[NO][0m
g0187: cpu_adam ............... [92m[YES][0m ......fused_lamb  [92m[OKAY][0m.............
g0187:  [92m[YES][0m cpu_adagrad......  ............[92m[OKAY][0m 
g0187: [92m[YES][0m ...... [92m[OKAY][0m
g0187: cpu_lionfused_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0187: 
g0187: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0187: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0187: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0194: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0194: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_lion .............async_io [92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0194: ...... [92m[OKAY][0m
g0194: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0194: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0194: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: async_io ............... [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0194:  [92m[OKAY][0mevoformer_attn
g0194:  ......... [93m[NO][0m ....... [93m[NO][0m
g0194: fused_adam .............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0194:  [92m[OKAY][0m
g0194: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0mfused_lion
g0194:  ............. [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0194:  ...... [92m[OKAY][0m
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0194: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0194: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0187: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0187: 
g0187: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0194: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0194: inference_core_ops ..... inference_core_ops[92m[YES][0m  ...........  [92m[OKAY][0m[92m[YES][0m 
g0194: ...... [92m[OKAY][0m
g0194: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0187: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... cutlass_ops[92m[OKAY][0m
g0194:  ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0mragged_device_ops
g0194:  ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0187: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0187: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0187: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0187: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0187: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0187: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0195: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0195: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0195: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_adam ............. [92m[YES][0m ...... async_io[92m[OKAY][0m
g0195:  ............... cpu_adam[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0195: ...... [92m[OKAY][0m
g0195: cpu_adagrad ............ [92m[YES][0m ...... fused_adam[92m[OKAY][0m 
g0195: ............. [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0195:  ...... [92m[OKAY][0mcpu_adam
g0195: async_io ...............  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0195: 
g0195:  [92m[OKAY][0mevoformer_attncpu_adagrad
g0195:   .....................  [93m[NO][0m[92m[YES][0m  .......fused_adam......   [93m[NO][0m[92m[OKAY][0m.............
g0195: 
g0195:  [92m[YES][0m fused_lamb......cpu_lion   .............[92m[OKAY][0m............... 
g0195:  [92m[YES][0m[92m[YES][0m  ............ cpu_adam [92m[OKAY][0m 
g0195: ...............[92m[OKAY][0m 
g0195: [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_lioncpu_adagrad  .........................[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH  
g0195: [92m[YES][0m[92m[YES][0m  ......evoformer_attn......   [92m[OKAY][0m[92m[OKAY][0m.........
g0195: 
g0195:  [93m[NO][0m .......cpu_lion  [93m[NO][0m...............
g0195:  [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0195: [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0195: evoformer_attnfused_lion  ......................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0195: 
g0195: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0194: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ ragged_ops[93m[NO][0m  ....................  [93m[NO][0m[92m[YES][0m
g0194:  ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: DeepSpeed general environment info:
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0187: DeepSpeed general environment info:
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0187: DeepSpeed general environment info:
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0187: DeepSpeed general environment info:
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0197: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0197: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0197: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0197: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0197: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0197: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0197: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0197: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0197: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............inference_core_ops   [92m[OKAY][0m[92m[OKAY][0m.....
g0195: 
g0195:  [92m[YES][0m ...... [92m[OKAY][0m
g0195: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0185: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adagrad ............ [92m[YES][0m ......async_io [92m[OKAY][0m 
g0185: ............... [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0185:  ...... [92m[OKAY][0m
g0185: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: cpu_adamevoformer_attn  ........................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0185: 
g0185: fused_lambcpu_adagrad  .........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0185: 
g0185: cpu_lion ............... [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0185: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0185: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0185: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: async_io ............... [92m[YES][0m ...... fused_lion[92m[OKAY][0m .............
g0185:  [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0185:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0185: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cpu_adam ............... [92m[YES][0masync_io ......  ...............[92m[OKAY][0m 
g0198: [92m[YES][0m ......cpu_adagrad  [92m[OKAY][0m............
g0198:  [92m[YES][0m ...... [92m[OKAY][0m
g0198: cpu_lion fused_adam...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0198: [92m[OKAY][0m
g0198: cpu_adam [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0198:  [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0198:  ....... cpu_adagrad[93m[NO][0m 
g0198: ............ fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0198: ...... [92m[OKAY][0mcpu_lion
g0198:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0198: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0198: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_lion ............. [92m[YES][0masync_io ......  ...............[92m[OKAY][0m 
g0198: [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0198: async_io cpu_adagrad...............  ............[92m[YES][0m  [92m[YES][0m......  ...... [92m[OKAY][0m[92m[OKAY][0m
g0198: 
g0198: cpu_lion ...............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0198:  [92m[OKAY][0m
g0198: cpu_adam ...............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0198: [92m[YES][0m evoformer_attn......  .........[92m[OKAY][0m 
g0198: [93m[NO][0m cpu_adagrad.......  ............[93m[NO][0m 
g0198: [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0198: [92m[YES][0m ...... cpu_lion[92m[OKAY][0m 
g0198: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_lion ............. [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH ......
g0198:  [92m[OKAY][0m
g0198: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0198: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0195: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0195: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0195: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0195: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatibleragged_ops
g0195:  .............sparse_attn  [92m[YES][0m............  ......[93m[NO][0m  [92m[OKAY][0m.......
g0195:  [93m[NO][0m
g0195: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0195: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0185: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0185: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0185: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0198: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0198: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0198: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0197: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0197: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0197: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0197: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0197: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0197: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: ragged_ops ............. [92m[YES][0m ragged_ops......  [92m[OKAY][0m.............
g0197:  [92m[YES][0m ......random_ltd  [92m[OKAY][0m.............
g0197:  [92m[YES][0m ......random_ltd  [92m[OKAY][0m.............
g0197:  [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0197: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0197: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0197: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0197: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0198: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0198: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0198: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0198: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0198: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0198: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0198: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0198: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0198: --------------------------------------------------
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0198: --------------------------------------------------
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: --------------------------------------------------
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0198: --------------------------------------------------
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0198: DeepSpeed general environment info:
g0198: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: shared memory (/dev/shm) size .... 188.13 GB
g0198: DeepSpeed general environment info:
g0198: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: shared memory (/dev/shm) size .... 188.13 GB
g0198: DeepSpeed general environment info:
g0198: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: shared memory (/dev/shm) size .... 188.13 GB
g0198: DeepSpeed general environment info:
g0198: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: shared memory (/dev/shm) size .... 188.13 GB
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0187: [2024-08-10 08:04:18,590] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [2024-08-10 08:04:18,591] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [2024-08-10 08:04:18,591] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: [2024-08-10 08:04:18,591] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: [2024-08-10 08:04:18,591] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [2024-08-10 08:04:18,593] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: [2024-08-10 08:04:18,593] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [2024-08-10 08:04:18,599] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [2024-08-10 08:04:18,607] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [2024-08-10 08:04:18,607] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [2024-08-10 08:04:18,608] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [2024-08-10 08:04:18,609] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [2024-08-10 08:04:18,622] [INFO] [comm.py:637:init_distributed] cdb=None
g0185: [2024-08-10 08:04:18,622] [INFO] [comm.py:637:init_distributed] cdb=None
g0185: [2024-08-10 08:04:18,623] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [2024-08-10 08:04:18,625] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [2024-08-10 08:04:18,625] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [2024-08-10 08:04:18,626] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [2024-08-10 08:04:18,626] [INFO] [comm.py:637:init_distributed] cdb=None
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [2024-08-10 08:04:18,634] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [2024-08-10 08:04:18,636] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: [2024-08-10 08:04:18,637] [INFO] [comm.py:637:init_distributed] cdb=None
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [2024-08-10 08:04:18,641] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: ninjaninjaninja   ninja......................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m..................
g0188: 
g0188: 
g0188:  --------------------------------------------------[92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0188: 
g0188: 
g0188: 
g0188: op name--------------------------------------------------op nameop name 
g0188:   ................................................ op name   installedinstalledinstalled................    ......installed    compatiblecompatiblecompatible..
g0188: 
g0188: 
g0188:  --------------------------------------------------compatible----------------------------------------------------------------------------------------------------
g0188: 
g0188: 
g0188: 
g0188: --------------------------------------------------
g0188: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0188: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0188: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0188: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0188: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: async_io ...............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0188:  [92m[OKAY][0m
g0188: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHasync_io
g0188:  evoformer_attn...............  .........[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0188: [93m[NO][0m
g0188: fused_lamb .............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0188:  [92m[OKAY][0m
g0188: cpu_adam ...............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0188:  [92m[OKAY][0m
g0188: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0188: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0188: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0188: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0188: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0188: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: > setting tensorboard ...
g0198: [2024-08-10 08:04:19,085] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [2024-08-10 08:04:19,157] [INFO] [comm.py:637:init_distributed] cdb=None
g0188: [2024-08-10 08:04:19,157] [INFO] [comm.py:637:init_distributed] cdb=None
g0188: [2024-08-10 08:04:19,157] [INFO] [comm.py:637:init_distributed] cdb=None
g0188: [2024-08-10 08:04:19,159] [INFO] [comm.py:637:init_distributed] cdb=None
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: > initialized tensor model parallel with size 1
g0184: > initialized pipeline model parallel with size 8
g0184: > setting random seeds to 1234 ...
g0184: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0184: > compiling dataset index builder ...
g0184: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0184: make: Nothing to be done for 'default'.
g0184: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0184: >>> done with dataset index builder. Compilation time: 0.082 seconds
g0184: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0184: > compiling and loading fused kernels ...
g0184: Detected CUDA files, patching ldflags
g0184: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0184: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0184: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0184: ninja: no work to do.
g0184: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0184: Detected CUDA files, patching ldflags
g0184: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0184: Building extension module scaled_masked_softmax_cuda...
g0184: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0184: ninja: no work to do.
g0184: Loading extension module scaled_masked_softmax_cuda...
g0184: Detected CUDA files, patching ldflags
g0184: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0184: Building extension module scaled_softmax_cuda...
g0184: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0184: ninja: no work to do.
g0184: Loading extension module scaled_softmax_cuda...
g0184: >>> done with compiling and loading fused kernels. Compilation time: 7.801 seconds
g0184: time to initialize megatron (seconds): 22.183
g0184: [after megatron is initialized] datetime: 2024-08-10 08:04:29 
g0195: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0198: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0184: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0188: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0187: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0194: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0197: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0185: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0195: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0195: wandb:  $ pip install wandb --upgrade
g0195: wandb: Tracking run with wandb version 0.17.5
g0195: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-op7t1kxd
g0195: wandb: Run `wandb offline` to turn off syncing.
g0197: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0197: wandb:  $ pip install wandb --upgrade
g0197: wandb: Tracking run with wandb version 0.17.5
g0197: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-ru2bhkb0
g0197: wandb: Run `wandb offline` to turn off syncing.
g0195: wandb: Syncing run g0195.abci.local
g0195: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0195: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/op7t1kxd
g0197: wandb: Syncing run g0197.abci.local
g0197: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0197: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ru2bhkb0
g0184: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0184: wandb:  $ pip install wandb --upgrade
g0184: wandb: Tracking run with wandb version 0.17.5
g0184: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-ls2sen6j
g0184: wandb: Run `wandb offline` to turn off syncing.
g0184: wandb: Syncing run g0184.abci.local
g0184: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0184: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ls2sen6j
g0198: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0198: wandb:  $ pip install wandb --upgrade
g0198: wandb: Tracking run with wandb version 0.17.5
g0198: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-z6m12myl
g0198: wandb: Run `wandb offline` to turn off syncing.
g0198: wandb: Syncing run g0198.abci.local
g0198: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0198: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/z6m12myl
g0188: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0188: wandb:  $ pip install wandb --upgrade
g0188: wandb: Tracking run with wandb version 0.17.5
g0188: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-q3cilbhm
g0188: wandb: Run `wandb offline` to turn off syncing.
g0188: wandb: Syncing run g0188.abci.local
g0188: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0188: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/q3cilbhm
g0185: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0185: wandb:  $ pip install wandb --upgrade
g0185: wandb: Tracking run with wandb version 0.17.5
g0185: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-x9z68ht8
g0185: wandb: Run `wandb offline` to turn off syncing.
g0185: wandb: Syncing run g0185.abci.local
g0185: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0185: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/x9z68ht8
g0194: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0194: wandb:  $ pip install wandb --upgrade
g0194: wandb: Tracking run with wandb version 0.17.5
g0194: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-y2kqcg7b
g0194: wandb: Run `wandb offline` to turn off syncing.
g0187: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0187: wandb:  $ pip install wandb --upgrade
g0187: wandb: Tracking run with wandb version 0.17.5
g0187: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080431-fma5w1fx
g0187: wandb: Run `wandb offline` to turn off syncing.
g0194: wandb: Syncing run g0194.abci.local
g0194: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0194: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/y2kqcg7b
g0187: wandb: Syncing run g0187.abci.local
g0187: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0187: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/fma5w1fx
g0184: building GPT model ...
g0184: [2024-08-10 08:04:32,152] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0184: [2024-08-10 08:04:32,153] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0184: [2024-08-10 08:04:32,153] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 55.34 GB, percent = 14.7%
g0184: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0184: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0184: [2024-08-10 08:04:32,669] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0184: stage=0 layers=5
g0184:      0: _to_float16
g0184:      1: EmbeddingPipe
g0184:      2: ParallelTransformerLayerPipe
g0184:      3: ParallelTransformerLayerPipe
g0184:      4: ParallelTransformerLayerPipe
g0184: stage=1 layers=3
g0184:      5: ParallelTransformerLayerPipe
g0184:      6: ParallelTransformerLayerPipe
g0184:      7: ParallelTransformerLayerPipe
g0184: stage=2 layers=3
g0184:      8: ParallelTransformerLayerPipe
g0184:      9: ParallelTransformerLayerPipe
g0184:     10: ParallelTransformerLayerPipe
g0184: stage=3 layers=3
g0184:     11: ParallelTransformerLayerPipe
g0184:     12: ParallelTransformerLayerPipe
g0184:     13: ParallelTransformerLayerPipe
g0184: stage=4 layers=3
g0184:     14: ParallelTransformerLayerPipe
g0184:     15: ParallelTransformerLayerPipe
g0184:     16: ParallelTransformerLayerPipe
g0184: stage=5 layers=3
g0184:     17: ParallelTransformerLayerPipe
g0184:     18: ParallelTransformerLayerPipe
g0184:     19: ParallelTransformerLayerPipe
g0184: stage=6 layers=3
g0184:     20: ParallelTransformerLayerPipe
g0184:     21: ParallelTransformerLayerPipe
g0184:     22: ParallelTransformerLayerPipe
g0184: stage=7 layers=3
g0184:     23: ParallelTransformerLayerPipe
g0184:     24: MixedFusedRMSNorm
g0184:     25: LMHeadPipe
g0184:   loss: CrossEntropy
g0194:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0185:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0187:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0197:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0198:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0195:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0188:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0184: [2024-08-10 08:04:33,204] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0184: [2024-08-10 08:04:33,205] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0184: [2024-08-10 08:04:33,205] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 55.4 GB, percent = 14.7%
g0184:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0184: setting training iterations to 10000000
g0184: > learning rate decay style: cosine
g0184: DeepSpeed is enabled.
g0184: [2024-08-10 08:04:33,207] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0197: [2024-08-10 08:04:33,226] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0197: [2024-08-10 08:04:33,227] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0197: [2024-08-10 08:04:33,227] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0197: [2024-08-10 08:04:33,229] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-10 08:04:33,242] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-10 08:04:33,242] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-10 08:04:33,242] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-10 08:04:33,242] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-10 08:04:33,247] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-10 08:04:33,248] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-10 08:04:33,251] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-10 08:04:33,251] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-10 08:04:33,251] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-10 08:04:33,251] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-10 08:04:33,255] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-10 08:04:33,255] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-10 08:04:33,255] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-10 08:04:33,255] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-10 08:04:33,256] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-10 08:04:33,257] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0187: [2024-08-10 08:04:33,259] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0187: [2024-08-10 08:04:33,259] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0187: [2024-08-10 08:04:33,260] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0187: [2024-08-10 08:04:33,260] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-10 08:04:33,278] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-10 08:04:33,279] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-10 08:04:33,279] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-10 08:04:33,279] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-10 08:04:33,399] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0184: [2024-08-10 08:04:33,400] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0184: [2024-08-10 08:04:33,400] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0184: [2024-08-10 08:04:33,400] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0184: [2024-08-10 08:04:33,401] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0184: [2024-08-10 08:04:33,435] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-10 08:04:33,435] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0184: [2024-08-10 08:04:33,435] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-10 08:04:33,436] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0184: [2024-08-10 08:04:33,436] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-10 08:04:33,436] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fcb692a8b10>
g0184: [2024-08-10 08:04:33,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: [2024-08-10 08:04:33,436] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0184: [2024-08-10 08:04:33,437] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0184:     "partition_activations": false, 
g0184:     "contiguous_memory_optimization": false, 
g0184:     "cpu_checkpointing": false, 
g0184:     "number_checkpoints": null, 
g0184:     "synchronize_checkpoint_boundary": false, 
g0184:     "profile": false
g0184: }
g0184: [2024-08-10 08:04:33,437] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0184: [2024-08-10 08:04:33,437] [INFO] [config.py:983:print]   amp_enabled .................. False
g0184: [2024-08-10 08:04:33,437] [INFO] [config.py:983:print]   amp_params ................... False
g0184: [2024-08-10 08:04:33,437] [INFO] [config.py:983:print]   autotuning_config ............ {
g0184:     "enabled": false, 
g0184:     "start_step": null, 
g0184:     "end_step": null, 
g0184:     "metric_path": null, 
g0184:     "arg_mappings": null, 
g0184:     "metric": "throughput", 
g0184:     "model_info": null, 
g0184:     "results_dir": "autotuning_results", 
g0184:     "exps_dir": "autotuning_exps", 
g0184:     "overwrite": true, 
g0184:     "fast": true, 
g0184:     "start_profile_step": 3, 
g0184:     "end_profile_step": 5, 
g0184:     "tuner_type": "gridsearch", 
g0184:     "tuner_early_stopping": 5, 
g0184:     "tuner_num_trials": 50, 
g0184:     "model_info_path": null, 
g0184:     "mp_size": 1, 
g0184:     "max_train_batch_size": null, 
g0184:     "min_train_batch_size": 1, 
g0184:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0184:     "min_train_micro_batch_size_per_gpu": 1, 
g0184:     "num_tuning_micro_batch_sizes": 3
g0184: }
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcb241e34d0>
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   communication_data_type ...... None
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0184: [2024-08-10 08:04:33,438] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   disable_allgather ............ False
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   dump_state ................... False
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0184: [2024-08-10 08:04:33,439] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0184: [2024-08-10 08:04:33,440] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0184:     "enabled": false, 
g0184:     "recompute_fwd_factor": 0.0, 
g0184:     "profile_step": 1, 
g0184:     "module_depth": -1, 
g0184:     "top_modules": 1, 
g0184:     "detailed": true, 
g0184:     "output_file": null
g0184: }
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   global_rank .................. 0
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0184: [2024-08-10 08:04:33,441] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   loss_scale ................... 0
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0184: [2024-08-10 08:04:33,442] [INFO] [config.py:983:print]   nebula_config ................ {
g0184:     "enabled": false, 
g0184:     "persistent_storage_path": null, 
g0184:     "persistent_time_interval": 100, 
g0184:     "num_of_version_in_retention": 2, 
g0184:     "enable_nebula_load": true, 
g0184:     "load_path": null
g0184: }
g0184: [2024-08-10 08:04:33,443] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0184: [2024-08-10 08:04:33,443] [INFO] [config.py:983:print]   optimizer_name ............... None
g0184: [2024-08-10 08:04:33,443] [INFO] [config.py:983:print]   optimizer_params ............. None
g0184: [2024-08-10 08:04:33,443] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0184: [2024-08-10 08:04:33,443] [INFO] [config.py:983:print]   pld_enabled .................. False
g0184: [2024-08-10 08:04:33,443] [INFO] [config.py:983:print]   pld_params ................... False
g0184: [2024-08-10 08:04:33,443] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   scheduler_name ............... None
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   scheduler_params ............. None
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   sparse_attention ............. None
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0184: [2024-08-10 08:04:33,444] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   world_size ................... 4
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   zero_enabled ................. False
g0184: [2024-08-10 08:04:33,445] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0184: [2024-08-10 08:04:33,446] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0184: [2024-08-10 08:04:33,446] [INFO] [config.py:969:print_user_config]   json = {
g0184:     "train_batch_size": 128, 
g0184:     "train_micro_batch_size_per_gpu": 1, 
g0184:     "steps_per_print": 10, 
g0184:     "zero_optimization": {
g0184:         "stage": 0
g0184:     }, 
g0184:     "gradient_clipping": 1.0, 
g0184:     "prescale_gradients": true, 
g0184:     "fp16": {
g0184:         "enabled": true, 
g0184:         "loss_scale": 0, 
g0184:         "loss_scale_window": 500, 
g0184:         "hysteresis": 2, 
g0184:         "min_loss_scale": 1, 
g0184:         "initial_scale_power": 11
g0184:     }, 
g0184:     "wall_clock_breakdown": false
g0184: }
g0184: [2024-08-10 08:04:33,446] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0184: [2024-08-10 08:04:33,446] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-10 08:04:34,149] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0194: [2024-08-10 08:04:34,149] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0197: [2024-08-10 08:04:34,150] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0187: [2024-08-10 08:04:34,150] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0185: [2024-08-10 08:04:34,150] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0195: [2024-08-10 08:04:34,150] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0188: [2024-08-10 08:04:34,150] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0198: [2024-08-10 08:04:34,150] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0197: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0197: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0197: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0185: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0185: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0187: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0194: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0187: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0187: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0195: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0195: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0185: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0195: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0185: [2024-08-10 08:04:34,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0195: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0194: [2024-08-10 08:04:34,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0194: [2024-08-10 08:04:34,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0194: [2024-08-10 08:04:34,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0197: [2024-08-10 08:04:34,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0187: [2024-08-10 08:04:34,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 08:04:34,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0188: [2024-08-10 08:04:34,946] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0188: [2024-08-10 08:04:34,946] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0188: [2024-08-10 08:04:34,946] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0188: [2024-08-10 08:04:34,947] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0197: [2024-08-10 08:04:38,101] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0197: [2024-08-10 08:04:38,101] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0197: [2024-08-10 08:04:38,101] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0194: [2024-08-10 08:04:38,102] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0194: [2024-08-10 08:04:38,102] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0195: [2024-08-10 08:04:38,102] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0195: [2024-08-10 08:04:38,102] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0195: [2024-08-10 08:04:38,103] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0195: [2024-08-10 08:04:38,103] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0197: [2024-08-10 08:04:38,104] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0194: [2024-08-10 08:04:38,106] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0198: [2024-08-10 08:04:38,106] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0198: [2024-08-10 08:04:38,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0185: [2024-08-10 08:04:38,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0185: [2024-08-10 08:04:38,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0185: [2024-08-10 08:04:38,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0185: [2024-08-10 08:04:38,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0194: [2024-08-10 08:04:38,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0187: [2024-08-10 08:04:38,108] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0187: [2024-08-10 08:04:38,108] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0197: [2024-08-10 08:04:38,109] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt...
g0187: [2024-08-10 08:04:38,109] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0197: [2024-08-10 08:04:38,110] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt...
g0195: [2024-08-10 08:04:38,110] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt...
g0195: [2024-08-10 08:04:38,110] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt...
g0195: [2024-08-10 08:04:38,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt...
g0195: [2024-08-10 08:04:38,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt...
g0194: [2024-08-10 08:04:38,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt...
g0194: [2024-08-10 08:04:38,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt...
g0197: [2024-08-10 08:04:38,112] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt...
g0187: [2024-08-10 08:04:38,112] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0198: [2024-08-10 08:04:38,113] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0194: [2024-08-10 08:04:38,114] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt...
g0197: [2024-08-10 08:04:38,114] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt...
g0198: [2024-08-10 08:04:38,115] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt...
g0198: [2024-08-10 08:04:38,115] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt...
g0194: [2024-08-10 08:04:38,115] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt...
g0185: [2024-08-10 08:04:38,115] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt...
g0185: [2024-08-10 08:04:38,115] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt...
g0185: [2024-08-10 08:04:38,116] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt...
g0185: [2024-08-10 08:04:38,116] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt...
g0187: [2024-08-10 08:04:38,116] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt...
g0187: [2024-08-10 08:04:38,116] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt...
g0198: [2024-08-10 08:04:38,117] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0187: [2024-08-10 08:04:38,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt...
g0198: [2024-08-10 08:04:38,120] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt...
g0187: [2024-08-10 08:04:38,120] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt...
g0198: [2024-08-10 08:04:38,123] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt...
g0184: [2024-08-10 08:04:38,274] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:38,274] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:38,274] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:38,275] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:38,281] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 08:04:38,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 08:04:38,283] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 08:04:38,283] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0194: [2024-08-10 08:04:40,065] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 08:04:40,066] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 08:04:40,066] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 08:04:40,067] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,067] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,067] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,068] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 08:04:40,068] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0188: [2024-08-10 08:04:40,112] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0188: [2024-08-10 08:04:40,112] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0188: [2024-08-10 08:04:40,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0188: [2024-08-10 08:04:40,112] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0188: [2024-08-10 08:04:40,120] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt...
g0188: [2024-08-10 08:04:40,120] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt...
g0188: [2024-08-10 08:04:40,121] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt...
g0188: [2024-08-10 08:04:40,121] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt...
g0195: [2024-08-10 08:04:40,197] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 08:04:40,198] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 08:04:40,199] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,199] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 08:04:40,199] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,200] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,202] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 08:04:40,203] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,497] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:40,498] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:40,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,499] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,516] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:40,517] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 08:04:40,517] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,518] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,554] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,555] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,555] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,558] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,558] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,558] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,559] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,559] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,559] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,559] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,560] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,588] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,588] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,591] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,591] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,594] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,594] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,596] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,596] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,604] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,606] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,610] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,611] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,612] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,612] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,618] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,618] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0187: [2024-08-10 08:04:40,672] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 08:04:40,672] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 08:04:40,673] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 08:04:40,673] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 08:04:40,674] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0187: [2024-08-10 08:04:40,674] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0187: [2024-08-10 08:04:40,674] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0187: [2024-08-10 08:04:40,674] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,702] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 08:04:40,702] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 08:04:40,702] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 08:04:40,703] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 08:04:40,703] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,703] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,703] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,703] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,746] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 08:04:40,746] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 08:04:40,753] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 08:04:40,753] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0197: [2024-08-10 08:04:40,775] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 08:04:40,776] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 08:04:40,776] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt.
g0184: [2024-08-10 08:04:40,776] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,776] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0197: [2024-08-10 08:04:40,777] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0197: [2024-08-10 08:04:40,777] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 08:04:40,777] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0197: [2024-08-10 08:04:40,777] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0197: [2024-08-10 08:04:40,777] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,782] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0184: [2024-08-10 08:04:40,783] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0185: [2024-08-10 08:04:40,809] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 08:04:40,809] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 08:04:40,809] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 08:04:40,810] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0185: [2024-08-10 08:04:40,810] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0185: [2024-08-10 08:04:40,810] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 08:04:40,810] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0185: [2024-08-10 08:04:40,811] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,861] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,861] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,861] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,861] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,862] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,862] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,862] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,862] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,867] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,867] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,868] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,868] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,868] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,868] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,868] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,868] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,892] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,892] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,893] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,893] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,899] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,899] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,901] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 08:04:40,901] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0194: [2024-08-10 08:04:40,910] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,910] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,914] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0194: [2024-08-10 08:04:40,914] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,917] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,917] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,922] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0195: [2024-08-10 08:04:40,922] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,037] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,037] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,038] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,038] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,038] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,038] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,038] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,040] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,040] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,041] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,041] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,042] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,070] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,070] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,074] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,074] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,075] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,078] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,078] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,078] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,090] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,091] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,091] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,099] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,099] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,099] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0194: [2024-08-10 08:04:41,101] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 08:04:41,101] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 08:04:41,101] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 08:04:41,102] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 08:04:41,102] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0194: [2024-08-10 08:04:41,102] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0194: [2024-08-10 08:04:41,102] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0194: [2024-08-10 08:04:41,102] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,116] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,116] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,116] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,117] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0194: [2024-08-10 08:04:41,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 08:04:41,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,134] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,134] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,134] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0194: [2024-08-10 08:04:41,134] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 08:04:41,134] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,134] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,134] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,149] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,149] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,153] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,153] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,164] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,167] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,168] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,169] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0197: [2024-08-10 08:04:41,170] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,171] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,183] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,191] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0197: [2024-08-10 08:04:41,192] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,386] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,386] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,386] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,387] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,417] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,417] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,419] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,420] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,438] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,438] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,440] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,441] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,511] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,511] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,512] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,512] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,512] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,512] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,512] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,512] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0195: [2024-08-10 08:04:41,524] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,525] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,525] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,525] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0195: [2024-08-10 08:04:41,525] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,525] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0195: [2024-08-10 08:04:41,525] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0195: [2024-08-10 08:04:41,526] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,542] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,542] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,545] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,546] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,558] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 08:04:41,558] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0185: [2024-08-10 08:04:41,558] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,559] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,561] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0185: [2024-08-10 08:04:41,564] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0198: [2024-08-10 08:04:41,769] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 08:04:41,769] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 08:04:41,770] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 08:04:41,770] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 08:04:41,770] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0198: [2024-08-10 08:04:41,770] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0198: [2024-08-10 08:04:41,770] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0198: [2024-08-10 08:04:41,771] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,807] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,807] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,807] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,808] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,808] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,808] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,808] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,808] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 08:04:41,837] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,837] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,842] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 08:04:41,842] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,949] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,949] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,949] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,950] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,950] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,950] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,950] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,950] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0187: [2024-08-10 08:04:41,981] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,981] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,981] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,982] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 08:04:41,996] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,003] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,003] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,003] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,050] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,050] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,050] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,051] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,051] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,051] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,051] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,051] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0185: [2024-08-10 08:04:42,054] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,054] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,054] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,054] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0185: [2024-08-10 08:04:42,054] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,054] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0185: [2024-08-10 08:04:42,054] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0185: [2024-08-10 08:04:42,055] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,085] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,085] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,086] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,087] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,087] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,088] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,088] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 08:04:42,089] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,100] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,106] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,108] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,109] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,112] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,112] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,112] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,112] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,113] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,114] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,182] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 08:04:42,182] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 08:04:42,182] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 08:04:42,182] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 08:04:42,183] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,183] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,183] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,183] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 08:04:42,214] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 08:04:42,214] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 08:04:42,214] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 08:04:42,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,293] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,293] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,293] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,293] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,293] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,293] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,293] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,294] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,324] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,324] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,326] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,326] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,333] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,334] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,334] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,334] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,334] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,340] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,340] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0198: [2024-08-10 08:04:42,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,385] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 08:04:42,385] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,509] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,509] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,510] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,510] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,510] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 08:04:42,540] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,540] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 08:04:42,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0184:  > overriding learning rate value to 0.0002
g0184:  > overriding minimum learning rate value to 1e-05
g0184:  > overriding warmup iterations value to 0
g0184:  > overriding warmup tokens value to 3000000000
g0184:  > overriding total number of iterations value to 1280000000
g0184:  > overriding decay tokens value to 300000000000
g0184:  > overriding learning rate decay style value to cosine
g0184:  > overriding start weight decay value to 0.1
g0184:  > overriding end weight decay value to 0.1
g0184:  > overriding total number of weight decay iterations value to 1280000000
g0184:  > overriding weight decay incr style value to constant
g0184:  checkpoint version 3.0
g0188: [2024-08-10 08:04:47,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 08:04:47,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 08:04:47,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 08:04:47,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 08:04:47,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,685] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,685] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,686] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,686] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,687] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,721] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,720] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,723] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 08:04:47,740] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,740] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,742] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:47,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,351] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,351] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,351] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,351] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,351] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,352] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,352] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,352] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,383] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,383] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,385] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,385] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,400] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,400] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,406] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,406] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,764] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,765] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,767] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,767] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,767] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,767] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,768] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,768] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 08:04:48,796] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,796] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,801] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 08:04:48,802] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0184:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 21000
g0198: (min, max) time across ranks (ms):
g0198:     load-checkpoint ................................: (15020.63, 15024.35)
g0184: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-10 08:04:49 
g0184: > building train, validation, and test datasets ...
g0184:  > datasets target sizes (minimum size):
g0184:     train:      1280000000
g0184:     validation: 128012800
g0184:     test:       12800
g0184: > building train, validation, and test datasets for GPT ...
g0184: Single data path provided for train, valid & test
g0184:  > building dataset index ...
g0184:     reading sizes...
g0184:     reading pointers...
g0184:     reading document index...
g0184:     creating numpy buffer of mmap...
g0184:     creating memory view of numpy buffer...
g0184:  > finished creating indexed dataset in 0.152357 seconds
g0184:     number of documents: 2237032
g0184:  > dataset split:
g0184:     train:
g0184:      document indices in [0, 2122943) total of 2122943 documents
g0184:     validation:
g0184:      document indices in [2122943, 2234795) total of 111852 documents
g0184:     test:
g0184:      document indices in [2234795, 2237032) total of 2237 documents
g0184:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_doc_idx.npy
g0184:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_sample_idx.npy
g0184:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_shuffle_idx.npy
g0184:     loaded indexed file in 0.130 seconds
g0184:     total number of samples: 10738039
g0184:     total number of epochs: 1
g0184:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_doc_idx.npy
g0184:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_sample_idx.npy
g0184:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_shuffle_idx.npy
g0184:     loaded indexed file in 0.233 seconds
g0184:     total number of samples: 128391300
g0184:     total number of epochs: 228
g0184:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_doc_idx.npy
g0184:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_sample_idx.npy
g0184:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_shuffle_idx.npy
g0184:     loaded indexed file in 0.066 seconds
g0184:     total number of samples: 14458
g0184:     total number of epochs: 2
g0184: > finished creating GPT datasets ...
g0184: [after dataloaders are built] datetime: 2024-08-10 08:04:52 
g0184: done with setup ...
g0184: training ...
g0198: (min, max) time across ranks (ms):
g0198:     model-and-optimizer-setup ......................: (17895.49, 17910.72)
g0198:     train/valid/test-data-iterators-setup ..........: (2256.50, 2313.37)
g0184: [before the start of training step] datetime: 2024-08-10 08:04:52 
g0184: [2024-08-10 08:05:55,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=21010, skipped=27, lr=[0.00019996658846301013, 0.00019996658846301013], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21010 loss: 0.8129 iter time (s): 6.319 samples/sec: 20.258
g0198:  iteration    21010/10000000 | consumed samples:      2689280 | consumed tokens:   5507645440 | elapsed time per iteration (ms): 6355.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.154830E-01 | loss scale: 131072.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.140 | tokens per gpu per second (tgs): 1288.955 | TFLOPs: 10.37 |
g0184: [Rank 0] (after 21010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 10902.0 | max reserved: 10902.0
g0195: [Rank 20] (after 21010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5434.0 | max reserved: 5434.0
g0194: [Rank 16] (after 21010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6328.0 | max reserved: 6328.0
g0198: [Rank 28] (after 21010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0185: [Rank 4] (after 21010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 9010.0 | max reserved: 9010.0
g0197: [Rank 24] (after 21010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0188: [Rank 12] (after 21010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7222.0 | max reserved: 7222.0
g0187: [Rank 8] (after 21010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8116.0 | max reserved: 8116.0
g0184: [2024-08-10 08:06:42,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=21020, skipped=27, lr=[0.0001999665185678512, 0.0001999665185678512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21020 loss: 0.7950 iter time (s): 4.464 samples/sec: 28.673
g0198:  iteration    21020/10000000 | consumed samples:      2690560 | consumed tokens:   5510266880 | elapsed time per iteration (ms): 4633.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.153241E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.624 | tokens per gpu per second (tgs): 1767.906 | TFLOPs: 14.23 |
g0184: [2024-08-10 08:07:24,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=21030, skipped=27, lr=[0.00019996644859967308, 0.00019996644859967308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21030 loss: 0.8392 iter time (s): 4.200 samples/sec: 30.478
g0198:  iteration    21030/10000000 | consumed samples:      2691840 | consumed tokens:   5512888320 | elapsed time per iteration (ms): 4232.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.179423E-01 | loss scale: 131072.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.240 | tokens per gpu per second (tgs): 1935.335 | TFLOPs: 15.57 |
g0184: [2024-08-10 08:08:07,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=21040, skipped=27, lr=[0.00019996637855847576, 0.00019996637855847576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21040 loss: 0.8452 iter time (s): 4.275 samples/sec: 29.938
g0198:  iteration    21040/10000000 | consumed samples:      2693120 | consumed tokens:   5515509760 | elapsed time per iteration (ms): 4308.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.204841E-01 | loss scale: 131072.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.708 | tokens per gpu per second (tgs): 1901.340 | TFLOPs: 15.30 |
g0184: [2024-08-10 08:08:50,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=21050, skipped=27, lr=[0.00019996630844425938, 0.00019996630844425938], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21050 loss: 0.8182 iter time (s): 4.290 samples/sec: 29.837
g0198:  iteration    21050/10000000 | consumed samples:      2694400 | consumed tokens:   5518131200 | elapsed time per iteration (ms): 4323.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.265211E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.608 | tokens per gpu per second (tgs): 1894.881 | TFLOPs: 15.25 |
g0184: [2024-08-10 08:09:36,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=21060, skipped=27, lr=[0.00019996623825702388, 0.00019996623825702388], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21060 loss: 0.8035 iter time (s): 4.482 samples/sec: 28.556
g0198:  iteration    21060/10000000 | consumed samples:      2695680 | consumed tokens:   5520752640 | elapsed time per iteration (ms): 4515.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.088608E-01 | loss scale: 131072.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.347 | tokens per gpu per second (tgs): 1814.201 | TFLOPs: 14.60 |
g0197: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 08:09:59,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 08:09:59,783] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 08:10:23,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=21070, skipped=27, lr=[0.0001999661679967694, 0.0001999661679967694], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21070 loss: 0.7696 iter time (s): 4.671 samples/sec: 27.404
g0198:  iteration    21070/10000000 | consumed samples:      2696960 | consumed tokens:   5523374080 | elapsed time per iteration (ms): 4705.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.117843E-01 | loss scale: 262144.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.203 | tokens per gpu per second (tgs): 1740.977 | TFLOPs: 14.01 |
g0184: [2024-08-10 08:11:06,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=21080, skipped=27, lr=[0.00019996609766349599, 0.00019996609766349599], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21080 loss: 0.8580 iter time (s): 4.268 samples/sec: 29.990
g0198:  iteration    21080/10000000 | consumed samples:      2698240 | consumed tokens:   5525995520 | elapsed time per iteration (ms): 4300.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301769E-01 | loss scale: 262144.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.762 | tokens per gpu per second (tgs): 1904.757 | TFLOPs: 15.33 |
g0184: [2024-08-10 08:11:48,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=21090, skipped=27, lr=[0.00019996602725720364, 0.00019996602725720364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21090 loss: 0.8692 iter time (s): 4.230 samples/sec: 30.262
g0198:  iteration    21090/10000000 | consumed samples:      2699520 | consumed tokens:   5528616960 | elapsed time per iteration (ms): 4262.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.256503E-01 | loss scale: 262144.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.029 | tokens per gpu per second (tgs): 1921.861 | TFLOPs: 15.47 |
g0184: [2024-08-10 08:12:36,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=21100, skipped=27, lr=[0.00019996595677789246, 0.00019996595677789246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21100 loss: 0.8046 iter time (s): 4.701 samples/sec: 27.231
g0198:  iteration    21100/10000000 | consumed samples:      2700800 | consumed tokens:   5531238400 | elapsed time per iteration (ms): 4733.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.248905E-01 | loss scale: 262144.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.042 | tokens per gpu per second (tgs): 1730.712 | TFLOPs: 13.93 |
g0184: [2024-08-10 08:13:20,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=21110, skipped=27, lr=[0.00019996588622556247, 0.00019996588622556247], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21110 loss: 0.8113 iter time (s): 4.447 samples/sec: 28.782
g0198:  iteration    21110/10000000 | consumed samples:      2702080 | consumed tokens:   5533859840 | elapsed time per iteration (ms): 4480.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.198750E-01 | loss scale: 262144.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.572 | tokens per gpu per second (tgs): 1828.578 | TFLOPs: 14.71 |
g0184: [2024-08-10 08:14:09,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=21120, skipped=27, lr=[0.00019996581560021375, 0.00019996581560021375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21120 loss: 0.8503 iter time (s): 4.815 samples/sec: 26.583
g0198:  iteration    21120/10000000 | consumed samples:      2703360 | consumed tokens:   5536481280 | elapsed time per iteration (ms): 4848.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.204679E-01 | loss scale: 262144.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.398 | tokens per gpu per second (tgs): 1689.442 | TFLOPs: 13.60 |
g0184: [2024-08-10 08:14:54,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=21130, skipped=27, lr=[0.00019996574490184637, 0.00019996574490184637], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21130 loss: 0.7795 iter time (s): 4.531 samples/sec: 28.250
g0198:  iteration    21130/10000000 | consumed samples:      2704640 | consumed tokens:   5539102720 | elapsed time per iteration (ms): 4564.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301196E-01 | loss scale: 262144.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.046 | tokens per gpu per second (tgs): 1794.927 | TFLOPs: 14.44 |
g0184: [2024-08-10 08:15:42,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=21140, skipped=27, lr=[0.00019996567413046036, 0.00019996567413046036], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21140 loss: 0.8252 iter time (s): 4.684 samples/sec: 27.326
g0198:  iteration    21140/10000000 | consumed samples:      2705920 | consumed tokens:   5541724160 | elapsed time per iteration (ms): 4717.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301115E-01 | loss scale: 262144.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.132 | tokens per gpu per second (tgs): 1736.459 | TFLOPs: 13.97 |
g0184: [2024-08-10 08:16:27,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=21150, skipped=27, lr=[0.00019996560328605574, 0.00019996560328605574], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21150 loss: 0.7896 iter time (s): 4.547 samples/sec: 28.148
g0198:  iteration    21150/10000000 | consumed samples:      2707200 | consumed tokens:   5544345600 | elapsed time per iteration (ms): 4580.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.088492E-01 | loss scale: 262144.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.944 | tokens per gpu per second (tgs): 1788.411 | TFLOPs: 14.39 |
g0184: [2024-08-10 08:17:12,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=21160, skipped=27, lr=[0.00019996553236863266, 0.00019996553236863266], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21160 loss: 0.7992 iter time (s): 4.376 samples/sec: 29.251
g0198:  iteration    21160/10000000 | consumed samples:      2708480 | consumed tokens:   5546967040 | elapsed time per iteration (ms): 4409.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303592E-01 | loss scale: 262144.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.028 | tokens per gpu per second (tgs): 1857.767 | TFLOPs: 14.95 |
g0184: [2024-08-10 08:18:00,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=21170, skipped=27, lr=[0.00019996546137819105, 0.00019996546137819105], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21170 loss: 0.8449 iter time (s): 4.810 samples/sec: 26.610
g0198:  iteration    21170/10000000 | consumed samples:      2709760 | consumed tokens:   5549588480 | elapsed time per iteration (ms): 4843.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.174400E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.427 | tokens per gpu per second (tgs): 1691.324 | TFLOPs: 13.61 |
g0184: [2024-08-10 08:18:48,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=21180, skipped=27, lr=[0.00019996539031473108, 0.00019996539031473108], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21180 loss: 0.8353 iter time (s): 4.725 samples/sec: 27.088
g0198:  iteration    21180/10000000 | consumed samples:      2711040 | consumed tokens:   5552209920 | elapsed time per iteration (ms): 4758.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303212E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.900 | tokens per gpu per second (tgs): 1721.628 | TFLOPs: 13.85 |
g0184: [2024-08-10 08:19:33,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=21190, skipped=27, lr=[0.00019996531917825273, 0.00019996531917825273], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21190 loss: 0.8225 iter time (s): 4.503 samples/sec: 28.428
g0198:  iteration    21190/10000000 | consumed samples:      2712320 | consumed tokens:   5554831360 | elapsed time per iteration (ms): 4535.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.230814E-01 | loss scale: 262144.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.222 | tokens per gpu per second (tgs): 1806.200 | TFLOPs: 14.53 |
g0184: [2024-08-10 08:20:16,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=21200, skipped=27, lr=[0.00019996524796875607, 0.00019996524796875607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21200 loss: 0.8470 iter time (s): 4.239 samples/sec: 30.199
g0198:  iteration    21200/10000000 | consumed samples:      2713600 | consumed tokens:   5557452800 | elapsed time per iteration (ms): 4271.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.145921E-01 | loss scale: 262144.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.963 | tokens per gpu per second (tgs): 1917.651 | TFLOPs: 15.43 |
g0184: [2024-08-10 08:20:58,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=21210, skipped=27, lr=[0.00019996517668624116, 0.00019996517668624116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21210 loss: 0.8157 iter time (s): 4.185 samples/sec: 30.585
g0198:  iteration    21210/10000000 | consumed samples:      2714880 | consumed tokens:   5560074240 | elapsed time per iteration (ms): 4217.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.257791E-01 | loss scale: 262144.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.348 | tokens per gpu per second (tgs): 1942.245 | TFLOPs: 15.63 |
g0184: [2024-08-10 08:21:39,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=21220, skipped=27, lr=[0.0001999651053307081, 0.0001999651053307081], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21220 loss: 0.8054 iter time (s): 4.125 samples/sec: 31.028
g0198:  iteration    21220/10000000 | consumed samples:      2716160 | consumed tokens:   5562695680 | elapsed time per iteration (ms): 4158.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.217115E-01 | loss scale: 262144.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.778 | tokens per gpu per second (tgs): 1969.763 | TFLOPs: 15.85 |
g0184: [2024-08-10 08:22:26,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=21230, skipped=27, lr=[0.00019996503390215688, 0.00019996503390215688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21230 loss: 0.7888 iter time (s): 4.629 samples/sec: 27.653
g0198:  iteration    21230/10000000 | consumed samples:      2717440 | consumed tokens:   5565317120 | elapsed time per iteration (ms): 4679.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.113398E-01 | loss scale: 262144.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.356 | tokens per gpu per second (tgs): 1750.770 | TFLOPs: 14.09 |
g0184: [2024-08-10 08:23:13,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=21240, skipped=27, lr=[0.00019996496240058757, 0.00019996496240058757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21240 loss: 0.8103 iter time (s): 4.623 samples/sec: 27.689
g0198:  iteration    21240/10000000 | consumed samples:      2718720 | consumed tokens:   5567938560 | elapsed time per iteration (ms): 4655.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.296234E-01 | loss scale: 262144.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.493 | tokens per gpu per second (tgs): 1759.555 | TFLOPs: 14.16 |
g0184: [2024-08-10 08:24:00,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=21250, skipped=27, lr=[0.00019996489082600024, 0.00019996489082600024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21250 loss: 0.8218 iter time (s): 4.695 samples/sec: 27.260
g0198:  iteration    21250/10000000 | consumed samples:      2720000 | consumed tokens:   5570560000 | elapsed time per iteration (ms): 4728.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.300486E-01 | loss scale: 262144.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.072 | tokens per gpu per second (tgs): 1732.615 | TFLOPs: 13.94 |
g0184: [2024-08-10 08:24:47,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=21260, skipped=27, lr=[0.00019996481917839494, 0.00019996481917839494], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21260 loss: 0.7729 iter time (s): 4.712 samples/sec: 27.167
g0198:  iteration    21260/10000000 | consumed samples:      2721280 | consumed tokens:   5573181440 | elapsed time per iteration (ms): 4744.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.198295E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.980 | tokens per gpu per second (tgs): 1726.705 | TFLOPs: 13.90 |
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21264
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21264
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21264
g0198: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21264
g0194: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: Grad overflow on iteration 21264
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21264
g0198: Grad overflow on iteration 21264
g0194: Grad overflow on iteration 21264
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21264
g0194: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21264
g0197: Grad overflow on iteration 21264
g0194: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21264
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21264
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21264
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21264
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21264
g0194: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 21264
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21264
g0198: Grad overflow on iteration 21264
g0185: Grad overflow on iteration 21264
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21264
g0187: Grad overflow on iteration 21264
g0198: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: Grad overflow on iteration 21264
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21264
g0195: Grad overflow on iteration 21264
g0194: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 21264
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: Grad overflow on iteration 21264
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21264
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21264
g0194: Grad overflow on iteration 21264
g0195: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21264
g0198: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 08:25:13,455] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 08:25:13,455] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0197: [2024-08-10 08:25:13,455] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 08:25:13,455] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 08:25:13,454] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21264
g0195: [2024-08-10 08:25:13,456] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 08:25:35,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=21270, skipped=28, lr=[0.00019996474745777174, 0.00019996474745777174], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21270 loss: 0.8185 iter time (s): 4.741 samples/sec: 26.999
g0198:  iteration    21270/10000000 | consumed samples:      2722560 | consumed tokens:   5575802880 | elapsed time per iteration (ms): 4775.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.202408E-01 | loss scale: 131072.0 | grad norm: 0.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.803 | tokens per gpu per second (tgs): 1715.403 | TFLOPs: 13.80 |
g0184: [2024-08-10 08:26:25,982] [INFO] [logging.py:96:log_dist] [Rank 0] step=21280, skipped=28, lr=[0.00019996467566413064, 0.00019996467566413064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21280 loss: 0.8052 iter time (s): 4.991 samples/sec: 25.647
g0198:  iteration    21280/10000000 | consumed samples:      2723840 | consumed tokens:   5578424320 | elapsed time per iteration (ms): 5024.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.248118E-01 | loss scale: 131072.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.478 | tokens per gpu per second (tgs): 1630.579 | TFLOPs: 13.12 |
g0184: [2024-08-10 08:27:09,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=21290, skipped=28, lr=[0.00019996460379747175, 0.00019996460379747175], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21290 loss: 0.7994 iter time (s): 4.368 samples/sec: 29.303
g0198:  iteration    21290/10000000 | consumed samples:      2725120 | consumed tokens:   5581045760 | elapsed time per iteration (ms): 4400.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.106094E-01 | loss scale: 131072.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.085 | tokens per gpu per second (tgs): 1861.463 | TFLOPs: 14.98 |
g0184: [2024-08-10 08:27:56,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=21300, skipped=28, lr=[0.00019996453185779512, 0.00019996453185779512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21300 loss: 0.7973 iter time (s): 4.582 samples/sec: 27.933
g0198:  iteration    21300/10000000 | consumed samples:      2726400 | consumed tokens:   5583667200 | elapsed time per iteration (ms): 4615.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.248207E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.735 | tokens per gpu per second (tgs): 1775.070 | TFLOPs: 14.28 |
g0184: [2024-08-10 08:28:44,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=21310, skipped=28, lr=[0.00019996445984510078, 0.00019996445984510078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21310 loss: 0.8147 iter time (s): 4.797 samples/sec: 26.685
g0198:  iteration    21310/10000000 | consumed samples:      2727680 | consumed tokens:   5586288640 | elapsed time per iteration (ms): 4829.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.251177E-01 | loss scale: 131072.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.504 | tokens per gpu per second (tgs): 1696.254 | TFLOPs: 13.65 |
g0184: [2024-08-10 08:29:28,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=21320, skipped=28, lr=[0.00019996438775938885, 0.00019996438775938885], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21320 loss: 0.8212 iter time (s): 4.333 samples/sec: 29.541
g0198:  iteration    21320/10000000 | consumed samples:      2728960 | consumed tokens:   5588910080 | elapsed time per iteration (ms): 4365.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.259329E-01 | loss scale: 131072.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.320 | tokens per gpu per second (tgs): 1876.495 | TFLOPs: 15.10 |
g0184: [2024-08-10 08:30:11,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=21330, skipped=28, lr=[0.00019996431560065925, 0.00019996431560065925], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21330 loss: 0.8185 iter time (s): 4.298 samples/sec: 29.780
g0198:  iteration    21330/10000000 | consumed samples:      2730240 | consumed tokens:   5591531520 | elapsed time per iteration (ms): 4330.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.195889E-01 | loss scale: 131072.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.555 | tokens per gpu per second (tgs): 1891.532 | TFLOPs: 15.22 |
g0184: [2024-08-10 08:30:53,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=21340, skipped=28, lr=[0.0001999642433689122, 0.0001999642433689122], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21340 loss: 0.8145 iter time (s): 4.173 samples/sec: 30.671
g0198:  iteration    21340/10000000 | consumed samples:      2731520 | consumed tokens:   5594152960 | elapsed time per iteration (ms): 4205.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.198565E-01 | loss scale: 131072.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.433 | tokens per gpu per second (tgs): 1947.726 | TFLOPs: 15.67 |
g0184: [2024-08-10 08:31:37,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=21350, skipped=28, lr=[0.00019996417106414764, 0.00019996417106414764], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21350 loss: 0.8149 iter time (s): 4.353 samples/sec: 29.404
g0198:  iteration    21350/10000000 | consumed samples:      2732800 | consumed tokens:   5596774400 | elapsed time per iteration (ms): 4386.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.259473E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.184 | tokens per gpu per second (tgs): 1867.780 | TFLOPs: 15.03 |
g0184: [2024-08-10 08:32:21,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=21360, skipped=28, lr=[0.00019996409868636565, 0.00019996409868636565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21360 loss: 0.8082 iter time (s): 4.336 samples/sec: 29.520
g0198:  iteration    21360/10000000 | consumed samples:      2734080 | consumed tokens:   5599395840 | elapsed time per iteration (ms): 4369.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.132401E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.292 | tokens per gpu per second (tgs): 1874.720 | TFLOPs: 15.09 |
g0184: [2024-08-10 08:33:07,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=21370, skipped=28, lr=[0.0001999640262355663, 0.0001999640262355663], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21370 loss: 0.8168 iter time (s): 4.622 samples/sec: 27.693
g0198:  iteration    21370/10000000 | consumed samples:      2735360 | consumed tokens:   5602017280 | elapsed time per iteration (ms): 4655.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.182970E-01 | loss scale: 131072.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.498 | tokens per gpu per second (tgs): 1759.844 | TFLOPs: 14.16 |
g0184: [2024-08-10 08:33:53,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=21380, skipped=28, lr=[0.00019996395371174965, 0.00019996395371174965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21380 loss: 0.8083 iter time (s): 4.525 samples/sec: 28.286
g0198:  iteration    21380/10000000 | consumed samples:      2736640 | consumed tokens:   5604638720 | elapsed time per iteration (ms): 4558.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.242384E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.083 | tokens per gpu per second (tgs): 1797.291 | TFLOPs: 14.46 |
g0184: [2024-08-10 08:34:38,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=21390, skipped=28, lr=[0.00019996388111491578, 0.00019996388111491578], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21390 loss: 0.8111 iter time (s): 4.529 samples/sec: 28.264
g0198:  iteration    21390/10000000 | consumed samples:      2737920 | consumed tokens:   5607260160 | elapsed time per iteration (ms): 4562.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.223612E-01 | loss scale: 131072.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.054 | tokens per gpu per second (tgs): 1795.470 | TFLOPs: 14.45 |
g0184: [2024-08-10 08:35:26,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=21400, skipped=28, lr=[0.0001999638084450647, 0.0001999638084450647], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21400 loss: 0.8463 iter time (s): 4.759 samples/sec: 26.898
g0198:  iteration    21400/10000000 | consumed samples:      2739200 | consumed tokens:   5609881600 | elapsed time per iteration (ms): 4791.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.214541E-01 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.715 | tokens per gpu per second (tgs): 1709.747 | TFLOPs: 13.76 |
g0184: [2024-08-10 08:36:10,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=21410, skipped=28, lr=[0.00019996373570219645, 0.00019996373570219645], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21410 loss: 0.8065 iter time (s): 4.357 samples/sec: 29.375
g0198:  iteration    21410/10000000 | consumed samples:      2740480 | consumed tokens:   5612503040 | elapsed time per iteration (ms): 4391.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.177557E-01 | loss scale: 131072.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.149 | tokens per gpu per second (tgs): 1865.562 | TFLOPs: 15.01 |
g0184: [2024-08-10 08:36:56,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=21420, skipped=28, lr=[0.00019996366288631113, 0.00019996366288631113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21420 loss: 0.8120 iter time (s): 4.579 samples/sec: 27.952
g0198:  iteration    21420/10000000 | consumed samples:      2741760 | consumed tokens:   5615124480 | elapsed time per iteration (ms): 4614.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.253704E-01 | loss scale: 131072.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.739 | tokens per gpu per second (tgs): 1775.265 | TFLOPs: 14.29 |
g0184: [2024-08-10 08:37:42,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=21430, skipped=28, lr=[0.00019996358999740878, 0.00019996358999740878], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21430 loss: 0.7697 iter time (s): 4.514 samples/sec: 28.358
g0198:  iteration    21430/10000000 | consumed samples:      2743040 | consumed tokens:   5617745920 | elapsed time per iteration (ms): 4546.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.193189E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.156 | tokens per gpu per second (tgs): 1801.953 | TFLOPs: 14.50 |
g0184: [2024-08-10 08:38:27,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=21440, skipped=28, lr=[0.0001999635170354895, 0.0001999635170354895], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21440 loss: 0.8102 iter time (s): 4.468 samples/sec: 28.647
g0198:  iteration    21440/10000000 | consumed samples:      2744320 | consumed tokens:   5620367360 | elapsed time per iteration (ms): 4501.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.251944E-01 | loss scale: 131072.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.438 | tokens per gpu per second (tgs): 1820.007 | TFLOPs: 14.65 |
g0184: [2024-08-10 08:39:12,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=21450, skipped=28, lr=[0.00019996344400055328, 0.00019996344400055328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21450 loss: 0.7932 iter time (s): 4.534 samples/sec: 28.228
g0198:  iteration    21450/10000000 | consumed samples:      2745600 | consumed tokens:   5622988800 | elapsed time per iteration (ms): 4573.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.159881E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.986 | tokens per gpu per second (tgs): 1791.099 | TFLOPs: 14.41 |
g0184: [2024-08-10 08:39:58,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=21460, skipped=28, lr=[0.00019996337089260018, 0.00019996337089260018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21460 loss: 0.8252 iter time (s): 4.498 samples/sec: 28.456
g0198:  iteration    21460/10000000 | consumed samples:      2746880 | consumed tokens:   5625610240 | elapsed time per iteration (ms): 4530.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.216597E-01 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.251 | tokens per gpu per second (tgs): 1808.094 | TFLOPs: 14.55 |
g0184: [2024-08-10 08:40:44,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=21470, skipped=28, lr=[0.00019996329771163027, 0.00019996329771163027], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21470 loss: 0.8129 iter time (s): 4.618 samples/sec: 27.718
g0198:  iteration    21470/10000000 | consumed samples:      2748160 | consumed tokens:   5628231680 | elapsed time per iteration (ms): 4650.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.239012E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.524 | tokens per gpu per second (tgs): 1761.542 | TFLOPs: 14.18 |
g0184: [2024-08-10 08:41:28,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=21480, skipped=28, lr=[0.00019996322445764366, 0.00019996322445764366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21480 loss: 0.8387 iter time (s): 4.331 samples/sec: 29.552
g0198:  iteration    21480/10000000 | consumed samples:      2749440 | consumed tokens:   5630853120 | elapsed time per iteration (ms): 4364.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.315763E-01 | loss scale: 131072.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.331 | tokens per gpu per second (tgs): 1877.156 | TFLOPs: 15.11 |
g0184: [2024-08-10 08:42:12,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=21490, skipped=28, lr=[0.0001999631511306403, 0.0001999631511306403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21490 loss: 0.8005 iter time (s): 4.364 samples/sec: 29.331
g0198:  iteration    21490/10000000 | consumed samples:      2750720 | consumed tokens:   5633474560 | elapsed time per iteration (ms): 4396.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.200489E-01 | loss scale: 131072.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.114 | tokens per gpu per second (tgs): 1863.274 | TFLOPs: 14.99 |
g0184: [2024-08-10 08:43:00,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=21500, skipped=28, lr=[0.00019996307773062033, 0.00019996307773062033], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21500 loss: 0.8350 iter time (s): 4.778 samples/sec: 26.789
g0198:  iteration    21500/10000000 | consumed samples:      2752000 | consumed tokens:   5636096000 | elapsed time per iteration (ms): 4811.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.139373E-01 | loss scale: 131072.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.605 | tokens per gpu per second (tgs): 1702.741 | TFLOPs: 13.70 |
g0184: [2024-08-10 08:43:46,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=21510, skipped=28, lr=[0.0001999630042575838, 0.0001999630042575838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21510 loss: 0.8434 iter time (s): 4.615 samples/sec: 27.735
g0198:  iteration    21510/10000000 | consumed samples:      2753280 | consumed tokens:   5638717440 | elapsed time per iteration (ms): 4647.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.189349E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.541 | tokens per gpu per second (tgs): 1762.603 | TFLOPs: 14.18 |
g0184: [2024-08-10 08:44:33,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=21520, skipped=28, lr=[0.00019996293071153075, 0.00019996293071153075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21520 loss: 0.8266 iter time (s): 4.650 samples/sec: 27.525
g0198:  iteration    21520/10000000 | consumed samples:      2754560 | consumed tokens:   5641338880 | elapsed time per iteration (ms): 4683.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.197277E-01 | loss scale: 131072.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.331 | tokens per gpu per second (tgs): 1749.206 | TFLOPs: 14.08 |
g0184: [2024-08-10 08:45:18,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=21530, skipped=28, lr=[0.00019996285709246125, 0.00019996285709246125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21530 loss: 0.8289 iter time (s): 4.389 samples/sec: 29.165
g0198:  iteration    21530/10000000 | consumed samples:      2755840 | consumed tokens:   5643960320 | elapsed time per iteration (ms): 4421.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.235585E-01 | loss scale: 131072.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.951 | tokens per gpu per second (tgs): 1852.869 | TFLOPs: 14.91 |
g0184: [2024-08-10 08:46:00,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=21540, skipped=28, lr=[0.0001999627834003753, 0.0001999627834003753], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21540 loss: 0.8075 iter time (s): 4.167 samples/sec: 30.721
g0198:  iteration    21540/10000000 | consumed samples:      2757120 | consumed tokens:   5646581760 | elapsed time per iteration (ms): 4210.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.142382E-01 | loss scale: 131072.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.397 | tokens per gpu per second (tgs): 1945.422 | TFLOPs: 15.66 |
g0184: [2024-08-10 08:46:46,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=21550, skipped=28, lr=[0.00019996270963527302, 0.00019996270963527302], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21550 loss: 0.7953 iter time (s): 4.608 samples/sec: 27.779
g0198:  iteration    21550/10000000 | consumed samples:      2758400 | consumed tokens:   5649203200 | elapsed time per iteration (ms): 4641.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.181905E-01 | loss scale: 131072.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.579 | tokens per gpu per second (tgs): 1765.053 | TFLOPs: 14.20 |
g0184: [2024-08-10 08:47:31,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=21560, skipped=28, lr=[0.00019996263579715443, 0.00019996263579715443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21560 loss: 0.8564 iter time (s): 4.488 samples/sec: 28.522
g0198:  iteration    21560/10000000 | consumed samples:      2759680 | consumed tokens:   5651824640 | elapsed time per iteration (ms): 4520.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.203644E-01 | loss scale: 131072.0 | grad norm: 0.282 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.315 | tokens per gpu per second (tgs): 1812.152 | TFLOPs: 14.58 |
g0184: [2024-08-10 08:48:19,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=21570, skipped=28, lr=[0.00019996256188601963, 0.00019996256188601963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21570 loss: 0.8417 iter time (s): 4.696 samples/sec: 27.255
g0198:  iteration    21570/10000000 | consumed samples:      2760960 | consumed tokens:   5654446080 | elapsed time per iteration (ms): 4729.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.250987E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.064 | tokens per gpu per second (tgs): 1732.094 | TFLOPs: 13.94 |
g0184: [2024-08-10 08:49:05,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=21580, skipped=28, lr=[0.00019996248790186863, 0.00019996248790186863], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21580 loss: 0.8253 iter time (s): 4.653 samples/sec: 27.510
g0198:  iteration    21580/10000000 | consumed samples:      2762240 | consumed tokens:   5657067520 | elapsed time per iteration (ms): 4685.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.166257E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.316 | tokens per gpu per second (tgs): 1748.247 | TFLOPs: 14.07 |
g0184: [2024-08-10 08:49:52,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=21590, skipped=28, lr=[0.00019996241384470152, 0.00019996241384470152], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21590 loss: 0.8106 iter time (s): 4.636 samples/sec: 27.607
g0198:  iteration    21590/10000000 | consumed samples:      2763520 | consumed tokens:   5659688960 | elapsed time per iteration (ms): 4669.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.130845E-01 | loss scale: 131072.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.415 | tokens per gpu per second (tgs): 1754.552 | TFLOPs: 14.12 |
g0184: [2024-08-10 08:50:36,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=21600, skipped=28, lr=[0.00019996233971451832, 0.00019996233971451832], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21600 loss: 0.8189 iter time (s): 4.378 samples/sec: 29.240
g0198:  iteration    21600/10000000 | consumed samples:      2764800 | consumed tokens:   5662310400 | elapsed time per iteration (ms): 4411.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.217013E-01 | loss scale: 131072.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.012 | tokens per gpu per second (tgs): 1856.777 | TFLOPs: 14.94 |
g0184: [2024-08-10 08:51:22,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=21610, skipped=28, lr=[0.00019996226551131913, 0.00019996226551131913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21610 loss: 0.8050 iter time (s): 4.507 samples/sec: 28.400
g0198:  iteration    21610/10000000 | consumed samples:      2766080 | consumed tokens:   5664931840 | elapsed time per iteration (ms): 4540.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.173427E-01 | loss scale: 131072.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.193 | tokens per gpu per second (tgs): 1804.367 | TFLOPs: 14.52 |
g0184: [2024-08-10 08:52:05,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=21620, skipped=28, lr=[0.00019996219123510396, 0.00019996219123510396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21620 loss: 0.8149 iter time (s): 4.270 samples/sec: 29.979
g0198:  iteration    21620/10000000 | consumed samples:      2767360 | consumed tokens:   5667553280 | elapsed time per iteration (ms): 4302.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.194928E-01 | loss scale: 131072.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.752 | tokens per gpu per second (tgs): 1904.138 | TFLOPs: 15.32 |
g0184: [2024-08-10 08:52:48,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=21630, skipped=28, lr=[0.00019996211688587292, 0.00019996211688587292], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21630 loss: 0.7886 iter time (s): 4.280 samples/sec: 29.905
g0198:  iteration    21630/10000000 | consumed samples:      2768640 | consumed tokens:   5670174720 | elapsed time per iteration (ms): 4315.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.162224E-01 | loss scale: 131072.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.662 | tokens per gpu per second (tgs): 1898.360 | TFLOPs: 15.28 |
g0184: [2024-08-10 08:53:38,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=21640, skipped=28, lr=[0.00019996204246362605, 0.00019996204246362605], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21640 loss: 0.8044 iter time (s): 5.011 samples/sec: 25.544
g0198:  iteration    21640/10000000 | consumed samples:      2769920 | consumed tokens:   5672796160 | elapsed time per iteration (ms): 5044.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.144114E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.377 | tokens per gpu per second (tgs): 1624.120 | TFLOPs: 13.07 |
g0184: [2024-08-10 08:54:23,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=21650, skipped=28, lr=[0.0001999619679683634, 0.0001999619679683634], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21650 loss: 0.7760 iter time (s): 4.416 samples/sec: 28.988
g0198:  iteration    21650/10000000 | consumed samples:      2771200 | consumed tokens:   5675417600 | elapsed time per iteration (ms): 4449.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.168027E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.769 | tokens per gpu per second (tgs): 1841.195 | TFLOPs: 14.82 |
g0184: [2024-08-10 08:55:06,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=21660, skipped=28, lr=[0.000199961893400085, 0.000199961893400085], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21660 loss: 0.7886 iter time (s): 4.316 samples/sec: 29.660
g0198:  iteration    21660/10000000 | consumed samples:      2772480 | consumed tokens:   5678039040 | elapsed time per iteration (ms): 4348.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.078343E-01 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.434 | tokens per gpu per second (tgs): 1883.764 | TFLOPs: 15.16 |
g0184: [2024-08-10 08:55:56,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=21670, skipped=28, lr=[0.00019996181875879096, 0.00019996181875879096], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21670 loss: 0.8449 iter time (s): 4.934 samples/sec: 25.942
g0198:  iteration    21670/10000000 | consumed samples:      2773760 | consumed tokens:   5680660480 | elapsed time per iteration (ms): 4967.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.112787E-01 | loss scale: 131072.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.768 | tokens per gpu per second (tgs): 1649.152 | TFLOPs: 13.27 |
g0184: [2024-08-10 08:56:43,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=21680, skipped=28, lr=[0.00019996174404448126, 0.00019996174404448126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21680 loss: 0.7966 iter time (s): 4.689 samples/sec: 27.300
g0198:  iteration    21680/10000000 | consumed samples:      2775040 | consumed tokens:   5683281920 | elapsed time per iteration (ms): 4721.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.102739E-01 | loss scale: 131072.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.111 | tokens per gpu per second (tgs): 1735.131 | TFLOPs: 13.96 |
g0184: [2024-08-10 08:57:29,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=21690, skipped=28, lr=[0.00019996166925715606, 0.00019996166925715606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21690 loss: 0.8229 iter time (s): 4.553 samples/sec: 28.111
g0198:  iteration    21690/10000000 | consumed samples:      2776320 | consumed tokens:   5685903360 | elapsed time per iteration (ms): 4586.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.155647E-01 | loss scale: 131072.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.911 | tokens per gpu per second (tgs): 1786.284 | TFLOPs: 14.37 |
g0184: [2024-08-10 08:58:18,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=21700, skipped=28, lr=[0.00019996159439681534, 0.00019996159439681534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21700 loss: 0.8528 iter time (s): 4.852 samples/sec: 26.382
g0198:  iteration    21700/10000000 | consumed samples:      2777600 | consumed tokens:   5688524800 | elapsed time per iteration (ms): 4884.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.135835E-01 | loss scale: 131072.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.207 | tokens per gpu per second (tgs): 1677.247 | TFLOPs: 13.50 |
g0184: [2024-08-10 08:59:02,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=21710, skipped=28, lr=[0.0001999615194634592, 0.0001999615194634592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21710 loss: 0.7987 iter time (s): 4.418 samples/sec: 28.972
g0198:  iteration    21710/10000000 | consumed samples:      2778880 | consumed tokens:   5691146240 | elapsed time per iteration (ms): 4451.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.157267E-01 | loss scale: 131072.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.757 | tokens per gpu per second (tgs): 1840.435 | TFLOPs: 14.81 |
g0184: [2024-08-10 08:59:49,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=21720, skipped=28, lr=[0.00019996144445708766, 0.00019996144445708766], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21720 loss: 0.8073 iter time (s): 4.652 samples/sec: 27.516
g0198:  iteration    21720/10000000 | consumed samples:      2780160 | consumed tokens:   5693767680 | elapsed time per iteration (ms): 4684.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.215962E-01 | loss scale: 131072.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.323 | tokens per gpu per second (tgs): 1748.676 | TFLOPs: 14.07 |
g0184: [2024-08-10 09:00:36,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=21730, skipped=28, lr=[0.0001999613693777008, 0.0001999613693777008], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21730 loss: 0.8349 iter time (s): 4.652 samples/sec: 27.515
g0198:  iteration    21730/10000000 | consumed samples:      2781440 | consumed tokens:   5696389120 | elapsed time per iteration (ms): 4685.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.264356E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.320 | tokens per gpu per second (tgs): 1748.483 | TFLOPs: 14.07 |
g0184: [2024-08-10 09:01:22,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=21740, skipped=28, lr=[0.0001999612942252987, 0.0001999612942252987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21740 loss: 0.8386 iter time (s): 4.559 samples/sec: 28.079
g0198:  iteration    21740/10000000 | consumed samples:      2782720 | consumed tokens:   5699010560 | elapsed time per iteration (ms): 4592.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.282373E-01 | loss scale: 131072.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.874 | tokens per gpu per second (tgs): 1783.936 | TFLOPs: 14.36 |
g0184: [2024-08-10 09:02:06,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=21750, skipped=28, lr=[0.00019996121899988137, 0.00019996121899988137], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21750 loss: 0.8100 iter time (s): 4.378 samples/sec: 29.239
g0198:  iteration    21750/10000000 | consumed samples:      2784000 | consumed tokens:   5701632000 | elapsed time per iteration (ms): 4410.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.246889E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.021 | tokens per gpu per second (tgs): 1857.362 | TFLOPs: 14.95 |
g0184: [2024-08-10 09:02:53,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=21760, skipped=28, lr=[0.0001999611437014489, 0.0001999611437014489], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21760 loss: 0.7892 iter time (s): 4.636 samples/sec: 27.610
g0198:  iteration    21760/10000000 | consumed samples:      2785280 | consumed tokens:   5704253440 | elapsed time per iteration (ms): 4669.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.082565E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.412 | tokens per gpu per second (tgs): 1754.364 | TFLOPs: 14.12 |
g0184: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 09:03:19,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 09:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 09:03:39,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=21770, skipped=28, lr=[0.00019996106833000134, 0.00019996106833000134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21770 loss: 0.8126 iter time (s): 4.610 samples/sec: 27.764
g0198:  iteration    21770/10000000 | consumed samples:      2786560 | consumed tokens:   5706874880 | elapsed time per iteration (ms): 4645.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.237465E-01 | loss scale: 262144.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.555 | tokens per gpu per second (tgs): 1763.498 | TFLOPs: 14.19 |
g0184: [2024-08-10 09:04:26,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=21780, skipped=28, lr=[0.00019996099288553876, 0.00019996099288553876], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21780 loss: 0.8257 iter time (s): 4.657 samples/sec: 27.483
g0198:  iteration    21780/10000000 | consumed samples:      2787840 | consumed tokens:   5709496320 | elapsed time per iteration (ms): 4690.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.234581E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.289 | tokens per gpu per second (tgs): 1746.501 | TFLOPs: 14.05 |
g0184: [2024-08-10 09:05:16,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=21790, skipped=28, lr=[0.00019996091736806118, 0.00019996091736806118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21790 loss: 0.7999 iter time (s): 4.958 samples/sec: 25.816
g0198:  iteration    21790/10000000 | consumed samples:      2789120 | consumed tokens:   5712117760 | elapsed time per iteration (ms): 4991.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.143291E-01 | loss scale: 262144.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.645 | tokens per gpu per second (tgs): 1641.268 | TFLOPs: 13.21 |
g0184: [2024-08-10 09:06:02,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=21800, skipped=28, lr=[0.00019996084177756871, 0.00019996084177756871], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21800 loss: 0.8290 iter time (s): 4.608 samples/sec: 27.780
g0198:  iteration    21800/10000000 | consumed samples:      2790400 | consumed tokens:   5714739200 | elapsed time per iteration (ms): 4640.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.207781E-01 | loss scale: 262144.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.582 | tokens per gpu per second (tgs): 1765.245 | TFLOPs: 14.21 |
g0184: [2024-08-10 09:06:52,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=21810, skipped=28, lr=[0.00019996076611406135, 0.00019996076611406135], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21810 loss: 0.8178 iter time (s): 4.891 samples/sec: 26.169
g0198:  iteration    21810/10000000 | consumed samples:      2791680 | consumed tokens:   5717360640 | elapsed time per iteration (ms): 4924.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.196209E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.992 | tokens per gpu per second (tgs): 1663.486 | TFLOPs: 13.39 |
g0184: [2024-08-10 09:07:37,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=21820, skipped=28, lr=[0.00019996069037753924, 0.00019996069037753924], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21820 loss: 0.7785 iter time (s): 4.536 samples/sec: 28.220
g0198:  iteration    21820/10000000 | consumed samples:      2792960 | consumed tokens:   5719982080 | elapsed time per iteration (ms): 4568.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.110644E-01 | loss scale: 262144.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.017 | tokens per gpu per second (tgs): 1793.085 | TFLOPs: 14.43 |
g0184: [2024-08-10 09:08:24,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=21830, skipped=28, lr=[0.00019996061456800234, 0.00019996061456800234], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21830 loss: 0.7976 iter time (s): 4.621 samples/sec: 27.701
g0198:  iteration    21830/10000000 | consumed samples:      2794240 | consumed tokens:   5722603520 | elapsed time per iteration (ms): 4653.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.109680E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.505 | tokens per gpu per second (tgs): 1760.333 | TFLOPs: 14.17 |
g0184: [2024-08-10 09:09:13,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=21840, skipped=28, lr=[0.00019996053868545077, 0.00019996053868545077], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21840 loss: 0.7902 iter time (s): 4.906 samples/sec: 26.092
g0198:  iteration    21840/10000000 | consumed samples:      2795520 | consumed tokens:   5725224960 | elapsed time per iteration (ms): 4949.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.031067E-01 | loss scale: 262144.0 | grad norm: 0.348 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.859 | tokens per gpu per second (tgs): 1654.980 | TFLOPs: 13.32 |
g0184: [2024-08-10 09:10:02,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=21850, skipped=28, lr=[0.00019996046272988458, 0.00019996046272988458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21850 loss: 0.8599 iter time (s): 4.802 samples/sec: 26.654
g0198:  iteration    21850/10000000 | consumed samples:      2796800 | consumed tokens:   5727846400 | elapsed time per iteration (ms): 4860.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.250339E-01 | loss scale: 262144.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.336 | tokens per gpu per second (tgs): 1685.514 | TFLOPs: 13.56 |
g0184: [2024-08-10 09:11:04,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=21860, skipped=28, lr=[0.00019996038670130382, 0.00019996038670130382], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21860 loss: 0.7958 iter time (s): 6.164 samples/sec: 20.766
g0198:  iteration    21860/10000000 | consumed samples:      2798080 | consumed tokens:   5730467840 | elapsed time per iteration (ms): 6198.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.159517E-01 | loss scale: 262144.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.651 | tokens per gpu per second (tgs): 1321.686 | TFLOPs: 10.64 |
g0184: [2024-08-10 09:12:04,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=21870, skipped=28, lr=[0.00019996031059970858, 0.00019996031059970858], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21870 loss: 0.7930 iter time (s): 5.942 samples/sec: 21.543
g0198:  iteration    21870/10000000 | consumed samples:      2799360 | consumed tokens:   5733089280 | elapsed time per iteration (ms): 5975.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.147636E-01 | loss scale: 262144.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.422 | tokens per gpu per second (tgs): 1370.985 | TFLOPs: 11.03 |
g0184: [2024-08-10 09:12:55,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=21880, skipped=28, lr=[0.00019996023442509886, 0.00019996023442509886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21880 loss: 0.8427 iter time (s): 5.078 samples/sec: 25.209
g0198:  iteration    21880/10000000 | consumed samples:      2800640 | consumed tokens:   5735710720 | elapsed time per iteration (ms): 5111.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.171829E-01 | loss scale: 262144.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.042 | tokens per gpu per second (tgs): 1602.718 | TFLOPs: 12.90 |
g0184: [2024-08-10 09:13:49,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=21890, skipped=28, lr=[0.00019996015817747473, 0.00019996015817747473], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21890 loss: 0.8032 iter time (s): 5.341 samples/sec: 23.967
g0198:  iteration    21890/10000000 | consumed samples:      2801920 | consumed tokens:   5738332160 | elapsed time per iteration (ms): 5374.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.234239E-01 | loss scale: 262144.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.817 | tokens per gpu per second (tgs): 1524.301 | TFLOPs: 12.27 |
g0184: [2024-08-10 09:14:51,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=21900, skipped=28, lr=[0.0001999600818568363, 0.0001999600818568363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21900 loss: 0.8198 iter time (s): 6.242 samples/sec: 20.508
g0198:  iteration    21900/10000000 | consumed samples:      2803200 | consumed tokens:   5740953600 | elapsed time per iteration (ms): 6274.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.162002E-01 | loss scale: 262144.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.400 | tokens per gpu per second (tgs): 1305.586 | TFLOPs: 10.51 |
g0184: [2024-08-10 09:15:54,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=21910, skipped=28, lr=[0.0001999600054631836, 0.0001999600054631836], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21910 loss: 0.8308 iter time (s): 6.258 samples/sec: 20.454
g0198:  iteration    21910/10000000 | consumed samples:      2804480 | consumed tokens:   5743575040 | elapsed time per iteration (ms): 6290.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.174202E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.347 | tokens per gpu per second (tgs): 1302.226 | TFLOPs: 10.48 |
g0197: [2024-08-10 09:16:14,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21913
g0197: [2024-08-10 09:16:14,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21913
g0197: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 09:16:14,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21913
g0185: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21913
g0184: [2024-08-10 09:16:14,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21913
g0185: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:16:14,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21913
g0185: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: Grad overflow on iteration 21913
g0184: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21913
g0197: Grad overflow on iteration 21913
g0195: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21913
g0197: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: Grad overflow on iteration 21913
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: Grad overflow on iteration 21913
g0184: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: Grad overflow on iteration 21913
g0197: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21913
g0188: Grad overflow on iteration 21913
g0197: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 09:16:14,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21913
g0184: Grad overflow on iteration 21913
g0195: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:16:14,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21913
g0195: Grad overflow on iteration 21913
g0184: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: Grad overflow on iteration 21913
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 21913
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21913
g0195: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21913
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: Grad overflow on iteration 21913
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: Grad overflow on iteration 21913
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 21913
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: Grad overflow on iteration 21913
g0188: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: Grad overflow on iteration 21913
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21913
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21913
g0187: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21913
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 09:16:14,860] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21913
g0185: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 09:16:14,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21914
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21914
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21914
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21914
g0195: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: Grad overflow on iteration 21914
g0195: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21914
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21914
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21914
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21914
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21914
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21914
g0194: Grad overflow on iteration 21914
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: Grad overflow on iteration 21914
g0194: Grad overflow on iteration 21914
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21914
g0198: Grad overflow on iteration 21914
g0194: Grad overflow on iteration 21914
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21914
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21914
g0195: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: Grad overflow on iteration 21914
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21914
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: Grad overflow on iteration 21914
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21914
g0194: Grad overflow on iteration 21914
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: Grad overflow on iteration 21914
g0184: Grad overflow on iteration 21914
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: Grad overflow on iteration 21914
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21914
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21914
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21914
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21914
g0188: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21914
g0184: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 09:16:19,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 09:16:19,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0195: [2024-08-10 09:16:19,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 09:16:45,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=21920, skipped=30, lr=[0.00019995992899651666, 0.00019995992899651666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21920 loss: 0.8331 iter time (s): 5.021 samples/sec: 25.495
g0198:  iteration    21920/10000000 | consumed samples:      2805760 | consumed tokens:   5746196480 | elapsed time per iteration (ms): 5053.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.181497E-01 | loss scale: 65536.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.328 | tokens per gpu per second (tgs): 1620.988 | TFLOPs: 13.04 |
g0187: [2024-08-10 09:16:49,404] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21920
g0187: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21920
g0187: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21920
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21920
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21920
g0187: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21920
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: Grad overflow on iteration 21920
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21920
g0187: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21920
g0184: Grad overflow on iteration 21920
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: Grad overflow on iteration 21920
g0194: Grad overflow on iteration 21920
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21920
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21920
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: Grad overflow on iteration 21920
g0185: Grad overflow on iteration 21920
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21920
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21920
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21920
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21920
g0188: Grad overflow on iteration 21920
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21920
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21920
g0184: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21920
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: Grad overflow on iteration 21920
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21920
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21920
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: Grad overflow on iteration 21920
g0184: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21920
g0185: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 09:16:49,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21920
g0195: Grad overflow on iteration 21920
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21920
g0184: [2024-08-10 09:16:49,406] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 09:16:49,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21922
g0195: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21922
g0195: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: Grad overflow on iteration 21922
g0194: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21922
g0187: Grad overflow on iteration 21922
g0187: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21922
g0197: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: Grad overflow on iteration 21922
g0197: Grad overflow on iteration 21922
g0194: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: Grad overflow on iteration 21922
g0197: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21922
g0197: Grad overflow on iteration 21922
g0188: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21922
g0197: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: Grad overflow on iteration 21922
g0184: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: Grad overflow on iteration 21922
g0184: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21922
g0184: Grad overflow on iteration 21922
g0194: Grad overflow on iteration 21922
g0194: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: Grad overflow on iteration 21922
g0187: Grad overflow on iteration 21922
g0184: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21922
g0194: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: Grad overflow on iteration 21922
g0185: Grad overflow on iteration 21922
g0188: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21922
g0187: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21922
g0184: [2024-08-10 09:16:59,760] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0198: [2024-08-10 09:16:59,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 21922
g0187: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 21922
g0188: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 21922
g0194: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21922
g0195: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21922
g0198: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 21922
g0197: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21922
g0185: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 09:16:59,761] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21922
g0184: [2024-08-10 09:16:59,762] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 21923
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21923
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21923
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21923
g0197: Grad overflow on iteration 21923
g0195: Grad overflow on iteration 21923
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21923
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21923
g0187: Grad overflow on iteration 21923
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 21923
g0187: Grad overflow on iteration 21923
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 21923
g0185: Grad overflow on iteration 21923
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: Grad overflow on iteration 21923
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 21923
g0194: Grad overflow on iteration 21923
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: Grad overflow on iteration 21923
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21923
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 21923
g0197: Grad overflow on iteration 21923
g0188: Grad overflow on iteration 21923
g0184: Grad overflow on iteration 21923
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: Grad overflow on iteration 21923
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 21923
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 21923
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 21923
g0195: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: Grad overflow on iteration 21923
g0184: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 21923
g0184: [2024-08-10 09:17:05,073] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0187: Grad overflow on iteration 21923
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: Grad overflow on iteration 21923
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 21923
g0187: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: Grad overflow on iteration 21923
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 09:17:05,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 09:17:34,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=21930, skipped=33, lr=[0.00019995985245683555, 0.00019995985245683555], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21930 loss: 0.8183 iter time (s): 4.871 samples/sec: 26.277
g0198:  iteration    21930/10000000 | consumed samples:      2807040 | consumed tokens:   5748817920 | elapsed time per iteration (ms): 4904.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.284441E-01 | loss scale: 8192.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.097 | tokens per gpu per second (tgs): 1670.215 | TFLOPs: 13.44 |
g0184: [2024-08-10 09:18:25,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=21940, skipped=33, lr=[0.00019995977584414038, 0.00019995977584414038], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21940 loss: 0.8184 iter time (s): 5.049 samples/sec: 25.351
g0198:  iteration    21940/10000000 | consumed samples:      2808320 | consumed tokens:   5751439360 | elapsed time per iteration (ms): 5082.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.202598E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.184 | tokens per gpu per second (tgs): 1611.793 | TFLOPs: 12.97 |
g0184: [2024-08-10 09:19:18,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=21950, skipped=33, lr=[0.00019995969915843114, 0.00019995969915843114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21950 loss: 0.8223 iter time (s): 5.256 samples/sec: 24.353
g0198:  iteration    21950/10000000 | consumed samples:      2809600 | consumed tokens:   5754060800 | elapsed time per iteration (ms): 5289.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.093019E-01 | loss scale: 8192.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.197 | tokens per gpu per second (tgs): 1548.597 | TFLOPs: 12.46 |
g0184: [2024-08-10 09:20:09,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=21960, skipped=33, lr=[0.00019995962239970796, 0.00019995962239970796], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21960 loss: 0.8099 iter time (s): 5.073 samples/sec: 25.234
g0198:  iteration    21960/10000000 | consumed samples:      2810880 | consumed tokens:   5756682240 | elapsed time per iteration (ms): 5106.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.127788E-01 | loss scale: 8192.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.069 | tokens per gpu per second (tgs): 1604.391 | TFLOPs: 12.91 |
g0184: [2024-08-10 09:20:53,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=21970, skipped=33, lr=[0.00019995954556797083, 0.00019995954556797083], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21970 loss: 0.8014 iter time (s): 4.425 samples/sec: 28.926
g0198:  iteration    21970/10000000 | consumed samples:      2812160 | consumed tokens:   5759303680 | elapsed time per iteration (ms): 4457.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.280532E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.715 | tokens per gpu per second (tgs): 1837.737 | TFLOPs: 14.79 |
g0184: [2024-08-10 09:21:38,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=21980, skipped=33, lr=[0.00019995946866321987, 0.00019995946866321987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21980 loss: 0.8154 iter time (s): 4.495 samples/sec: 28.476
g0198:  iteration    21980/10000000 | consumed samples:      2813440 | consumed tokens:   5761925120 | elapsed time per iteration (ms): 4528.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.095665E-01 | loss scale: 8192.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.267 | tokens per gpu per second (tgs): 1809.090 | TFLOPs: 14.56 |
g0184: [2024-08-10 09:22:26,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=21990, skipped=33, lr=[0.00019995939168545508, 0.00019995939168545508], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 21990 loss: 0.8003 iter time (s): 4.732 samples/sec: 27.049
g0198:  iteration    21990/10000000 | consumed samples:      2814720 | consumed tokens:   5764546560 | elapsed time per iteration (ms): 4765.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.228912E-01 | loss scale: 8192.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.858 | tokens per gpu per second (tgs): 1718.935 | TFLOPs: 13.83 |
g0184: [2024-08-10 09:23:12,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=33, lr=[0.00019995931463467654, 0.00019995931463467654], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22000 loss: 0.7862 iter time (s): 4.551 samples/sec: 28.128
g0198:  iteration    22000/10000000 | consumed samples:      2816000 | consumed tokens:   5767168000 | elapsed time per iteration (ms): 4583.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.049831E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.928 | tokens per gpu per second (tgs): 1787.364 | TFLOPs: 14.38 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 22000 | lm loss value: 8.166532E-01 | lm loss PPL: 2.262914E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   22000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 09:30:43,526] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step22000 is about to be saved!
g0184: [2024-08-10 09:30:43,535] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0198: [2024-08-10 09:30:43,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0198: [2024-08-10 09:30:43,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0184: [2024-08-10 09:30:43,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0198: [2024-08-10 09:30:43,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0184: [2024-08-10 09:30:43,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0188: [2024-08-10 09:30:43,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0188: [2024-08-10 09:30:43,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0188: [2024-08-10 09:30:43,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0195: [2024-08-10 09:30:43,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0195: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0195: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0187: [2024-08-10 09:30:43,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0187: [2024-08-10 09:30:43,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0185: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0185: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0187: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0197: [2024-08-10 09:30:43,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0197: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0197: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0194: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0194: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0185: [2024-08-10 09:30:43,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0194: [2024-08-10 09:30:43,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0198: [2024-08-10 09:30:43,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_23-model_00-model_states.pt...
g0188: [2024-08-10 09:30:43,571] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_11-model_00-model_states.pt...
g0187: [2024-08-10 09:30:43,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_08-model_00-model_states.pt...
g0185: [2024-08-10 09:30:43,578] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_05-model_00-model_states.pt...
g0194: [2024-08-10 09:30:43,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_14-model_00-model_states.pt...
g0197: [2024-08-10 09:30:43,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_20-model_00-model_states.pt...
g0195: [2024-08-10 09:30:43,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_17-model_00-model_states.pt...
g0184: [2024-08-10 09:30:43,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_01-model_00-model_states.pt...
g0185: [2024-08-10 09:30:43,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 09:30:43,720] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_06-model_00-model_states.pt...
g0184: [2024-08-10 09:30:43,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_01-model_00-model_states.pt.
g0188: [2024-08-10 09:30:43,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_11-model_00-model_states.pt.
g0184: [2024-08-10 09:30:43,791] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_02-model_00-model_states.pt...
g0194: [2024-08-10 09:30:43,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_14-model_00-model_states.pt.
g0188: [2024-08-10 09:30:43,805] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_12-model_00-model_states.pt...
g0194: [2024-08-10 09:30:43,836] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_15-model_00-model_states.pt...
g0187: [2024-08-10 09:30:43,850] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_08-model_00-model_states.pt.
g0185: [2024-08-10 09:30:43,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_06-model_00-model_states.pt.
g0197: [2024-08-10 09:30:43,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_20-model_00-model_states.pt.
g0187: [2024-08-10 09:30:43,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_09-model_00-model_states.pt...
g0185: [2024-08-10 09:30:43,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_07-model_00-model_states.pt...
g0197: [2024-08-10 09:30:43,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_21-model_00-model_states.pt...
g0188: [2024-08-10 09:30:43,932] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_12-model_00-model_states.pt.
g0184: [2024-08-10 09:30:43,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_02-model_00-model_states.pt.
g0194: [2024-08-10 09:30:43,950] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_15-model_00-model_states.pt.
g0188: [2024-08-10 09:30:43,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_13-model_00-model_states.pt...
g0195: [2024-08-10 09:30:43,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_17-model_00-model_states.pt.
g0184: [2024-08-10 09:30:43,983] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_03-model_00-model_states.pt...
g0194: [2024-08-10 09:30:43,984] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_16-model_00-model_states.pt...
g0195: [2024-08-10 09:30:44,004] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_18-model_00-model_states.pt...
g0185: [2024-08-10 09:30:44,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 09:30:44,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_01_model_states.pt...
g0197: [2024-08-10 09:30:44,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_21-model_00-model_states.pt.
g0187: [2024-08-10 09:30:44,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_09-model_00-model_states.pt.
g0197: [2024-08-10 09:30:44,047] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_22-model_00-model_states.pt...
g0187: [2024-08-10 09:30:44,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_10-model_00-model_states.pt...
g0184: [2024-08-10 09:30:44,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 09:30:44,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_04-model_00-model_states.pt...
g0195: [2024-08-10 09:30:44,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_18-model_00-model_states.pt.
g0197: [2024-08-10 09:30:44,147] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 09:30:44,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_06_model_states.pt...
g0194: [2024-08-10 09:30:44,155] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 09:30:44,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_04_model_states.pt...
g0195: [2024-08-10 09:30:44,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_19-model_00-model_states.pt...
g0198: [2024-08-10 09:30:44,205] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 09:30:44,206] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 09:30:44,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 09:30:44,254] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_25-model_00-model_states.pt...
g0195: [2024-08-10 09:30:44,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 09:30:44,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_05_model_states.pt...
g0184: [2024-08-10 09:30:44,326] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 09:30:44,328] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_00_model_states.pt
g0184: [2024-08-10 09:30:44,329] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_00_model_states.pt...
g0187: [2024-08-10 09:30:44,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 09:30:44,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_02_model_states.pt...
g0188: [2024-08-10 09:30:44,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 09:30:44,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_03_model_states.pt...
g0198: [2024-08-10 09:30:44,668] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 09:30:44,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_07_model_states.pt...
g0197: [2024-08-10 09:30:46,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 09:30:46,572] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0194: [2024-08-10 09:30:46,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 09:30:46,629] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0188: [2024-08-10 09:30:46,644] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 09:30:46,645] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0185: [2024-08-10 09:30:46,660] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 09:30:46,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0187: [2024-08-10 09:30:46,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 09:30:46,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0195: [2024-08-10 09:30:46,723] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 09:30:46,723] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0198: [2024-08-10 09:30:46,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 09:30:46,824] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0184: [2024-08-10 09:30:47,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step22000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 09:30:47,748] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22000 is ready now!
g0184:   successfully saved checkpoint at iteration   22000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.26, Latency(second): 4.282
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4281.87, 4282.36)
g0184: [2024-08-10 09:31:41,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=22010, skipped=33, lr=[0.00019995923751088432, 0.00019995923751088432], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22010 loss: 0.8580 iter time (s): 5.342 samples/sec: 23.962
g0198:  iteration    22010/10000000 | consumed samples:      2817280 | consumed tokens:   5769789440 | elapsed time per iteration (ms): 50909.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.102242E-01 | loss scale: 8192.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.514 | tokens per gpu per second (tgs): 160.915 | TFLOPs: 1.29 |
g0184: [2024-08-10 09:32:30,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=22020, skipped=33, lr=[0.0001999591603140785, 0.0001999591603140785], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22020 loss: 0.8172 iter time (s): 4.837 samples/sec: 26.465
g0198:  iteration    22020/10000000 | consumed samples:      2818560 | consumed tokens:   5772410880 | elapsed time per iteration (ms): 4869.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.125098E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.285 | tokens per gpu per second (tgs): 1682.240 | TFLOPs: 13.54 |
g0184: [2024-08-10 09:33:16,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=22030, skipped=33, lr=[0.0001999590830442591, 0.0001999590830442591], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22030 loss: 0.8290 iter time (s): 4.574 samples/sec: 27.986
g0198:  iteration    22030/10000000 | consumed samples:      2819840 | consumed tokens:   5775032320 | elapsed time per iteration (ms): 4606.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.131636E-01 | loss scale: 8192.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.790 | tokens per gpu per second (tgs): 1778.534 | TFLOPs: 14.31 |
g0184: [2024-08-10 09:34:00,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=22040, skipped=33, lr=[0.0001999590057014262, 0.0001999590057014262], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22040 loss: 0.8171 iter time (s): 4.416 samples/sec: 28.983
g0198:  iteration    22040/10000000 | consumed samples:      2821120 | consumed tokens:   5777653760 | elapsed time per iteration (ms): 4449.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.171327E-01 | loss scale: 8192.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.766 | tokens per gpu per second (tgs): 1841.029 | TFLOPs: 14.82 |
g0184: [2024-08-10 09:34:50,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=22050, skipped=33, lr=[0.00019995892828557983, 0.00019995892828557983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22050 loss: 0.8074 iter time (s): 4.974 samples/sec: 25.734
g0198:  iteration    22050/10000000 | consumed samples:      2822400 | consumed tokens:   5780275200 | elapsed time per iteration (ms): 5006.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.217864E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.565 | tokens per gpu per second (tgs): 1636.189 | TFLOPs: 13.17 |
g0184: [2024-08-10 09:35:38,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=22060, skipped=33, lr=[0.00019995885079672008, 0.00019995885079672008], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22060 loss: 0.7739 iter time (s): 4.756 samples/sec: 26.911
g0198:  iteration    22060/10000000 | consumed samples:      2823680 | consumed tokens:   5782896640 | elapsed time per iteration (ms): 4789.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.076511E-01 | loss scale: 8192.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.727 | tokens per gpu per second (tgs): 1710.523 | TFLOPs: 13.76 |
g0184: [2024-08-10 09:36:31,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=22070, skipped=33, lr=[0.000199958773234847, 0.000199958773234847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22070 loss: 0.8093 iter time (s): 5.274 samples/sec: 24.271
g0198:  iteration    22070/10000000 | consumed samples:      2824960 | consumed tokens:   5785518080 | elapsed time per iteration (ms): 5306.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.191361E-01 | loss scale: 8192.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.121 | tokens per gpu per second (tgs): 1543.753 | TFLOPs: 12.42 |
g0184: [2024-08-10 09:37:18,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=22080, skipped=33, lr=[0.0001999586955999607, 0.0001999586955999607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22080 loss: 0.8120 iter time (s): 4.640 samples/sec: 27.584
g0198:  iteration    22080/10000000 | consumed samples:      2826240 | consumed tokens:   5788139520 | elapsed time per iteration (ms): 4673.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.173628E-01 | loss scale: 8192.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.391 | tokens per gpu per second (tgs): 1752.996 | TFLOPs: 14.11 |
g0184: [2024-08-10 09:38:07,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=22090, skipped=33, lr=[0.00019995861789206114, 0.00019995861789206114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22090 loss: 0.8437 iter time (s): 4.823 samples/sec: 26.541
g0198:  iteration    22090/10000000 | consumed samples:      2827520 | consumed tokens:   5790760960 | elapsed time per iteration (ms): 4855.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.166360E-01 | loss scale: 8192.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.361 | tokens per gpu per second (tgs): 1687.089 | TFLOPs: 13.58 |
g0184: [2024-08-10 09:38:52,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=22100, skipped=33, lr=[0.00019995854011114843, 0.00019995854011114843], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22100 loss: 0.7918 iter time (s): 4.490 samples/sec: 28.506
g0198:  iteration    22100/10000000 | consumed samples:      2828800 | consumed tokens:   5793382400 | elapsed time per iteration (ms): 4523.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.200206E-01 | loss scale: 8192.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.299 | tokens per gpu per second (tgs): 1811.130 | TFLOPs: 14.57 |
g0184: [2024-08-10 09:39:40,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=22110, skipped=33, lr=[0.00019995846225722265, 0.00019995846225722265], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22110 loss: 0.8423 iter time (s): 4.763 samples/sec: 26.873
g0198:  iteration    22110/10000000 | consumed samples:      2830080 | consumed tokens:   5796003840 | elapsed time per iteration (ms): 4796.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.095529E-01 | loss scale: 8192.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.688 | tokens per gpu per second (tgs): 1708.061 | TFLOPs: 13.75 |
g0184: [2024-08-10 09:40:32,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=22120, skipped=33, lr=[0.00019995838433028382, 0.00019995838433028382], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22120 loss: 0.7973 iter time (s): 5.196 samples/sec: 24.635
g0198:  iteration    22120/10000000 | consumed samples:      2831360 | consumed tokens:   5798625280 | elapsed time per iteration (ms): 5228.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.062733E-01 | loss scale: 8192.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.480 | tokens per gpu per second (tgs): 1566.690 | TFLOPs: 12.61 |
g0184: [2024-08-10 09:41:20,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=22130, skipped=33, lr=[0.00019995830633033206, 0.00019995830633033206], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22130 loss: 0.7845 iter time (s): 4.789 samples/sec: 26.729
g0198:  iteration    22130/10000000 | consumed samples:      2832640 | consumed tokens:   5801246720 | elapsed time per iteration (ms): 4830.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.121826E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.500 | tokens per gpu per second (tgs): 1695.981 | TFLOPs: 13.65 |
g0184: [2024-08-10 09:42:08,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=22140, skipped=33, lr=[0.00019995822825736735, 0.00019995822825736735], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22140 loss: 0.8104 iter time (s): 4.718 samples/sec: 27.129
g0198:  iteration    22140/10000000 | consumed samples:      2833920 | consumed tokens:   5803868160 | elapsed time per iteration (ms): 4750.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.261570E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.942 | tokens per gpu per second (tgs): 1724.290 | TFLOPs: 13.88 |
g0184: [2024-08-10 09:42:54,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=22150, skipped=33, lr=[0.00019995815011138982, 0.00019995815011138982], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22150 loss: 0.8041 iter time (s): 4.552 samples/sec: 28.117
g0198:  iteration    22150/10000000 | consumed samples:      2835200 | consumed tokens:   5806489600 | elapsed time per iteration (ms): 4586.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.058718E-01 | loss scale: 8192.0 | grad norm: 0.284 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.911 | tokens per gpu per second (tgs): 1786.309 | TFLOPs: 14.37 |
g0184: [2024-08-10 09:43:40,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=22160, skipped=33, lr=[0.0001999580718923995, 0.0001999580718923995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22160 loss: 0.8177 iter time (s): 4.597 samples/sec: 27.843
g0198:  iteration    22160/10000000 | consumed samples:      2836480 | consumed tokens:   5809111040 | elapsed time per iteration (ms): 4631.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.187118E-01 | loss scale: 8192.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.639 | tokens per gpu per second (tgs): 1768.908 | TFLOPs: 14.23 |
g0184: [2024-08-10 09:44:24,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=22170, skipped=33, lr=[0.00019995799360039647, 0.00019995799360039647], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22170 loss: 0.8251 iter time (s): 4.354 samples/sec: 29.397
g0198:  iteration    22170/10000000 | consumed samples:      2837760 | consumed tokens:   5811732480 | elapsed time per iteration (ms): 4387.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.180136E-01 | loss scale: 8192.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.174 | tokens per gpu per second (tgs): 1867.138 | TFLOPs: 15.03 |
g0184: [2024-08-10 09:45:14,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=22180, skipped=33, lr=[0.0001999579152353807, 0.0001999579152353807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22180 loss: 0.8147 iter time (s): 4.980 samples/sec: 25.705
g0198:  iteration    22180/10000000 | consumed samples:      2839040 | consumed tokens:   5814353920 | elapsed time per iteration (ms): 5012.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.180552E-01 | loss scale: 8192.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.537 | tokens per gpu per second (tgs): 1634.345 | TFLOPs: 13.15 |
g0184: [2024-08-10 09:46:01,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=22190, skipped=33, lr=[0.0001999578367973524, 0.0001999578367973524], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22190 loss: 0.8319 iter time (s): 4.675 samples/sec: 27.379
g0198:  iteration    22190/10000000 | consumed samples:      2840320 | consumed tokens:   5816975360 | elapsed time per iteration (ms): 4707.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.140718E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.189 | tokens per gpu per second (tgs): 1740.092 | TFLOPs: 14.00 |
g0184: [2024-08-10 09:46:49,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=22200, skipped=33, lr=[0.00019995775828631152, 0.00019995775828631152], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22200 loss: 0.8090 iter time (s): 4.774 samples/sec: 26.812
g0198:  iteration    22200/10000000 | consumed samples:      2841600 | consumed tokens:   5819596800 | elapsed time per iteration (ms): 4806.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.248541E-01 | loss scale: 8192.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.630 | tokens per gpu per second (tgs): 1704.331 | TFLOPs: 13.72 |
g0184: [2024-08-10 09:47:33,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=22210, skipped=33, lr=[0.00019995767970225814, 0.00019995767970225814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22210 loss: 0.8181 iter time (s): 4.363 samples/sec: 29.340
g0198:  iteration    22210/10000000 | consumed samples:      2842880 | consumed tokens:   5822218240 | elapsed time per iteration (ms): 4395.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.202969E-01 | loss scale: 8192.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.119 | tokens per gpu per second (tgs): 1863.644 | TFLOPs: 15.00 |
g0184: [2024-08-10 09:48:36,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=22220, skipped=33, lr=[0.00019995760104519236, 0.00019995760104519236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22220 loss: 0.8203 iter time (s): 6.228 samples/sec: 20.551
g0198:  iteration    22220/10000000 | consumed samples:      2844160 | consumed tokens:   5824839680 | elapsed time per iteration (ms): 6262.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.172390E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.440 | tokens per gpu per second (tgs): 1308.129 | TFLOPs: 10.53 |
g0184: [2024-08-10 09:49:42,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=22230, skipped=33, lr=[0.00019995752231511418, 0.00019995752231511418], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22230 loss: 0.8011 iter time (s): 6.549 samples/sec: 19.544
g0198:  iteration    22230/10000000 | consumed samples:      2845440 | consumed tokens:   5827461120 | elapsed time per iteration (ms): 6582.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.175883E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.447 | tokens per gpu per second (tgs): 1244.587 | TFLOPs: 10.02 |
g0184: [2024-08-10 09:50:35,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=22240, skipped=33, lr=[0.00019995744351202375, 0.00019995744351202375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22240 loss: 0.8130 iter time (s): 5.324 samples/sec: 24.043
g0198:  iteration    22240/10000000 | consumed samples:      2846720 | consumed tokens:   5830082560 | elapsed time per iteration (ms): 5356.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.184034E-01 | loss scale: 8192.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.894 | tokens per gpu per second (tgs): 1529.247 | TFLOPs: 12.31 |
g0184: [2024-08-10 09:51:21,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=22250, skipped=33, lr=[0.00019995736463592102, 0.00019995736463592102], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22250 loss: 0.8135 iter time (s): 4.534 samples/sec: 28.234
g0198:  iteration    22250/10000000 | consumed samples:      2848000 | consumed tokens:   5832704000 | elapsed time per iteration (ms): 4566.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.083151E-01 | loss scale: 8192.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.029 | tokens per gpu per second (tgs): 1793.830 | TFLOPs: 14.44 |
g0184: [2024-08-10 09:52:07,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=22260, skipped=33, lr=[0.00019995728568680614, 0.00019995728568680614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22260 loss: 0.8028 iter time (s): 4.559 samples/sec: 28.074
g0198:  iteration    22260/10000000 | consumed samples:      2849280 | consumed tokens:   5835325440 | elapsed time per iteration (ms): 4600.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.210303E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.821 | tokens per gpu per second (tgs): 1780.522 | TFLOPs: 14.33 |
g0184: [2024-08-10 09:52:58,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=22270, skipped=33, lr=[0.00019995720666467911, 0.00019995720666467911], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22270 loss: 0.8137 iter time (s): 5.105 samples/sec: 25.075
g0198:  iteration    22270/10000000 | consumed samples:      2850560 | consumed tokens:   5837946880 | elapsed time per iteration (ms): 5137.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.127444E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.914 | tokens per gpu per second (tgs): 1594.466 | TFLOPs: 12.83 |
g0184: [2024-08-10 09:53:42,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=22280, skipped=33, lr=[0.00019995712756954, 0.00019995712756954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22280 loss: 0.8378 iter time (s): 4.296 samples/sec: 29.795
g0198:  iteration    22280/10000000 | consumed samples:      2851840 | consumed tokens:   5840568320 | elapsed time per iteration (ms): 4328.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.104295E-01 | loss scale: 8192.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.571 | tokens per gpu per second (tgs): 1892.530 | TFLOPs: 15.23 |
g0184: [2024-08-10 09:54:29,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=22290, skipped=33, lr=[0.00019995704840138892, 0.00019995704840138892], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22290 loss: 0.7687 iter time (s): 4.693 samples/sec: 27.274
g0198:  iteration    22290/10000000 | consumed samples:      2853120 | consumed tokens:   5843189760 | elapsed time per iteration (ms): 4726.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.092265E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.083 | tokens per gpu per second (tgs): 1733.309 | TFLOPs: 13.95 |
g0184: [2024-08-10 09:55:13,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=22300, skipped=33, lr=[0.00019995696916022592, 0.00019995696916022592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22300 loss: 0.8174 iter time (s): 4.367 samples/sec: 29.308
g0198:  iteration    22300/10000000 | consumed samples:      2854400 | consumed tokens:   5845811200 | elapsed time per iteration (ms): 4401.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.179383E-01 | loss scale: 8192.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.084 | tokens per gpu per second (tgs): 1861.406 | TFLOPs: 14.98 |
g0184: [2024-08-10 09:56:02,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=22310, skipped=33, lr=[0.000199956889846051, 0.000199956889846051], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22310 loss: 0.7952 iter time (s): 4.878 samples/sec: 26.238
g0198:  iteration    22310/10000000 | consumed samples:      2855680 | consumed tokens:   5848432640 | elapsed time per iteration (ms): 4911.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.229978E-01 | loss scale: 8192.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.062 | tokens per gpu per second (tgs): 1667.977 | TFLOPs: 13.42 |
g0184: [2024-08-10 09:56:53,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=22320, skipped=33, lr=[0.00019995681045886427, 0.00019995681045886427], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22320 loss: 0.8126 iter time (s): 5.031 samples/sec: 25.444
g0198:  iteration    22320/10000000 | consumed samples:      2856960 | consumed tokens:   5851054080 | elapsed time per iteration (ms): 5063.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.087622E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.277 | tokens per gpu per second (tgs): 1617.753 | TFLOPs: 13.02 |
g0184: [2024-08-10 09:57:40,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=22330, skipped=33, lr=[0.00019995673099866578, 0.00019995673099866578], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22330 loss: 0.8495 iter time (s): 4.757 samples/sec: 26.905
g0198:  iteration    22330/10000000 | consumed samples:      2858240 | consumed tokens:   5853675520 | elapsed time per iteration (ms): 4790.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.171714E-01 | loss scale: 8192.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.719 | tokens per gpu per second (tgs): 1710.033 | TFLOPs: 13.76 |
g0184: [2024-08-10 09:58:26,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=22340, skipped=33, lr=[0.0001999566514654556, 0.0001999566514654556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22340 loss: 0.8131 iter time (s): 4.484 samples/sec: 28.548
g0198:  iteration    22340/10000000 | consumed samples:      2859520 | consumed tokens:   5856296960 | elapsed time per iteration (ms): 4516.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.108188E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.340 | tokens per gpu per second (tgs): 1813.736 | TFLOPs: 14.60 |
g0184: [2024-08-10 09:59:09,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=22350, skipped=33, lr=[0.0001999565718592338, 0.0001999565718592338], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22350 loss: 0.7855 iter time (s): 4.351 samples/sec: 29.420
g0198:  iteration    22350/10000000 | consumed samples:      2860800 | consumed tokens:   5858918400 | elapsed time per iteration (ms): 4383.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.069700E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.202 | tokens per gpu per second (tgs): 1868.936 | TFLOPs: 15.04 |
g0184: [2024-08-10 10:00:00,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=22360, skipped=33, lr=[0.0001999564921800004, 0.0001999564921800004], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22360 loss: 0.8173 iter time (s): 5.037 samples/sec: 25.410
g0198:  iteration    22360/10000000 | consumed samples:      2862080 | consumed tokens:   5861539840 | elapsed time per iteration (ms): 5085.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.069668E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.171 | tokens per gpu per second (tgs): 1610.916 | TFLOPs: 12.96 |
g0184: [2024-08-10 10:00:45,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=22370, skipped=33, lr=[0.0001999564124277555, 0.0001999564124277555], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22370 loss: 0.8357 iter time (s): 4.420 samples/sec: 28.959
g0198:  iteration    22370/10000000 | consumed samples:      2863360 | consumed tokens:   5864161280 | elapsed time per iteration (ms): 4452.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.140615E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.748 | tokens per gpu per second (tgs): 1839.851 | TFLOPs: 14.81 |
g0184: [2024-08-10 10:01:28,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=22380, skipped=33, lr=[0.0001999563326024991, 0.0001999563326024991], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22380 loss: 0.8133 iter time (s): 4.248 samples/sec: 30.132
g0198:  iteration    22380/10000000 | consumed samples:      2864640 | consumed tokens:   5866782720 | elapsed time per iteration (ms): 4281.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.125048E-01 | loss scale: 8192.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.899 | tokens per gpu per second (tgs): 1913.566 | TFLOPs: 15.40 |
g0184: [2024-08-10 10:02:15,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=22390, skipped=33, lr=[0.00019995625270423137, 0.00019995625270423137], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22390 loss: 0.7855 iter time (s): 4.691 samples/sec: 27.284
g0198:  iteration    22390/10000000 | consumed samples:      2865920 | consumed tokens:   5869404160 | elapsed time per iteration (ms): 4724.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.178221E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.096 | tokens per gpu per second (tgs): 1734.133 | TFLOPs: 13.95 |
g0184: [2024-08-10 10:03:00,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=22400, skipped=33, lr=[0.00019995617273295226, 0.00019995617273295226], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22400 loss: 0.7709 iter time (s): 4.462 samples/sec: 28.689
g0198:  iteration    22400/10000000 | consumed samples:      2867200 | consumed tokens:   5872025600 | elapsed time per iteration (ms): 4494.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.087587E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.478 | tokens per gpu per second (tgs): 1822.622 | TFLOPs: 14.67 |
g0184: [2024-08-10 10:03:42,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=22410, skipped=33, lr=[0.00019995609268866194, 0.00019995609268866194], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22410 loss: 0.8090 iter time (s): 4.206 samples/sec: 30.429
g0198:  iteration    22410/10000000 | consumed samples:      2868480 | consumed tokens:   5874647040 | elapsed time per iteration (ms): 4240.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.174006E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.184 | tokens per gpu per second (tgs): 1931.799 | TFLOPs: 15.55 |
g0184: [2024-08-10 10:04:27,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=22420, skipped=33, lr=[0.0001999560125713604, 0.0001999560125713604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22420 loss: 0.8130 iter time (s): 4.467 samples/sec: 28.653
g0198:  iteration    22420/10000000 | consumed samples:      2869760 | consumed tokens:   5877268480 | elapsed time per iteration (ms): 4500.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.099038E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.444 | tokens per gpu per second (tgs): 1820.446 | TFLOPs: 14.65 |
g0185: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 10:04:49,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 10:04:49,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 10:05:09,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=22430, skipped=33, lr=[0.00019995593238104767, 0.00019995593238104767], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22430 loss: 0.8206 iter time (s): 4.149 samples/sec: 30.851
g0198:  iteration    22430/10000000 | consumed samples:      2871040 | consumed tokens:   5879889920 | elapsed time per iteration (ms): 4181.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.160852E-01 | loss scale: 16384.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.609 | tokens per gpu per second (tgs): 1958.958 | TFLOPs: 15.76 |
g0184: [2024-08-10 10:05:55,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=22440, skipped=33, lr=[0.00019995585211772388, 0.00019995585211772388], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22440 loss: 0.8377 iter time (s): 4.525 samples/sec: 28.286
g0198:  iteration    22440/10000000 | consumed samples:      2872320 | consumed tokens:   5882511360 | elapsed time per iteration (ms): 4558.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.114114E-01 | loss scale: 16384.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.078 | tokens per gpu per second (tgs): 1797.008 | TFLOPs: 14.46 |
g0184: [2024-08-10 10:06:38,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=22450, skipped=33, lr=[0.00019995577178138907, 0.00019995577178138907], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22450 loss: 0.8055 iter time (s): 4.270 samples/sec: 29.977
g0198:  iteration    22450/10000000 | consumed samples:      2873600 | consumed tokens:   5885132800 | elapsed time per iteration (ms): 4302.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.157592E-01 | loss scale: 16384.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.749 | tokens per gpu per second (tgs): 1903.957 | TFLOPs: 15.32 |
g0184: [2024-08-10 10:07:20,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=22460, skipped=33, lr=[0.00019995569137204333, 0.00019995569137204333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22460 loss: 0.8120 iter time (s): 4.226 samples/sec: 30.288
g0198:  iteration    22460/10000000 | consumed samples:      2874880 | consumed tokens:   5887754240 | elapsed time per iteration (ms): 4258.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.226534E-01 | loss scale: 16384.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.055 | tokens per gpu per second (tgs): 1923.533 | TFLOPs: 15.48 |
g0184: [2024-08-10 10:08:03,844] [INFO] [logging.py:96:log_dist] [Rank 0] step=22470, skipped=33, lr=[0.00019995561088968666, 0.00019995561088968666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22470 loss: 0.8402 iter time (s): 4.276 samples/sec: 29.932
g0198:  iteration    22470/10000000 | consumed samples:      2876160 | consumed tokens:   5890375680 | elapsed time per iteration (ms): 4310.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.096285E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.697 | tokens per gpu per second (tgs): 1900.585 | TFLOPs: 15.29 |
g0184: [2024-08-10 10:08:47,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=22480, skipped=33, lr=[0.00019995553033431914, 0.00019995553033431914], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22480 loss: 0.8247 iter time (s): 4.376 samples/sec: 29.249
g0198:  iteration    22480/10000000 | consumed samples:      2877440 | consumed tokens:   5892997120 | elapsed time per iteration (ms): 4408.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.067129E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.035 | tokens per gpu per second (tgs): 1858.210 | TFLOPs: 14.95 |
g0184: [2024-08-10 10:09:31,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=22490, skipped=33, lr=[0.00019995544970594087, 0.00019995544970594087], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22490 loss: 0.8090 iter time (s): 4.316 samples/sec: 29.655
g0198:  iteration    22490/10000000 | consumed samples:      2878720 | consumed tokens:   5895618560 | elapsed time per iteration (ms): 4348.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.114824E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.433 | tokens per gpu per second (tgs): 1883.728 | TFLOPs: 15.16 |
g0184: [2024-08-10 10:10:15,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=22500, skipped=33, lr=[0.00019995536900455186, 0.00019995536900455186], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22500 loss: 0.8056 iter time (s): 4.394 samples/sec: 29.131
g0198:  iteration    22500/10000000 | consumed samples:      2880000 | consumed tokens:   5898240000 | elapsed time per iteration (ms): 4426.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.073975E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.918 | tokens per gpu per second (tgs): 1850.730 | TFLOPs: 14.89 |
g0184: [2024-08-10 10:11:03,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=22510, skipped=33, lr=[0.00019995528823015224, 0.00019995528823015224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22510 loss: 0.8138 iter time (s): 4.709 samples/sec: 27.181
g0198:  iteration    22510/10000000 | consumed samples:      2881280 | consumed tokens:   5900861440 | elapsed time per iteration (ms): 4742.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.109528E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.991 | tokens per gpu per second (tgs): 1727.434 | TFLOPs: 13.90 |
g0184: [2024-08-10 10:11:47,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=22520, skipped=33, lr=[0.000199955207382742, 0.000199955207382742], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22520 loss: 0.8482 iter time (s): 4.353 samples/sec: 29.402
g0198:  iteration    22520/10000000 | consumed samples:      2882560 | consumed tokens:   5903482880 | elapsed time per iteration (ms): 4392.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.126247E-01 | loss scale: 16384.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.143 | tokens per gpu per second (tgs): 1865.139 | TFLOPs: 15.01 |
g0184: [2024-08-10 10:12:30,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=22530, skipped=33, lr=[0.00019995512646232125, 0.00019995512646232125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22530 loss: 0.7991 iter time (s): 4.293 samples/sec: 29.819
g0198:  iteration    22530/10000000 | consumed samples:      2883840 | consumed tokens:   5906104320 | elapsed time per iteration (ms): 4326.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.166471E-01 | loss scale: 16384.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.582 | tokens per gpu per second (tgs): 1893.269 | TFLOPs: 15.24 |
g0184: [2024-08-10 10:13:17,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=22540, skipped=33, lr=[0.00019995504546889005, 0.00019995504546889005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22540 loss: 0.8240 iter time (s): 4.693 samples/sec: 27.273
g0198:  iteration    22540/10000000 | consumed samples:      2885120 | consumed tokens:   5908725760 | elapsed time per iteration (ms): 4727.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.085649E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.077 | tokens per gpu per second (tgs): 1732.929 | TFLOPs: 13.95 |
g0184: [2024-08-10 10:14:02,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=22550, skipped=33, lr=[0.0001999549644024484, 0.0001999549644024484], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22550 loss: 0.7799 iter time (s): 4.442 samples/sec: 28.817
g0198:  iteration    22550/10000000 | consumed samples:      2886400 | consumed tokens:   5911347200 | elapsed time per iteration (ms): 4474.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.170510E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.605 | tokens per gpu per second (tgs): 1830.723 | TFLOPs: 14.73 |
g0184: [2024-08-10 10:14:45,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=22560, skipped=33, lr=[0.00019995488326299648, 0.00019995488326299648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22560 loss: 0.8376 iter time (s): 4.267 samples/sec: 29.999
g0198:  iteration    22560/10000000 | consumed samples:      2887680 | consumed tokens:   5913968640 | elapsed time per iteration (ms): 4322.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.124295E-01 | loss scale: 16384.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.614 | tokens per gpu per second (tgs): 1895.279 | TFLOPs: 15.25 |
g0184: [2024-08-10 10:15:33,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=22570, skipped=33, lr=[0.00019995480205053424, 0.00019995480205053424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22570 loss: 0.8228 iter time (s): 4.748 samples/sec: 26.958
g0198:  iteration    22570/10000000 | consumed samples:      2888960 | consumed tokens:   5916590080 | elapsed time per iteration (ms): 4780.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.179534E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.775 | tokens per gpu per second (tgs): 1713.587 | TFLOPs: 13.79 |
g0184: [2024-08-10 10:16:18,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=22580, skipped=33, lr=[0.0001999547207650618, 0.0001999547207650618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22580 loss: 0.8272 iter time (s): 4.497 samples/sec: 28.461
g0198:  iteration    22580/10000000 | consumed samples:      2890240 | consumed tokens:   5919211520 | elapsed time per iteration (ms): 4530.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.048819E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.253 | tokens per gpu per second (tgs): 1808.223 | TFLOPs: 14.55 |
g0184: [2024-08-10 10:17:03,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=22590, skipped=33, lr=[0.0001999546394065792, 0.0001999546394065792], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22590 loss: 0.8400 iter time (s): 4.482 samples/sec: 28.559
g0198:  iteration    22590/10000000 | consumed samples:      2891520 | consumed tokens:   5921832960 | elapsed time per iteration (ms): 4514.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.154683E-01 | loss scale: 16384.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.352 | tokens per gpu per second (tgs): 1814.543 | TFLOPs: 14.60 |
g0184: [2024-08-10 10:17:48,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=22600, skipped=33, lr=[0.0001999545579750865, 0.0001999545579750865], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22600 loss: 0.8081 iter time (s): 4.483 samples/sec: 28.552
g0198:  iteration    22600/10000000 | consumed samples:      2892800 | consumed tokens:   5924454400 | elapsed time per iteration (ms): 4515.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.097771E-01 | loss scale: 16384.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.347 | tokens per gpu per second (tgs): 1814.221 | TFLOPs: 14.60 |
g0184: [2024-08-10 10:18:35,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=22610, skipped=33, lr=[0.0001999544764705838, 0.0001999544764705838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22610 loss: 0.8010 iter time (s): 4.604 samples/sec: 27.803
g0198:  iteration    22610/10000000 | consumed samples:      2894080 | consumed tokens:   5927075840 | elapsed time per iteration (ms): 4637.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.065792E-01 | loss scale: 16384.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.601 | tokens per gpu per second (tgs): 1766.450 | TFLOPs: 14.21 |
g0184: [2024-08-10 10:19:24,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=22620, skipped=33, lr=[0.00019995439489307111, 0.00019995439489307111], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22620 loss: 0.8013 iter time (s): 4.926 samples/sec: 25.983
g0198:  iteration    22620/10000000 | consumed samples:      2895360 | consumed tokens:   5929697280 | elapsed time per iteration (ms): 4958.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.101444E-01 | loss scale: 16384.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.812 | tokens per gpu per second (tgs): 1651.976 | TFLOPs: 13.29 |
g0184: [2024-08-10 10:20:09,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=22630, skipped=33, lr=[0.0001999543132425485, 0.0001999543132425485], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22630 loss: 0.8223 iter time (s): 4.472 samples/sec: 28.622
g0198:  iteration    22630/10000000 | consumed samples:      2896640 | consumed tokens:   5932318720 | elapsed time per iteration (ms): 4504.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.195359E-01 | loss scale: 16384.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.413 | tokens per gpu per second (tgs): 1818.458 | TFLOPs: 14.63 |
g0184: [2024-08-10 10:20:54,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=22640, skipped=33, lr=[0.0001999542315190161, 0.0001999542315190161], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22640 loss: 0.7983 iter time (s): 4.458 samples/sec: 28.715
g0198:  iteration    22640/10000000 | consumed samples:      2897920 | consumed tokens:   5934940160 | elapsed time per iteration (ms): 4490.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.125402E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.506 | tokens per gpu per second (tgs): 1824.372 | TFLOPs: 14.68 |
g0184: [2024-08-10 10:21:39,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=22650, skipped=33, lr=[0.0001999541497224739, 0.0001999541497224739], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22650 loss: 0.8488 iter time (s): 4.445 samples/sec: 28.799
g0198:  iteration    22650/10000000 | consumed samples:      2899200 | consumed tokens:   5937561600 | elapsed time per iteration (ms): 4477.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.151387E-01 | loss scale: 16384.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.588 | tokens per gpu per second (tgs): 1829.629 | TFLOPs: 14.72 |
g0184: [2024-08-10 10:22:26,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=22660, skipped=33, lr=[0.000199954067852922, 0.000199954067852922], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22660 loss: 0.8002 iter time (s): 4.636 samples/sec: 27.608
g0198:  iteration    22660/10000000 | consumed samples:      2900480 | consumed tokens:   5940183040 | elapsed time per iteration (ms): 4669.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.108192E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.414 | tokens per gpu per second (tgs): 1754.480 | TFLOPs: 14.12 |
g0184: [2024-08-10 10:23:14,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=22670, skipped=33, lr=[0.00019995398591036043, 0.00019995398591036043], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22670 loss: 0.8642 iter time (s): 4.776 samples/sec: 26.800
g0198:  iteration    22670/10000000 | consumed samples:      2901760 | consumed tokens:   5942804480 | elapsed time per iteration (ms): 4809.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.168036E-01 | loss scale: 16384.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.613 | tokens per gpu per second (tgs): 1703.258 | TFLOPs: 13.71 |
g0184: [2024-08-10 10:23:59,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=22680, skipped=33, lr=[0.00019995390389478929, 0.00019995390389478929], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22680 loss: 0.7921 iter time (s): 4.437 samples/sec: 28.847
g0198:  iteration    22680/10000000 | consumed samples:      2903040 | consumed tokens:   5945425920 | elapsed time per iteration (ms): 4469.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.059057E-01 | loss scale: 16384.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.636 | tokens per gpu per second (tgs): 1832.723 | TFLOPs: 14.75 |
g0184: [2024-08-10 10:24:47,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=22690, skipped=33, lr=[0.00019995382180620862, 0.00019995382180620862], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22690 loss: 0.8444 iter time (s): 4.800 samples/sec: 26.667
g0198:  iteration    22690/10000000 | consumed samples:      2904320 | consumed tokens:   5948047360 | elapsed time per iteration (ms): 4832.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.112129E-01 | loss scale: 16384.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.488 | tokens per gpu per second (tgs): 1695.245 | TFLOPs: 13.64 |
g0184: [2024-08-10 10:25:31,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=22700, skipped=33, lr=[0.0001999537396446185, 0.0001999537396446185], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22700 loss: 0.8017 iter time (s): 4.419 samples/sec: 28.969
g0198:  iteration    22700/10000000 | consumed samples:      2905600 | consumed tokens:   5950668800 | elapsed time per iteration (ms): 4453.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.107991E-01 | loss scale: 16384.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.743 | tokens per gpu per second (tgs): 1839.539 | TFLOPs: 14.80 |
g0184: [2024-08-10 10:26:17,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=22710, skipped=33, lr=[0.00019995365741001898, 0.00019995365741001898], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22710 loss: 0.7768 iter time (s): 4.557 samples/sec: 28.091
g0198:  iteration    22710/10000000 | consumed samples:      2906880 | consumed tokens:   5953290240 | elapsed time per iteration (ms): 4589.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.060312E-01 | loss scale: 16384.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.891 | tokens per gpu per second (tgs): 1785.048 | TFLOPs: 14.36 |
g0184: [2024-08-10 10:27:03,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=22720, skipped=33, lr=[0.0001999535751024101, 0.0001999535751024101], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22720 loss: 0.7872 iter time (s): 4.573 samples/sec: 27.990
g0198:  iteration    22720/10000000 | consumed samples:      2908160 | consumed tokens:   5955911680 | elapsed time per iteration (ms): 4605.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.102013E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.792 | tokens per gpu per second (tgs): 1778.673 | TFLOPs: 14.31 |
g0184: [2024-08-10 10:27:50,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=22730, skipped=33, lr=[0.00019995349272179197, 0.00019995349272179197], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22730 loss: 0.8359 iter time (s): 4.608 samples/sec: 27.778
g0198:  iteration    22730/10000000 | consumed samples:      2909440 | consumed tokens:   5958533120 | elapsed time per iteration (ms): 4640.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.017200E-01 | loss scale: 16384.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.583 | tokens per gpu per second (tgs): 1765.296 | TFLOPs: 14.21 |
g0184: [2024-08-10 10:28:38,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=22740, skipped=33, lr=[0.00019995341026816463, 0.00019995341026816463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22740 loss: 0.8241 iter time (s): 4.804 samples/sec: 26.643
g0198:  iteration    22740/10000000 | consumed samples:      2910720 | consumed tokens:   5961154560 | elapsed time per iteration (ms): 4841.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.118524E-01 | loss scale: 16384.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.438 | tokens per gpu per second (tgs): 1692.018 | TFLOPs: 13.62 |
g0184: [2024-08-10 10:29:23,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=22750, skipped=33, lr=[0.00019995332774152817, 0.00019995332774152817], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22750 loss: 0.7893 iter time (s): 4.430 samples/sec: 28.893
g0198:  iteration    22750/10000000 | consumed samples:      2912000 | consumed tokens:   5963776000 | elapsed time per iteration (ms): 4463.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.035325E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.679 | tokens per gpu per second (tgs): 1835.443 | TFLOPs: 14.77 |
g0184: [2024-08-10 10:30:09,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=22760, skipped=33, lr=[0.0001999532451418826, 0.0001999532451418826], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22760 loss: 0.8038 iter time (s): 4.541 samples/sec: 28.187
g0198:  iteration    22760/10000000 | consumed samples:      2913280 | consumed tokens:   5966397440 | elapsed time per iteration (ms): 4586.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.116372E-01 | loss scale: 16384.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.906 | tokens per gpu per second (tgs): 1785.988 | TFLOPs: 14.37 |
g0184: [2024-08-10 10:30:55,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=22770, skipped=33, lr=[0.00019995316246922801, 0.00019995316246922801], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22770 loss: 0.8171 iter time (s): 4.639 samples/sec: 27.593
g0198:  iteration    22770/10000000 | consumed samples:      2914560 | consumed tokens:   5969018880 | elapsed time per iteration (ms): 4671.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.181252E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.401 | tokens per gpu per second (tgs): 1753.667 | TFLOPs: 14.11 |
g0184: [2024-08-10 10:31:43,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=22780, skipped=33, lr=[0.00019995307972356448, 0.00019995307972356448], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22780 loss: 0.8212 iter time (s): 4.717 samples/sec: 27.135
g0198:  iteration    22780/10000000 | consumed samples:      2915840 | consumed tokens:   5971640320 | elapsed time per iteration (ms): 4749.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.144855E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.948 | tokens per gpu per second (tgs): 1724.694 | TFLOPs: 13.88 |
g0184: [2024-08-10 10:32:36,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=22790, skipped=33, lr=[0.00019995299690489204, 0.00019995299690489204], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22790 loss: 0.7840 iter time (s): 5.255 samples/sec: 24.357
g0198:  iteration    22790/10000000 | consumed samples:      2917120 | consumed tokens:   5974261760 | elapsed time per iteration (ms): 5287.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.180713E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.207 | tokens per gpu per second (tgs): 1549.227 | TFLOPs: 12.47 |
g0184: [2024-08-10 10:33:25,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=22800, skipped=33, lr=[0.00019995291401321078, 0.00019995291401321078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22800 loss: 0.7895 iter time (s): 4.893 samples/sec: 26.161
g0198:  iteration    22800/10000000 | consumed samples:      2918400 | consumed tokens:   5976883200 | elapsed time per iteration (ms): 4925.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.163721E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.987 | tokens per gpu per second (tgs): 1663.158 | TFLOPs: 13.38 |
g0184: [2024-08-10 10:34:27,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=22810, skipped=33, lr=[0.00019995283104852078, 0.00019995283104852078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22810 loss: 0.8030 iter time (s): 6.167 samples/sec: 20.755
g0198:  iteration    22810/10000000 | consumed samples:      2919680 | consumed tokens:   5979504640 | elapsed time per iteration (ms): 6200.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.220217E-01 | loss scale: 16384.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.644 | tokens per gpu per second (tgs): 1321.202 | TFLOPs: 10.63 |
g0184: [2024-08-10 10:35:17,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=22820, skipped=33, lr=[0.00019995274801082208, 0.00019995274801082208], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22820 loss: 0.8227 iter time (s): 4.917 samples/sec: 26.033
g0198:  iteration    22820/10000000 | consumed samples:      2920960 | consumed tokens:   5982126080 | elapsed time per iteration (ms): 4949.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.195722E-01 | loss scale: 16384.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.862 | tokens per gpu per second (tgs): 1655.164 | TFLOPs: 13.32 |
g0184: [2024-08-10 10:36:17,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=22830, skipped=33, lr=[0.00019995266490011472, 0.00019995266490011472], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22830 loss: 0.8295 iter time (s): 5.990 samples/sec: 21.367
g0198:  iteration    22830/10000000 | consumed samples:      2922240 | consumed tokens:   5984747520 | elapsed time per iteration (ms): 6023.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.210950E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.251 | tokens per gpu per second (tgs): 1360.063 | TFLOPs: 10.94 |
g0184: [2024-08-10 10:37:24,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=22840, skipped=33, lr=[0.00019995258171639878, 0.00019995258171639878], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22840 loss: 0.8557 iter time (s): 6.688 samples/sec: 19.139
g0198:  iteration    22840/10000000 | consumed samples:      2923520 | consumed tokens:   5987368960 | elapsed time per iteration (ms): 6721.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.137741E-01 | loss scale: 16384.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.045 | tokens per gpu per second (tgs): 1218.855 | TFLOPs: 9.81 |
g0184: [2024-08-10 10:38:13,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=22850, skipped=33, lr=[0.0001999524984596744, 0.0001999524984596744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22850 loss: 0.8088 iter time (s): 4.858 samples/sec: 26.350
g0198:  iteration    22850/10000000 | consumed samples:      2924800 | consumed tokens:   5989990400 | elapsed time per iteration (ms): 4890.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.087623E-01 | loss scale: 16384.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.174 | tokens per gpu per second (tgs): 1675.128 | TFLOPs: 13.48 |
g0184: [2024-08-10 10:38:59,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=22860, skipped=33, lr=[0.0001999524151299415, 0.0001999524151299415], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22860 loss: 0.7669 iter time (s): 4.615 samples/sec: 27.733
g0198:  iteration    22860/10000000 | consumed samples:      2926080 | consumed tokens:   5992611840 | elapsed time per iteration (ms): 4648.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.203349E-01 | loss scale: 16384.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.538 | tokens per gpu per second (tgs): 1762.464 | TFLOPs: 14.18 |
g0184: [2024-08-10 10:39:43,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=22870, skipped=33, lr=[0.00019995233172720025, 0.00019995233172720025], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22870 loss: 0.8278 iter time (s): 4.323 samples/sec: 29.612
g0198:  iteration    22870/10000000 | consumed samples:      2927360 | consumed tokens:   5995233280 | elapsed time per iteration (ms): 4355.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.061441E-01 | loss scale: 16384.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.387 | tokens per gpu per second (tgs): 1880.793 | TFLOPs: 15.14 |
g0184: [2024-08-10 10:40:26,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=22880, skipped=33, lr=[0.00019995224825145069, 0.00019995224825145069], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22880 loss: 0.8074 iter time (s): 4.256 samples/sec: 30.076
g0198:  iteration    22880/10000000 | consumed samples:      2928640 | consumed tokens:   5997854720 | elapsed time per iteration (ms): 4288.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.152876E-01 | loss scale: 16384.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.848 | tokens per gpu per second (tgs): 1910.244 | TFLOPs: 15.37 |
g0184: [2024-08-10 10:41:12,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=22890, skipped=33, lr=[0.00019995216470269288, 0.00019995216470269288], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22890 loss: 0.8056 iter time (s): 4.567 samples/sec: 28.028
g0198:  iteration    22890/10000000 | consumed samples:      2929920 | consumed tokens:   6000476160 | elapsed time per iteration (ms): 4599.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.033979E-01 | loss scale: 16384.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.829 | tokens per gpu per second (tgs): 1781.034 | TFLOPs: 14.33 |
g0184: [2024-08-10 10:41:59,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=22900, skipped=33, lr=[0.00019995208108092687, 0.00019995208108092687], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22900 loss: 0.8256 iter time (s): 4.682 samples/sec: 27.339
g0198:  iteration    22900/10000000 | consumed samples:      2931200 | consumed tokens:   6003097600 | elapsed time per iteration (ms): 4715.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.023589E-01 | loss scale: 16384.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.146 | tokens per gpu per second (tgs): 1737.346 | TFLOPs: 13.98 |
g0184: [2024-08-10 10:42:42,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=22910, skipped=33, lr=[0.00019995199738615275, 0.00019995199738615275], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22910 loss: 0.8245 iter time (s): 4.271 samples/sec: 29.972
g0198:  iteration    22910/10000000 | consumed samples:      2932480 | consumed tokens:   6005719040 | elapsed time per iteration (ms): 4303.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.152586E-01 | loss scale: 16384.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.742 | tokens per gpu per second (tgs): 1903.462 | TFLOPs: 15.32 |
g0184: [2024-08-10 10:43:31,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=22920, skipped=33, lr=[0.00019995191361837057, 0.00019995191361837057], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22920 loss: 0.8108 iter time (s): 4.843 samples/sec: 26.428
g0198:  iteration    22920/10000000 | consumed samples:      2933760 | consumed tokens:   6008340480 | elapsed time per iteration (ms): 4879.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.044407E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.233 | tokens per gpu per second (tgs): 1678.909 | TFLOPs: 13.51 |
g0185: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 10:43:56,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 10:43:56,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 10:43:56,430] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 10:44:19,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=22930, skipped=33, lr=[0.00019995182977758038, 0.00019995182977758038], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22930 loss: 0.8286 iter time (s): 4.762 samples/sec: 26.878
g0198:  iteration    22930/10000000 | consumed samples:      2935040 | consumed tokens:   6010961920 | elapsed time per iteration (ms): 4794.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.066411E-01 | loss scale: 32768.0 | grad norm: 0.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.697 | tokens per gpu per second (tgs): 1708.621 | TFLOPs: 13.75 |
g0184: [2024-08-10 10:45:04,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=22940, skipped=33, lr=[0.0001999517458637823, 0.0001999517458637823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22940 loss: 0.8048 iter time (s): 4.524 samples/sec: 28.294
g0198:  iteration    22940/10000000 | consumed samples:      2936320 | consumed tokens:   6013583360 | elapsed time per iteration (ms): 4556.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.151760E-01 | loss scale: 32768.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.092 | tokens per gpu per second (tgs): 1797.885 | TFLOPs: 14.47 |
g0184: [2024-08-10 10:45:54,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=22950, skipped=33, lr=[0.00019995166187697635, 0.00019995166187697635], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22950 loss: 0.7964 iter time (s): 4.933 samples/sec: 25.948
g0198:  iteration    22950/10000000 | consumed samples:      2937600 | consumed tokens:   6016204800 | elapsed time per iteration (ms): 4965.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.025692E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.779 | tokens per gpu per second (tgs): 1649.869 | TFLOPs: 13.28 |
g0184: [2024-08-10 10:46:53,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=22960, skipped=33, lr=[0.00019995157781716255, 0.00019995157781716255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22960 loss: 0.7962 iter time (s): 5.855 samples/sec: 21.860
g0198:  iteration    22960/10000000 | consumed samples:      2938880 | consumed tokens:   6018826240 | elapsed time per iteration (ms): 5891.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.188822E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.725 | tokens per gpu per second (tgs): 1390.405 | TFLOPs: 11.19 |
g0184: [2024-08-10 10:47:37,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=22970, skipped=33, lr=[0.00019995149368434107, 0.00019995149368434107], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22970 loss: 0.7800 iter time (s): 4.419 samples/sec: 28.965
g0198:  iteration    22970/10000000 | consumed samples:      2940160 | consumed tokens:   6021447680 | elapsed time per iteration (ms): 4451.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.069743E-01 | loss scale: 32768.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.752 | tokens per gpu per second (tgs): 1840.152 | TFLOPs: 14.81 |
g0184: [2024-08-10 10:48:24,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=22980, skipped=33, lr=[0.00019995140947851189, 0.00019995140947851189], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22980 loss: 0.7739 iter time (s): 4.568 samples/sec: 28.021
g0198:  iteration    22980/10000000 | consumed samples:      2941440 | consumed tokens:   6024069120 | elapsed time per iteration (ms): 4605.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.140598E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.790 | tokens per gpu per second (tgs): 1778.584 | TFLOPs: 14.31 |
g0184: [2024-08-10 10:49:12,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=22990, skipped=33, lr=[0.0001999513251996751, 0.0001999513251996751], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 22990 loss: 0.8408 iter time (s): 4.783 samples/sec: 26.761
g0198:  iteration    22990/10000000 | consumed samples:      2942720 | consumed tokens:   6026690560 | elapsed time per iteration (ms): 4815.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.090187E-01 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.581 | tokens per gpu per second (tgs): 1701.171 | TFLOPs: 13.69 |
g0184: [2024-08-10 10:50:00,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=33, lr=[0.00019995124084783077, 0.00019995124084783077], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23000 loss: 0.8133 iter time (s): 4.819 samples/sec: 26.563
g0198:  iteration    23000/10000000 | consumed samples:      2944000 | consumed tokens:   6029312000 | elapsed time per iteration (ms): 4851.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.027360E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.384 | tokens per gpu per second (tgs): 1688.585 | TFLOPs: 13.59 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 23000 | lm loss value: 8.093579E-01 | lm loss PPL: 2.246465E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   23000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 10:58:18,382] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step23000 is about to be saved!
g0198: [2024-08-10 10:58:18,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0198: [2024-08-10 10:58:18,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0198: [2024-08-10 10:58:18,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0184: [2024-08-10 10:58:18,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0184: [2024-08-10 10:58:18,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0184: [2024-08-10 10:58:18,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0197: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0197: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0197: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0188: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0188: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0188: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0194: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0194: [2024-08-10 10:58:18,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0194: [2024-08-10 10:58:18,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0187: [2024-08-10 10:58:18,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0187: [2024-08-10 10:58:18,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0187: [2024-08-10 10:58:18,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0185: [2024-08-10 10:58:18,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0185: [2024-08-10 10:58:18,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0185: [2024-08-10 10:58:18,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0195: [2024-08-10 10:58:18,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0195: [2024-08-10 10:58:18,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0195: [2024-08-10 10:58:18,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0198: [2024-08-10 10:58:18,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_23-model_00-model_states.pt...
g0188: [2024-08-10 10:58:18,422] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_11-model_00-model_states.pt...
g0187: [2024-08-10 10:58:18,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_08-model_00-model_states.pt...
g0194: [2024-08-10 10:58:18,428] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_14-model_00-model_states.pt...
g0197: [2024-08-10 10:58:18,429] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_20-model_00-model_states.pt...
g0195: [2024-08-10 10:58:18,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_17-model_00-model_states.pt...
g0185: [2024-08-10 10:58:18,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_05-model_00-model_states.pt...
g0184: [2024-08-10 10:58:18,438] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_01-model_00-model_states.pt...
g0195: [2024-08-10 10:58:18,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_17-model_00-model_states.pt.
g0185: [2024-08-10 10:58:18,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_05-model_00-model_states.pt.
g0197: [2024-08-10 10:58:18,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_20-model_00-model_states.pt.
g0187: [2024-08-10 10:58:18,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_08-model_00-model_states.pt.
g0194: [2024-08-10 10:58:18,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_14-model_00-model_states.pt.
g0188: [2024-08-10 10:58:18,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_11-model_00-model_states.pt.
g0195: [2024-08-10 10:58:18,626] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_18-model_00-model_states.pt...
g0185: [2024-08-10 10:58:18,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_06-model_00-model_states.pt...
g0198: [2024-08-10 10:58:18,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 10:58:18,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_24-model_00-model_states.pt...
g0197: [2024-08-10 10:58:18,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_21-model_00-model_states.pt...
g0198: [2024-08-10 10:58:18,634] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_24-model_00-model_states.pt.
g0187: [2024-08-10 10:58:18,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_09-model_00-model_states.pt...
g0188: [2024-08-10 10:58:18,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_12-model_00-model_states.pt...
g0194: [2024-08-10 10:58:18,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_15-model_00-model_states.pt...
g0198: [2024-08-10 10:58:18,678] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_25-model_00-model_states.pt...
g0184: [2024-08-10 10:58:18,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 10:58:18,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_02-model_00-model_states.pt...
g0195: [2024-08-10 10:58:18,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_18-model_00-model_states.pt.
g0188: [2024-08-10 10:58:18,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_12-model_00-model_states.pt.
g0194: [2024-08-10 10:58:18,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_15-model_00-model_states.pt.
g0197: [2024-08-10 10:58:18,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_21-model_00-model_states.pt.
g0185: [2024-08-10 10:58:18,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_06-model_00-model_states.pt.
g0188: [2024-08-10 10:58:18,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_13-model_00-model_states.pt...
g0195: [2024-08-10 10:58:18,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_19-model_00-model_states.pt...
g0194: [2024-08-10 10:58:18,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_16-model_00-model_states.pt...
g0197: [2024-08-10 10:58:18,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_22-model_00-model_states.pt...
g0185: [2024-08-10 10:58:18,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_07-model_00-model_states.pt...
g0198: [2024-08-10 10:58:18,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 10:58:18,857] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_07_model_states.pt...
g0184: [2024-08-10 10:58:18,912] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_02-model_00-model_states.pt.
g0195: [2024-08-10 10:58:18,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_19-model_00-model_states.pt.
g0188: [2024-08-10 10:58:18,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_13-model_00-model_states.pt.
g0195: [2024-08-10 10:58:18,921] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_05_model_states.pt...
g0188: [2024-08-10 10:58:18,921] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_03_model_states.pt...
g0185: [2024-08-10 10:58:18,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 10:58:18,931] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_01_model_states.pt...
g0184: [2024-08-10 10:58:18,940] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_03-model_00-model_states.pt...
g0187: [2024-08-10 10:58:18,961] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 10:58:18,988] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_10-model_00-model_states.pt...
g0197: [2024-08-10 10:58:18,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 10:58:18,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_06_model_states.pt...
g0194: [2024-08-10 10:58:19,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 10:58:19,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_04_model_states.pt...
g0184: [2024-08-10 10:58:19,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 10:58:19,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_04-model_00-model_states.pt...
g0187: [2024-08-10 10:58:19,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 10:58:19,160] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_02_model_states.pt...
g0184: [2024-08-10 10:58:19,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 10:58:19,294] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_00_model_states.pt
g0184: [2024-08-10 10:58:19,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 10:58:20,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 10:58:20,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0188: [2024-08-10 10:58:21,261] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 10:58:21,262] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0197: [2024-08-10 10:58:21,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 10:58:21,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0187: [2024-08-10 10:58:21,423] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 10:58:21,424] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0194: [2024-08-10 10:58:21,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 10:58:21,500] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0195: [2024-08-10 10:58:21,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 10:58:21,650] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0185: [2024-08-10 10:58:21,770] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 10:58:21,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0184: [2024-08-10 10:58:23,138] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step23000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 10:58:23,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23000 is ready now!
g0184:   successfully saved checkpoint at iteration   23000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.68, Latency(second): 4.816
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4815.40, 4815.80)
g0184: [2024-08-10 10:59:16,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=23010, skipped=33, lr=[0.000199951156422979, 0.000199951156422979], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23010 loss: 0.8155 iter time (s): 5.297 samples/sec: 24.166
g0198:  iteration    23010/10000000 | consumed samples:      2945280 | consumed tokens:   6031933440 | elapsed time per iteration (ms): 55576.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.040606E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.303 | tokens per gpu per second (tgs): 147.401 | TFLOPs: 1.19 |
g0184: [2024-08-10 11:00:04,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=23020, skipped=33, lr=[0.0001999510719251198, 0.0001999510719251198], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23020 loss: 0.7922 iter time (s): 4.761 samples/sec: 26.885
g0198:  iteration    23020/10000000 | consumed samples:      2946560 | consumed tokens:   6034554880 | elapsed time per iteration (ms): 4794.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.053941E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.700 | tokens per gpu per second (tgs): 1708.819 | TFLOPs: 13.75 |
g0184: [2024-08-10 11:00:49,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=23030, skipped=33, lr=[0.00019995098735425322, 0.00019995098735425322], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23030 loss: 0.8089 iter time (s): 4.508 samples/sec: 28.395
g0198:  iteration    23030/10000000 | consumed samples:      2947840 | consumed tokens:   6037176320 | elapsed time per iteration (ms): 4540.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.063343E-01 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.193 | tokens per gpu per second (tgs): 1804.332 | TFLOPs: 14.52 |
g0184: [2024-08-10 11:01:37,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=23040, skipped=33, lr=[0.0001999509027103794, 0.0001999509027103794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23040 loss: 0.8066 iter time (s): 4.743 samples/sec: 26.985
g0198:  iteration    23040/10000000 | consumed samples:      2949120 | consumed tokens:   6039797760 | elapsed time per iteration (ms): 4776.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.183250E-01 | loss scale: 32768.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.800 | tokens per gpu per second (tgs): 1715.169 | TFLOPs: 13.80 |
g0184: [2024-08-10 11:02:31,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=23050, skipped=33, lr=[0.00019995081799349833, 0.00019995081799349833], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23050 loss: 0.8275 iter time (s): 5.337 samples/sec: 23.986
g0198:  iteration    23050/10000000 | consumed samples:      2950400 | consumed tokens:   6042419200 | elapsed time per iteration (ms): 5391.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.147568E-01 | loss scale: 32768.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.743 | tokens per gpu per second (tgs): 1519.559 | TFLOPs: 12.23 |
g0184: [2024-08-10 11:03:20,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=23060, skipped=33, lr=[0.00019995073320361014, 0.00019995073320361014], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23060 loss: 0.8298 iter time (s): 4.833 samples/sec: 26.485
g0198:  iteration    23060/10000000 | consumed samples:      2951680 | consumed tokens:   6045040640 | elapsed time per iteration (ms): 4865.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.048067E-01 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.306 | tokens per gpu per second (tgs): 1683.584 | TFLOPs: 13.55 |
g0184: [2024-08-10 11:04:07,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=23070, skipped=33, lr=[0.00019995064834071486, 0.00019995064834071486], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23070 loss: 0.8113 iter time (s): 4.720 samples/sec: 27.121
g0198:  iteration    23070/10000000 | consumed samples:      2952960 | consumed tokens:   6047662080 | elapsed time per iteration (ms): 4752.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.163689E-01 | loss scale: 32768.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.936 | tokens per gpu per second (tgs): 1723.898 | TFLOPs: 13.87 |
g0184: [2024-08-10 11:04:55,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=23080, skipped=33, lr=[0.00019995056340481256, 0.00019995056340481256], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23080 loss: 0.8211 iter time (s): 4.779 samples/sec: 26.782
g0198:  iteration    23080/10000000 | consumed samples:      2954240 | consumed tokens:   6050283520 | elapsed time per iteration (ms): 4811.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.085663E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.601 | tokens per gpu per second (tgs): 1702.456 | TFLOPs: 13.70 |
g0184: [2024-08-10 11:05:43,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=23090, skipped=33, lr=[0.0001999504783959033, 0.0001999504783959033], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23090 loss: 0.8241 iter time (s): 4.682 samples/sec: 27.340
g0198:  iteration    23090/10000000 | consumed samples:      2955520 | consumed tokens:   6052904960 | elapsed time per iteration (ms): 4741.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.046557E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.996 | tokens per gpu per second (tgs): 1727.766 | TFLOPs: 13.90 |
g0184: [2024-08-10 11:06:27,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=23100, skipped=33, lr=[0.00019995039331398713, 0.00019995039331398713], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23100 loss: 0.8020 iter time (s): 4.404 samples/sec: 29.063
g0198:  iteration    23100/10000000 | consumed samples:      2956800 | consumed tokens:   6055526400 | elapsed time per iteration (ms): 4436.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.045561E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.852 | tokens per gpu per second (tgs): 1846.504 | TFLOPs: 14.86 |
g0184: [2024-08-10 11:07:13,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=23110, skipped=33, lr=[0.00019995030815906414, 0.00019995030815906414], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23110 loss: 0.8182 iter time (s): 4.607 samples/sec: 27.785
g0198:  iteration    23110/10000000 | consumed samples:      2958080 | consumed tokens:   6058147840 | elapsed time per iteration (ms): 4639.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.085824E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.591 | tokens per gpu per second (tgs): 1765.837 | TFLOPs: 14.21 |
g0184: [2024-08-10 11:08:02,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=23120, skipped=33, lr=[0.00019995022293113442, 0.00019995022293113442], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23120 loss: 0.8134 iter time (s): 4.849 samples/sec: 26.395
g0198:  iteration    23120/10000000 | consumed samples:      2959360 | consumed tokens:   6060769280 | elapsed time per iteration (ms): 4882.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 7.988514E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.218 | tokens per gpu per second (tgs): 1677.937 | TFLOPs: 13.50 |
g0184: [2024-08-10 11:08:51,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=23130, skipped=33, lr=[0.00019995013763019796, 0.00019995013763019796], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23130 loss: 0.8157 iter time (s): 4.798 samples/sec: 26.677
g0198:  iteration    23130/10000000 | consumed samples:      2960640 | consumed tokens:   6063390720 | elapsed time per iteration (ms): 4830.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.035111E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.496 | tokens per gpu per second (tgs): 1695.760 | TFLOPs: 13.65 |
g0184: [2024-08-10 11:09:37,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=23140, skipped=33, lr=[0.00019995005225625491, 0.00019995005225625491], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23140 loss: 0.7919 iter time (s): 4.633 samples/sec: 27.627
g0198:  iteration    23140/10000000 | consumed samples:      2961920 | consumed tokens:   6066012160 | elapsed time per iteration (ms): 4666.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 7.948570E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.432 | tokens per gpu per second (tgs): 1755.633 | TFLOPs: 14.13 |
g0184: [2024-08-10 11:10:27,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=23150, skipped=33, lr=[0.0001999499668093053, 0.0001999499668093053], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23150 loss: 0.7887 iter time (s): 4.957 samples/sec: 25.821
g0198:  iteration    23150/10000000 | consumed samples:      2963200 | consumed tokens:   6068633600 | elapsed time per iteration (ms): 4990.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.127472E-01 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.651 | tokens per gpu per second (tgs): 1641.633 | TFLOPs: 13.21 |
g0184: [2024-08-10 11:11:12,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=23160, skipped=33, lr=[0.0001999498812893492, 0.0001999498812893492], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23160 loss: 0.7905 iter time (s): 4.494 samples/sec: 28.479
g0198:  iteration    23160/10000000 | consumed samples:      2964480 | consumed tokens:   6071255040 | elapsed time per iteration (ms): 4526.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.104853E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.275 | tokens per gpu per second (tgs): 1809.623 | TFLOPs: 14.56 |
g0184: [2024-08-10 11:11:59,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=23170, skipped=33, lr=[0.00019994979569638665, 0.00019994979569638665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23170 loss: 0.8048 iter time (s): 4.566 samples/sec: 28.036
g0198:  iteration    23170/10000000 | consumed samples:      2965760 | consumed tokens:   6073876480 | elapsed time per iteration (ms): 4636.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.155064E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.606 | tokens per gpu per second (tgs): 1766.765 | TFLOPs: 14.22 |
g0184: [2024-08-10 11:12:49,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=23180, skipped=33, lr=[0.00019994971003041773, 0.00019994971003041773], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23180 loss: 0.8197 iter time (s): 4.946 samples/sec: 25.881
g0198:  iteration    23180/10000000 | consumed samples:      2967040 | consumed tokens:   6076497920 | elapsed time per iteration (ms): 4978.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.151592E-01 | loss scale: 32768.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.710 | tokens per gpu per second (tgs): 1645.454 | TFLOPs: 13.24 |
g0184: [2024-08-10 11:13:35,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=23190, skipped=33, lr=[0.00019994962429144254, 0.00019994962429144254], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23190 loss: 0.8237 iter time (s): 4.652 samples/sec: 27.514
g0198:  iteration    23190/10000000 | consumed samples:      2968320 | consumed tokens:   6079119360 | elapsed time per iteration (ms): 4684.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.141278E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.323 | tokens per gpu per second (tgs): 1748.685 | TFLOPs: 14.07 |
g0184: [2024-08-10 11:14:23,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=23200, skipped=33, lr=[0.00019994953847946107, 0.00019994953847946107], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23200 loss: 0.7855 iter time (s): 4.702 samples/sec: 27.223
g0198:  iteration    23200/10000000 | consumed samples:      2969600 | consumed tokens:   6081740800 | elapsed time per iteration (ms): 4735.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.062639E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.030 | tokens per gpu per second (tgs): 1729.941 | TFLOPs: 13.92 |
g0184: [2024-08-10 11:15:12,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=23210, skipped=33, lr=[0.00019994945259447347, 0.00019994945259447347], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23210 loss: 0.7860 iter time (s): 4.854 samples/sec: 26.369
g0198:  iteration    23210/10000000 | consumed samples:      2970880 | consumed tokens:   6084362240 | elapsed time per iteration (ms): 4897.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.090933E-01 | loss scale: 32768.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.138 | tokens per gpu per second (tgs): 1672.823 | TFLOPs: 13.46 |
g0184: [2024-08-10 11:15:54,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=23220, skipped=33, lr=[0.00019994936663647974, 0.00019994936663647974], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23220 loss: 0.7886 iter time (s): 4.216 samples/sec: 30.357
g0198:  iteration    23220/10000000 | consumed samples:      2972160 | consumed tokens:   6086983680 | elapsed time per iteration (ms): 4249.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.107529E-01 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.124 | tokens per gpu per second (tgs): 1927.939 | TFLOPs: 15.51 |
g0184: [2024-08-10 11:16:41,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=23230, skipped=33, lr=[0.00019994928060548, 0.00019994928060548], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23230 loss: 0.7926 iter time (s): 4.647 samples/sec: 27.547
g0198:  iteration    23230/10000000 | consumed samples:      2973440 | consumed tokens:   6089605120 | elapsed time per iteration (ms): 4681.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.054213E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.342 | tokens per gpu per second (tgs): 1749.917 | TFLOPs: 14.08 |
g0184: [2024-08-10 11:17:26,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=23240, skipped=33, lr=[0.00019994919450147427, 0.00019994919450147427], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23240 loss: 0.8195 iter time (s): 4.425 samples/sec: 28.923
g0198:  iteration    23240/10000000 | consumed samples:      2974720 | consumed tokens:   6092226560 | elapsed time per iteration (ms): 4458.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.987310E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.709 | tokens per gpu per second (tgs): 1837.400 | TFLOPs: 14.79 |
g0184: [2024-08-10 11:18:11,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=23250, skipped=33, lr=[0.00019994910832446265, 0.00019994910832446265], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23250 loss: 0.7674 iter time (s): 4.543 samples/sec: 28.172
g0198:  iteration    23250/10000000 | consumed samples:      2976000 | consumed tokens:   6094848000 | elapsed time per iteration (ms): 4576.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.024474E-01 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.968 | tokens per gpu per second (tgs): 1789.944 | TFLOPs: 14.40 |
g0184: [2024-08-10 11:19:00,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=23260, skipped=33, lr=[0.0001999490220744452, 0.0001999490220744452], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23260 loss: 0.8240 iter time (s): 4.838 samples/sec: 26.456
g0198:  iteration    23260/10000000 | consumed samples:      2977280 | consumed tokens:   6097469440 | elapsed time per iteration (ms): 4872.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.119593E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.270 | tokens per gpu per second (tgs): 1681.273 | TFLOPs: 13.53 |
g0184: [2024-08-10 11:19:49,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=23270, skipped=33, lr=[0.00019994893575142196, 0.00019994893575142196], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23270 loss: 0.7924 iter time (s): 4.896 samples/sec: 26.144
g0198:  iteration    23270/10000000 | consumed samples:      2978560 | consumed tokens:   6100090880 | elapsed time per iteration (ms): 4937.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.039279E-01 | loss scale: 32768.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.926 | tokens per gpu per second (tgs): 1659.243 | TFLOPs: 13.35 |
g0184: [2024-08-10 11:20:35,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=23280, skipped=33, lr=[0.00019994884935539304, 0.00019994884935539304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23280 loss: 0.7897 iter time (s): 4.536 samples/sec: 28.216
g0198:  iteration    23280/10000000 | consumed samples:      2979840 | consumed tokens:   6102712320 | elapsed time per iteration (ms): 4569.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.097953E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.012 | tokens per gpu per second (tgs): 1792.799 | TFLOPs: 14.43 |
g0184: [2024-08-10 11:21:20,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=23290, skipped=33, lr=[0.00019994876288635847, 0.00019994876288635847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23290 loss: 0.8095 iter time (s): 4.410 samples/sec: 29.026
g0198:  iteration    23290/10000000 | consumed samples:      2981120 | consumed tokens:   6105333760 | elapsed time per iteration (ms): 4442.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.116940E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.813 | tokens per gpu per second (tgs): 1844.063 | TFLOPs: 14.84 |
g0184: [2024-08-10 11:22:05,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=23300, skipped=33, lr=[0.00019994867634431834, 0.00019994867634431834], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23300 loss: 0.7925 iter time (s): 4.488 samples/sec: 28.523
g0198:  iteration    23300/10000000 | consumed samples:      2982400 | consumed tokens:   6107955200 | elapsed time per iteration (ms): 4520.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.111932E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.316 | tokens per gpu per second (tgs): 1812.200 | TFLOPs: 14.58 |
g0184: [2024-08-10 11:22:55,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=23310, skipped=33, lr=[0.0001999485897292727, 0.0001999485897292727], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23310 loss: 0.7945 iter time (s): 5.020 samples/sec: 25.500
g0198:  iteration    23310/10000000 | consumed samples:      2983680 | consumed tokens:   6110576640 | elapsed time per iteration (ms): 5052.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.016998E-01 | loss scale: 32768.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.333 | tokens per gpu per second (tgs): 1621.283 | TFLOPs: 13.05 |
g0184: [2024-08-10 11:23:43,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=23320, skipped=33, lr=[0.0001999485030412216, 0.0001999485030412216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23320 loss: 0.7991 iter time (s): 4.750 samples/sec: 26.950
g0198:  iteration    23320/10000000 | consumed samples:      2984960 | consumed tokens:   6113198080 | elapsed time per iteration (ms): 4781.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.044230E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.767 | tokens per gpu per second (tgs): 1713.118 | TFLOPs: 13.79 |
g0184: [2024-08-10 11:24:31,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=23330, skipped=33, lr=[0.00019994841628016515, 0.00019994841628016515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23330 loss: 0.7972 iter time (s): 4.799 samples/sec: 26.671
g0198:  iteration    23330/10000000 | consumed samples:      2986240 | consumed tokens:   6115819520 | elapsed time per iteration (ms): 4832.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.997111E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.487 | tokens per gpu per second (tgs): 1695.161 | TFLOPs: 13.64 |
g0184: [2024-08-10 11:25:19,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=23340, skipped=33, lr=[0.0001999483294461034, 0.0001999483294461034], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23340 loss: 0.8007 iter time (s): 4.738 samples/sec: 27.013
g0198:  iteration    23340/10000000 | consumed samples:      2987520 | consumed tokens:   6118440960 | elapsed time per iteration (ms): 4774.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.134068E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.812 | tokens per gpu per second (tgs): 1715.979 | TFLOPs: 13.81 |
g0184: [2024-08-10 11:26:06,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=23350, skipped=33, lr=[0.0001999482425390364, 0.0001999482425390364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23350 loss: 0.8705 iter time (s): 4.618 samples/sec: 27.715
g0198:  iteration    23350/10000000 | consumed samples:      2988800 | consumed tokens:   6121062400 | elapsed time per iteration (ms): 4653.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.126080E-01 | loss scale: 32768.0 | grad norm: 0.503 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.509 | tokens per gpu per second (tgs): 1760.552 | TFLOPs: 14.17 |
g0184: [2024-08-10 11:26:53,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=23360, skipped=33, lr=[0.00019994815555896426, 0.00019994815555896426], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23360 loss: 0.8119 iter time (s): 4.691 samples/sec: 27.284
g0198:  iteration    23360/10000000 | consumed samples:      2990080 | consumed tokens:   6123683840 | elapsed time per iteration (ms): 4726.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.071898E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.084 | tokens per gpu per second (tgs): 1733.360 | TFLOPs: 13.95 |
g0184: [2024-08-10 11:27:49,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=23370, skipped=33, lr=[0.000199948068505887, 0.000199948068505887], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23370 loss: 0.8607 iter time (s): 5.530 samples/sec: 23.148
g0198:  iteration    23370/10000000 | consumed samples:      2991360 | consumed tokens:   6126305280 | elapsed time per iteration (ms): 5563.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.224956E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.009 | tokens per gpu per second (tgs): 1472.589 | TFLOPs: 11.85 |
g0184: [2024-08-10 11:28:41,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=23380, skipped=33, lr=[0.00019994798137980467, 0.00019994798137980467], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23380 loss: 0.8124 iter time (s): 5.175 samples/sec: 24.732
g0198:  iteration    23380/10000000 | consumed samples:      2992640 | consumed tokens:   6128926720 | elapsed time per iteration (ms): 5208.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.180201E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.576 | tokens per gpu per second (tgs): 1572.852 | TFLOPs: 12.66 |
g0184: [2024-08-10 11:29:25,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=23390, skipped=33, lr=[0.0001999478941807174, 0.0001999478941807174], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23390 loss: 0.8061 iter time (s): 4.441 samples/sec: 28.823
g0198:  iteration    23390/10000000 | consumed samples:      2993920 | consumed tokens:   6131548160 | elapsed time per iteration (ms): 4473.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.083429E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.613 | tokens per gpu per second (tgs): 1831.261 | TFLOPs: 14.74 |
g0184: [2024-08-10 11:30:15,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=23400, skipped=33, lr=[0.00019994780690862522, 0.00019994780690862522], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23400 loss: 0.8365 iter time (s): 4.884 samples/sec: 26.207
g0198:  iteration    23400/10000000 | consumed samples:      2995200 | consumed tokens:   6134169600 | elapsed time per iteration (ms): 4916.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.116370E-01 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.033 | tokens per gpu per second (tgs): 1666.110 | TFLOPs: 13.41 |
g0184: [2024-08-10 11:31:01,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=23410, skipped=33, lr=[0.00019994771956352822, 0.00019994771956352822], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23410 loss: 0.7531 iter time (s): 4.637 samples/sec: 27.603
g0198:  iteration    23410/10000000 | consumed samples:      2996480 | consumed tokens:   6136791040 | elapsed time per iteration (ms): 4677.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.007153E-01 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.363 | tokens per gpu per second (tgs): 1751.201 | TFLOPs: 14.09 |
g0184: [2024-08-10 11:31:47,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=23420, skipped=33, lr=[0.00019994763214542643, 0.00019994763214542643], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23420 loss: 0.7783 iter time (s): 4.478 samples/sec: 28.582
g0198:  iteration    23420/10000000 | consumed samples:      2997760 | consumed tokens:   6139412480 | elapsed time per iteration (ms): 4511.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.014512E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.371 | tokens per gpu per second (tgs): 1815.762 | TFLOPs: 14.61 |
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 11:32:11,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 11:32:34,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=23430, skipped=33, lr=[0.00019994754465431992, 0.00019994754465431992], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23430 loss: 0.8068 iter time (s): 4.752 samples/sec: 26.936
g0198:  iteration    23430/10000000 | consumed samples:      2999040 | consumed tokens:   6142033920 | elapsed time per iteration (ms): 4785.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.080197E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.747 | tokens per gpu per second (tgs): 1711.781 | TFLOPs: 13.77 |
g0184: [2024-08-10 11:33:21,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=23440, skipped=33, lr=[0.00019994745709020882, 0.00019994745709020882], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23440 loss: 0.8331 iter time (s): 4.634 samples/sec: 27.625
g0198:  iteration    23440/10000000 | consumed samples:      3000320 | consumed tokens:   6144655360 | elapsed time per iteration (ms): 4666.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.197427E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.432 | tokens per gpu per second (tgs): 1755.620 | TFLOPs: 14.13 |
g0184: [2024-08-10 11:34:10,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=23450, skipped=33, lr=[0.00019994736945309313, 0.00019994736945309313], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23450 loss: 0.7739 iter time (s): 4.822 samples/sec: 26.545
g0198:  iteration    23450/10000000 | consumed samples:      3001600 | consumed tokens:   6147276800 | elapsed time per iteration (ms): 4856.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.993635E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.354 | tokens per gpu per second (tgs): 1686.666 | TFLOPs: 13.57 |
g0184: [2024-08-10 11:34:55,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=23460, skipped=33, lr=[0.00019994728174297296, 0.00019994728174297296], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23460 loss: 0.7816 iter time (s): 4.526 samples/sec: 28.284
g0198:  iteration    23460/10000000 | consumed samples:      3002880 | consumed tokens:   6149898240 | elapsed time per iteration (ms): 4558.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.115229E-01 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.080 | tokens per gpu per second (tgs): 1797.116 | TFLOPs: 14.46 |
g0184: [2024-08-10 11:35:48,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=23470, skipped=33, lr=[0.00019994719395984833, 0.00019994719395984833], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23470 loss: 0.7818 iter time (s): 5.275 samples/sec: 24.264
g0198:  iteration    23470/10000000 | consumed samples:      3004160 | consumed tokens:   6152519680 | elapsed time per iteration (ms): 5309.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.020166E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.110 | tokens per gpu per second (tgs): 1543.050 | TFLOPs: 12.42 |
g0184: [2024-08-10 11:36:36,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=23480, skipped=33, lr=[0.00019994710610371933, 0.00019994710610371933], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23480 loss: 0.8049 iter time (s): 4.722 samples/sec: 27.105
g0198:  iteration    23480/10000000 | consumed samples:      3005440 | consumed tokens:   6155141120 | elapsed time per iteration (ms): 4755.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.013238E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.917 | tokens per gpu per second (tgs): 1722.718 | TFLOPs: 13.86 |
g0184: [2024-08-10 11:37:22,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=23490, skipped=33, lr=[0.00019994701817458606, 0.00019994701817458606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23490 loss: 0.8260 iter time (s): 4.626 samples/sec: 27.672
g0198:  iteration    23490/10000000 | consumed samples:      3006720 | consumed tokens:   6157762560 | elapsed time per iteration (ms): 4658.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.101300E-01 | loss scale: 65536.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.476 | tokens per gpu per second (tgs): 1758.472 | TFLOPs: 14.15 |
g0184: [2024-08-10 11:38:10,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=23500, skipped=33, lr=[0.00019994693017244856, 0.00019994693017244856], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23500 loss: 0.8071 iter time (s): 4.738 samples/sec: 27.016
g0198:  iteration    23500/10000000 | consumed samples:      3008000 | consumed tokens:   6160384000 | elapsed time per iteration (ms): 4770.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.009166E-01 | loss scale: 65536.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.832 | tokens per gpu per second (tgs): 1717.257 | TFLOPs: 13.82 |
g0184: [2024-08-10 11:38:57,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=23510, skipped=33, lr=[0.0001999468420973069, 0.0001999468420973069], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23510 loss: 0.8371 iter time (s): 4.688 samples/sec: 27.303
g0198:  iteration    23510/10000000 | consumed samples:      3009280 | consumed tokens:   6163005440 | elapsed time per iteration (ms): 4722.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.072656E-01 | loss scale: 65536.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.106 | tokens per gpu per second (tgs): 1734.773 | TFLOPs: 13.96 |
g0184: [2024-08-10 11:39:45,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=23520, skipped=33, lr=[0.00019994675394916113, 0.00019994675394916113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23520 loss: 0.8198 iter time (s): 4.687 samples/sec: 27.311
g0198:  iteration    23520/10000000 | consumed samples:      3010560 | consumed tokens:   6165626880 | elapsed time per iteration (ms): 4721.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.114149E-01 | loss scale: 65536.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.113 | tokens per gpu per second (tgs): 1735.220 | TFLOPs: 13.96 |
g0184: [2024-08-10 11:40:36,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=23530, skipped=33, lr=[0.00019994666572801132, 0.00019994666572801132], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23530 loss: 0.8250 iter time (s): 5.133 samples/sec: 24.939
g0198:  iteration    23530/10000000 | consumed samples:      3011840 | consumed tokens:   6168248320 | elapsed time per iteration (ms): 5167.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.184508E-01 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.771 | tokens per gpu per second (tgs): 1585.355 | TFLOPs: 12.76 |
g0184: [2024-08-10 11:41:20,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=23540, skipped=33, lr=[0.0001999465774338576, 0.0001999465774338576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23540 loss: 0.7801 iter time (s): 4.373 samples/sec: 29.273
g0198:  iteration    23540/10000000 | consumed samples:      3013120 | consumed tokens:   6170869760 | elapsed time per iteration (ms): 4417.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.014324E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.976 | tokens per gpu per second (tgs): 1854.453 | TFLOPs: 14.92 |
g0184: [2024-08-10 11:42:07,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=23550, skipped=33, lr=[0.00019994648906669995, 0.00019994648906669995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23550 loss: 0.8136 iter time (s): 4.595 samples/sec: 27.858
g0198:  iteration    23550/10000000 | consumed samples:      3014400 | consumed tokens:   6173491200 | elapsed time per iteration (ms): 4627.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.048632E-01 | loss scale: 65536.0 | grad norm: 0.273 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.662 | tokens per gpu per second (tgs): 1770.366 | TFLOPs: 14.25 |
g0184: [2024-08-10 11:42:53,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=23560, skipped=33, lr=[0.00019994640062653848, 0.00019994640062653848], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23560 loss: 0.8008 iter time (s): 4.615 samples/sec: 27.737
g0198:  iteration    23560/10000000 | consumed samples:      3015680 | consumed tokens:   6176112640 | elapsed time per iteration (ms): 4647.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.100256E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.544 | tokens per gpu per second (tgs): 1762.788 | TFLOPs: 14.19 |
g0184: [2024-08-10 11:43:42,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=23570, skipped=33, lr=[0.00019994631211337329, 0.00019994631211337329], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23570 loss: 0.8175 iter time (s): 4.873 samples/sec: 26.266
g0198:  iteration    23570/10000000 | consumed samples:      3016960 | consumed tokens:   6178734080 | elapsed time per iteration (ms): 4905.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.091159E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.092 | tokens per gpu per second (tgs): 1669.866 | TFLOPs: 13.44 |
g0184: [2024-08-10 11:44:35,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=23580, skipped=33, lr=[0.00019994622352720437, 0.00019994622352720437], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23580 loss: 0.8077 iter time (s): 5.241 samples/sec: 24.423
g0198:  iteration    23580/10000000 | consumed samples:      3018240 | consumed tokens:   6181355520 | elapsed time per iteration (ms): 5273.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.033630E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.273 | tokens per gpu per second (tgs): 1553.488 | TFLOPs: 12.50 |
g0184: [2024-08-10 11:45:24,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=23590, skipped=33, lr=[0.00019994613486803186, 0.00019994613486803186], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23590 loss: 0.8280 iter time (s): 4.886 samples/sec: 26.199
g0198:  iteration    23590/10000000 | consumed samples:      3019520 | consumed tokens:   6183976960 | elapsed time per iteration (ms): 4918.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.146523E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.026 | tokens per gpu per second (tgs): 1665.656 | TFLOPs: 13.40 |
g0184: [2024-08-10 11:46:13,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=23600, skipped=33, lr=[0.0001999460461358558, 0.0001999460461358558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23600 loss: 0.8162 iter time (s): 4.876 samples/sec: 26.249
g0198:  iteration    23600/10000000 | consumed samples:      3020800 | consumed tokens:   6186598400 | elapsed time per iteration (ms): 4910.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.119756E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.064 | tokens per gpu per second (tgs): 1668.128 | TFLOPs: 13.42 |
g0184: [2024-08-10 11:47:01,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=23610, skipped=33, lr=[0.00019994595733067625, 0.00019994595733067625], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23610 loss: 0.8051 iter time (s): 4.766 samples/sec: 26.854
g0198:  iteration    23610/10000000 | consumed samples:      3022080 | consumed tokens:   6189219840 | elapsed time per iteration (ms): 4799.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.007271E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.672 | tokens per gpu per second (tgs): 1707.012 | TFLOPs: 13.74 |
g0184: [2024-08-10 11:47:49,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=23620, skipped=33, lr=[0.00019994586845249328, 0.00019994586845249328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23620 loss: 0.8000 iter time (s): 4.739 samples/sec: 27.012
g0198:  iteration    23620/10000000 | consumed samples:      3023360 | consumed tokens:   6191841280 | elapsed time per iteration (ms): 4771.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.077510E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.825 | tokens per gpu per second (tgs): 1716.790 | TFLOPs: 13.82 |
g0184: [2024-08-10 11:48:42,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=23630, skipped=33, lr=[0.00019994577950130696, 0.00019994577950130696], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23630 loss: 0.8127 iter time (s): 5.258 samples/sec: 24.343
g0198:  iteration    23630/10000000 | consumed samples:      3024640 | consumed tokens:   6194462720 | elapsed time per iteration (ms): 5291.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.018745E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.190 | tokens per gpu per second (tgs): 1548.133 | TFLOPs: 12.46 |
g0184: [2024-08-10 11:49:31,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=23640, skipped=33, lr=[0.00019994569047711736, 0.00019994569047711736], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23640 loss: 0.7979 iter time (s): 4.927 samples/sec: 25.979
g0198:  iteration    23640/10000000 | consumed samples:      3025920 | consumed tokens:   6197084160 | elapsed time per iteration (ms): 4959.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.055082E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.807 | tokens per gpu per second (tgs): 1651.635 | TFLOPs: 13.29 |
g0184: [2024-08-10 11:50:19,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=23650, skipped=33, lr=[0.00019994560137992458, 0.00019994560137992458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23650 loss: 0.7809 iter time (s): 4.701 samples/sec: 27.229
g0198:  iteration    23650/10000000 | consumed samples:      3027200 | consumed tokens:   6199705600 | elapsed time per iteration (ms): 4733.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.044550E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.040 | tokens per gpu per second (tgs): 1730.555 | TFLOPs: 13.93 |
g0184: [2024-08-10 11:51:05,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=23660, skipped=33, lr=[0.00019994551220972864, 0.00019994551220972864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23660 loss: 0.8256 iter time (s): 4.552 samples/sec: 28.120
g0198:  iteration    23660/10000000 | consumed samples:      3028480 | consumed tokens:   6202327040 | elapsed time per iteration (ms): 4584.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.012987E-01 | loss scale: 65536.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.921 | tokens per gpu per second (tgs): 1786.915 | TFLOPs: 14.38 |
g0184: [2024-08-10 11:51:51,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=23670, skipped=33, lr=[0.00019994542296652962, 0.00019994542296652962], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23670 loss: 0.8066 iter time (s): 4.586 samples/sec: 27.912
g0198:  iteration    23670/10000000 | consumed samples:      3029760 | consumed tokens:   6204948480 | elapsed time per iteration (ms): 4621.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.958841E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.695 | tokens per gpu per second (tgs): 1772.470 | TFLOPs: 14.26 |
g0184: [2024-08-10 11:52:36,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=23680, skipped=33, lr=[0.00019994533365032762, 0.00019994533365032762], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23680 loss: 0.7714 iter time (s): 4.516 samples/sec: 28.343
g0198:  iteration    23680/10000000 | consumed samples:      3031040 | consumed tokens:   6207569920 | elapsed time per iteration (ms): 4549.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.109879E-01 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.133 | tokens per gpu per second (tgs): 1800.544 | TFLOPs: 14.49 |
g0184: [2024-08-10 11:53:22,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=23690, skipped=33, lr=[0.00019994524426112268, 0.00019994524426112268], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23690 loss: 0.8013 iter time (s): 4.560 samples/sec: 28.072
g0198:  iteration    23690/10000000 | consumed samples:      3032320 | consumed tokens:   6210191360 | elapsed time per iteration (ms): 4593.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.175256E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.864 | tokens per gpu per second (tgs): 1783.272 | TFLOPs: 14.35 |
g0184: [2024-08-10 11:54:11,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=23700, skipped=33, lr=[0.00019994515479891485, 0.00019994515479891485], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23700 loss: 0.8261 iter time (s): 4.886 samples/sec: 26.195
g0198:  iteration    23700/10000000 | consumed samples:      3033600 | consumed tokens:   6212812800 | elapsed time per iteration (ms): 4919.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.120131E-01 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.017 | tokens per gpu per second (tgs): 1665.116 | TFLOPs: 13.40 |
g0184: [2024-08-10 11:55:00,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=23710, skipped=33, lr=[0.00019994506526370426, 0.00019994506526370426], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23710 loss: 0.8126 iter time (s): 4.815 samples/sec: 26.585
g0198:  iteration    23710/10000000 | consumed samples:      3034880 | consumed tokens:   6215434240 | elapsed time per iteration (ms): 4847.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.970590E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.407 | tokens per gpu per second (tgs): 1690.073 | TFLOPs: 13.60 |
g0184: [2024-08-10 11:55:46,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=23720, skipped=33, lr=[0.00019994497565549094, 0.00019994497565549094], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23720 loss: 0.7761 iter time (s): 4.530 samples/sec: 28.258
g0198:  iteration    23720/10000000 | consumed samples:      3036160 | consumed tokens:   6218055680 | elapsed time per iteration (ms): 4562.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.063482E-01 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.053 | tokens per gpu per second (tgs): 1795.411 | TFLOPs: 14.45 |
g0184: [2024-08-10 11:56:31,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=23730, skipped=33, lr=[0.00019994488597427492, 0.00019994488597427492], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23730 loss: 0.8163 iter time (s): 4.549 samples/sec: 28.140
g0198:  iteration    23730/10000000 | consumed samples:      3037440 | consumed tokens:   6220677120 | elapsed time per iteration (ms): 4581.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.016315E-01 | loss scale: 65536.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.940 | tokens per gpu per second (tgs): 1788.158 | TFLOPs: 14.39 |
g0184: [2024-08-10 11:57:18,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=23740, skipped=33, lr=[0.00019994479622005635, 0.00019994479622005635], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23740 loss: 0.7792 iter time (s): 4.611 samples/sec: 27.757
g0198:  iteration    23740/10000000 | consumed samples:      3038720 | consumed tokens:   6223298560 | elapsed time per iteration (ms): 4643.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.043439E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.563 | tokens per gpu per second (tgs): 1764.017 | TFLOPs: 14.20 |
g0184: [2024-08-10 11:58:10,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=23750, skipped=33, lr=[0.00019994470639283523, 0.00019994470639283523], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23750 loss: 0.8104 iter time (s): 5.140 samples/sec: 24.900
g0198:  iteration    23750/10000000 | consumed samples:      3040000 | consumed tokens:   6225920000 | elapsed time per iteration (ms): 5173.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.030519E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.743 | tokens per gpu per second (tgs): 1583.583 | TFLOPs: 12.74 |
g0184: [2024-08-10 11:58:57,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=23760, skipped=33, lr=[0.0001999446164926117, 0.0001999446164926117], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23760 loss: 0.7961 iter time (s): 4.730 samples/sec: 27.060
g0198:  iteration    23760/10000000 | consumed samples:      3041280 | consumed tokens:   6228541440 | elapsed time per iteration (ms): 4770.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.051685E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.832 | tokens per gpu per second (tgs): 1717.275 | TFLOPs: 13.82 |
g0184: [2024-08-10 11:59:44,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=23770, skipped=33, lr=[0.00019994452651938576, 0.00019994452651938576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23770 loss: 0.8026 iter time (s): 4.678 samples/sec: 27.364
g0198:  iteration    23770/10000000 | consumed samples:      3042560 | consumed tokens:   6231162880 | elapsed time per iteration (ms): 4711.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.930083E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.169 | tokens per gpu per second (tgs): 1738.791 | TFLOPs: 13.99 |
g0184: [2024-08-10 12:00:29,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=23780, skipped=33, lr=[0.0001999444364731575, 0.0001999444364731575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23780 loss: 0.8049 iter time (s): 4.442 samples/sec: 28.818
g0198:  iteration    23780/10000000 | consumed samples:      3043840 | consumed tokens:   6233784320 | elapsed time per iteration (ms): 4474.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.001140E-01 | loss scale: 65536.0 | grad norm: 0.585 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.609 | tokens per gpu per second (tgs): 1830.958 | TFLOPs: 14.73 |
g0184: [2024-08-10 12:01:14,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=23790, skipped=33, lr=[0.000199944346353927, 0.000199944346353927], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23790 loss: 0.8112 iter time (s): 4.488 samples/sec: 28.523
g0198:  iteration    23790/10000000 | consumed samples:      3045120 | consumed tokens:   6236405760 | elapsed time per iteration (ms): 4520.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.099453E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.318 | tokens per gpu per second (tgs): 1812.369 | TFLOPs: 14.58 |
g0184: [2024-08-10 12:02:00,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=23800, skipped=33, lr=[0.00019994425616169434, 0.00019994425616169434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23800 loss: 0.7847 iter time (s): 4.520 samples/sec: 28.316
g0198:  iteration    23800/10000000 | consumed samples:      3046400 | consumed tokens:   6239027200 | elapsed time per iteration (ms): 4554.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.068612E-01 | loss scale: 65536.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.105 | tokens per gpu per second (tgs): 1798.729 | TFLOPs: 14.47 |
g0184: [2024-08-10 12:02:50,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=23810, skipped=33, lr=[0.00019994416589645954, 0.00019994416589645954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23810 loss: 0.8123 iter time (s): 4.954 samples/sec: 25.839
g0198:  iteration    23810/10000000 | consumed samples:      3047680 | consumed tokens:   6241648640 | elapsed time per iteration (ms): 4987.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.018066E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.665 | tokens per gpu per second (tgs): 1642.579 | TFLOPs: 13.22 |
g0184: [2024-08-10 12:03:39,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=23820, skipped=33, lr=[0.00019994407555822273, 0.00019994407555822273], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23820 loss: 0.8058 iter time (s): 4.875 samples/sec: 26.258
g0198:  iteration    23820/10000000 | consumed samples:      3048960 | consumed tokens:   6244270080 | elapsed time per iteration (ms): 4908.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.965118E-01 | loss scale: 65536.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.079 | tokens per gpu per second (tgs): 1669.048 | TFLOPs: 13.43 |
g0184: [2024-08-10 12:04:26,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=23830, skipped=33, lr=[0.00019994398514698393, 0.00019994398514698393], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23830 loss: 0.8211 iter time (s): 4.689 samples/sec: 27.300
g0198:  iteration    23830/10000000 | consumed samples:      3050240 | consumed tokens:   6246891520 | elapsed time per iteration (ms): 4721.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.035813E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.109 | tokens per gpu per second (tgs): 1734.977 | TFLOPs: 13.96 |
g0184: [2024-08-10 12:05:14,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=23840, skipped=33, lr=[0.00019994389466274326, 0.00019994389466274326], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23840 loss: 0.7840 iter time (s): 4.805 samples/sec: 26.640
g0198:  iteration    23840/10000000 | consumed samples:      3051520 | consumed tokens:   6249512960 | elapsed time per iteration (ms): 4837.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.989514E-01 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.458 | tokens per gpu per second (tgs): 1693.312 | TFLOPs: 13.63 |
g0184: [2024-08-10 12:06:03,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=23850, skipped=33, lr=[0.00019994380410550075, 0.00019994380410550075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23850 loss: 0.7923 iter time (s): 4.832 samples/sec: 26.488
g0198:  iteration    23850/10000000 | consumed samples:      3052800 | consumed tokens:   6252134400 | elapsed time per iteration (ms): 4864.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.931960E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.311 | tokens per gpu per second (tgs): 1683.930 | TFLOPs: 13.55 |
g0184: [2024-08-10 12:06:55,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=23860, skipped=33, lr=[0.0001999437134752565, 0.0001999437134752565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23860 loss: 0.8368 iter time (s): 5.151 samples/sec: 24.849
g0198:  iteration    23860/10000000 | consumed samples:      3054080 | consumed tokens:   6254755840 | elapsed time per iteration (ms): 5184.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.079302E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.689 | tokens per gpu per second (tgs): 1580.118 | TFLOPs: 12.72 |
g0184: [2024-08-10 12:07:47,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=23870, skipped=33, lr=[0.00019994362277201056, 0.00019994362277201056], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23870 loss: 0.7955 iter time (s): 5.149 samples/sec: 24.858
g0198:  iteration    23870/10000000 | consumed samples:      3055360 | consumed tokens:   6257377280 | elapsed time per iteration (ms): 5182.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.062698E-01 | loss scale: 65536.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.700 | tokens per gpu per second (tgs): 1580.823 | TFLOPs: 12.72 |
g0184: [2024-08-10 12:08:33,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=23880, skipped=33, lr=[0.000199943531995763, 0.000199943531995763], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23880 loss: 0.8033 iter time (s): 4.590 samples/sec: 27.888
g0198:  iteration    23880/10000000 | consumed samples:      3056640 | consumed tokens:   6259998720 | elapsed time per iteration (ms): 4622.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.000125E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.692 | tokens per gpu per second (tgs): 1772.303 | TFLOPs: 14.26 |
g0184: [2024-08-10 12:09:27,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=23890, skipped=33, lr=[0.00019994344114651386, 0.00019994344114651386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23890 loss: 0.8195 iter time (s): 5.326 samples/sec: 24.034
g0198:  iteration    23890/10000000 | consumed samples:      3057920 | consumed tokens:   6262620160 | elapsed time per iteration (ms): 5358.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.096523E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.886 | tokens per gpu per second (tgs): 1528.730 | TFLOPs: 12.30 |
g0184: [2024-08-10 12:10:18,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=23900, skipped=33, lr=[0.00019994335022426325, 0.00019994335022426325], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23900 loss: 0.8101 iter time (s): 5.156 samples/sec: 24.824
g0198:  iteration    23900/10000000 | consumed samples:      3059200 | consumed tokens:   6265241600 | elapsed time per iteration (ms): 5189.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.971667E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.666 | tokens per gpu per second (tgs): 1578.598 | TFLOPs: 12.70 |
g0184: [2024-08-10 12:11:11,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=23910, skipped=33, lr=[0.00019994325922901126, 0.00019994325922901126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23910 loss: 0.7751 iter time (s): 5.177 samples/sec: 24.727
g0198:  iteration    23910/10000000 | consumed samples:      3060480 | consumed tokens:   6267863040 | elapsed time per iteration (ms): 5210.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.996849E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.566 | tokens per gpu per second (tgs): 1572.250 | TFLOPs: 12.65 |
g0184: [2024-08-10 12:12:00,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=23920, skipped=33, lr=[0.0001999431681607579, 0.0001999431681607579], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23920 loss: 0.8072 iter time (s): 4.896 samples/sec: 26.145
g0198:  iteration    23920/10000000 | consumed samples:      3061760 | consumed tokens:   6270484480 | elapsed time per iteration (ms): 4930.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.967708E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.963 | tokens per gpu per second (tgs): 1661.613 | TFLOPs: 13.37 |
g0188: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 12:12:24,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 12:12:24,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 12:12:48,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=23930, skipped=33, lr=[0.00019994307701950328, 0.00019994307701950328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23930 loss: 0.8237 iter time (s): 4.748 samples/sec: 26.957
g0198:  iteration    23930/10000000 | consumed samples:      3063040 | consumed tokens:   6273105920 | elapsed time per iteration (ms): 4780.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.064777E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.773 | tokens per gpu per second (tgs): 1713.493 | TFLOPs: 13.79 |
g0184: [2024-08-10 12:13:42,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=23940, skipped=33, lr=[0.00019994298580524748, 0.00019994298580524748], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23940 loss: 0.7853 iter time (s): 5.381 samples/sec: 23.787
g0198:  iteration    23940/10000000 | consumed samples:      3064320 | consumed tokens:   6275727360 | elapsed time per iteration (ms): 5413.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.032217E-01 | loss scale: 131072.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.645 | tokens per gpu per second (tgs): 1513.249 | TFLOPs: 12.18 |
g0184: [2024-08-10 12:14:33,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=23950, skipped=33, lr=[0.00019994289451799054, 0.00019994289451799054], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23950 loss: 0.7972 iter time (s): 5.096 samples/sec: 25.119
g0198:  iteration    23950/10000000 | consumed samples:      3065600 | consumed tokens:   6278348800 | elapsed time per iteration (ms): 5128.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.041702E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.957 | tokens per gpu per second (tgs): 1597.278 | TFLOPs: 12.85 |
g0184: [2024-08-10 12:15:21,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=23960, skipped=33, lr=[0.00019994280315773253, 0.00019994280315773253], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23960 loss: 0.7820 iter time (s): 4.734 samples/sec: 27.037
g0198:  iteration    23960/10000000 | consumed samples:      3066880 | consumed tokens:   6280970240 | elapsed time per iteration (ms): 4767.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.996388E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.849 | tokens per gpu per second (tgs): 1718.366 | TFLOPs: 13.83 |
g0184: [2024-08-10 12:16:09,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=23970, skipped=33, lr=[0.00019994271172447354, 0.00019994271172447354], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23970 loss: 0.7916 iter time (s): 4.764 samples/sec: 26.870
g0198:  iteration    23970/10000000 | consumed samples:      3068160 | consumed tokens:   6283591680 | elapsed time per iteration (ms): 4796.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.969378E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.688 | tokens per gpu per second (tgs): 1708.007 | TFLOPs: 13.74 |
g0184: [2024-08-10 12:16:56,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=23980, skipped=33, lr=[0.00019994262021821364, 0.00019994262021821364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23980 loss: 0.8110 iter time (s): 4.670 samples/sec: 27.412
g0198:  iteration    23980/10000000 | consumed samples:      3069440 | consumed tokens:   6286213120 | elapsed time per iteration (ms): 4705.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.140564E-01 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.205 | tokens per gpu per second (tgs): 1741.123 | TFLOPs: 14.01 |
g0184: [2024-08-10 12:17:44,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=23990, skipped=33, lr=[0.00019994252863895286, 0.00019994252863895286], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 23990 loss: 0.8078 iter time (s): 4.767 samples/sec: 26.851
g0198:  iteration    23990/10000000 | consumed samples:      3070720 | consumed tokens:   6288834560 | elapsed time per iteration (ms): 4800.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.037687E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.667 | tokens per gpu per second (tgs): 1706.667 | TFLOPs: 13.73 |
g0184: [2024-08-10 12:18:32,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=33, lr=[0.00019994243698669134, 0.00019994243698669134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24000 loss: 0.7768 iter time (s): 4.777 samples/sec: 26.796
g0198:  iteration    24000/10000000 | consumed samples:      3072000 | consumed tokens:   6291456000 | elapsed time per iteration (ms): 4810.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.004658E-01 | loss scale: 131072.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.610 | tokens per gpu per second (tgs): 1703.043 | TFLOPs: 13.70 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 24000 | lm loss value: 8.018880E-01 | lm loss PPL: 2.229747E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   24000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 12:26:18,499] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step24000 is about to be saved!
g0198: [2024-08-10 12:26:18,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0198: [2024-08-10 12:26:18,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0198: [2024-08-10 12:26:18,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0184: [2024-08-10 12:26:18,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0184: [2024-08-10 12:26:18,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0184: [2024-08-10 12:26:18,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0194: [2024-08-10 12:26:18,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0194: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0194: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0195: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0195: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0195: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0197: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0197: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0197: [2024-08-10 12:26:18,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0187: [2024-08-10 12:26:18,513] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0187: [2024-08-10 12:26:18,513] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0187: [2024-08-10 12:26:18,514] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0185: [2024-08-10 12:26:18,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0185: [2024-08-10 12:26:18,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0185: [2024-08-10 12:26:18,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0188: [2024-08-10 12:26:18,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0188: [2024-08-10 12:26:18,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0188: [2024-08-10 12:26:18,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0198: [2024-08-10 12:26:18,531] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_23-model_00-model_states.pt...
g0187: [2024-08-10 12:26:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_08-model_00-model_states.pt...
g0194: [2024-08-10 12:26:18,550] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_14-model_00-model_states.pt...
g0197: [2024-08-10 12:26:18,550] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_20-model_00-model_states.pt...
g0195: [2024-08-10 12:26:18,550] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_17-model_00-model_states.pt...
g0185: [2024-08-10 12:26:18,554] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_05-model_00-model_states.pt...
g0188: [2024-08-10 12:26:18,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_11-model_00-model_states.pt...
g0184: [2024-08-10 12:26:18,556] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_01-model_00-model_states.pt...
g0194: [2024-08-10 12:26:18,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_14-model_00-model_states.pt.
g0198: [2024-08-10 12:26:18,701] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 12:26:18,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 12:26:18,704] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_24-model_00-model_states.pt.
g0194: [2024-08-10 12:26:18,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_15-model_00-model_states.pt...
g0187: [2024-08-10 12:26:18,733] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_08-model_00-model_states.pt.
g0195: [2024-08-10 12:26:18,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_17-model_00-model_states.pt.
g0185: [2024-08-10 12:26:18,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_05-model_00-model_states.pt.
g0198: [2024-08-10 12:26:18,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_25-model_00-model_states.pt...
g0197: [2024-08-10 12:26:18,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_20-model_00-model_states.pt.
g0187: [2024-08-10 12:26:18,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_09-model_00-model_states.pt...
g0185: [2024-08-10 12:26:18,772] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_06-model_00-model_states.pt...
g0184: [2024-08-10 12:26:18,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_01-model_00-model_states.pt.
g0195: [2024-08-10 12:26:18,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_18-model_00-model_states.pt...
g0188: [2024-08-10 12:26:18,787] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_11-model_00-model_states.pt.
g0184: [2024-08-10 12:26:18,789] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_02-model_00-model_states.pt...
g0197: [2024-08-10 12:26:18,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_21-model_00-model_states.pt...
g0188: [2024-08-10 12:26:18,825] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_12-model_00-model_states.pt...
g0185: [2024-08-10 12:26:18,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_06-model_00-model_states.pt.
g0184: [2024-08-10 12:26:18,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_02-model_00-model_states.pt.
g0195: [2024-08-10 12:26:18,907] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_18-model_00-model_states.pt.
g0185: [2024-08-10 12:26:18,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_07-model_00-model_states.pt...
g0184: [2024-08-10 12:26:18,927] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_03-model_00-model_states.pt...
g0194: [2024-08-10 12:26:18,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_15-model_00-model_states.pt.
g0195: [2024-08-10 12:26:18,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_19-model_00-model_states.pt...
g0187: [2024-08-10 12:26:18,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_09-model_00-model_states.pt.
g0197: [2024-08-10 12:26:18,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_21-model_00-model_states.pt.
g0194: [2024-08-10 12:26:18,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_16-model_00-model_states.pt...
g0187: [2024-08-10 12:26:18,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_10-model_00-model_states.pt...
g0198: [2024-08-10 12:26:18,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 12:26:18,984] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_07_model_states.pt...
g0188: [2024-08-10 12:26:18,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_12-model_00-model_states.pt.
g0197: [2024-08-10 12:26:18,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_22-model_00-model_states.pt...
g0188: [2024-08-10 12:26:19,028] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_13-model_00-model_states.pt...
g0195: [2024-08-10 12:26:19,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 12:26:19,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_05_model_states.pt...
g0184: [2024-08-10 12:26:19,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 12:26:19,143] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_04-model_00-model_states.pt...
g0194: [2024-08-10 12:26:19,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 12:26:19,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_04_model_states.pt...
g0185: [2024-08-10 12:26:19,150] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 12:26:19,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_01_model_states.pt...
g0197: [2024-08-10 12:26:19,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 12:26:19,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_06_model_states.pt...
g0187: [2024-08-10 12:26:19,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 12:26:19,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_02_model_states.pt...
g0188: [2024-08-10 12:26:19,199] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 12:26:19,200] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_03_model_states.pt...
g0184: [2024-08-10 12:26:19,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 12:26:19,282] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_00_model_states.pt
g0184: [2024-08-10 12:26:19,282] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 12:26:20,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 12:26:21,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0195: [2024-08-10 12:26:21,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 12:26:21,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0185: [2024-08-10 12:26:21,601] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 12:26:21,601] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0194: [2024-08-10 12:26:21,621] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 12:26:21,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0187: [2024-08-10 12:26:21,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 12:26:21,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0197: [2024-08-10 12:26:21,756] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 12:26:21,757] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0188: [2024-08-10 12:26:22,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 12:26:22,290] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0184: [2024-08-10 12:26:22,764] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step24000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 12:26:22,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24000 is ready now!
g0184:   successfully saved checkpoint at iteration   24000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.22, Latency(second): 4.318
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4316.82, 4317.89)
g0184: [2024-08-10 12:27:17,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=24010, skipped=33, lr=[0.0001999423452614291, 0.0001999423452614291], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24010 loss: 0.7991 iter time (s): 5.429 samples/sec: 23.578
g0198:  iteration    24010/10000000 | consumed samples:      3073280 | consumed tokens:   6294077440 | elapsed time per iteration (ms): 52507.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.073222E-01 | loss scale: 131072.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.438 | tokens per gpu per second (tgs): 156.016 | TFLOPs: 1.26 |
g0184: [2024-08-10 12:28:06,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=24020, skipped=33, lr=[0.00019994225346316622, 0.00019994225346316622], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24020 loss: 0.8114 iter time (s): 4.898 samples/sec: 26.132
g0198:  iteration    24020/10000000 | consumed samples:      3074560 | consumed tokens:   6296698880 | elapsed time per iteration (ms): 4931.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.049120E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.956 | tokens per gpu per second (tgs): 1661.170 | TFLOPs: 13.37 |
g0184: [2024-08-10 12:28:51,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=24030, skipped=33, lr=[0.00019994216159190277, 0.00019994216159190277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24030 loss: 0.7890 iter time (s): 4.422 samples/sec: 28.947
g0198:  iteration    24030/10000000 | consumed samples:      3075840 | consumed tokens:   6299320320 | elapsed time per iteration (ms): 4454.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.004333E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.735 | tokens per gpu per second (tgs): 1839.024 | TFLOPs: 14.80 |
g0184: [2024-08-10 12:29:40,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=24040, skipped=33, lr=[0.00019994206964763883, 0.00019994206964763883], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24040 loss: 0.8045 iter time (s): 4.833 samples/sec: 26.485
g0198:  iteration    24040/10000000 | consumed samples:      3077120 | consumed tokens:   6301941760 | elapsed time per iteration (ms): 4875.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.119390E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.253 | tokens per gpu per second (tgs): 1680.180 | TFLOPs: 13.52 |
g0184: [2024-08-10 12:30:28,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=24050, skipped=33, lr=[0.00019994197763037447, 0.00019994197763037447], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24050 loss: 0.7965 iter time (s): 4.771 samples/sec: 26.826
g0198:  iteration    24050/10000000 | consumed samples:      3078400 | consumed tokens:   6304563200 | elapsed time per iteration (ms): 4804.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.146696E-01 | loss scale: 131072.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.642 | tokens per gpu per second (tgs): 1705.071 | TFLOPs: 13.72 |
g0184: [2024-08-10 12:31:12,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=24060, skipped=33, lr=[0.00019994188554010975, 0.00019994188554010975], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24060 loss: 0.7893 iter time (s): 4.434 samples/sec: 28.871
g0198:  iteration    24060/10000000 | consumed samples:      3079680 | consumed tokens:   6307184640 | elapsed time per iteration (ms): 4466.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.946167E-01 | loss scale: 131072.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.660 | tokens per gpu per second (tgs): 1834.220 | TFLOPs: 14.76 |
g0184: [2024-08-10 12:32:02,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=24070, skipped=33, lr=[0.00019994179337684477, 0.00019994179337684477], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24070 loss: 0.8211 iter time (s): 4.945 samples/sec: 25.885
g0198:  iteration    24070/10000000 | consumed samples:      3080960 | consumed tokens:   6309806080 | elapsed time per iteration (ms): 4978.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.888910E-01 | loss scale: 131072.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.712 | tokens per gpu per second (tgs): 1645.569 | TFLOPs: 13.24 |
g0184: [2024-08-10 12:32:49,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=24080, skipped=33, lr=[0.00019994170114057954, 0.00019994170114057954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24080 loss: 0.8107 iter time (s): 4.615 samples/sec: 27.738
g0198:  iteration    24080/10000000 | consumed samples:      3082240 | consumed tokens:   6312427520 | elapsed time per iteration (ms): 4647.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.070274E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.544 | tokens per gpu per second (tgs): 1762.842 | TFLOPs: 14.19 |
g0184: [2024-08-10 12:33:36,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=24090, skipped=33, lr=[0.0001999416088313142, 0.0001999416088313142], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24090 loss: 0.8028 iter time (s): 4.726 samples/sec: 27.083
g0198:  iteration    24090/10000000 | consumed samples:      3083520 | consumed tokens:   6315048960 | elapsed time per iteration (ms): 4759.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.024106E-01 | loss scale: 131072.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.896 | tokens per gpu per second (tgs): 1721.342 | TFLOPs: 13.85 |
g0184: [2024-08-10 12:34:21,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=24100, skipped=33, lr=[0.00019994151644904876, 0.00019994151644904876], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24100 loss: 0.7911 iter time (s): 4.440 samples/sec: 28.830
g0198:  iteration    24100/10000000 | consumed samples:      3084800 | consumed tokens:   6317670400 | elapsed time per iteration (ms): 4472.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.968513E-01 | loss scale: 131072.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.620 | tokens per gpu per second (tgs): 1831.705 | TFLOPs: 14.74 |
g0184: [2024-08-10 12:35:06,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=24110, skipped=33, lr=[0.00019994142399378334, 0.00019994142399378334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24110 loss: 0.8068 iter time (s): 4.447 samples/sec: 28.785
g0198:  iteration    24110/10000000 | consumed samples:      3086080 | consumed tokens:   6320291840 | elapsed time per iteration (ms): 4480.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.901859E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.571 | tokens per gpu per second (tgs): 1828.543 | TFLOPs: 14.71 |
g0184: [2024-08-10 12:36:02,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=24120, skipped=33, lr=[0.00019994133146551799, 0.00019994133146551799], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24120 loss: 0.7807 iter time (s): 5.653 samples/sec: 22.643
g0198:  iteration    24120/10000000 | consumed samples:      3087360 | consumed tokens:   6322913280 | elapsed time per iteration (ms): 5686.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.095601E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.508 | tokens per gpu per second (tgs): 1440.512 | TFLOPs: 11.59 |
g0184: [2024-08-10 12:36:53,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=24130, skipped=33, lr=[0.0001999412388642528, 0.0001999412388642528], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24130 loss: 0.7898 iter time (s): 5.007 samples/sec: 25.563
g0198:  iteration    24130/10000000 | consumed samples:      3088640 | consumed tokens:   6325534720 | elapsed time per iteration (ms): 5041.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.973496E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.388 | tokens per gpu per second (tgs): 1624.859 | TFLOPs: 13.08 |
g0184: [2024-08-10 12:37:54,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=24140, skipped=33, lr=[0.0001999411461899878, 0.0001999411461899878], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24140 loss: 0.7799 iter time (s): 6.074 samples/sec: 21.073
g0198:  iteration    24140/10000000 | consumed samples:      3089920 | consumed tokens:   6328156160 | elapsed time per iteration (ms): 6107.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.015431E-01 | loss scale: 131072.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.957 | tokens per gpu per second (tgs): 1341.274 | TFLOPs: 10.79 |
g0184: [2024-08-10 12:38:42,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=24150, skipped=33, lr=[0.0001999410534427231, 0.0001999410534427231], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24150 loss: 0.7898 iter time (s): 4.761 samples/sec: 26.883
g0198:  iteration    24150/10000000 | consumed samples:      3091200 | consumed tokens:   6330777600 | elapsed time per iteration (ms): 4797.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.105372E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.683 | tokens per gpu per second (tgs): 1707.719 | TFLOPs: 13.74 |
g0184: [2024-08-10 12:39:38,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=24160, skipped=33, lr=[0.00019994096062245874, 0.00019994096062245874], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24160 loss: 0.8063 iter time (s): 5.601 samples/sec: 22.852
g0198:  iteration    24160/10000000 | consumed samples:      3092480 | consumed tokens:   6333399040 | elapsed time per iteration (ms): 5634.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.000463E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.716 | tokens per gpu per second (tgs): 1453.821 | TFLOPs: 11.70 |
g0184: [2024-08-10 12:40:30,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=24170, skipped=33, lr=[0.00019994086772919483, 0.00019994086772919483], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24170 loss: 0.8072 iter time (s): 5.104 samples/sec: 25.078
g0198:  iteration    24170/10000000 | consumed samples:      3093760 | consumed tokens:   6336020480 | elapsed time per iteration (ms): 5138.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.951658E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.913 | tokens per gpu per second (tgs): 1594.409 | TFLOPs: 12.83 |
g0184: [2024-08-10 12:41:18,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=24180, skipped=33, lr=[0.0001999407747629314, 0.0001999407747629314], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24180 loss: 0.8158 iter time (s): 4.754 samples/sec: 26.925
g0198:  iteration    24180/10000000 | consumed samples:      3095040 | consumed tokens:   6338641920 | elapsed time per iteration (ms): 4786.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.994565E-01 | loss scale: 131072.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.740 | tokens per gpu per second (tgs): 1711.345 | TFLOPs: 13.77 |
g0184: [2024-08-10 12:42:05,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=24190, skipped=33, lr=[0.00019994068172366855, 0.00019994068172366855], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24190 loss: 0.7826 iter time (s): 4.739 samples/sec: 27.012
g0198:  iteration    24190/10000000 | consumed samples:      3096320 | consumed tokens:   6341263360 | elapsed time per iteration (ms): 4789.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.994912E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.724 | tokens per gpu per second (tgs): 1710.364 | TFLOPs: 13.76 |
g0184: [2024-08-10 12:42:50,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=24200, skipped=33, lr=[0.00019994058861140635, 0.00019994058861140635], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24200 loss: 0.7908 iter time (s): 4.471 samples/sec: 28.629
g0198:  iteration    24200/10000000 | consumed samples:      3097600 | consumed tokens:   6343884800 | elapsed time per iteration (ms): 4504.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.995402E-01 | loss scale: 131072.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.419 | tokens per gpu per second (tgs): 1818.815 | TFLOPs: 14.64 |
g0184: [2024-08-10 12:43:39,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=24210, skipped=33, lr=[0.00019994049542614487, 0.00019994049542614487], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24210 loss: 0.8048 iter time (s): 4.817 samples/sec: 26.570
g0198:  iteration    24210/10000000 | consumed samples:      3098880 | consumed tokens:   6346506240 | elapsed time per iteration (ms): 4851.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.082099E-01 | loss scale: 131072.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.385 | tokens per gpu per second (tgs): 1688.634 | TFLOPs: 13.59 |
g0184: [2024-08-10 12:44:26,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=24220, skipped=33, lr=[0.00019994040216788413, 0.00019994040216788413], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24220 loss: 0.8026 iter time (s): 4.684 samples/sec: 27.326
g0198:  iteration    24220/10000000 | consumed samples:      3100160 | consumed tokens:   6349127680 | elapsed time per iteration (ms): 4723.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.969532E-01 | loss scale: 131072.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.097 | tokens per gpu per second (tgs): 1734.238 | TFLOPs: 13.96 |
g0184: [2024-08-10 12:45:15,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=24230, skipped=33, lr=[0.00019994030883662428, 0.00019994030883662428], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24230 loss: 0.7941 iter time (s): 4.884 samples/sec: 26.208
g0198:  iteration    24230/10000000 | consumed samples:      3101440 | consumed tokens:   6351749120 | elapsed time per iteration (ms): 4917.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.036791E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.031 | tokens per gpu per second (tgs): 1665.992 | TFLOPs: 13.41 |
g0184: [2024-08-10 12:46:07,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=24240, skipped=33, lr=[0.00019994021543236537, 0.00019994021543236537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24240 loss: 0.8039 iter time (s): 5.128 samples/sec: 24.959
g0198:  iteration    24240/10000000 | consumed samples:      3102720 | consumed tokens:   6354370560 | elapsed time per iteration (ms): 5161.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.035380E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.800 | tokens per gpu per second (tgs): 1587.173 | TFLOPs: 12.77 |
g0184: [2024-08-10 12:46:58,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=24250, skipped=33, lr=[0.00019994012195510746, 0.00019994012195510746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24250 loss: 0.8222 iter time (s): 5.034 samples/sec: 25.426
g0198:  iteration    24250/10000000 | consumed samples:      3104000 | consumed tokens:   6356992000 | elapsed time per iteration (ms): 5067.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.932909E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.262 | tokens per gpu per second (tgs): 1616.742 | TFLOPs: 13.01 |
g0184: [2024-08-10 12:47:45,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=24260, skipped=33, lr=[0.0001999400284048506, 0.0001999400284048506], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24260 loss: 0.8306 iter time (s): 4.684 samples/sec: 27.324
g0198:  iteration    24260/10000000 | consumed samples:      3105280 | consumed tokens:   6359613440 | elapsed time per iteration (ms): 4716.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.089787E-01 | loss scale: 131072.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.136 | tokens per gpu per second (tgs): 1736.735 | TFLOPs: 13.98 |
g0184: [2024-08-10 12:48:32,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=24270, skipped=33, lr=[0.0001999399347815949, 0.0001999399347815949], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24270 loss: 0.7997 iter time (s): 4.691 samples/sec: 27.287
g0198:  iteration    24270/10000000 | consumed samples:      3106560 | consumed tokens:   6362234880 | elapsed time per iteration (ms): 4723.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.861189E-01 | loss scale: 131072.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.097 | tokens per gpu per second (tgs): 1734.206 | TFLOPs: 13.96 |
g0184: [2024-08-10 12:49:18,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=24280, skipped=33, lr=[0.00019993984108534042, 0.00019993984108534042], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24280 loss: 0.8020 iter time (s): 4.535 samples/sec: 28.225
g0198:  iteration    24280/10000000 | consumed samples:      3107840 | consumed tokens:   6364856320 | elapsed time per iteration (ms): 4567.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.994233E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.024 | tokens per gpu per second (tgs): 1793.521 | TFLOPs: 14.43 |
g0184: [2024-08-10 12:50:05,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=24290, skipped=33, lr=[0.0001999397473160872, 0.0001999397473160872], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24290 loss: 0.8209 iter time (s): 4.652 samples/sec: 27.517
g0198:  iteration    24290/10000000 | consumed samples:      3109120 | consumed tokens:   6367477760 | elapsed time per iteration (ms): 4684.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.073554E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.326 | tokens per gpu per second (tgs): 1748.896 | TFLOPs: 14.07 |
g0184: [2024-08-10 12:50:51,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=24300, skipped=33, lr=[0.00019993965347383537, 0.00019993965347383537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24300 loss: 0.7921 iter time (s): 4.648 samples/sec: 27.537
g0198:  iteration    24300/10000000 | consumed samples:      3110400 | consumed tokens:   6370099200 | elapsed time per iteration (ms): 4681.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.964753E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.339 | tokens per gpu per second (tgs): 1749.712 | TFLOPs: 14.08 |
g0184: [2024-08-10 12:51:37,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=24310, skipped=33, lr=[0.00019993955955858496, 0.00019993955955858496], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24310 loss: 0.7951 iter time (s): 4.531 samples/sec: 28.250
g0198:  iteration    24310/10000000 | consumed samples:      3111680 | consumed tokens:   6372720640 | elapsed time per iteration (ms): 4563.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.957416E-01 | loss scale: 131072.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.050 | tokens per gpu per second (tgs): 1795.189 | TFLOPs: 14.45 |
g0184: [2024-08-10 12:52:24,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=24320, skipped=33, lr=[0.0001999394655703361, 0.0001999394655703361], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24320 loss: 0.7899 iter time (s): 4.653 samples/sec: 27.508
g0198:  iteration    24320/10000000 | consumed samples:      3112960 | consumed tokens:   6375342080 | elapsed time per iteration (ms): 4690.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.979510E-01 | loss scale: 131072.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.290 | tokens per gpu per second (tgs): 1746.578 | TFLOPs: 14.06 |
g0184: [2024-08-10 12:53:12,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=24330, skipped=33, lr=[0.00019993937150908876, 0.00019993937150908876], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24330 loss: 0.8158 iter time (s): 4.759 samples/sec: 26.897
g0198:  iteration    24330/10000000 | consumed samples:      3114240 | consumed tokens:   6377963520 | elapsed time per iteration (ms): 4792.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.975636E-01 | loss scale: 131072.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.711 | tokens per gpu per second (tgs): 1709.495 | TFLOPs: 13.76 |
g0184: [2024-08-10 12:53:59,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=24340, skipped=33, lr=[0.0001999392773748431, 0.0001999392773748431], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24340 loss: 0.7914 iter time (s): 4.666 samples/sec: 27.435
g0198:  iteration    24340/10000000 | consumed samples:      3115520 | consumed tokens:   6380584960 | elapsed time per iteration (ms): 4699.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.928903E-01 | loss scale: 131072.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.237 | tokens per gpu per second (tgs): 1743.140 | TFLOPs: 14.03 |
g0184: [2024-08-10 12:54:48,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=24350, skipped=33, lr=[0.00019993918316759914, 0.00019993918316759914], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24350 loss: 0.8097 iter time (s): 4.915 samples/sec: 26.042
g0198:  iteration    24350/10000000 | consumed samples:      3116800 | consumed tokens:   6383206400 | elapsed time per iteration (ms): 4949.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.089164E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.864 | tokens per gpu per second (tgs): 1655.300 | TFLOPs: 13.32 |
g0184: [2024-08-10 12:55:34,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=24360, skipped=33, lr=[0.000199939088887357, 0.000199939088887357], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24360 loss: 0.7966 iter time (s): 4.513 samples/sec: 28.361
g0198:  iteration    24360/10000000 | consumed samples:      3118080 | consumed tokens:   6385827840 | elapsed time per iteration (ms): 4546.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.068735E-01 | loss scale: 131072.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.156 | tokens per gpu per second (tgs): 1801.962 | TFLOPs: 14.50 |
g0184: [2024-08-10 12:56:19,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=24370, skipped=33, lr=[0.00019993899453411672, 0.00019993899453411672], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24370 loss: 0.7753 iter time (s): 4.447 samples/sec: 28.786
g0198:  iteration    24370/10000000 | consumed samples:      3119360 | consumed tokens:   6388449280 | elapsed time per iteration (ms): 4480.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.016288E-01 | loss scale: 131072.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.572 | tokens per gpu per second (tgs): 1828.577 | TFLOPs: 14.71 |
g0184: [2024-08-10 12:57:10,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=24380, skipped=33, lr=[0.00019993890010787838, 0.00019993890010787838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24380 loss: 0.8069 iter time (s): 5.092 samples/sec: 25.138
g0198:  iteration    24380/10000000 | consumed samples:      3120640 | consumed tokens:   6391070720 | elapsed time per iteration (ms): 5127.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.018269E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.965 | tokens per gpu per second (tgs): 1597.782 | TFLOPs: 12.86 |
g0184: [2024-08-10 12:57:57,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=24390, skipped=33, lr=[0.00019993880560864206, 0.00019993880560864206], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24390 loss: 0.7984 iter time (s): 4.717 samples/sec: 27.138
g0198:  iteration    24390/10000000 | consumed samples:      3121920 | consumed tokens:   6393692160 | elapsed time per iteration (ms): 4749.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.024899E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.952 | tokens per gpu per second (tgs): 1724.958 | TFLOPs: 13.88 |
g0184: [2024-08-10 12:58:42,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=24400, skipped=33, lr=[0.00019993871103640782, 0.00019993871103640782], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24400 loss: 0.8199 iter time (s): 4.380 samples/sec: 29.227
g0198:  iteration    24400/10000000 | consumed samples:      3123200 | consumed tokens:   6396313600 | elapsed time per iteration (ms): 4412.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.095454E-01 | loss scale: 131072.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.008 | tokens per gpu per second (tgs): 1856.487 | TFLOPs: 14.94 |
g0184: [2024-08-10 12:59:26,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=24410, skipped=33, lr=[0.0001999386163911757, 0.0001999386163911757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24410 loss: 0.7745 iter time (s): 4.421 samples/sec: 28.955
g0198:  iteration    24410/10000000 | consumed samples:      3124480 | consumed tokens:   6398935040 | elapsed time per iteration (ms): 4453.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.883147E-01 | loss scale: 131072.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.741 | tokens per gpu per second (tgs): 1839.425 | TFLOPs: 14.80 |
g0184: [2024-08-10 13:00:12,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=24420, skipped=33, lr=[0.00019993852167294587, 0.00019993852167294587], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24420 loss: 0.8222 iter time (s): 4.577 samples/sec: 27.967
g0198:  iteration    24420/10000000 | consumed samples:      3125760 | consumed tokens:   6401556480 | elapsed time per iteration (ms): 4611.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.027806E-01 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.759 | tokens per gpu per second (tgs): 1776.584 | TFLOPs: 14.30 |
g0184: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 13:00:35,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 13:00:35,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 13:00:59,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=24430, skipped=33, lr=[0.0001999384268817183, 0.0001999384268817183], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24430 loss: 0.8007 iter time (s): 4.642 samples/sec: 27.576
g0198:  iteration    24430/10000000 | consumed samples:      3127040 | consumed tokens:   6404177920 | elapsed time per iteration (ms): 4674.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.938009E-01 | loss scale: 262144.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.384 | tokens per gpu per second (tgs): 1752.594 | TFLOPs: 14.10 |
g0184: [2024-08-10 13:01:44,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=24440, skipped=33, lr=[0.00019993833201749315, 0.00019993833201749315], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24440 loss: 0.7779 iter time (s): 4.490 samples/sec: 28.506
g0198:  iteration    24440/10000000 | consumed samples:      3128320 | consumed tokens:   6406799360 | elapsed time per iteration (ms): 4523.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.013351E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.299 | tokens per gpu per second (tgs): 1811.167 | TFLOPs: 14.57 |
g0184: [2024-08-10 13:02:30,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=24450, skipped=33, lr=[0.0001999382370802704, 0.0001999382370802704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24450 loss: 0.7991 iter time (s): 4.591 samples/sec: 27.879
g0198:  iteration    24450/10000000 | consumed samples:      3129600 | consumed tokens:   6409420800 | elapsed time per iteration (ms): 4624.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.959346E-01 | loss scale: 262144.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.681 | tokens per gpu per second (tgs): 1771.597 | TFLOPs: 14.26 |
g0184: [2024-08-10 13:03:19,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=24460, skipped=33, lr=[0.0001999381420700502, 0.0001999381420700502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24460 loss: 0.8330 iter time (s): 4.832 samples/sec: 26.488
g0198:  iteration    24460/10000000 | consumed samples:      3130880 | consumed tokens:   6412042240 | elapsed time per iteration (ms): 4866.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.988590E-01 | loss scale: 262144.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.303 | tokens per gpu per second (tgs): 1683.400 | TFLOPs: 13.55 |
g0184: [2024-08-10 13:04:10,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=24470, skipped=33, lr=[0.0001999380469868326, 0.0001999380469868326], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24470 loss: 0.7819 iter time (s): 5.075 samples/sec: 25.221
g0198:  iteration    24470/10000000 | consumed samples:      3132160 | consumed tokens:   6414663680 | elapsed time per iteration (ms): 5107.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.810325E-01 | loss scale: 262144.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.059 | tokens per gpu per second (tgs): 1603.793 | TFLOPs: 12.91 |
g0184: [2024-08-10 13:05:00,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=24480, skipped=33, lr=[0.00019993795183061765, 0.00019993795183061765], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24480 loss: 0.7731 iter time (s): 4.974 samples/sec: 25.735
g0198:  iteration    24480/10000000 | consumed samples:      3133440 | consumed tokens:   6417285120 | elapsed time per iteration (ms): 5018.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.053939E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.508 | tokens per gpu per second (tgs): 1632.494 | TFLOPs: 13.14 |
g0184: [2024-08-10 13:05:47,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=24490, skipped=33, lr=[0.00019993785660140547, 0.00019993785660140547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24490 loss: 0.8220 iter time (s): 4.606 samples/sec: 27.792
g0198:  iteration    24490/10000000 | consumed samples:      3134720 | consumed tokens:   6419906560 | elapsed time per iteration (ms): 4640.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.988151E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.585 | tokens per gpu per second (tgs): 1765.410 | TFLOPs: 14.21 |
g0184: [2024-08-10 13:06:31,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=24500, skipped=33, lr=[0.00019993776129919607, 0.00019993776129919607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24500 loss: 0.8149 iter time (s): 4.374 samples/sec: 29.264
g0198:  iteration    24500/10000000 | consumed samples:      3136000 | consumed tokens:   6422528000 | elapsed time per iteration (ms): 4407.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.041339E-01 | loss scale: 262144.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.041 | tokens per gpu per second (tgs): 1858.629 | TFLOPs: 14.96 |
g0184: [2024-08-10 13:07:20,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=24510, skipped=33, lr=[0.00019993766592398956, 0.00019993766592398956], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24510 loss: 0.8181 iter time (s): 4.872 samples/sec: 26.271
g0198:  iteration    24510/10000000 | consumed samples:      3137280 | consumed tokens:   6425149440 | elapsed time per iteration (ms): 4905.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.068982E-01 | loss scale: 262144.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.096 | tokens per gpu per second (tgs): 1670.122 | TFLOPs: 13.44 |
g0184: [2024-08-10 13:08:09,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=24520, skipped=33, lr=[0.00019993757047578602, 0.00019993757047578602], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24520 loss: 0.8200 iter time (s): 4.867 samples/sec: 26.301
g0198:  iteration    24520/10000000 | consumed samples:      3138560 | consumed tokens:   6427770880 | elapsed time per iteration (ms): 4899.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.109399E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.125 | tokens per gpu per second (tgs): 1672.011 | TFLOPs: 13.45 |
g0184: [2024-08-10 13:08:55,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=24530, skipped=33, lr=[0.00019993747495458556, 0.00019993747495458556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24530 loss: 0.8137 iter time (s): 4.554 samples/sec: 28.107
g0198:  iteration    24530/10000000 | consumed samples:      3139840 | consumed tokens:   6430392320 | elapsed time per iteration (ms): 4600.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.976976E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.823 | tokens per gpu per second (tgs): 1780.642 | TFLOPs: 14.33 |
g0184: [2024-08-10 13:09:39,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=24540, skipped=33, lr=[0.00019993737936038815, 0.00019993737936038815], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24540 loss: 0.8224 iter time (s): 4.376 samples/sec: 29.252
g0198:  iteration    24540/10000000 | consumed samples:      3141120 | consumed tokens:   6433013760 | elapsed time per iteration (ms): 4408.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.990416E-01 | loss scale: 262144.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.036 | tokens per gpu per second (tgs): 1858.325 | TFLOPs: 14.95 |
g0184: [2024-08-10 13:10:27,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=24550, skipped=33, lr=[0.00019993728369319396, 0.00019993728369319396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24550 loss: 0.7972 iter time (s): 4.809 samples/sec: 26.615
g0198:  iteration    24550/10000000 | consumed samples:      3142400 | consumed tokens:   6435635200 | elapsed time per iteration (ms): 4856.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.957218E-01 | loss scale: 262144.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.358 | tokens per gpu per second (tgs): 1686.912 | TFLOPs: 13.57 |
g0184: [2024-08-10 13:11:16,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=24560, skipped=33, lr=[0.00019993718795300298, 0.00019993718795300298], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24560 loss: 0.7885 iter time (s): 4.782 samples/sec: 26.765
g0198:  iteration    24560/10000000 | consumed samples:      3143680 | consumed tokens:   6438256640 | elapsed time per iteration (ms): 4819.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.981967E-01 | loss scale: 262144.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.562 | tokens per gpu per second (tgs): 1699.946 | TFLOPs: 13.68 |
g0184: [2024-08-10 13:12:02,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=24570, skipped=33, lr=[0.00019993709213981538, 0.00019993709213981538], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24570 loss: 0.8189 iter time (s): 4.594 samples/sec: 27.863
g0198:  iteration    24570/10000000 | consumed samples:      3144960 | consumed tokens:   6440878080 | elapsed time per iteration (ms): 4627.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.974209E-01 | loss scale: 262144.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.663 | tokens per gpu per second (tgs): 1770.430 | TFLOPs: 14.25 |
g0184: [2024-08-10 13:12:49,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=24580, skipped=33, lr=[0.00019993699625363114, 0.00019993699625363114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24580 loss: 0.7845 iter time (s): 4.652 samples/sec: 27.514
g0198:  iteration    24580/10000000 | consumed samples:      3146240 | consumed tokens:   6443499520 | elapsed time per iteration (ms): 4684.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.049111E-01 | loss scale: 262144.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.324 | tokens per gpu per second (tgs): 1748.741 | TFLOPs: 14.07 |
g0184: [2024-08-10 13:13:38,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=24590, skipped=33, lr=[0.0001999369002944504, 0.0001999369002944504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24590 loss: 0.8182 iter time (s): 4.866 samples/sec: 26.306
g0198:  iteration    24590/10000000 | consumed samples:      3147520 | consumed tokens:   6446120960 | elapsed time per iteration (ms): 4898.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.058347E-01 | loss scale: 262144.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.132 | tokens per gpu per second (tgs): 1672.471 | TFLOPs: 13.46 |
g0184: [2024-08-10 13:14:27,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=24600, skipped=33, lr=[0.00019993680426227319, 0.00019993680426227319], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24600 loss: 0.8204 iter time (s): 4.856 samples/sec: 26.361
g0198:  iteration    24600/10000000 | consumed samples:      3148800 | consumed tokens:   6448742400 | elapsed time per iteration (ms): 4888.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.885030E-01 | loss scale: 262144.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.187 | tokens per gpu per second (tgs): 1675.956 | TFLOPs: 13.49 |
g0184: [2024-08-10 13:15:15,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=24610, skipped=33, lr=[0.00019993670815709964, 0.00019993670815709964], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24610 loss: 0.7897 iter time (s): 4.822 samples/sec: 26.546
g0198:  iteration    24610/10000000 | consumed samples:      3150080 | consumed tokens:   6451363840 | elapsed time per iteration (ms): 4875.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.025090E-01 | loss scale: 262144.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.253 | tokens per gpu per second (tgs): 1680.211 | TFLOPs: 13.52 |
g0184: [2024-08-10 13:16:04,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=24620, skipped=33, lr=[0.00019993661197892975, 0.00019993661197892975], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24620 loss: 0.7898 iter time (s): 4.860 samples/sec: 26.336
g0198:  iteration    24620/10000000 | consumed samples:      3151360 | consumed tokens:   6453985280 | elapsed time per iteration (ms): 4894.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.917793E-01 | loss scale: 262144.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.151 | tokens per gpu per second (tgs): 1673.641 | TFLOPs: 13.47 |
g0184: [2024-08-10 13:16:56,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=24630, skipped=33, lr=[0.00019993651572776364, 0.00019993651572776364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24630 loss: 0.8085 iter time (s): 5.129 samples/sec: 24.954
g0198:  iteration    24630/10000000 | consumed samples:      3152640 | consumed tokens:   6456606720 | elapsed time per iteration (ms): 5162.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.989372E-01 | loss scale: 262144.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.796 | tokens per gpu per second (tgs): 1586.915 | TFLOPs: 12.77 |
g0184: [2024-08-10 13:17:46,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=24640, skipped=33, lr=[0.0001999364194036014, 0.0001999364194036014], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24640 loss: 0.7827 iter time (s): 4.936 samples/sec: 25.931
g0198:  iteration    24640/10000000 | consumed samples:      3153920 | consumed tokens:   6459228160 | elapsed time per iteration (ms): 4969.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.014907E-01 | loss scale: 262144.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.759 | tokens per gpu per second (tgs): 1648.589 | TFLOPs: 13.27 |
g0184: [2024-08-10 13:18:34,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=24650, skipped=33, lr=[0.00019993632300644308, 0.00019993632300644308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24650 loss: 0.7768 iter time (s): 4.735 samples/sec: 27.034
g0198:  iteration    24650/10000000 | consumed samples:      3155200 | consumed tokens:   6461849600 | elapsed time per iteration (ms): 4784.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.981740E-01 | loss scale: 262144.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.752 | tokens per gpu per second (tgs): 1712.139 | TFLOPs: 13.78 |
g0184: [2024-08-10 13:19:24,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=24660, skipped=33, lr=[0.00019993622653628873, 0.00019993622653628873], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24660 loss: 0.7989 iter time (s): 4.966 samples/sec: 25.775
g0198:  iteration    24660/10000000 | consumed samples:      3156480 | consumed tokens:   6464471040 | elapsed time per iteration (ms): 4998.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.984913E-01 | loss scale: 262144.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.606 | tokens per gpu per second (tgs): 1638.780 | TFLOPs: 13.19 |
g0184: [2024-08-10 13:20:09,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=24670, skipped=33, lr=[0.00019993612999313846, 0.00019993612999313846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24670 loss: 0.8223 iter time (s): 4.477 samples/sec: 28.588
g0198:  iteration    24670/10000000 | consumed samples:      3157760 | consumed tokens:   6467092480 | elapsed time per iteration (ms): 4510.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.059006E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.380 | tokens per gpu per second (tgs): 1816.332 | TFLOPs: 14.62 |
g0184: [2024-08-10 13:20:53,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=24680, skipped=33, lr=[0.00019993603337699234, 0.00019993603337699234], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24680 loss: 0.8051 iter time (s): 4.406 samples/sec: 29.051
g0198:  iteration    24680/10000000 | consumed samples:      3159040 | consumed tokens:   6469713920 | elapsed time per iteration (ms): 4438.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.019037E-01 | loss scale: 262144.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.837 | tokens per gpu per second (tgs): 1845.600 | TFLOPs: 14.85 |
g0184: [2024-08-10 13:21:41,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=24690, skipped=33, lr=[0.00019993593668785043, 0.00019993593668785043], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24690 loss: 0.8223 iter time (s): 4.769 samples/sec: 26.842
g0198:  iteration    24690/10000000 | consumed samples:      3160320 | consumed tokens:   6472335360 | elapsed time per iteration (ms): 4801.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.983110E-01 | loss scale: 262144.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.656 | tokens per gpu per second (tgs): 1706.007 | TFLOPs: 13.73 |
g0184: [2024-08-10 13:22:28,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=24700, skipped=33, lr=[0.0001999358399257128, 0.0001999358399257128], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24700 loss: 0.8114 iter time (s): 4.682 samples/sec: 27.341
g0198:  iteration    24700/10000000 | consumed samples:      3161600 | consumed tokens:   6474956800 | elapsed time per iteration (ms): 4714.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.031454E-01 | loss scale: 262144.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.150 | tokens per gpu per second (tgs): 1737.596 | TFLOPs: 13.98 |
g0184: [2024-08-10 13:23:12,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=24710, skipped=33, lr=[0.00019993574309057954, 0.00019993574309057954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24710 loss: 0.7960 iter time (s): 4.383 samples/sec: 29.206
g0198:  iteration    24710/10000000 | consumed samples:      3162880 | consumed tokens:   6477578240 | elapsed time per iteration (ms): 4414.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.937822E-01 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.992 | tokens per gpu per second (tgs): 1855.518 | TFLOPs: 14.93 |
g0184: [2024-08-10 13:23:59,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=24720, skipped=33, lr=[0.00019993564618245072, 0.00019993564618245072], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24720 loss: 0.8148 iter time (s): 4.621 samples/sec: 27.702
g0198:  iteration    24720/10000000 | consumed samples:      3164160 | consumed tokens:   6480199680 | elapsed time per iteration (ms): 4653.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.988471E-01 | loss scale: 262144.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.507 | tokens per gpu per second (tgs): 1760.449 | TFLOPs: 14.17 |
g0184: [2024-08-10 13:24:43,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=24730, skipped=33, lr=[0.00019993554920132641, 0.00019993554920132641], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24730 loss: 0.8050 iter time (s): 4.350 samples/sec: 29.423
g0198:  iteration    24730/10000000 | consumed samples:      3165440 | consumed tokens:   6482821120 | elapsed time per iteration (ms): 4384.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.005716E-01 | loss scale: 262144.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.196 | tokens per gpu per second (tgs): 1868.521 | TFLOPs: 15.04 |
g0195: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 24737
g0184: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 24737
g0185: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: Grad overflow on iteration 24737
g0185: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 24737
g0185: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 24737
g0195: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: Grad overflow on iteration 24737
g0185: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: Grad overflow on iteration 24737
g0185: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 24737
g0188: Grad overflow on iteration 24737
g0185: Grad overflow on iteration 24737
g0188: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: Grad overflow on iteration 24737
g0185: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 24737
g0188: Grad overflow on iteration 24737
g0197: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 24737
g0195: Grad overflow on iteration 24737
g0194: Grad overflow on iteration 24737
g0195: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: Grad overflow on iteration 24737
g0188: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 24737
g0194: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 24737
g0194: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: Grad overflow on iteration 24737
g0194: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: Grad overflow on iteration 24737
g0188: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: Grad overflow on iteration 24737
g0198: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 24737
g0184: Grad overflow on iteration 24737
g0187: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 24737
g0198: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 24737
g0197: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 24737
g0198: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: Grad overflow on iteration 24737
g0187: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 24737
g0197: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 24737
g0197: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: Grad overflow on iteration 24737
g0197: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 13:25:18,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 24737
g0184: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 13:25:18,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 13:25:18,315] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0194: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 24739
g0194: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 24739
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 24739
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 24739
g0194: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 24739
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 24739
g0194: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: Grad overflow on iteration 24739
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 24739
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 24739
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 24739
g0198: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 24739
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 24739
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 24739
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 24739
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 24739
g0195: Grad overflow on iteration 24739
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: Grad overflow on iteration 24739
g0197: Grad overflow on iteration 24739
g0198: Grad overflow on iteration 24739
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 24739
g0197: Grad overflow on iteration 24739
g0198: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 24739
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 24739
g0194: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 24739
g0194: Grad overflow on iteration 24739
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: Grad overflow on iteration 24739
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 24739
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: Grad overflow on iteration 24739
g0198: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: Grad overflow on iteration 24739
g0188: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: Grad overflow on iteration 24739
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: Grad overflow on iteration 24739
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 13:25:27,097] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 24739
g0184: [2024-08-10 13:25:27,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0194: [2024-08-10 13:25:27,098] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 13:25:27,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=24740, skipped=35, lr=[0.00019993546185590347, 0.00019993546185590347], mom=[(0.9, 0.95), (0.9, 0.95)]
g0198: [2024-08-10 13:25:27,098] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: steps: 24740 loss: 0.8078 iter time (s): 4.357 samples/sec: 29.379
g0198:  iteration    24740/10000000 | consumed samples:      3166720 | consumed tokens:   6485442560 | elapsed time per iteration (ms): 4389.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.047960E-01 | loss scale: 65536.0 | grad norm: 0.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.160 | tokens per gpu per second (tgs): 1866.216 | TFLOPs: 15.02 |
g0195: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 24740
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 24740
g0184: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 24740
g0187: Grad overflow on iteration 24740
g0195: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 24740
g0187: Grad overflow on iteration 24740
g0184: Grad overflow on iteration 24740
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 24740
g0195: Grad overflow on iteration 24740
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 24740
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 24740
g0185: Grad overflow on iteration 24740
g0188: Grad overflow on iteration 24740
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: Grad overflow on iteration 24740
g0194: Grad overflow on iteration 24740
g0184: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 24740
g0184: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: Grad overflow on iteration 24740
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 24740
g0197: Grad overflow on iteration 24740
g0195: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 24740
g0195: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 24740
g0195: Grad overflow on iteration 24740
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 24740
g0185: Grad overflow on iteration 24740
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 24740
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 24740
g0185: Grad overflow on iteration 24740
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: Grad overflow on iteration 24740
g0197: Grad overflow on iteration 24740
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 24740
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: Grad overflow on iteration 24740
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: Grad overflow on iteration 24740
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 13:25:31,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 13:25:31,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 13:25:31,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 13:25:31,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0184: [2024-08-10 13:26:14,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=24750, skipped=36, lr=[0.00019993535502009163, 0.00019993535502009163], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24750 loss: 0.7924 iter time (s): 4.741 samples/sec: 27.001
g0198:  iteration    24750/10000000 | consumed samples:      3168000 | consumed tokens:   6488064000 | elapsed time per iteration (ms): 4773.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.971652E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.817 | tokens per gpu per second (tgs): 1716.264 | TFLOPs: 13.81 |
g0184: [2024-08-10 13:27:01,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=24760, skipped=36, lr=[0.00019993525781998134, 0.00019993525781998134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24760 loss: 0.8432 iter time (s): 4.673 samples/sec: 27.393
g0198:  iteration    24760/10000000 | consumed samples:      3169280 | consumed tokens:   6490685440 | elapsed time per iteration (ms): 4705.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.106323E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.205 | tokens per gpu per second (tgs): 1741.097 | TFLOPs: 14.01 |
g0184: [2024-08-10 13:27:50,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=24770, skipped=36, lr=[0.00019993516054687583, 0.00019993516054687583], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24770 loss: 0.8019 iter time (s): 4.801 samples/sec: 26.658
g0198:  iteration    24770/10000000 | consumed samples:      3170560 | consumed tokens:   6493306880 | elapsed time per iteration (ms): 4834.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.940388E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.474 | tokens per gpu per second (tgs): 1694.358 | TFLOPs: 13.63 |
g0184: [2024-08-10 13:28:37,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=24780, skipped=36, lr=[0.00019993506320077525, 0.00019993506320077525], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24780 loss: 0.7888 iter time (s): 4.668 samples/sec: 27.423
g0198:  iteration    24780/10000000 | consumed samples:      3171840 | consumed tokens:   6495928320 | elapsed time per iteration (ms): 4701.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.003547E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.228 | tokens per gpu per second (tgs): 1742.573 | TFLOPs: 14.02 |
g0184: [2024-08-10 13:29:24,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=24790, skipped=36, lr=[0.0001999349657816796, 0.0001999349657816796], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24790 loss: 0.7802 iter time (s): 4.717 samples/sec: 27.138
g0198:  iteration    24790/10000000 | consumed samples:      3173120 | consumed tokens:   6498549760 | elapsed time per iteration (ms): 4750.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.921926E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.946 | tokens per gpu per second (tgs): 1724.525 | TFLOPs: 13.88 |
g0184: [2024-08-10 13:30:10,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=24800, skipped=36, lr=[0.000199934868289589, 0.000199934868289589], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24800 loss: 0.8047 iter time (s): 4.556 samples/sec: 28.096
g0198:  iteration    24800/10000000 | consumed samples:      3174400 | consumed tokens:   6501171200 | elapsed time per iteration (ms): 4595.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.980313E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.852 | tokens per gpu per second (tgs): 1782.547 | TFLOPs: 14.34 |
g0184: [2024-08-10 13:30:59,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=24810, skipped=36, lr=[0.00019993477072450348, 0.00019993477072450348], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24810 loss: 0.7876 iter time (s): 4.820 samples/sec: 26.554
g0198:  iteration    24810/10000000 | consumed samples:      3175680 | consumed tokens:   6503792640 | elapsed time per iteration (ms): 4853.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.951119E-01 | loss scale: 32768.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.375 | tokens per gpu per second (tgs): 1688.026 | TFLOPs: 13.58 |
g0184: [2024-08-10 13:31:48,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=24820, skipped=36, lr=[0.00019993467308642316, 0.00019993467308642316], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24820 loss: 0.7904 iter time (s): 4.855 samples/sec: 26.363
g0198:  iteration    24820/10000000 | consumed samples:      3176960 | consumed tokens:   6506414080 | elapsed time per iteration (ms): 4888.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.986382E-01 | loss scale: 32768.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.182 | tokens per gpu per second (tgs): 1675.657 | TFLOPs: 13.48 |
g0184: [2024-08-10 13:32:35,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=24830, skipped=36, lr=[0.00019993457537534814, 0.00019993457537534814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24830 loss: 0.7929 iter time (s): 4.729 samples/sec: 27.065
g0198:  iteration    24830/10000000 | consumed samples:      3178240 | consumed tokens:   6509035520 | elapsed time per iteration (ms): 4763.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.071692E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.871 | tokens per gpu per second (tgs): 1719.734 | TFLOPs: 13.84 |
g0184: [2024-08-10 13:33:22,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=24840, skipped=36, lr=[0.0001999344775912784, 0.0001999344775912784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24840 loss: 0.8038 iter time (s): 4.605 samples/sec: 27.797
g0198:  iteration    24840/10000000 | consumed samples:      3179520 | consumed tokens:   6511656960 | elapsed time per iteration (ms): 4637.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.021137E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.601 | tokens per gpu per second (tgs): 1766.483 | TFLOPs: 14.22 |
g0184: [2024-08-10 13:34:08,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=24850, skipped=36, lr=[0.00019993437973421413, 0.00019993437973421413], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24850 loss: 0.8027 iter time (s): 4.639 samples/sec: 27.589
g0198:  iteration    24850/10000000 | consumed samples:      3180800 | consumed tokens:   6514278400 | elapsed time per iteration (ms): 4681.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.005385E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.342 | tokens per gpu per second (tgs): 1749.915 | TFLOPs: 14.08 |
g0184: [2024-08-10 13:34:56,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=24860, skipped=36, lr=[0.00019993428180415532, 0.00019993428180415532], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24860 loss: 0.7989 iter time (s): 4.737 samples/sec: 27.018
g0198:  iteration    24860/10000000 | consumed samples:      3182080 | consumed tokens:   6516899840 | elapsed time per iteration (ms): 4770.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.998452E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.831 | tokens per gpu per second (tgs): 1717.212 | TFLOPs: 13.82 |
g0184: [2024-08-10 13:35:43,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=24870, skipped=36, lr=[0.00019993418380110208, 0.00019993418380110208], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24870 loss: 0.7502 iter time (s): 4.616 samples/sec: 27.732
g0198:  iteration    24870/10000000 | consumed samples:      3183360 | consumed tokens:   6519521280 | elapsed time per iteration (ms): 4649.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.904776E-01 | loss scale: 32768.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.530 | tokens per gpu per second (tgs): 1761.896 | TFLOPs: 14.18 |
g0184: [2024-08-10 13:36:31,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=24880, skipped=36, lr=[0.00019993408572505444, 0.00019993408572505444], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24880 loss: 0.7719 iter time (s): 4.831 samples/sec: 26.497
g0198:  iteration    24880/10000000 | consumed samples:      3184640 | consumed tokens:   6522142720 | elapsed time per iteration (ms): 4863.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.022607E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.319 | tokens per gpu per second (tgs): 1684.442 | TFLOPs: 13.55 |
g0184: [2024-08-10 13:37:17,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=24890, skipped=36, lr=[0.00019993398757601257, 0.00019993398757601257], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24890 loss: 0.7799 iter time (s): 4.528 samples/sec: 28.266
g0198:  iteration    24890/10000000 | consumed samples:      3185920 | consumed tokens:   6524764160 | elapsed time per iteration (ms): 4561.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.977684E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.062 | tokens per gpu per second (tgs): 1795.995 | TFLOPs: 14.45 |
g0184: [2024-08-10 13:38:04,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=24900, skipped=36, lr=[0.0001999338893539765, 0.0001999338893539765], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24900 loss: 0.8262 iter time (s): 4.658 samples/sec: 27.477
g0198:  iteration    24900/10000000 | consumed samples:      3187200 | consumed tokens:   6527385600 | elapsed time per iteration (ms): 4691.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.048921E-01 | loss scale: 32768.0 | grad norm: 0.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.281 | tokens per gpu per second (tgs): 1745.982 | TFLOPs: 14.05 |
g0184: [2024-08-10 13:38:49,484] [INFO] [logging.py:96:log_dist] [Rank 0] step=24910, skipped=36, lr=[0.00019993379105894622, 0.00019993379105894622], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24910 loss: 0.7997 iter time (s): 4.486 samples/sec: 28.536
g0198:  iteration    24910/10000000 | consumed samples:      3188480 | consumed tokens:   6530007040 | elapsed time per iteration (ms): 4518.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.929207E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.329 | tokens per gpu per second (tgs): 1813.050 | TFLOPs: 14.59 |
g0184: [2024-08-10 13:39:37,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=24920, skipped=36, lr=[0.00019993369269092194, 0.00019993369269092194], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24920 loss: 0.8041 iter time (s): 4.732 samples/sec: 27.051
g0198:  iteration    24920/10000000 | consumed samples:      3189760 | consumed tokens:   6532628480 | elapsed time per iteration (ms): 4765.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.950056E-01 | loss scale: 32768.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.861 | tokens per gpu per second (tgs): 1719.136 | TFLOPs: 13.83 |
g0184: [2024-08-10 13:40:24,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=24930, skipped=36, lr=[0.00019993359424990364, 0.00019993359424990364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24930 loss: 0.8217 iter time (s): 4.703 samples/sec: 27.217
g0198:  iteration    24930/10000000 | consumed samples:      3191040 | consumed tokens:   6535249920 | elapsed time per iteration (ms): 4737.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.009948E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.018 | tokens per gpu per second (tgs): 1729.157 | TFLOPs: 13.91 |
g0184: [2024-08-10 13:41:13,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=24940, skipped=36, lr=[0.00019993349573589148, 0.00019993349573589148], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24940 loss: 0.8120 iter time (s): 4.901 samples/sec: 26.118
g0198:  iteration    24940/10000000 | consumed samples:      3192320 | consumed tokens:   6537871360 | elapsed time per iteration (ms): 4935.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.061855E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.936 | tokens per gpu per second (tgs): 1659.878 | TFLOPs: 13.36 |
g0184: [2024-08-10 13:42:00,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=24950, skipped=36, lr=[0.00019993339714888546, 0.00019993339714888546], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24950 loss: 0.7882 iter time (s): 4.584 samples/sec: 27.925
g0198:  iteration    24950/10000000 | consumed samples:      3193600 | consumed tokens:   6540492800 | elapsed time per iteration (ms): 4620.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.972317E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.706 | tokens per gpu per second (tgs): 1773.163 | TFLOPs: 14.27 |
g0184: [2024-08-10 13:42:46,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=24960, skipped=36, lr=[0.00019993329848888567, 0.00019993329848888567], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24960 loss: 0.7821 iter time (s): 4.651 samples/sec: 27.519
g0198:  iteration    24960/10000000 | consumed samples:      3194880 | consumed tokens:   6543114240 | elapsed time per iteration (ms): 4684.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.024746E-01 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.324 | tokens per gpu per second (tgs): 1748.722 | TFLOPs: 14.07 |
g0184: [2024-08-10 13:43:32,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=24970, skipped=36, lr=[0.00019993319975589223, 0.00019993319975589223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24970 loss: 0.8132 iter time (s): 4.500 samples/sec: 28.448
g0198:  iteration    24970/10000000 | consumed samples:      3196160 | consumed tokens:   6545735680 | elapsed time per iteration (ms): 4533.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.932909E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.232 | tokens per gpu per second (tgs): 1806.832 | TFLOPs: 14.54 |
g0184: [2024-08-10 13:44:18,812] [INFO] [logging.py:96:log_dist] [Rank 0] step=24980, skipped=36, lr=[0.00019993310094990516, 0.00019993310094990516], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24980 loss: 0.7709 iter time (s): 4.621 samples/sec: 27.698
g0198:  iteration    24980/10000000 | consumed samples:      3197440 | consumed tokens:   6548357120 | elapsed time per iteration (ms): 4655.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.044357E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.492 | tokens per gpu per second (tgs): 1759.473 | TFLOPs: 14.16 |
g0184: [2024-08-10 13:45:06,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=24990, skipped=36, lr=[0.0001999330020709246, 0.0001999330020709246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 24990 loss: 0.8022 iter time (s): 4.757 samples/sec: 26.906
g0198:  iteration    24990/10000000 | consumed samples:      3198720 | consumed tokens:   6550978560 | elapsed time per iteration (ms): 4790.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.055812E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.719 | tokens per gpu per second (tgs): 1710.048 | TFLOPs: 13.76 |
g0184: [2024-08-10 13:45:56,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=36, lr=[0.00019993290311895055, 0.00019993290311895055], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25000 loss: 0.8170 iter time (s): 4.911 samples/sec: 26.064
g0198:  iteration    25000/10000000 | consumed samples:      3200000 | consumed tokens:   6553600000 | elapsed time per iteration (ms): 4944.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.939660E-01 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.889 | tokens per gpu per second (tgs): 1656.920 | TFLOPs: 13.33 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 25000 | lm loss value: 7.971933E-01 | lm loss PPL: 2.219303E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   25000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 13:53:34,515] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step25000 is about to be saved!
g0184: [2024-08-10 13:53:34,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0198: [2024-08-10 13:53:34,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0198: [2024-08-10 13:53:34,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0184: [2024-08-10 13:53:34,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0184: [2024-08-10 13:53:34,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0198: [2024-08-10 13:53:34,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0197: [2024-08-10 13:53:34,524] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0197: [2024-08-10 13:53:34,524] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0197: [2024-08-10 13:53:34,524] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0187: [2024-08-10 13:53:34,524] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0188: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0187: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0188: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0187: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0188: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0185: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0185: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0185: [2024-08-10 13:53:34,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0194: [2024-08-10 13:53:34,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0194: [2024-08-10 13:53:34,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0194: [2024-08-10 13:53:34,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0195: [2024-08-10 13:53:34,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0195: [2024-08-10 13:53:34,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0195: [2024-08-10 13:53:34,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0198: [2024-08-10 13:53:34,546] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_23-model_00-model_states.pt...
g0187: [2024-08-10 13:53:34,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_08-model_00-model_states.pt...
g0188: [2024-08-10 13:53:34,561] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_11-model_00-model_states.pt...
g0197: [2024-08-10 13:53:34,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_20-model_00-model_states.pt...
g0185: [2024-08-10 13:53:34,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_05-model_00-model_states.pt...
g0194: [2024-08-10 13:53:34,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_14-model_00-model_states.pt...
g0195: [2024-08-10 13:53:34,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_17-model_00-model_states.pt...
g0184: [2024-08-10 13:53:34,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_01-model_00-model_states.pt...
g0194: [2024-08-10 13:53:34,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_14-model_00-model_states.pt.
g0195: [2024-08-10 13:53:34,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_17-model_00-model_states.pt.
g0188: [2024-08-10 13:53:34,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_11-model_00-model_states.pt.
g0187: [2024-08-10 13:53:34,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_08-model_00-model_states.pt.
g0194: [2024-08-10 13:53:34,710] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_15-model_00-model_states.pt...
g0188: [2024-08-10 13:53:34,710] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_12-model_00-model_states.pt...
g0195: [2024-08-10 13:53:34,713] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_18-model_00-model_states.pt...
g0187: [2024-08-10 13:53:34,715] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_09-model_00-model_states.pt...
g0197: [2024-08-10 13:53:34,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_20-model_00-model_states.pt.
g0198: [2024-08-10 13:53:34,756] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 13:53:34,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 13:53:34,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_24-model_00-model_states.pt.
g0197: [2024-08-10 13:53:34,765] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_21-model_00-model_states.pt...
g0198: [2024-08-10 13:53:34,803] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_25-model_00-model_states.pt...
g0185: [2024-08-10 13:53:34,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_05-model_00-model_states.pt.
g0194: [2024-08-10 13:53:34,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_15-model_00-model_states.pt.
g0185: [2024-08-10 13:53:34,851] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_06-model_00-model_states.pt...
g0194: [2024-08-10 13:53:34,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_16-model_00-model_states.pt...
g0197: [2024-08-10 13:53:34,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_21-model_00-model_states.pt.
g0187: [2024-08-10 13:53:34,890] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_09-model_00-model_states.pt.
g0197: [2024-08-10 13:53:34,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_22-model_00-model_states.pt...
g0184: [2024-08-10 13:53:34,915] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_01-model_00-model_states.pt.
g0188: [2024-08-10 13:53:34,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_12-model_00-model_states.pt.
g0187: [2024-08-10 13:53:34,920] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_10-model_00-model_states.pt...
g0184: [2024-08-10 13:53:34,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_02-model_00-model_states.pt...
g0195: [2024-08-10 13:53:34,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_18-model_00-model_states.pt.
g0188: [2024-08-10 13:53:34,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_13-model_00-model_states.pt...
g0185: [2024-08-10 13:53:34,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_06-model_00-model_states.pt.
g0195: [2024-08-10 13:53:34,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_19-model_00-model_states.pt...
g0198: [2024-08-10 13:53:35,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 13:53:35,004] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_07_model_states.pt...
g0185: [2024-08-10 13:53:35,004] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_07-model_00-model_states.pt...
g0194: [2024-08-10 13:53:35,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 13:53:35,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_04_model_states.pt...
g0187: [2024-08-10 13:53:35,037] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 13:53:35,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_02_model_states.pt...
g0197: [2024-08-10 13:53:35,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 13:53:35,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_06_model_states.pt...
g0188: [2024-08-10 13:53:35,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 13:53:35,107] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_03_model_states.pt...
g0195: [2024-08-10 13:53:35,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 13:53:35,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_05_model_states.pt...
g0184: [2024-08-10 13:53:35,150] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_02-model_00-model_states.pt.
g0185: [2024-08-10 13:53:35,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 13:53:35,164] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_01_model_states.pt...
g0184: [2024-08-10 13:53:35,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 13:53:35,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 13:53:35,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 13:53:35,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 13:53:35,488] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_00_model_states.pt
g0184: [2024-08-10 13:53:35,488] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 13:53:36,896] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 13:53:36,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0187: [2024-08-10 13:53:37,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 13:53:37,389] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0194: [2024-08-10 13:53:37,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 13:53:37,500] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0195: [2024-08-10 13:53:37,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 13:53:37,524] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0188: [2024-08-10 13:53:37,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 13:53:37,530] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0197: [2024-08-10 13:53:37,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 13:53:37,556] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0185: [2024-08-10 13:53:37,596] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 13:53:37,596] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0184: [2024-08-10 13:53:38,854] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step25000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 13:53:38,855] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!
g0184:   successfully saved checkpoint at iteration   25000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.17, Latency(second): 4.359
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4358.76, 4359.45)
g0184: [2024-08-10 13:54:30,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=25010, skipped=36, lr=[0.00019993280409398313, 0.00019993280409398313], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25010 loss: 0.7844 iter time (s): 5.156 samples/sec: 24.825
g0198:  iteration    25010/10000000 | consumed samples:      3201280 | consumed tokens:   6556221440 | elapsed time per iteration (ms): 51455.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.970264E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.488 | tokens per gpu per second (tgs): 159.206 | TFLOPs: 1.28 |
g0184: [2024-08-10 13:55:15,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=25020, skipped=36, lr=[0.0001999327049960224, 0.0001999327049960224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25020 loss: 0.7826 iter time (s): 4.477 samples/sec: 28.589
g0198:  iteration    25020/10000000 | consumed samples:      3202560 | consumed tokens:   6558842880 | elapsed time per iteration (ms): 4510.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.900460E-01 | loss scale: 32768.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.378 | tokens per gpu per second (tgs): 1816.161 | TFLOPs: 14.61 |
g0184: [2024-08-10 13:56:03,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=25030, skipped=36, lr=[0.00019993260582506847, 0.00019993260582506847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25030 loss: 0.8174 iter time (s): 4.692 samples/sec: 27.279
g0198:  iteration    25030/10000000 | consumed samples:      3203840 | consumed tokens:   6561464320 | elapsed time per iteration (ms): 4727.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.017561E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.077 | tokens per gpu per second (tgs): 1732.899 | TFLOPs: 13.94 |
g0184: [2024-08-10 13:56:49,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=25040, skipped=36, lr=[0.0001999325065811214, 0.0001999325065811214], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25040 loss: 0.7571 iter time (s): 4.571 samples/sec: 28.000
g0198:  iteration    25040/10000000 | consumed samples:      3205120 | consumed tokens:   6564085760 | elapsed time per iteration (ms): 4604.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.954995E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.799 | tokens per gpu per second (tgs): 1779.114 | TFLOPs: 14.32 |
g0184: [2024-08-10 13:57:36,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=25050, skipped=36, lr=[0.00019993240726418122, 0.00019993240726418122], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25050 loss: 0.7974 iter time (s): 4.731 samples/sec: 27.054
g0198:  iteration    25050/10000000 | consumed samples:      3206400 | consumed tokens:   6566707200 | elapsed time per iteration (ms): 4764.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.919978E-01 | loss scale: 32768.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.866 | tokens per gpu per second (tgs): 1719.434 | TFLOPs: 13.84 |
g0184: [2024-08-10 13:58:23,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=25060, skipped=36, lr=[0.0001999323078742481, 0.0001999323078742481], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25060 loss: 0.7811 iter time (s): 4.676 samples/sec: 27.374
g0198:  iteration    25060/10000000 | consumed samples:      3207680 | consumed tokens:   6569328640 | elapsed time per iteration (ms): 4709.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.033092E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.179 | tokens per gpu per second (tgs): 1739.473 | TFLOPs: 14.00 |
g0184: [2024-08-10 13:59:11,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=25070, skipped=36, lr=[0.00019993220841132197, 0.00019993220841132197], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25070 loss: 0.7937 iter time (s): 4.698 samples/sec: 27.245
g0198:  iteration    25070/10000000 | consumed samples:      3208960 | consumed tokens:   6571950080 | elapsed time per iteration (ms): 4730.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.922442E-01 | loss scale: 32768.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.057 | tokens per gpu per second (tgs): 1731.666 | TFLOPs: 13.94 |
g0184: [2024-08-10 13:59:55,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=25080, skipped=36, lr=[0.00019993210887540307, 0.00019993210887540307], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25080 loss: 0.8267 iter time (s): 4.415 samples/sec: 28.989
g0198:  iteration    25080/10000000 | consumed samples:      3210240 | consumed tokens:   6574571520 | elapsed time per iteration (ms): 4448.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.029801E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.777 | tokens per gpu per second (tgs): 1841.710 | TFLOPs: 14.82 |
g0184: [2024-08-10 14:00:44,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=25090, skipped=36, lr=[0.00019993200926649138, 0.00019993200926649138], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25090 loss: 0.8104 iter time (s): 4.891 samples/sec: 26.168
g0198:  iteration    25090/10000000 | consumed samples:      3211520 | consumed tokens:   6577192960 | elapsed time per iteration (ms): 4923.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.048948E-01 | loss scale: 32768.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.996 | tokens per gpu per second (tgs): 1663.717 | TFLOPs: 13.39 |
g0184: [2024-08-10 14:01:32,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=25100, skipped=36, lr=[0.00019993190958458702, 0.00019993190958458702], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25100 loss: 0.8337 iter time (s): 4.726 samples/sec: 27.082
g0198:  iteration    25100/10000000 | consumed samples:      3212800 | consumed tokens:   6579814400 | elapsed time per iteration (ms): 4760.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.036027E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.886 | tokens per gpu per second (tgs): 1720.721 | TFLOPs: 13.85 |
g0184: [2024-08-10 14:02:26,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=25110, skipped=36, lr=[0.00019993180982969005, 0.00019993180982969005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25110 loss: 0.7928 iter time (s): 5.361 samples/sec: 23.878
g0198:  iteration    25110/10000000 | consumed samples:      3214080 | consumed tokens:   6582435840 | elapsed time per iteration (ms): 5393.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.027328E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.733 | tokens per gpu per second (tgs): 1518.886 | TFLOPs: 12.22 |
g0184: [2024-08-10 14:03:13,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=25120, skipped=36, lr=[0.00019993171000180052, 0.00019993171000180052], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25120 loss: 0.8375 iter time (s): 4.668 samples/sec: 27.419
g0198:  iteration    25120/10000000 | consumed samples:      3215360 | consumed tokens:   6585057280 | elapsed time per iteration (ms): 4700.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.063562E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.230 | tokens per gpu per second (tgs): 1742.724 | TFLOPs: 14.02 |
g0184: [2024-08-10 14:04:01,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=25130, skipped=36, lr=[0.00019993161010091855, 0.00019993161010091855], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25130 loss: 0.8076 iter time (s): 4.804 samples/sec: 26.647
g0198:  iteration    25130/10000000 | consumed samples:      3216640 | consumed tokens:   6587678720 | elapsed time per iteration (ms): 4836.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.998072E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.468 | tokens per gpu per second (tgs): 1693.942 | TFLOPs: 13.63 |
g0184: [2024-08-10 14:04:48,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=25140, skipped=36, lr=[0.00019993151012704418, 0.00019993151012704418], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25140 loss: 0.7802 iter time (s): 4.598 samples/sec: 27.839
g0198:  iteration    25140/10000000 | consumed samples:      3217920 | consumed tokens:   6590300160 | elapsed time per iteration (ms): 4630.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.937587E-01 | loss scale: 32768.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.643 | tokens per gpu per second (tgs): 1769.182 | TFLOPs: 14.24 |
g0184: [2024-08-10 14:05:35,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=25150, skipped=36, lr=[0.0001999314100801775, 0.0001999314100801775], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25150 loss: 0.7562 iter time (s): 4.739 samples/sec: 27.012
g0198:  iteration    25150/10000000 | consumed samples:      3219200 | consumed tokens:   6592921600 | elapsed time per iteration (ms): 4771.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.062850E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.828 | tokens per gpu per second (tgs): 1716.985 | TFLOPs: 13.82 |
g0184: [2024-08-10 14:06:27,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=25160, skipped=36, lr=[0.00019993130996031862, 0.00019993130996031862], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25160 loss: 0.7957 iter time (s): 5.094 samples/sec: 25.128
g0198:  iteration    25160/10000000 | consumed samples:      3220480 | consumed tokens:   6595543040 | elapsed time per iteration (ms): 5135.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.940899E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.926 | tokens per gpu per second (tgs): 1595.263 | TFLOPs: 12.84 |
g0184: [2024-08-10 14:07:11,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=25170, skipped=36, lr=[0.00019993120976746756, 0.00019993120976746756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25170 loss: 0.8109 iter time (s): 4.382 samples/sec: 29.211
g0198:  iteration    25170/10000000 | consumed samples:      3221760 | consumed tokens:   6598164480 | elapsed time per iteration (ms): 4415.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.002266E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.992 | tokens per gpu per second (tgs): 1855.509 | TFLOPs: 14.93 |
g0184: [2024-08-10 14:07:58,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=25180, skipped=36, lr=[0.00019993110950162446, 0.00019993110950162446], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25180 loss: 0.7882 iter time (s): 4.697 samples/sec: 27.251
g0198:  iteration    25180/10000000 | consumed samples:      3223040 | consumed tokens:   6600785920 | elapsed time per iteration (ms): 4729.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.977296E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.063 | tokens per gpu per second (tgs): 1732.053 | TFLOPs: 13.94 |
g0184: [2024-08-10 14:08:52,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=25190, skipped=36, lr=[0.00019993100916278935, 0.00019993100916278935], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25190 loss: 0.7734 iter time (s): 5.348 samples/sec: 23.934
g0198:  iteration    25190/10000000 | consumed samples:      3224320 | consumed tokens:   6603407360 | elapsed time per iteration (ms): 5381.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.949035E-01 | loss scale: 32768.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.784 | tokens per gpu per second (tgs): 1522.195 | TFLOPs: 12.25 |
g0184: [2024-08-10 14:09:41,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=25200, skipped=36, lr=[0.0001999309087509623, 0.0001999309087509623], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25200 loss: 0.8205 iter time (s): 4.867 samples/sec: 26.299
g0198:  iteration    25200/10000000 | consumed samples:      3225600 | consumed tokens:   6606028800 | elapsed time per iteration (ms): 4899.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.935970E-01 | loss scale: 32768.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.123 | tokens per gpu per second (tgs): 1671.866 | TFLOPs: 13.45 |
g0184: [2024-08-10 14:10:27,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=25210, skipped=36, lr=[0.0001999308082661434, 0.0001999308082661434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25210 loss: 0.8123 iter time (s): 4.543 samples/sec: 28.175
g0198:  iteration    25210/10000000 | consumed samples:      3226880 | consumed tokens:   6608650240 | elapsed time per iteration (ms): 4575.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.035501E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.973 | tokens per gpu per second (tgs): 1790.285 | TFLOPs: 14.41 |
g0184: [2024-08-10 14:11:14,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=25220, skipped=36, lr=[0.00019993070770833276, 0.00019993070770833276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25220 loss: 0.8134 iter time (s): 4.692 samples/sec: 27.283
g0198:  iteration    25220/10000000 | consumed samples:      3228160 | consumed tokens:   6611271680 | elapsed time per iteration (ms): 4731.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.927001E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.051 | tokens per gpu per second (tgs): 1731.279 | TFLOPs: 13.93 |
g0184: [2024-08-10 14:12:02,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=25230, skipped=36, lr=[0.00019993060707753046, 0.00019993060707753046], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25230 loss: 0.7863 iter time (s): 4.773 samples/sec: 26.819
g0198:  iteration    25230/10000000 | consumed samples:      3229440 | consumed tokens:   6613893120 | elapsed time per iteration (ms): 4805.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.938170E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.634 | tokens per gpu per second (tgs): 1704.597 | TFLOPs: 13.72 |
g0184: [2024-08-10 14:12:51,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=25240, skipped=36, lr=[0.00019993050637373653, 0.00019993050637373653], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25240 loss: 0.7643 iter time (s): 4.858 samples/sec: 26.350
g0198:  iteration    25240/10000000 | consumed samples:      3230720 | consumed tokens:   6616514560 | elapsed time per iteration (ms): 4890.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.943720E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.173 | tokens per gpu per second (tgs): 1675.083 | TFLOPs: 13.48 |
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:13:01,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 14:13:01,206] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:13:01,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 14:13:37,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=25250, skipped=36, lr=[0.000199930405596951, 0.000199930405596951], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25250 loss: 0.7792 iter time (s): 4.588 samples/sec: 27.899
g0198:  iteration    25250/10000000 | consumed samples:      3232000 | consumed tokens:   6619136000 | elapsed time per iteration (ms): 4620.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.011547E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.705 | tokens per gpu per second (tgs): 1773.099 | TFLOPs: 14.27 |
g0184: [2024-08-10 14:14:25,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=25260, skipped=36, lr=[0.0001999303047471741, 0.0001999303047471741], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25260 loss: 0.7792 iter time (s): 4.731 samples/sec: 27.057
g0198:  iteration    25260/10000000 | consumed samples:      3233280 | consumed tokens:   6621757440 | elapsed time per iteration (ms): 4763.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.020864E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.872 | tokens per gpu per second (tgs): 1719.833 | TFLOPs: 13.84 |
g0184: [2024-08-10 14:15:17,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=25270, skipped=36, lr=[0.00019993020382440578, 0.00019993020382440578], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25270 loss: 0.8006 iter time (s): 5.162 samples/sec: 24.796
g0198:  iteration    25270/10000000 | consumed samples:      3234560 | consumed tokens:   6624378880 | elapsed time per iteration (ms): 5194.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.973115E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.640 | tokens per gpu per second (tgs): 1576.953 | TFLOPs: 12.69 |
g0184: [2024-08-10 14:16:04,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=25280, skipped=36, lr=[0.00019993010282864614, 0.00019993010282864614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25280 loss: 0.7722 iter time (s): 4.682 samples/sec: 27.337
g0198:  iteration    25280/10000000 | consumed samples:      3235840 | consumed tokens:   6627000320 | elapsed time per iteration (ms): 4715.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.850607E-01 | loss scale: 65536.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.146 | tokens per gpu per second (tgs): 1737.334 | TFLOPs: 13.98 |
g0184: [2024-08-10 14:16:52,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=25290, skipped=36, lr=[0.00019993000175989534, 0.00019993000175989534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25290 loss: 0.8071 iter time (s): 4.734 samples/sec: 27.039
g0198:  iteration    25290/10000000 | consumed samples:      3237120 | consumed tokens:   6629621760 | elapsed time per iteration (ms): 4767.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.022124E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.851 | tokens per gpu per second (tgs): 1718.453 | TFLOPs: 13.83 |
g0184: [2024-08-10 14:17:38,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=25300, skipped=36, lr=[0.00019992990061815335, 0.00019992990061815335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25300 loss: 0.8060 iter time (s): 4.608 samples/sec: 27.780
g0198:  iteration    25300/10000000 | consumed samples:      3238400 | consumed tokens:   6632243200 | elapsed time per iteration (ms): 4640.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.004073E-01 | loss scale: 65536.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.586 | tokens per gpu per second (tgs): 1765.507 | TFLOPs: 14.21 |
g0184: [2024-08-10 14:18:30,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=25310, skipped=36, lr=[0.00019992979940342028, 0.00019992979940342028], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25310 loss: 0.7986 iter time (s): 5.157 samples/sec: 24.821
g0198:  iteration    25310/10000000 | consumed samples:      3239680 | consumed tokens:   6634864640 | elapsed time per iteration (ms): 5190.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.044582E-01 | loss scale: 65536.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.662 | tokens per gpu per second (tgs): 1578.338 | TFLOPs: 12.70 |
g0184: [2024-08-10 14:19:17,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=25320, skipped=36, lr=[0.00019992969811569625, 0.00019992969811569625], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25320 loss: 0.7941 iter time (s): 4.683 samples/sec: 27.332
g0198:  iteration    25320/10000000 | consumed samples:      3240960 | consumed tokens:   6637486080 | elapsed time per iteration (ms): 4715.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.955413E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.142 | tokens per gpu per second (tgs): 1737.092 | TFLOPs: 13.98 |
g0184: [2024-08-10 14:20:03,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=25330, skipped=36, lr=[0.0001999295967549813, 0.0001999295967549813], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25330 loss: 0.8154 iter time (s): 4.581 samples/sec: 27.943
g0198:  iteration    25330/10000000 | consumed samples:      3242240 | consumed tokens:   6640107520 | elapsed time per iteration (ms): 4613.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.969263E-01 | loss scale: 65536.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.742 | tokens per gpu per second (tgs): 1775.495 | TFLOPs: 14.29 |
g0184: [2024-08-10 14:20:51,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=25340, skipped=36, lr=[0.0001999294953212755, 0.0001999294953212755], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25340 loss: 0.7722 iter time (s): 4.774 samples/sec: 26.812
g0198:  iteration    25340/10000000 | consumed samples:      3243520 | consumed tokens:   6642728960 | elapsed time per iteration (ms): 4808.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.845480E-01 | loss scale: 65536.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.619 | tokens per gpu per second (tgs): 1703.610 | TFLOPs: 13.71 |
g0184: [2024-08-10 14:21:37,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=25350, skipped=36, lr=[0.00019992939381457899, 0.00019992939381457899], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25350 loss: 0.8076 iter time (s): 4.505 samples/sec: 28.411
g0198:  iteration    25350/10000000 | consumed samples:      3244800 | consumed tokens:   6645350400 | elapsed time per iteration (ms): 4537.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.935509E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.208 | tokens per gpu per second (tgs): 1805.336 | TFLOPs: 14.53 |
g0184: [2024-08-10 14:22:25,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=25360, skipped=36, lr=[0.00019992929223489178, 0.00019992929223489178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25360 loss: 0.7918 iter time (s): 4.824 samples/sec: 26.533
g0198:  iteration    25360/10000000 | consumed samples:      3246080 | consumed tokens:   6647971840 | elapsed time per iteration (ms): 4856.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.990852E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.356 | tokens per gpu per second (tgs): 1686.753 | TFLOPs: 13.57 |
g0184: [2024-08-10 14:23:12,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=25370, skipped=36, lr=[0.0001999291905822139, 0.0001999291905822139], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25370 loss: 0.8033 iter time (s): 4.641 samples/sec: 27.580
g0198:  iteration    25370/10000000 | consumed samples:      3247360 | consumed tokens:   6650593280 | elapsed time per iteration (ms): 4673.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.905645E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.389 | tokens per gpu per second (tgs): 1752.869 | TFLOPs: 14.11 |
g0184: [2024-08-10 14:23:55,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=25380, skipped=36, lr=[0.00019992908885654558, 0.00019992908885654558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25380 loss: 0.8087 iter time (s): 4.298 samples/sec: 29.781
g0198:  iteration    25380/10000000 | consumed samples:      3248640 | consumed tokens:   6653214720 | elapsed time per iteration (ms): 4330.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.957089E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.555 | tokens per gpu per second (tgs): 1891.530 | TFLOPs: 15.22 |
g0184: [2024-08-10 14:24:45,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=25390, skipped=36, lr=[0.0001999289870578868, 0.0001999289870578868], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25390 loss: 0.7673 iter time (s): 4.955 samples/sec: 25.831
g0198:  iteration    25390/10000000 | consumed samples:      3249920 | consumed tokens:   6655836160 | elapsed time per iteration (ms): 5019.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.906497E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.503 | tokens per gpu per second (tgs): 1632.161 | TFLOPs: 13.13 |
g0184: [2024-08-10 14:25:34,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=25400, skipped=36, lr=[0.00019992888518623766, 0.00019992888518623766], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25400 loss: 0.7884 iter time (s): 4.782 samples/sec: 26.766
g0198:  iteration    25400/10000000 | consumed samples:      3251200 | consumed tokens:   6658457600 | elapsed time per iteration (ms): 4815.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.918599E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.583 | tokens per gpu per second (tgs): 1701.316 | TFLOPs: 13.69 |
g0184: [2024-08-10 14:26:23,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=25410, skipped=36, lr=[0.00019992878324159822, 0.00019992878324159822], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25410 loss: 0.7755 iter time (s): 4.869 samples/sec: 26.290
g0198:  iteration    25410/10000000 | consumed samples:      3252480 | consumed tokens:   6661079040 | elapsed time per iteration (ms): 4901.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.955227E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.113 | tokens per gpu per second (tgs): 1671.217 | TFLOPs: 13.45 |
g0184: [2024-08-10 14:27:11,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=25420, skipped=36, lr=[0.00019992868122396858, 0.00019992868122396858], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25420 loss: 0.7830 iter time (s): 4.793 samples/sec: 26.708
g0198:  iteration    25420/10000000 | consumed samples:      3253760 | consumed tokens:   6663700480 | elapsed time per iteration (ms): 4825.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.834012E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.525 | tokens per gpu per second (tgs): 1697.613 | TFLOPs: 13.66 |
g0184: [2024-08-10 14:27:56,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=25430, skipped=36, lr=[0.0001999285791333488, 0.0001999285791333488], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25430 loss: 0.7584 iter time (s): 4.445 samples/sec: 28.795
g0198:  iteration    25430/10000000 | consumed samples:      3255040 | consumed tokens:   6666321920 | elapsed time per iteration (ms): 4477.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.932276E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.586 | tokens per gpu per second (tgs): 1829.500 | TFLOPs: 14.72 |
g0184: [2024-08-10 14:28:41,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=25440, skipped=36, lr=[0.00019992847696973897, 0.00019992847696973897], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25440 loss: 0.7954 iter time (s): 4.452 samples/sec: 28.754
g0198:  iteration    25440/10000000 | consumed samples:      3256320 | consumed tokens:   6668943360 | elapsed time per iteration (ms): 4484.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.066655E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.543 | tokens per gpu per second (tgs): 1826.762 | TFLOPs: 14.70 |
g0184: [2024-08-10 14:29:31,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=25450, skipped=36, lr=[0.00019992837473313917, 0.00019992837473313917], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25450 loss: 0.8187 iter time (s): 5.052 samples/sec: 25.336
g0198:  iteration    25450/10000000 | consumed samples:      3257600 | consumed tokens:   6671564800 | elapsed time per iteration (ms): 5087.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.009849E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.160 | tokens per gpu per second (tgs): 1610.256 | TFLOPs: 12.96 |
g0184: [2024-08-10 14:30:20,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=25460, skipped=36, lr=[0.00019992827242354947, 0.00019992827242354947], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25460 loss: 0.8303 iter time (s): 4.844 samples/sec: 26.426
g0198:  iteration    25460/10000000 | consumed samples:      3258880 | consumed tokens:   6674186240 | elapsed time per iteration (ms): 4876.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.033246E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.248 | tokens per gpu per second (tgs): 1679.857 | TFLOPs: 13.52 |
g0184: [2024-08-10 14:31:08,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=25470, skipped=36, lr=[0.00019992817004096998, 0.00019992817004096998], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25470 loss: 0.7934 iter time (s): 4.746 samples/sec: 26.969
g0198:  iteration    25470/10000000 | consumed samples:      3260160 | consumed tokens:   6676807680 | elapsed time per iteration (ms): 4778.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.057758E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.785 | tokens per gpu per second (tgs): 1714.214 | TFLOPs: 13.79 |
g0184: [2024-08-10 14:31:57,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=25480, skipped=36, lr=[0.0001999280675854007, 0.0001999280675854007], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25480 loss: 0.7863 iter time (s): 4.834 samples/sec: 26.480
g0198:  iteration    25480/10000000 | consumed samples:      3261440 | consumed tokens:   6679429120 | elapsed time per iteration (ms): 4867.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.965002E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.297 | tokens per gpu per second (tgs): 1683.025 | TFLOPs: 13.54 |
g0184: [2024-08-10 14:32:45,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=25490, skipped=36, lr=[0.00019992796505684178, 0.00019992796505684178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25490 loss: 0.8096 iter time (s): 4.828 samples/sec: 26.510
g0198:  iteration    25490/10000000 | consumed samples:      3262720 | consumed tokens:   6682050560 | elapsed time per iteration (ms): 4861.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.930726E-01 | loss scale: 65536.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.331 | tokens per gpu per second (tgs): 1685.199 | TFLOPs: 13.56 |
g0184: [2024-08-10 14:33:30,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=25500, skipped=36, lr=[0.0001999278624552933, 0.0001999278624552933], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25500 loss: 0.7965 iter time (s): 4.466 samples/sec: 28.664
g0198:  iteration    25500/10000000 | consumed samples:      3264000 | consumed tokens:   6684672000 | elapsed time per iteration (ms): 4498.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.982829E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.455 | tokens per gpu per second (tgs): 1821.106 | TFLOPs: 14.65 |
g0184: [2024-08-10 14:34:20,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=25510, skipped=36, lr=[0.00019992775978075532, 0.00019992775978075532], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25510 loss: 0.8098 iter time (s): 4.911 samples/sec: 26.066
g0198:  iteration    25510/10000000 | consumed samples:      3265280 | consumed tokens:   6687293440 | elapsed time per iteration (ms): 4943.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.960583E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.895 | tokens per gpu per second (tgs): 1657.292 | TFLOPs: 13.34 |
g0184: [2024-08-10 14:35:11,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=25520, skipped=36, lr=[0.00019992765703322793, 0.00019992765703322793], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25520 loss: 0.7991 iter time (s): 5.089 samples/sec: 25.152
g0198:  iteration    25520/10000000 | consumed samples:      3266560 | consumed tokens:   6689914880 | elapsed time per iteration (ms): 5133.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.951401E-01 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.932 | tokens per gpu per second (tgs): 1595.659 | TFLOPs: 12.84 |
g0184: [2024-08-10 14:35:57,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=25530, skipped=36, lr=[0.00019992755421271115, 0.00019992755421271115], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25530 loss: 0.8081 iter time (s): 4.597 samples/sec: 27.844
g0198:  iteration    25530/10000000 | consumed samples:      3267840 | consumed tokens:   6692536320 | elapsed time per iteration (ms): 4640.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.975083E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.581 | tokens per gpu per second (tgs): 1765.169 | TFLOPs: 14.20 |
g0184: [2024-08-10 14:36:44,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=25540, skipped=36, lr=[0.00019992745131920515, 0.00019992745131920515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25540 loss: 0.8138 iter time (s): 4.675 samples/sec: 27.382
g0198:  iteration    25540/10000000 | consumed samples:      3269120 | consumed tokens:   6695157760 | elapsed time per iteration (ms): 4707.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.987442E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.191 | tokens per gpu per second (tgs): 1740.209 | TFLOPs: 14.00 |
g0184: [2024-08-10 14:37:32,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=25550, skipped=36, lr=[0.00019992734835270996, 0.00019992734835270996], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25550 loss: 0.8207 iter time (s): 4.681 samples/sec: 27.345
g0198:  iteration    25550/10000000 | consumed samples:      3270400 | consumed tokens:   6697779200 | elapsed time per iteration (ms): 4713.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.900724E-01 | loss scale: 65536.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.155 | tokens per gpu per second (tgs): 1737.900 | TFLOPs: 13.99 |
g0184: [2024-08-10 14:38:16,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=25560, skipped=36, lr=[0.00019992724531322565, 0.00019992724531322565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25560 loss: 0.7821 iter time (s): 4.425 samples/sec: 28.926
g0198:  iteration    25560/10000000 | consumed samples:      3271680 | consumed tokens:   6700400640 | elapsed time per iteration (ms): 4457.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.924006E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.715 | tokens per gpu per second (tgs): 1837.780 | TFLOPs: 14.79 |
g0184: [2024-08-10 14:39:07,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=25570, skipped=36, lr=[0.0001999271422007523, 0.0001999271422007523], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25570 loss: 0.7775 iter time (s): 5.025 samples/sec: 25.474
g0198:  iteration    25570/10000000 | consumed samples:      3272960 | consumed tokens:   6703022080 | elapsed time per iteration (ms): 5057.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.905547E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.311 | tokens per gpu per second (tgs): 1619.900 | TFLOPs: 13.04 |
g0184: [2024-08-10 14:39:57,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=25580, skipped=36, lr=[0.00019992703901529002, 0.00019992703901529002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25580 loss: 0.8041 iter time (s): 4.968 samples/sec: 25.767
g0198:  iteration    25580/10000000 | consumed samples:      3274240 | consumed tokens:   6705643520 | elapsed time per iteration (ms): 5000.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.942337E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.598 | tokens per gpu per second (tgs): 1638.259 | TFLOPs: 13.18 |
g0184: [2024-08-10 14:40:44,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=25590, skipped=36, lr=[0.00019992693575683891, 0.00019992693575683891], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25590 loss: 0.8330 iter time (s): 4.683 samples/sec: 27.334
g0198:  iteration    25590/10000000 | consumed samples:      3275520 | consumed tokens:   6708264960 | elapsed time per iteration (ms): 4715.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.980814E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.146 | tokens per gpu per second (tgs): 1737.353 | TFLOPs: 13.98 |
g0184: [2024-08-10 14:41:32,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=25600, skipped=36, lr=[0.00019992683242539894, 0.00019992683242539894], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25600 loss: 0.8085 iter time (s): 4.827 samples/sec: 26.518
g0198:  iteration    25600/10000000 | consumed samples:      3276800 | consumed tokens:   6710886400 | elapsed time per iteration (ms): 4859.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.851186E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.342 | tokens per gpu per second (tgs): 1685.908 | TFLOPs: 13.57 |
g0184: [2024-08-10 14:42:18,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=25610, skipped=36, lr=[0.0001999267290209703, 0.0001999267290209703], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25610 loss: 0.8156 iter time (s): 4.502 samples/sec: 28.430
g0198:  iteration    25610/10000000 | consumed samples:      3278080 | consumed tokens:   6713507840 | elapsed time per iteration (ms): 4535.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.954385E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.222 | tokens per gpu per second (tgs): 1806.187 | TFLOPs: 14.53 |
g0184: [2024-08-10 14:43:06,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=25620, skipped=36, lr=[0.00019992662554355303, 0.00019992662554355303], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25620 loss: 0.8157 iter time (s): 4.792 samples/sec: 26.709
g0198:  iteration    25620/10000000 | consumed samples:      3279360 | consumed tokens:   6716129280 | elapsed time per iteration (ms): 4825.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.897805E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.524 | tokens per gpu per second (tgs): 1697.556 | TFLOPs: 13.66 |
g0184: [2024-08-10 14:43:54,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=25630, skipped=36, lr=[0.0001999265219931472, 0.0001999265219931472], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25630 loss: 0.7885 iter time (s): 4.725 samples/sec: 27.090
g0198:  iteration    25630/10000000 | consumed samples:      3280640 | consumed tokens:   6718750720 | elapsed time per iteration (ms): 4757.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.856348E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.906 | tokens per gpu per second (tgs): 1721.969 | TFLOPs: 13.86 |
g0184: [2024-08-10 14:44:39,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=25640, skipped=36, lr=[0.00019992641836975291, 0.00019992641836975291], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25640 loss: 0.7880 iter time (s): 4.493 samples/sec: 28.489
g0198:  iteration    25640/10000000 | consumed samples:      3281920 | consumed tokens:   6721372160 | elapsed time per iteration (ms): 4525.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.937761E-01 | loss scale: 65536.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.286 | tokens per gpu per second (tgs): 1810.315 | TFLOPs: 14.57 |
g0184: [2024-08-10 14:45:27,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=25650, skipped=36, lr=[0.00019992631467337027, 0.00019992631467337027], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25650 loss: 0.8273 iter time (s): 4.735 samples/sec: 27.034
g0198:  iteration    25650/10000000 | consumed samples:      3283200 | consumed tokens:   6723993600 | elapsed time per iteration (ms): 4767.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.036831E-01 | loss scale: 65536.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.850 | tokens per gpu per second (tgs): 1718.374 | TFLOPs: 13.83 |
g0184: [2024-08-10 14:46:14,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=25660, skipped=36, lr=[0.00019992621090399924, 0.00019992621090399924], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25660 loss: 0.7896 iter time (s): 4.678 samples/sec: 27.361
g0198:  iteration    25660/10000000 | consumed samples:      3284480 | consumed tokens:   6726615040 | elapsed time per iteration (ms): 4710.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.033770E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.174 | tokens per gpu per second (tgs): 1739.132 | TFLOPs: 14.00 |
g0184: [2024-08-10 14:46:59,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=25670, skipped=36, lr=[0.00019992610706164003, 0.00019992610706164003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25670 loss: 0.7697 iter time (s): 4.453 samples/sec: 28.743
g0198:  iteration    25670/10000000 | consumed samples:      3285760 | consumed tokens:   6729236480 | elapsed time per iteration (ms): 4485.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.858947E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.535 | tokens per gpu per second (tgs): 1826.228 | TFLOPs: 14.70 |
g0184: [2024-08-10 14:47:48,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=25680, skipped=36, lr=[0.00019992600314629264, 0.00019992600314629264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25680 loss: 0.7840 iter time (s): 4.948 samples/sec: 25.870
g0198:  iteration    25680/10000000 | consumed samples:      3287040 | consumed tokens:   6731857920 | elapsed time per iteration (ms): 4980.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.816783E-01 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.699 | tokens per gpu per second (tgs): 1644.720 | TFLOPs: 13.24 |
g0184: [2024-08-10 14:48:38,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=25690, skipped=36, lr=[0.00019992589915795723, 0.00019992589915795723], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25690 loss: 0.7939 iter time (s): 4.908 samples/sec: 26.078
g0198:  iteration    25690/10000000 | consumed samples:      3288320 | consumed tokens:   6734479360 | elapsed time per iteration (ms): 4946.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.937675E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.878 | tokens per gpu per second (tgs): 1656.219 | TFLOPs: 13.33 |
g0184: [2024-08-10 14:49:30,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=25700, skipped=36, lr=[0.0001999257950966338, 0.0001999257950966338], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25700 loss: 0.7644 iter time (s): 5.162 samples/sec: 24.799
g0198:  iteration    25700/10000000 | consumed samples:      3289600 | consumed tokens:   6737100800 | elapsed time per iteration (ms): 5194.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.909259E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.640 | tokens per gpu per second (tgs): 1576.930 | TFLOPs: 12.69 |
g0184: [2024-08-10 14:50:24,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=25710, skipped=36, lr=[0.00019992569096232243, 0.00019992569096232243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25710 loss: 0.7940 iter time (s): 5.344 samples/sec: 23.952
g0198:  iteration    25710/10000000 | consumed samples:      3290880 | consumed tokens:   6739722240 | elapsed time per iteration (ms): 5376.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.869829E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.808 | tokens per gpu per second (tgs): 1523.691 | TFLOPs: 12.26 |
g0184: [2024-08-10 14:51:10,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=25720, skipped=36, lr=[0.00019992558675502325, 0.00019992558675502325], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25720 loss: 0.7916 iter time (s): 4.659 samples/sec: 27.472
g0198:  iteration    25720/10000000 | consumed samples:      3292160 | consumed tokens:   6742343680 | elapsed time per iteration (ms): 4694.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.906329E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.266 | tokens per gpu per second (tgs): 1745.007 | TFLOPs: 14.04 |
g0184: [2024-08-10 14:51:56,366] [INFO] [logging.py:96:log_dist] [Rank 0] step=25730, skipped=36, lr=[0.00019992548247473632, 0.00019992548247473632], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25730 loss: 0.7660 iter time (s): 4.505 samples/sec: 28.416
g0198:  iteration    25730/10000000 | consumed samples:      3293440 | consumed tokens:   6744965120 | elapsed time per iteration (ms): 4536.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.954985E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.213 | tokens per gpu per second (tgs): 1805.637 | TFLOPs: 14.53 |
g0184: [2024-08-10 14:52:43,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=25740, skipped=36, lr=[0.0001999253781214617, 0.0001999253781214617], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25740 loss: 0.7540 iter time (s): 4.728 samples/sec: 27.074
g0198:  iteration    25740/10000000 | consumed samples:      3294720 | consumed tokens:   6747586560 | elapsed time per iteration (ms): 4759.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.950006E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.893 | tokens per gpu per second (tgs): 1721.122 | TFLOPs: 13.85 |
g0184: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 14:52:53,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 14:52:53,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 14:53:45,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=25750, skipped=36, lr=[0.00019992527369519953, 0.00019992527369519953], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25750 loss: 0.7792 iter time (s): 6.157 samples/sec: 20.789
g0198:  iteration    25750/10000000 | consumed samples:      3296000 | consumed tokens:   6750208000 | elapsed time per iteration (ms): 6190.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.797200E-01 | loss scale: 131072.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.678 | tokens per gpu per second (tgs): 1323.419 | TFLOPs: 10.65 |
g0184: [2024-08-10 14:54:37,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=25760, skipped=36, lr=[0.0001999251691959498, 0.0001999251691959498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25760 loss: 0.7917 iter time (s): 5.095 samples/sec: 25.125
g0198:  iteration    25760/10000000 | consumed samples:      3297280 | consumed tokens:   6752829440 | elapsed time per iteration (ms): 5126.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.943941E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.966 | tokens per gpu per second (tgs): 1597.850 | TFLOPs: 12.86 |
g0184: [2024-08-10 14:55:26,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=25770, skipped=36, lr=[0.00019992506462371268, 0.00019992506462371268], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25770 loss: 0.8022 iter time (s): 4.927 samples/sec: 25.980
g0198:  iteration    25770/10000000 | consumed samples:      3298560 | consumed tokens:   6755450880 | elapsed time per iteration (ms): 4959.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.882261E-01 | loss scale: 131072.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.810 | tokens per gpu per second (tgs): 1651.831 | TFLOPs: 13.29 |
g0184: [2024-08-10 14:56:12,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=25780, skipped=36, lr=[0.00019992495997848823, 0.00019992495997848823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25780 loss: 0.7554 iter time (s): 4.562 samples/sec: 28.061
g0198:  iteration    25780/10000000 | consumed samples:      3299840 | consumed tokens:   6758072320 | elapsed time per iteration (ms): 4607.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.872213E-01 | loss scale: 131072.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.780 | tokens per gpu per second (tgs): 1777.934 | TFLOPs: 14.31 |
g0184: [2024-08-10 14:56:57,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=25790, skipped=36, lr=[0.00019992485526027648, 0.00019992485526027648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25790 loss: 0.7570 iter time (s): 4.426 samples/sec: 28.921
g0198:  iteration    25790/10000000 | consumed samples:      3301120 | consumed tokens:   6760693760 | elapsed time per iteration (ms): 4458.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.980866E-01 | loss scale: 131072.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.712 | tokens per gpu per second (tgs): 1837.555 | TFLOPs: 14.79 |
g0184: [2024-08-10 14:57:58,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=25800, skipped=36, lr=[0.00019992475046907754, 0.00019992475046907754], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25800 loss: 0.7917 iter time (s): 6.031 samples/sec: 21.222
g0198:  iteration    25800/10000000 | consumed samples:      3302400 | consumed tokens:   6763315200 | elapsed time per iteration (ms): 6064.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.025412E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.108 | tokens per gpu per second (tgs): 1350.905 | TFLOPs: 10.87 |
g0184: [2024-08-10 14:58:47,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=25810, skipped=36, lr=[0.0001999246456048915, 0.0001999246456048915], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25810 loss: 0.7874 iter time (s): 4.881 samples/sec: 26.222
g0198:  iteration    25810/10000000 | consumed samples:      3303680 | consumed tokens:   6765936640 | elapsed time per iteration (ms): 4914.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.951080E-01 | loss scale: 131072.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.044 | tokens per gpu per second (tgs): 1666.805 | TFLOPs: 13.41 |
g0184: [2024-08-10 14:59:33,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=25820, skipped=36, lr=[0.00019992454066771842, 0.00019992454066771842], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25820 loss: 0.7798 iter time (s): 4.617 samples/sec: 27.723
g0198:  iteration    25820/10000000 | consumed samples:      3304960 | consumed tokens:   6768558080 | elapsed time per iteration (ms): 4650.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.925416E-01 | loss scale: 131072.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.525 | tokens per gpu per second (tgs): 1761.625 | TFLOPs: 14.18 |
g0184: [2024-08-10 15:00:19,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=25830, skipped=36, lr=[0.0001999244356575584, 0.0001999244356575584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25830 loss: 0.7901 iter time (s): 4.546 samples/sec: 28.159
g0198:  iteration    25830/10000000 | consumed samples:      3306240 | consumed tokens:   6771179520 | elapsed time per iteration (ms): 4579.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.026583E-01 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.952 | tokens per gpu per second (tgs): 1788.910 | TFLOPs: 14.40 |
g0184: [2024-08-10 15:01:16,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=25840, skipped=36, lr=[0.00019992433057441152, 0.00019992433057441152], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25840 loss: 0.7957 iter time (s): 5.699 samples/sec: 22.460
g0198:  iteration    25840/10000000 | consumed samples:      3307520 | consumed tokens:   6773800960 | elapsed time per iteration (ms): 5731.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.002707E-01 | loss scale: 131072.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.332 | tokens per gpu per second (tgs): 1429.278 | TFLOPs: 11.50 |
g0184: [2024-08-10 15:02:07,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=25850, skipped=36, lr=[0.00019992422541827785, 0.00019992422541827785], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25850 loss: 0.8306 iter time (s): 5.003 samples/sec: 25.586
g0198:  iteration    25850/10000000 | consumed samples:      3308800 | consumed tokens:   6776422400 | elapsed time per iteration (ms): 5035.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.960430E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.418 | tokens per gpu per second (tgs): 1626.780 | TFLOPs: 13.09 |
g0184: [2024-08-10 15:02:50,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=25860, skipped=36, lr=[0.0001999241201891575, 0.0001999241201891575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25860 loss: 0.8202 iter time (s): 4.321 samples/sec: 29.624
g0198:  iteration    25860/10000000 | consumed samples:      3310080 | consumed tokens:   6779043840 | elapsed time per iteration (ms): 4353.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.907712E-01 | loss scale: 131072.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.404 | tokens per gpu per second (tgs): 1881.877 | TFLOPs: 15.14 |
g0184: [2024-08-10 15:03:43,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=25870, skipped=36, lr=[0.00019992401488705047, 0.00019992401488705047], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25870 loss: 0.8000 iter time (s): 5.226 samples/sec: 24.494
g0198:  iteration    25870/10000000 | consumed samples:      3311360 | consumed tokens:   6781665280 | elapsed time per iteration (ms): 5259.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.073715E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.339 | tokens per gpu per second (tgs): 1557.692 | TFLOPs: 12.54 |
g0184: [2024-08-10 15:04:26,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=25880, skipped=36, lr=[0.00019992390951195695, 0.00019992390951195695], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25880 loss: 0.7719 iter time (s): 4.311 samples/sec: 29.691
g0198:  iteration    25880/10000000 | consumed samples:      3312640 | consumed tokens:   6784286720 | elapsed time per iteration (ms): 4343.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.875027E-01 | loss scale: 131072.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.468 | tokens per gpu per second (tgs): 1885.924 | TFLOPs: 15.18 |
g0184: [2024-08-10 15:05:12,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=25890, skipped=36, lr=[0.00019992380406387697, 0.00019992380406387697], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25890 loss: 0.7831 iter time (s): 4.514 samples/sec: 28.355
g0198:  iteration    25890/10000000 | consumed samples:      3313920 | consumed tokens:   6786908160 | elapsed time per iteration (ms): 4547.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.892698E-01 | loss scale: 131072.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.151 | tokens per gpu per second (tgs): 1801.642 | TFLOPs: 14.50 |
g0184: [2024-08-10 15:06:13,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=25900, skipped=36, lr=[0.00019992369854281055, 0.00019992369854281055], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25900 loss: 0.8283 iter time (s): 6.113 samples/sec: 20.938
g0198:  iteration    25900/10000000 | consumed samples:      3315200 | consumed tokens:   6789529600 | elapsed time per iteration (ms): 6146.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.922931E-01 | loss scale: 131072.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.825 | tokens per gpu per second (tgs): 1332.807 | TFLOPs: 10.73 |
g0184: [2024-08-10 15:07:08,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=25910, skipped=36, lr=[0.0001999235929487579, 0.0001999235929487579], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25910 loss: 0.8047 iter time (s): 5.481 samples/sec: 23.355
g0198:  iteration    25910/10000000 | consumed samples:      3316480 | consumed tokens:   6792151040 | elapsed time per iteration (ms): 5527.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.926632E-01 | loss scale: 131072.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.159 | tokens per gpu per second (tgs): 1482.175 | TFLOPs: 11.93 |
g0184: [2024-08-10 15:07:55,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=25920, skipped=36, lr=[0.000199923487281719, 0.000199923487281719], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25920 loss: 0.8122 iter time (s): 4.583 samples/sec: 27.928
g0198:  iteration    25920/10000000 | consumed samples:      3317760 | consumed tokens:   6794772480 | elapsed time per iteration (ms): 4616.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.913483E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.729 | tokens per gpu per second (tgs): 1774.670 | TFLOPs: 14.28 |
g0184: [2024-08-10 15:08:37,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=25930, skipped=36, lr=[0.00019992338154169395, 0.00019992338154169395], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25930 loss: 0.7719 iter time (s): 4.221 samples/sec: 30.326
g0198:  iteration    25930/10000000 | consumed samples:      3319040 | consumed tokens:   6797393920 | elapsed time per iteration (ms): 4254.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.916435E-01 | loss scale: 131072.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.085 | tokens per gpu per second (tgs): 1925.464 | TFLOPs: 15.49 |
g0184: [2024-08-10 15:09:22,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=25940, skipped=36, lr=[0.00019992327572868285, 0.00019992327572868285], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25940 loss: 0.7955 iter time (s): 4.459 samples/sec: 28.707
g0198:  iteration    25940/10000000 | consumed samples:      3320320 | consumed tokens:   6800015360 | elapsed time per iteration (ms): 4494.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.934367E-01 | loss scale: 131072.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.481 | tokens per gpu per second (tgs): 1822.814 | TFLOPs: 14.67 |
g0184: [2024-08-10 15:10:08,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=25950, skipped=36, lr=[0.0001999231698426858, 0.0001999231698426858], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25950 loss: 0.7970 iter time (s): 4.562 samples/sec: 28.058
g0198:  iteration    25950/10000000 | consumed samples:      3321600 | consumed tokens:   6802636800 | elapsed time per iteration (ms): 4622.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.947466E-01 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.693 | tokens per gpu per second (tgs): 1772.333 | TFLOPs: 14.26 |
g0184: [2024-08-10 15:10:59,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=25960, skipped=36, lr=[0.00019992306388370284, 0.00019992306388370284], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25960 loss: 0.8096 iter time (s): 5.056 samples/sec: 25.317
g0198:  iteration    25960/10000000 | consumed samples:      3322880 | consumed tokens:   6805258240 | elapsed time per iteration (ms): 5089.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.849817E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.151 | tokens per gpu per second (tgs): 1609.665 | TFLOPs: 12.95 |
g0184: [2024-08-10 15:12:01,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=25970, skipped=36, lr=[0.00019992295785173407, 0.00019992295785173407], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25970 loss: 0.7787 iter time (s): 6.118 samples/sec: 20.922
g0198:  iteration    25970/10000000 | consumed samples:      3324160 | consumed tokens:   6807879680 | elapsed time per iteration (ms): 6150.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.958820E-01 | loss scale: 131072.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.810 | tokens per gpu per second (tgs): 1331.868 | TFLOPs: 10.72 |
g0184: [2024-08-10 15:12:53,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=25980, skipped=36, lr=[0.00019992285174677958, 0.00019992285174677958], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25980 loss: 0.7951 iter time (s): 5.231 samples/sec: 24.470
g0198:  iteration    25980/10000000 | consumed samples:      3325440 | consumed tokens:   6810501120 | elapsed time per iteration (ms): 5263.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.900502E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.318 | tokens per gpu per second (tgs): 1556.330 | TFLOPs: 12.52 |
g0184: [2024-08-10 15:13:42,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=25990, skipped=36, lr=[0.00019992274556883944, 0.00019992274556883944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 25990 loss: 0.8007 iter time (s): 4.813 samples/sec: 26.597
g0198:  iteration    25990/10000000 | consumed samples:      3326720 | consumed tokens:   6813122560 | elapsed time per iteration (ms): 4845.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.962780E-01 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.419 | tokens per gpu per second (tgs): 1690.817 | TFLOPs: 13.61 |
g0184: [2024-08-10 15:14:25,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=36, lr=[0.00019992263931791374, 0.00019992263931791374], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26000 loss: 0.7834 iter time (s): 4.326 samples/sec: 29.586
g0198:  iteration    26000/10000000 | consumed samples:      3328000 | consumed tokens:   6815744000 | elapsed time per iteration (ms): 4359.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.883771E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.364 | tokens per gpu per second (tgs): 1879.307 | TFLOPs: 15.12 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 26000 | lm loss value: 8.011364E-01 | lm loss PPL: 2.228071E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   26000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 15:21:47,909] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step26000 is about to be saved!
g0184: [2024-08-10 15:21:47,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0184: [2024-08-10 15:21:47,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0184: [2024-08-10 15:21:47,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0198: [2024-08-10 15:21:47,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0198: [2024-08-10 15:21:47,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0198: [2024-08-10 15:21:47,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0194: [2024-08-10 15:21:47,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0194: [2024-08-10 15:21:47,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0194: [2024-08-10 15:21:47,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0195: [2024-08-10 15:21:47,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0195: [2024-08-10 15:21:47,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0195: [2024-08-10 15:21:47,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0185: [2024-08-10 15:21:47,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0185: [2024-08-10 15:21:47,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0185: [2024-08-10 15:21:47,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0197: [2024-08-10 15:21:47,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0197: [2024-08-10 15:21:47,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0197: [2024-08-10 15:21:47,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0187: [2024-08-10 15:21:47,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0187: [2024-08-10 15:21:47,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0187: [2024-08-10 15:21:47,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0188: [2024-08-10 15:21:47,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0188: [2024-08-10 15:21:47,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0188: [2024-08-10 15:21:47,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0198: [2024-08-10 15:21:47,945] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_23-model_00-model_states.pt...
g0187: [2024-08-10 15:21:47,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_08-model_00-model_states.pt...
g0184: [2024-08-10 15:21:47,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_01-model_00-model_states.pt...
g0194: [2024-08-10 15:21:47,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_14-model_00-model_states.pt...
g0195: [2024-08-10 15:21:47,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_17-model_00-model_states.pt...
g0188: [2024-08-10 15:21:47,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_11-model_00-model_states.pt...
g0197: [2024-08-10 15:21:47,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_20-model_00-model_states.pt...
g0185: [2024-08-10 15:21:47,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_05-model_00-model_states.pt...
g0185: [2024-08-10 15:21:48,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_05-model_00-model_states.pt.
g0185: [2024-08-10 15:21:48,101] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_06-model_00-model_states.pt...
g0187: [2024-08-10 15:21:48,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_08-model_00-model_states.pt.
g0197: [2024-08-10 15:21:48,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_20-model_00-model_states.pt.
g0187: [2024-08-10 15:21:48,147] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_09-model_00-model_states.pt...
g0195: [2024-08-10 15:21:48,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_17-model_00-model_states.pt.
g0197: [2024-08-10 15:21:48,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_21-model_00-model_states.pt...
g0195: [2024-08-10 15:21:48,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_18-model_00-model_states.pt...
g0184: [2024-08-10 15:21:48,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 15:21:48,231] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_02-model_00-model_states.pt...
g0198: [2024-08-10 15:21:48,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 15:21:48,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 15:21:48,294] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_24-model_00-model_states.pt.
g0187: [2024-08-10 15:21:48,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_09-model_00-model_states.pt.
g0195: [2024-08-10 15:21:48,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_18-model_00-model_states.pt.
g0197: [2024-08-10 15:21:48,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_21-model_00-model_states.pt.
g0184: [2024-08-10 15:21:48,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_02-model_00-model_states.pt.
g0187: [2024-08-10 15:21:48,326] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_10-model_00-model_states.pt...
g0184: [2024-08-10 15:21:48,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_03-model_00-model_states.pt...
g0198: [2024-08-10 15:21:48,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_25-model_00-model_states.pt...
g0195: [2024-08-10 15:21:48,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_19-model_00-model_states.pt...
g0197: [2024-08-10 15:21:48,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_22-model_00-model_states.pt...
g0197: [2024-08-10 15:21:48,457] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 15:21:48,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_06_model_states.pt...
g0195: [2024-08-10 15:21:48,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 15:21:48,468] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_05_model_states.pt...
g0187: [2024-08-10 15:21:48,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 15:21:48,494] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_02_model_states.pt...
g0198: [2024-08-10 15:21:48,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 15:21:48,524] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_07_model_states.pt...
g0184: [2024-08-10 15:21:48,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 15:21:48,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 15:21:48,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 15:21:48,664] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_00_model_states.pt
g0184: [2024-08-10 15:21:48,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_00_model_states.pt...
g0188: [2024-08-10 15:21:48,845] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_11-model_00-model_states.pt.
g0194: [2024-08-10 15:21:48,845] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_14-model_00-model_states.pt.
g0188: [2024-08-10 15:21:48,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_12-model_00-model_states.pt...
g0194: [2024-08-10 15:21:48,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_15-model_00-model_states.pt...
g0185: [2024-08-10 15:21:48,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 15:21:48,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_07-model_00-model_states.pt...
g0194: [2024-08-10 15:21:49,008] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 15:21:49,039] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_16-model_00-model_states.pt...
g0185: [2024-08-10 15:21:49,045] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 15:21:49,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_01_model_states.pt...
g0188: [2024-08-10 15:21:49,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_12-model_00-model_states.pt.
g0194: [2024-08-10 15:21:49,150] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 15:21:49,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_04_model_states.pt...
g0188: [2024-08-10 15:21:49,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_13-model_00-model_states.pt...
g0188: [2024-08-10 15:21:49,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 15:21:49,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_03_model_states.pt...
g0198: [2024-08-10 15:21:50,492] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 15:21:50,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0195: [2024-08-10 15:21:50,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 15:21:50,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0187: [2024-08-10 15:21:50,821] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 15:21:50,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0197: [2024-08-10 15:21:50,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 15:21:50,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0185: [2024-08-10 15:21:51,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 15:21:51,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0194: [2024-08-10 15:21:51,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 15:21:51,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0184: [2024-08-10 15:21:52,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 15:21:52,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0188: [2024-08-10 15:21:53,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step26000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 15:21:53,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26000 is ready now!
g0184:   successfully saved checkpoint at iteration   26000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.17, Latency(second): 5.405
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (5405.35, 5406.03)
g0184: [2024-08-10 15:22:51,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=26010, skipped=36, lr=[0.00019992253299400253, 0.00019992253299400253], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26010 loss: 0.7937 iter time (s): 5.795 samples/sec: 22.086
g0198:  iteration    26010/10000000 | consumed samples:      3329280 | consumed tokens:   6818365440 | elapsed time per iteration (ms): 50570.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.860791E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.531 | tokens per gpu per second (tgs): 161.992 | TFLOPs: 1.30 |
g0184: [2024-08-10 15:23:44,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=26020, skipped=36, lr=[0.00019992242659710592, 0.00019992242659710592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26020 loss: 0.7847 iter time (s): 5.265 samples/sec: 24.309
g0198:  iteration    26020/10000000 | consumed samples:      3330560 | consumed tokens:   6820986880 | elapsed time per iteration (ms): 5298.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.856804E-01 | loss scale: 131072.0 | grad norm: 0.393 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.159 | tokens per gpu per second (tgs): 1546.200 | TFLOPs: 12.44 |
g0184: [2024-08-10 15:24:31,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=26030, skipped=36, lr=[0.00019992232012722402, 0.00019992232012722402], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26030 loss: 0.7884 iter time (s): 4.709 samples/sec: 27.179
g0198:  iteration    26030/10000000 | consumed samples:      3331840 | consumed tokens:   6823608320 | elapsed time per iteration (ms): 4742.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.962088E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.992 | tokens per gpu per second (tgs): 1727.504 | TFLOPs: 13.90 |
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26031
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26031
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26031
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26031
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26031
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26031
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26031
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26031
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: Grad overflow on iteration 26031
g0195: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26031
g0194: Grad overflow on iteration 26031
g0194: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26031
g0194: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26031
g0194: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: Grad overflow on iteration 26031
g0197: Grad overflow on iteration 26031
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 26031
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26031
g0188: Grad overflow on iteration 26031
g0195: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 26031
g0187: Grad overflow on iteration 26031
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26031
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26031
g0195: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26031
g0198: Grad overflow on iteration 26031
g0187: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: Grad overflow on iteration 26031
g0188: Grad overflow on iteration 26031
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26031
g0195: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: Grad overflow on iteration 26031
g0198: Grad overflow on iteration 26031
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 15:24:41,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: Grad overflow on iteration 26031
g0198: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26031
g0187: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 15:24:41,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 15:24:41,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 15:24:41,101] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26031
g0187: [2024-08-10 15:24:41,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 15:25:17,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=26040, skipped=37, lr=[0.00019992221358435685, 0.00019992221358435685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26040 loss: 0.8041 iter time (s): 4.556 samples/sec: 28.095
g0198:  iteration    26040/10000000 | consumed samples:      3333120 | consumed tokens:   6826229760 | elapsed time per iteration (ms): 4588.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.955425E-01 | loss scale: 65536.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.894 | tokens per gpu per second (tgs): 1785.218 | TFLOPs: 14.37 |
g0184: [2024-08-10 15:26:04,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=26050, skipped=37, lr=[0.00019992210696850456, 0.00019992210696850456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26050 loss: 0.7951 iter time (s): 4.620 samples/sec: 27.705
g0198:  iteration    26050/10000000 | consumed samples:      3334400 | consumed tokens:   6828851200 | elapsed time per iteration (ms): 4652.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.932771E-01 | loss scale: 65536.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.510 | tokens per gpu per second (tgs): 1760.610 | TFLOPs: 14.17 |
g0184: [2024-08-10 15:27:09,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=26060, skipped=37, lr=[0.00019992200027966716, 0.00019992200027966716], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26060 loss: 0.7913 iter time (s): 6.496 samples/sec: 19.704
g0198:  iteration    26060/10000000 | consumed samples:      3335680 | consumed tokens:   6831472640 | elapsed time per iteration (ms): 6529.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.952734E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.602 | tokens per gpu per second (tgs): 1254.545 | TFLOPs: 10.10 |
g0184: [2024-08-10 15:27:58,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=26070, skipped=37, lr=[0.00019992189351784477, 0.00019992189351784477], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26070 loss: 0.7989 iter time (s): 4.856 samples/sec: 26.360
g0198:  iteration    26070/10000000 | consumed samples:      3336960 | consumed tokens:   6834094080 | elapsed time per iteration (ms): 4889.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.986839E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.176 | tokens per gpu per second (tgs): 1675.289 | TFLOPs: 13.48 |
g0184: [2024-08-10 15:28:43,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=26080, skipped=37, lr=[0.0001999217866830375, 0.0001999217866830375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26080 loss: 0.7623 iter time (s): 4.416 samples/sec: 28.987
g0198:  iteration    26080/10000000 | consumed samples:      3338240 | consumed tokens:   6836715520 | elapsed time per iteration (ms): 4448.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.859397E-01 | loss scale: 65536.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.773 | tokens per gpu per second (tgs): 1841.464 | TFLOPs: 14.82 |
g0184: [2024-08-10 15:29:26,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=26090, skipped=37, lr=[0.00019992167977524536, 0.00019992167977524536], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26090 loss: 0.7880 iter time (s): 4.359 samples/sec: 29.363
g0198:  iteration    26090/10000000 | consumed samples:      3339520 | consumed tokens:   6839336960 | elapsed time per iteration (ms): 4391.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.901689E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.145 | tokens per gpu per second (tgs): 1865.286 | TFLOPs: 15.01 |
g0184: [2024-08-10 15:30:11,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=26100, skipped=37, lr=[0.0001999215727944685, 0.0001999215727944685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26100 loss: 0.7480 iter time (s): 4.428 samples/sec: 28.909
g0198:  iteration    26100/10000000 | consumed samples:      3340800 | consumed tokens:   6841958400 | elapsed time per iteration (ms): 4460.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.903679E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.694 | tokens per gpu per second (tgs): 1836.390 | TFLOPs: 14.78 |
g0184: [2024-08-10 15:31:02,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=26110, skipped=37, lr=[0.00019992146574070695, 0.00019992146574070695], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26110 loss: 0.8060 iter time (s): 5.014 samples/sec: 25.528
g0198:  iteration    26110/10000000 | consumed samples:      3342080 | consumed tokens:   6844579840 | elapsed time per iteration (ms): 5047.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.902191E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.358 | tokens per gpu per second (tgs): 1622.937 | TFLOPs: 13.06 |
g0184: [2024-08-10 15:31:59,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=26120, skipped=37, lr=[0.00019992135861396083, 0.00019992135861396083], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26120 loss: 0.8291 iter time (s): 5.701 samples/sec: 22.451
g0198:  iteration    26120/10000000 | consumed samples:      3343360 | consumed tokens:   6847201280 | elapsed time per iteration (ms): 5734.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.962880E-01 | loss scale: 65536.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.322 | tokens per gpu per second (tgs): 1428.605 | TFLOPs: 11.50 |
g0184: [2024-08-10 15:33:00,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=26130, skipped=37, lr=[0.00019992125141423024, 0.00019992125141423024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26130 loss: 0.7681 iter time (s): 6.093 samples/sec: 21.009
g0198:  iteration    26130/10000000 | consumed samples:      3344640 | consumed tokens:   6849822720 | elapsed time per iteration (ms): 6125.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.926123E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.897 | tokens per gpu per second (tgs): 1337.397 | TFLOPs: 10.76 |
g0184: [2024-08-10 15:33:46,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=26140, skipped=37, lr=[0.0001999211441415152, 0.0001999211441415152], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26140 loss: 0.8134 iter time (s): 4.580 samples/sec: 27.948
g0198:  iteration    26140/10000000 | consumed samples:      3345920 | consumed tokens:   6852444160 | elapsed time per iteration (ms): 4612.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.866433E-01 | loss scale: 65536.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.748 | tokens per gpu per second (tgs): 1775.881 | TFLOPs: 14.29 |
g0184: [2024-08-10 15:34:37,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=26150, skipped=37, lr=[0.0001999210367958158, 0.0001999210367958158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26150 loss: 0.7819 iter time (s): 5.055 samples/sec: 25.321
g0198:  iteration    26150/10000000 | consumed samples:      3347200 | consumed tokens:   6855065600 | elapsed time per iteration (ms): 5087.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.894428E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.160 | tokens per gpu per second (tgs): 1610.222 | TFLOPs: 12.96 |
g0184: [2024-08-10 15:35:22,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=26160, skipped=37, lr=[0.0001999209293771322, 0.0001999209293771322], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26160 loss: 0.7668 iter time (s): 4.453 samples/sec: 28.743
g0198:  iteration    26160/10000000 | consumed samples:      3348480 | consumed tokens:   6857687040 | elapsed time per iteration (ms): 4485.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.911248E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.534 | tokens per gpu per second (tgs): 1826.166 | TFLOPs: 14.70 |
g0184: [2024-08-10 15:36:09,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=26170, skipped=37, lr=[0.00019992082188546441, 0.00019992082188546441], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26170 loss: 0.7866 iter time (s): 4.693 samples/sec: 27.273
g0198:  iteration    26170/10000000 | consumed samples:      3349760 | consumed tokens:   6860308480 | elapsed time per iteration (ms): 4726.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.789902E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.083 | tokens per gpu per second (tgs): 1733.326 | TFLOPs: 13.95 |
g0184: [2024-08-10 15:36:58,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=26180, skipped=37, lr=[0.0001999207143208125, 0.0001999207143208125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26180 loss: 0.7732 iter time (s): 4.811 samples/sec: 26.605
g0198:  iteration    26180/10000000 | consumed samples:      3351040 | consumed tokens:   6862929920 | elapsed time per iteration (ms): 4845.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.821820E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.417 | tokens per gpu per second (tgs): 1690.683 | TFLOPs: 13.61 |
g0184: [2024-08-10 15:37:47,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=26190, skipped=37, lr=[0.00019992060668317665, 0.00019992060668317665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26190 loss: 0.7942 iter time (s): 4.921 samples/sec: 26.009
g0198:  iteration    26190/10000000 | consumed samples:      3352320 | consumed tokens:   6865551360 | elapsed time per iteration (ms): 4954.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.898105E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.835 | tokens per gpu per second (tgs): 1653.457 | TFLOPs: 13.31 |
g0184: [2024-08-10 15:38:34,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=26200, skipped=37, lr=[0.00019992049897255685, 0.00019992049897255685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26200 loss: 0.8000 iter time (s): 4.610 samples/sec: 27.765
g0198:  iteration    26200/10000000 | consumed samples:      3353600 | consumed tokens:   6868172800 | elapsed time per iteration (ms): 4642.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.949322E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.569 | tokens per gpu per second (tgs): 1764.447 | TFLOPs: 14.20 |
g0184: [2024-08-10 15:39:19,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=26210, skipped=37, lr=[0.0001999203911889532, 0.0001999203911889532], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26210 loss: 0.8071 iter time (s): 4.489 samples/sec: 28.516
g0198:  iteration    26210/10000000 | consumed samples:      3354880 | consumed tokens:   6870794240 | elapsed time per iteration (ms): 4521.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.896831E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.308 | tokens per gpu per second (tgs): 1811.727 | TFLOPs: 14.58 |
g0184: [2024-08-10 15:40:08,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=26220, skipped=37, lr=[0.00019992028333236579, 0.00019992028333236579], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26220 loss: 0.7842 iter time (s): 4.905 samples/sec: 26.098
g0198:  iteration    26220/10000000 | consumed samples:      3356160 | consumed tokens:   6873415680 | elapsed time per iteration (ms): 4950.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.058019E-01 | loss scale: 65536.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.857 | tokens per gpu per second (tgs): 1654.838 | TFLOPs: 13.32 |
g0184: [2024-08-10 15:40:53,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=26230, skipped=37, lr=[0.00019992017540279472, 0.00019992017540279472], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26230 loss: 0.7848 iter time (s): 4.391 samples/sec: 29.148
g0198:  iteration    26230/10000000 | consumed samples:      3357440 | consumed tokens:   6876037120 | elapsed time per iteration (ms): 4424.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.877378E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.932 | tokens per gpu per second (tgs): 1851.678 | TFLOPs: 14.90 |
g0184: [2024-08-10 15:41:39,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=26240, skipped=37, lr=[0.00019992006740024006, 0.00019992006740024006], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26240 loss: 0.7784 iter time (s): 4.644 samples/sec: 27.561
g0198:  iteration    26240/10000000 | consumed samples:      3358720 | consumed tokens:   6878658560 | elapsed time per iteration (ms): 4676.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.897025E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.371 | tokens per gpu per second (tgs): 1751.739 | TFLOPs: 14.10 |
g0184: [2024-08-10 15:42:26,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=26250, skipped=37, lr=[0.00019991995932470188, 0.00019991995932470188], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26250 loss: 0.7857 iter time (s): 4.585 samples/sec: 27.918
g0198:  iteration    26250/10000000 | consumed samples:      3360000 | consumed tokens:   6881280000 | elapsed time per iteration (ms): 4617.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.740149E-01 | loss scale: 65536.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.722 | tokens per gpu per second (tgs): 1774.183 | TFLOPs: 14.28 |
g0184: [2024-08-10 15:43:12,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=26260, skipped=37, lr=[0.00019991985117618027, 0.00019991985117618027], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26260 loss: 0.7928 iter time (s): 4.613 samples/sec: 27.749
g0198:  iteration    26260/10000000 | consumed samples:      3361280 | consumed tokens:   6883901440 | elapsed time per iteration (ms): 4645.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.054316E-01 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.554 | tokens per gpu per second (tgs): 1763.426 | TFLOPs: 14.19 |
g0184: [2024-08-10 15:44:01,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=26270, skipped=37, lr=[0.00019991974295467536, 0.00019991974295467536], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26270 loss: 0.8243 iter time (s): 4.831 samples/sec: 26.495
g0198:  iteration    26270/10000000 | consumed samples:      3362560 | consumed tokens:   6886522880 | elapsed time per iteration (ms): 4864.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.099739E-01 | loss scale: 65536.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.315 | tokens per gpu per second (tgs): 1684.145 | TFLOPs: 13.55 |
g0184: [2024-08-10 15:44:48,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=26280, skipped=37, lr=[0.00019991963466018715, 0.00019991963466018715], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26280 loss: 0.7911 iter time (s): 4.678 samples/sec: 27.365
g0198:  iteration    26280/10000000 | consumed samples:      3363840 | consumed tokens:   6889144320 | elapsed time per iteration (ms): 4710.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.051212E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.175 | tokens per gpu per second (tgs): 1739.184 | TFLOPs: 14.00 |
g0184: [2024-08-10 15:45:34,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=26290, skipped=37, lr=[0.0001999195262927158, 0.0001999195262927158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26290 loss: 0.8020 iter time (s): 4.614 samples/sec: 27.743
g0198:  iteration    26290/10000000 | consumed samples:      3365120 | consumed tokens:   6891765760 | elapsed time per iteration (ms): 4646.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.014689E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.549 | tokens per gpu per second (tgs): 1763.120 | TFLOPs: 14.19 |
g0184: [2024-08-10 15:46:19,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=26300, skipped=37, lr=[0.00019991941785226134, 0.00019991941785226134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26300 loss: 0.7940 iter time (s): 4.468 samples/sec: 28.649
g0198:  iteration    26300/10000000 | consumed samples:      3366400 | consumed tokens:   6894387200 | elapsed time per iteration (ms): 4509.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.896747E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.387 | tokens per gpu per second (tgs): 1816.782 | TFLOPs: 14.62 |
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26308
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26308
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26308
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 15:47:00,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26308
g0195: [2024-08-10 15:47:00,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: Grad overflow on iteration 26308
g0185: [2024-08-10 15:47:00,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26308
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26308
g0188: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26308
g0194: Grad overflow on iteration 26308
g0197: [2024-08-10 15:47:00,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26308
g0195: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 26308
g0197: Grad overflow on iteration 26308
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26308
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26308
g0198: Grad overflow on iteration 26308
g0194: Grad overflow on iteration 26308
g0188: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26308
g0197: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26308
g0184: Grad overflow on iteration 26308
g0197: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: Grad overflow on iteration 26308
g0185: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26308
g0185: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26308
g0185: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: Grad overflow on iteration 26308
g0195: Grad overflow on iteration 26308
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: Grad overflow on iteration 26308
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26308
g0187: Grad overflow on iteration 26308
g0197: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26308
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 26308
g0187: Grad overflow on iteration 26308
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: Grad overflow on iteration 26308
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26308
g0195: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 15:47:00,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 15:47:00,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0184: [2024-08-10 15:47:04,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=26310, skipped=38, lr=[0.00019991930933882388, 0.00019991930933882388], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26310 loss: 0.7768 iter time (s): 4.391 samples/sec: 29.148
g0198:  iteration    26310/10000000 | consumed samples:      3367680 | consumed tokens:   6897008640 | elapsed time per iteration (ms): 4423.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.916792E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.934 | tokens per gpu per second (tgs): 1851.773 | TFLOPs: 14.90 |
g0184: [2024-08-10 15:47:52,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=26320, skipped=38, lr=[0.0001999192007524035, 0.0001999192007524035], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26320 loss: 0.8010 iter time (s): 4.765 samples/sec: 26.864
g0198:  iteration    26320/10000000 | consumed samples:      3368960 | consumed tokens:   6899630080 | elapsed time per iteration (ms): 4812.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.952456E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.599 | tokens per gpu per second (tgs): 1702.317 | TFLOPs: 13.70 |
g0184: [2024-08-10 15:48:40,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=26330, skipped=38, lr=[0.00019991909209300025, 0.00019991909209300025], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26330 loss: 0.7617 iter time (s): 4.762 samples/sec: 26.880
g0198:  iteration    26330/10000000 | consumed samples:      3370240 | consumed tokens:   6902251520 | elapsed time per iteration (ms): 4794.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.807931E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.696 | tokens per gpu per second (tgs): 1708.562 | TFLOPs: 13.75 |
g0185: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26335
g0185: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26335
g0185: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26335
g0187: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26335
g0187: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26335
g0185: Grad overflow on iteration 26335
g0187: Grad overflow on iteration 26335
g0195: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26335
g0184: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26335
g0184: Grad overflow on iteration 26335
g0188: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26335
g0188: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26335
g0188: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26335
g0194: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: Grad overflow on iteration 26335
g0198: Grad overflow on iteration 26335
g0194: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: Grad overflow on iteration 26335
g0197: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26335
g0197: Grad overflow on iteration 26335
g0184: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26335
g0197: Grad overflow on iteration 26335
g0184: Grad overflow on iteration 26335
g0187: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: Grad overflow on iteration 26335
g0198: [2024-08-10 15:49:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 26335
g0198: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26335
g0198: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 15:49:09,859] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0188: [2024-08-10 15:49:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26335
g0187: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26335
g0194: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26335
g0188: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: Grad overflow on iteration 26335
g0188: Grad overflow on iteration 26335
g0188: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26335
g0198: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26335
g0198: Grad overflow on iteration 26335
g0184: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 15:49:09,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 15:49:27,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=26340, skipped=39, lr=[0.00019991898336061426, 0.00019991898336061426], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26340 loss: 0.7938 iter time (s): 4.658 samples/sec: 27.477
g0198:  iteration    26340/10000000 | consumed samples:      3371520 | consumed tokens:   6904872960 | elapsed time per iteration (ms): 4691.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.008895E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.284 | tokens per gpu per second (tgs): 1746.190 | TFLOPs: 14.05 |
g0184: [2024-08-10 15:50:11,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=26350, skipped=39, lr=[0.0001999188745552456, 0.0001999188745552456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26350 loss: 0.7987 iter time (s): 4.396 samples/sec: 29.116
g0198:  iteration    26350/10000000 | consumed samples:      3372800 | consumed tokens:   6907494400 | elapsed time per iteration (ms): 4428.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.807893E-01 | loss scale: 16384.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.902 | tokens per gpu per second (tgs): 1849.701 | TFLOPs: 14.88 |
g0184: [2024-08-10 15:50:58,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=26360, skipped=39, lr=[0.00019991876567689436, 0.00019991876567689436], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26360 loss: 0.8315 iter time (s): 4.686 samples/sec: 27.316
g0198:  iteration    26360/10000000 | consumed samples:      3374080 | consumed tokens:   6910115840 | elapsed time per iteration (ms): 4726.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.962317E-01 | loss scale: 16384.0 | grad norm: 0.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.082 | tokens per gpu per second (tgs): 1733.268 | TFLOPs: 13.95 |
g0184: [2024-08-10 15:51:47,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=26370, skipped=39, lr=[0.00019991865672556062, 0.00019991865672556062], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26370 loss: 0.7921 iter time (s): 4.813 samples/sec: 26.596
g0198:  iteration    26370/10000000 | consumed samples:      3375360 | consumed tokens:   6912737280 | elapsed time per iteration (ms): 4846.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.908743E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.411 | tokens per gpu per second (tgs): 1690.275 | TFLOPs: 13.60 |
g0184: [2024-08-10 15:52:30,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=26380, skipped=39, lr=[0.00019991854770124443, 0.00019991854770124443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26380 loss: 0.7844 iter time (s): 4.331 samples/sec: 29.556
g0198:  iteration    26380/10000000 | consumed samples:      3376640 | consumed tokens:   6915358720 | elapsed time per iteration (ms): 4364.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.923301E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.329 | tokens per gpu per second (tgs): 1877.060 | TFLOPs: 15.11 |
g0184: [2024-08-10 15:53:23,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=26390, skipped=39, lr=[0.0001999184386039459, 0.0001999184386039459], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26390 loss: 0.8077 iter time (s): 5.257 samples/sec: 24.350
g0198:  iteration    26390/10000000 | consumed samples:      3377920 | consumed tokens:   6917980160 | elapsed time per iteration (ms): 5289.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.913909E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.200 | tokens per gpu per second (tgs): 1548.821 | TFLOPs: 12.46 |
g0184: [2024-08-10 15:54:10,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=26400, skipped=39, lr=[0.00019991832943366515, 0.00019991832943366515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26400 loss: 0.8012 iter time (s): 4.617 samples/sec: 27.724
g0198:  iteration    26400/10000000 | consumed samples:      3379200 | consumed tokens:   6920601600 | elapsed time per iteration (ms): 4655.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.951549E-01 | loss scale: 16384.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.496 | tokens per gpu per second (tgs): 1759.732 | TFLOPs: 14.16 |
g0184: [2024-08-10 15:54:56,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=26410, skipped=39, lr=[0.0001999182201904022, 0.0001999182201904022], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26410 loss: 0.7625 iter time (s): 4.627 samples/sec: 27.662
g0198:  iteration    26410/10000000 | consumed samples:      3380480 | consumed tokens:   6923223040 | elapsed time per iteration (ms): 4660.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.833731E-01 | loss scale: 16384.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.466 | tokens per gpu per second (tgs): 1757.829 | TFLOPs: 14.15 |
g0184: [2024-08-10 15:55:45,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=26420, skipped=39, lr=[0.00019991811087415715, 0.00019991811087415715], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26420 loss: 0.8131 iter time (s): 4.825 samples/sec: 26.531
g0198:  iteration    26420/10000000 | consumed samples:      3381760 | consumed tokens:   6925844480 | elapsed time per iteration (ms): 4857.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.001521E-01 | loss scale: 16384.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.353 | tokens per gpu per second (tgs): 1686.562 | TFLOPs: 13.57 |
g0184: [2024-08-10 15:56:35,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=26430, skipped=39, lr=[0.00019991800148493015, 0.00019991800148493015], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26430 loss: 0.8059 iter time (s): 4.919 samples/sec: 26.021
g0198:  iteration    26430/10000000 | consumed samples:      3383040 | consumed tokens:   6928465920 | elapsed time per iteration (ms): 4969.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.924983E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.757 | tokens per gpu per second (tgs): 1648.426 | TFLOPs: 13.27 |
g0184: [2024-08-10 15:57:19,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=26440, skipped=39, lr=[0.00019991789202272118, 0.00019991789202272118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26440 loss: 0.7880 iter time (s): 4.365 samples/sec: 29.321
g0198:  iteration    26440/10000000 | consumed samples:      3384320 | consumed tokens:   6931087360 | elapsed time per iteration (ms): 4398.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.940103E-01 | loss scale: 16384.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.101 | tokens per gpu per second (tgs): 1862.469 | TFLOPs: 14.99 |
g0184: [2024-08-10 15:58:11,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=26450, skipped=39, lr=[0.0001999177824875304, 0.0001999177824875304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26450 loss: 0.7882 iter time (s): 5.175 samples/sec: 24.736
g0198:  iteration    26450/10000000 | consumed samples:      3385600 | consumed tokens:   6933708800 | elapsed time per iteration (ms): 5209.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.979899E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.569 | tokens per gpu per second (tgs): 1572.437 | TFLOPs: 12.65 |
g0184: [2024-08-10 15:59:08,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=26460, skipped=39, lr=[0.00019991767287935784, 0.00019991767287935784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26460 loss: 0.7759 iter time (s): 5.720 samples/sec: 22.377
g0198:  iteration    26460/10000000 | consumed samples:      3386880 | consumed tokens:   6936330240 | elapsed time per iteration (ms): 5752.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.043320E-01 | loss scale: 16384.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.250 | tokens per gpu per second (tgs): 1424.013 | TFLOPs: 11.46 |
g0184: [2024-08-10 15:59:58,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=26470, skipped=39, lr=[0.00019991756319820365, 0.00019991756319820365], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26470 loss: 0.7940 iter time (s): 4.981 samples/sec: 25.700
g0198:  iteration    26470/10000000 | consumed samples:      3388160 | consumed tokens:   6938951680 | elapsed time per iteration (ms): 5013.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.864827E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.533 | tokens per gpu per second (tgs): 1634.088 | TFLOPs: 13.15 |
g0184: [2024-08-10 16:00:50,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=26480, skipped=39, lr=[0.0001999174534440679, 0.0001999174534440679], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26480 loss: 0.8113 iter time (s): 5.163 samples/sec: 24.791
g0198:  iteration    26480/10000000 | consumed samples:      3389440 | consumed tokens:   6941573120 | elapsed time per iteration (ms): 5195.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.910577E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.636 | tokens per gpu per second (tgs): 1576.691 | TFLOPs: 12.69 |
g0184: [2024-08-10 16:01:40,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=26490, skipped=39, lr=[0.0001999173436169506, 0.0001999173436169506], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26490 loss: 0.7666 iter time (s): 4.969 samples/sec: 25.762
g0198:  iteration    26490/10000000 | consumed samples:      3390720 | consumed tokens:   6944194560 | elapsed time per iteration (ms): 5014.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.930620E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.525 | tokens per gpu per second (tgs): 1633.582 | TFLOPs: 13.15 |
g0184: [2024-08-10 16:02:26,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=26500, skipped=39, lr=[0.0001999172337168519, 0.0001999172337168519], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26500 loss: 0.7857 iter time (s): 4.498 samples/sec: 28.457
g0198:  iteration    26500/10000000 | consumed samples:      3392000 | consumed tokens:   6946816000 | elapsed time per iteration (ms): 4530.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.959972E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.255 | tokens per gpu per second (tgs): 1808.307 | TFLOPs: 14.55 |
g0184: [2024-08-10 16:03:09,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=26510, skipped=39, lr=[0.00019991712374377185, 0.00019991712374377185], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26510 loss: 0.7877 iter time (s): 4.288 samples/sec: 29.849
g0198:  iteration    26510/10000000 | consumed samples:      3393280 | consumed tokens:   6949437440 | elapsed time per iteration (ms): 4320.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.904004E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.625 | tokens per gpu per second (tgs): 1895.975 | TFLOPs: 15.26 |
g0184: [2024-08-10 16:04:02,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=26520, skipped=39, lr=[0.00019991701369771059, 0.00019991701369771059], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26520 loss: 0.7963 iter time (s): 5.308 samples/sec: 24.115
g0198:  iteration    26520/10000000 | consumed samples:      3394560 | consumed tokens:   6952058880 | elapsed time per iteration (ms): 5341.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.933753E-01 | loss scale: 16384.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.965 | tokens per gpu per second (tgs): 1533.765 | TFLOPs: 12.34 |
g0184: [2024-08-10 16:04:51,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=26530, skipped=39, lr=[0.00019991690357866817, 0.00019991690357866817], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26530 loss: 0.7791 iter time (s): 4.845 samples/sec: 26.417
g0198:  iteration    26530/10000000 | consumed samples:      3395840 | consumed tokens:   6954680320 | elapsed time per iteration (ms): 4879.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.974509E-01 | loss scale: 16384.0 | grad norm: 0.437 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.235 | tokens per gpu per second (tgs): 1679.045 | TFLOPs: 13.51 |
g0184: [2024-08-10 16:05:36,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=26540, skipped=39, lr=[0.0001999167933866447, 0.0001999167933866447], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26540 loss: 0.8074 iter time (s): 4.420 samples/sec: 28.961
g0198:  iteration    26540/10000000 | consumed samples:      3397120 | consumed tokens:   6957301760 | elapsed time per iteration (ms): 4452.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.906277E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.749 | tokens per gpu per second (tgs): 1839.915 | TFLOPs: 14.81 |
g0184: [2024-08-10 16:06:19,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=26550, skipped=39, lr=[0.0001999166831216402, 0.0001999166831216402], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26550 loss: 0.8104 iter time (s): 4.326 samples/sec: 29.586
g0198:  iteration    26550/10000000 | consumed samples:      3398400 | consumed tokens:   6959923200 | elapsed time per iteration (ms): 4372.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.892135E-01 | loss scale: 16384.0 | grad norm: 1.682 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.272 | tokens per gpu per second (tgs): 1873.438 | TFLOPs: 15.08 |
g0184: [2024-08-10 16:07:03,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=26560, skipped=39, lr=[0.00019991657278365478, 0.00019991657278365478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26560 loss: 0.8229 iter time (s): 4.342 samples/sec: 29.480
g0198:  iteration    26560/10000000 | consumed samples:      3399680 | consumed tokens:   6962544640 | elapsed time per iteration (ms): 4376.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.056731E-01 | loss scale: 16384.0 | grad norm: 0.600 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.250 | tokens per gpu per second (tgs): 1872.014 | TFLOPs: 15.06 |
g0184: [2024-08-10 16:07:47,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=26570, skipped=39, lr=[0.00019991646237268857, 0.00019991646237268857], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26570 loss: 0.8315 iter time (s): 4.390 samples/sec: 29.154
g0198:  iteration    26570/10000000 | consumed samples:      3400960 | consumed tokens:   6965166080 | elapsed time per iteration (ms): 4423.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.133158E-01 | loss scale: 16384.0 | grad norm: 0.320 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.939 | tokens per gpu per second (tgs): 1852.077 | TFLOPs: 14.90 |
g0184: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26574
g0184: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26574
g0184: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26574
g0184: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 26574
g0184: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26574
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26574
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26574
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 26574
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26574
g0195: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26574
g0195: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26574
g0195: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 16:08:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 26574
g0194: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26574
g0185: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26574
g0194: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 26574
g0194: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 26574
g0188: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26574
g0188: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 26574
g0188: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 26574
g0188: Grad overflow on iteration 26574
g0198: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: Grad overflow on iteration 26574
g0198: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 26574
g0187: Grad overflow on iteration 26574
g0198: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: Grad overflow on iteration 26574
g0188: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 16:08:10,122] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0187: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 26574
g0187: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 16:08:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: Grad overflow on iteration 26574
g0198: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 26574
g0187: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: Grad overflow on iteration 26574
g0198: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: Grad overflow on iteration 26574
g0187: [2024-08-10 16:08:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26574
g0195: [2024-08-10 16:08:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 16:08:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26574
g0197: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 16:08:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 26574
g0197: [2024-08-10 16:08:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 16:08:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 16:08:33,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=26580, skipped=40, lr=[0.00019991635188874166, 0.00019991635188874166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26580 loss: 0.8012 iter time (s): 4.521 samples/sec: 28.315
g0198:  iteration    26580/10000000 | consumed samples:      3402240 | consumed tokens:   6967787520 | elapsed time per iteration (ms): 4553.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.111546E-01 | loss scale: 8192.0 | grad norm: 0.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.113 | tokens per gpu per second (tgs): 1799.213 | TFLOPs: 14.48 |
g0184: [2024-08-10 16:09:24,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=26590, skipped=40, lr=[0.00019991624133181403, 0.00019991624133181403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26590 loss: 0.7873 iter time (s): 5.108 samples/sec: 25.061
g0198:  iteration    26590/10000000 | consumed samples:      3403520 | consumed tokens:   6970408960 | elapsed time per iteration (ms): 5140.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.030323E-01 | loss scale: 8192.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.901 | tokens per gpu per second (tgs): 1593.687 | TFLOPs: 12.82 |
g0184: [2024-08-10 16:10:14,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=26600, skipped=40, lr=[0.00019991613070190588, 0.00019991613070190588], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26600 loss: 0.8216 iter time (s): 4.904 samples/sec: 26.103
g0198:  iteration    26600/10000000 | consumed samples:      3404800 | consumed tokens:   6973030400 | elapsed time per iteration (ms): 4936.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.026523E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.931 | tokens per gpu per second (tgs): 1659.582 | TFLOPs: 13.35 |
g0184: [2024-08-10 16:11:01,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=26610, skipped=40, lr=[0.00019991601999901721, 0.00019991601999901721], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26610 loss: 0.8351 iter time (s): 4.704 samples/sec: 27.209
g0198:  iteration    26610/10000000 | consumed samples:      3406080 | consumed tokens:   6975651840 | elapsed time per iteration (ms): 4737.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.955443E-01 | loss scale: 8192.0 | grad norm: 3.600 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.021 | tokens per gpu per second (tgs): 1729.336 | TFLOPs: 13.92 |
g0184: [2024-08-10 16:11:49,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=26620, skipped=40, lr=[0.00019991590922314821, 0.00019991590922314821], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26620 loss: 0.8307 iter time (s): 4.742 samples/sec: 26.995
g0198:  iteration    26620/10000000 | consumed samples:      3407360 | consumed tokens:   6978273280 | elapsed time per iteration (ms): 4786.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.921103E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.740 | tokens per gpu per second (tgs): 1711.348 | TFLOPs: 13.77 |
g0184: [2024-08-10 16:12:36,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=26630, skipped=40, lr=[0.00019991579837429883, 0.00019991579837429883], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26630 loss: 0.8077 iter time (s): 4.667 samples/sec: 27.425
g0198:  iteration    26630/10000000 | consumed samples:      3408640 | consumed tokens:   6980894720 | elapsed time per iteration (ms): 4699.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.978756E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.235 | tokens per gpu per second (tgs): 1743.051 | TFLOPs: 14.03 |
g0184: [2024-08-10 16:13:22,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=26640, skipped=40, lr=[0.00019991568745246927, 0.00019991568745246927], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26640 loss: 0.7497 iter time (s): 4.608 samples/sec: 27.780
g0198:  iteration    26640/10000000 | consumed samples:      3409920 | consumed tokens:   6983516160 | elapsed time per iteration (ms): 4640.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.823522E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.584 | tokens per gpu per second (tgs): 1765.383 | TFLOPs: 14.21 |
g0184: [2024-08-10 16:14:25,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=26650, skipped=40, lr=[0.00019991557645765955, 0.00019991557645765955], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26650 loss: 0.7590 iter time (s): 6.278 samples/sec: 20.387
g0198:  iteration    26650/10000000 | consumed samples:      3411200 | consumed tokens:   6986137600 | elapsed time per iteration (ms): 6312.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.865689E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.276 | tokens per gpu per second (tgs): 1297.673 | TFLOPs: 10.44 |
g0184: [2024-08-10 16:15:18,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=26660, skipped=40, lr=[0.0001999154653898698, 0.0001999154653898698], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26660 loss: 0.7979 iter time (s): 5.256 samples/sec: 24.352
g0198:  iteration    26660/10000000 | consumed samples:      3412480 | consumed tokens:   6988759040 | elapsed time per iteration (ms): 5289.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.901829E-01 | loss scale: 8192.0 | grad norm: 0.322 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.201 | tokens per gpu per second (tgs): 1548.854 | TFLOPs: 12.46 |
g0184: [2024-08-10 16:16:18,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=26670, skipped=40, lr=[0.00019991535424910008, 0.00019991535424910008], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26670 loss: 0.7752 iter time (s): 5.903 samples/sec: 21.684
g0198:  iteration    26670/10000000 | consumed samples:      3413760 | consumed tokens:   6991380480 | elapsed time per iteration (ms): 5935.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.834758E-01 | loss scale: 8192.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.564 | tokens per gpu per second (tgs): 1380.093 | TFLOPs: 11.11 |
g0184: [2024-08-10 16:17:15,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=26680, skipped=40, lr=[0.00019991524303535044, 0.00019991524303535044], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26680 loss: 0.7835 iter time (s): 5.678 samples/sec: 22.544
g0198:  iteration    26680/10000000 | consumed samples:      3415040 | consumed tokens:   6994001920 | elapsed time per iteration (ms): 5710.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.881125E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.414 | tokens per gpu per second (tgs): 1434.504 | TFLOPs: 11.54 |
g0184: [2024-08-10 16:18:18,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=26690, skipped=40, lr=[0.00019991513174862102, 0.00019991513174862102], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26690 loss: 0.7755 iter time (s): 6.284 samples/sec: 20.368
g0198:  iteration    26690/10000000 | consumed samples:      3416320 | consumed tokens:   6996623360 | elapsed time per iteration (ms): 6316.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.859893E-01 | loss scale: 8192.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.264 | tokens per gpu per second (tgs): 1296.887 | TFLOPs: 10.44 |
g0184: [2024-08-10 16:19:14,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=26700, skipped=40, lr=[0.0001999150203889119, 0.0001999150203889119], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26700 loss: 0.7830 iter time (s): 5.590 samples/sec: 22.896
g0198:  iteration    26700/10000000 | consumed samples:      3417600 | consumed tokens:   6999244800 | elapsed time per iteration (ms): 5623.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.822246E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.762 | tokens per gpu per second (tgs): 1456.766 | TFLOPs: 11.72 |
g0184: [2024-08-10 16:20:15,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=26710, skipped=40, lr=[0.00019991490895622313, 0.00019991490895622313], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26710 loss: 0.7708 iter time (s): 6.012 samples/sec: 21.289
g0198:  iteration    26710/10000000 | consumed samples:      3418880 | consumed tokens:   7001866240 | elapsed time per iteration (ms): 6045.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.907637E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.174 | tokens per gpu per second (tgs): 1355.140 | TFLOPs: 10.91 |
g0184: [2024-08-10 16:21:03,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=26720, skipped=40, lr=[0.00019991479745055485, 0.00019991479745055485], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26720 loss: 0.7810 iter time (s): 4.805 samples/sec: 26.639
g0198:  iteration    26720/10000000 | consumed samples:      3420160 | consumed tokens:   7004487680 | elapsed time per iteration (ms): 4837.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.805366E-01 | loss scale: 8192.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.460 | tokens per gpu per second (tgs): 1693.452 | TFLOPs: 13.63 |
g0184: [2024-08-10 16:21:59,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=26730, skipped=40, lr=[0.0001999146858719071, 0.0001999146858719071], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26730 loss: 0.7880 iter time (s): 5.575 samples/sec: 22.959
g0198:  iteration    26730/10000000 | consumed samples:      3421440 | consumed tokens:   7007109120 | elapsed time per iteration (ms): 5607.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.965844E-01 | loss scale: 8192.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.825 | tokens per gpu per second (tgs): 1460.818 | TFLOPs: 11.76 |
g0184: [2024-08-10 16:23:03,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=26740, skipped=40, lr=[0.00019991457422027998, 0.00019991457422027998], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26740 loss: 0.7618 iter time (s): 6.309 samples/sec: 20.288
g0198:  iteration    26740/10000000 | consumed samples:      3422720 | consumed tokens:   7009730560 | elapsed time per iteration (ms): 6358.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.856125E-01 | loss scale: 8192.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.129 | tokens per gpu per second (tgs): 1288.268 | TFLOPs: 10.37 |
g0184: [2024-08-10 16:24:11,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=26750, skipped=40, lr=[0.00019991446249567358, 0.00019991446249567358], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26750 loss: 0.8022 iter time (s): 6.812 samples/sec: 18.790
g0198:  iteration    26750/10000000 | consumed samples:      3424000 | consumed tokens:   7012352000 | elapsed time per iteration (ms): 6873.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.775765E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.622 | tokens per gpu per second (tgs): 1191.802 | TFLOPs: 9.59 |
g0184: [2024-08-10 16:25:01,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=26760, skipped=40, lr=[0.00019991435069808796, 0.00019991435069808796], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26760 loss: 0.7903 iter time (s): 4.955 samples/sec: 25.833
g0198:  iteration    26760/10000000 | consumed samples:      3425280 | consumed tokens:   7014973440 | elapsed time per iteration (ms): 4988.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.876245E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.662 | tokens per gpu per second (tgs): 1642.350 | TFLOPs: 13.22 |
g0184: [2024-08-10 16:25:47,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=26770, skipped=40, lr=[0.00019991423882752325, 0.00019991423882752325], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26770 loss: 0.7968 iter time (s): 4.520 samples/sec: 28.318
g0198:  iteration    26770/10000000 | consumed samples:      3426560 | consumed tokens:   7017594880 | elapsed time per iteration (ms): 4552.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.971594E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.116 | tokens per gpu per second (tgs): 1799.424 | TFLOPs: 14.48 |
g0184: [2024-08-10 16:26:34,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=26780, skipped=40, lr=[0.00019991412688397952, 0.00019991412688397952], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26780 loss: 0.8041 iter time (s): 4.685 samples/sec: 27.320
g0198:  iteration    26780/10000000 | consumed samples:      3427840 | consumed tokens:   7020216320 | elapsed time per iteration (ms): 4717.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.904927E-01 | loss scale: 8192.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.131 | tokens per gpu per second (tgs): 1736.368 | TFLOPs: 13.97 |
g0184: [2024-08-10 16:27:30,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=26790, skipped=40, lr=[0.00019991401486745683, 0.00019991401486745683], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26790 loss: 0.7860 iter time (s): 5.526 samples/sec: 23.163
g0198:  iteration    26790/10000000 | consumed samples:      3429120 | consumed tokens:   7022837760 | elapsed time per iteration (ms): 5559.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.941225E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.024 | tokens per gpu per second (tgs): 1473.534 | TFLOPs: 11.86 |
g0184: [2024-08-10 16:28:26,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=26800, skipped=40, lr=[0.0001999139027779553, 0.0001999139027779553], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26800 loss: 0.8272 iter time (s): 5.647 samples/sec: 22.668
g0198:  iteration    26800/10000000 | consumed samples:      3430400 | consumed tokens:   7025459200 | elapsed time per iteration (ms): 5679.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.911620E-01 | loss scale: 8192.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.538 | tokens per gpu per second (tgs): 1442.436 | TFLOPs: 11.61 |
g0184: [2024-08-10 16:29:20,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=26810, skipped=40, lr=[0.000199913790615475, 0.000199913790615475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26810 loss: 0.7926 iter time (s): 5.335 samples/sec: 23.994
g0198:  iteration    26810/10000000 | consumed samples:      3431680 | consumed tokens:   7028080640 | elapsed time per iteration (ms): 5367.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.863819E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.847 | tokens per gpu per second (tgs): 1526.230 | TFLOPs: 12.28 |
g0184: [2024-08-10 16:30:09,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=26820, skipped=40, lr=[0.00019991367838001603, 0.00019991367838001603], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26820 loss: 0.7785 iter time (s): 4.897 samples/sec: 26.137
g0198:  iteration    26820/10000000 | consumed samples:      3432960 | consumed tokens:   7030702080 | elapsed time per iteration (ms): 4940.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.865220E-01 | loss scale: 8192.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.906 | tokens per gpu per second (tgs): 1658.005 | TFLOPs: 13.34 |
g0184: [2024-08-10 16:31:00,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=26830, skipped=40, lr=[0.00019991356607157847, 0.00019991356607157847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26830 loss: 0.8113 iter time (s): 5.043 samples/sec: 25.384
g0198:  iteration    26830/10000000 | consumed samples:      3434240 | consumed tokens:   7033323520 | elapsed time per iteration (ms): 5075.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.898127E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.222 | tokens per gpu per second (tgs): 1614.197 | TFLOPs: 12.99 |
g0184: [2024-08-10 16:31:52,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=26840, skipped=40, lr=[0.00019991345369016237, 0.00019991345369016237], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26840 loss: 0.7700 iter time (s): 5.169 samples/sec: 24.761
g0198:  iteration    26840/10000000 | consumed samples:      3435520 | consumed tokens:   7035944960 | elapsed time per iteration (ms): 5203.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.893979E-01 | loss scale: 8192.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.600 | tokens per gpu per second (tgs): 1574.394 | TFLOPs: 12.67 |
g0184: [2024-08-10 16:32:51,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=26850, skipped=40, lr=[0.00019991334123576788, 0.00019991334123576788], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26850 loss: 0.7893 iter time (s): 5.839 samples/sec: 21.923
g0198:  iteration    26850/10000000 | consumed samples:      3436800 | consumed tokens:   7038566400 | elapsed time per iteration (ms): 5871.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.982012E-01 | loss scale: 8192.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.800 | tokens per gpu per second (tgs): 1395.205 | TFLOPs: 11.23 |
g0184: [2024-08-10 16:33:38,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=26860, skipped=40, lr=[0.00019991322870839505, 0.00019991322870839505], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26860 loss: 0.7641 iter time (s): 4.675 samples/sec: 27.380
g0198:  iteration    26860/10000000 | consumed samples:      3438080 | consumed tokens:   7041187840 | elapsed time per iteration (ms): 4708.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.863493E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.183 | tokens per gpu per second (tgs): 1739.696 | TFLOPs: 14.00 |
g0184: [2024-08-10 16:34:33,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=26870, skipped=40, lr=[0.00019991311610804394, 0.00019991311610804394], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26870 loss: 0.7868 iter time (s): 5.487 samples/sec: 23.328
g0198:  iteration    26870/10000000 | consumed samples:      3439360 | consumed tokens:   7043809280 | elapsed time per iteration (ms): 5519.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.928310E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.189 | tokens per gpu per second (tgs): 1484.109 | TFLOPs: 11.94 |
g0184: [2024-08-10 16:35:20,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=26880, skipped=40, lr=[0.0001999130034347147, 0.0001999130034347147], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26880 loss: 0.8390 iter time (s): 4.600 samples/sec: 27.828
g0198:  iteration    26880/10000000 | consumed samples:      3440640 | consumed tokens:   7046430720 | elapsed time per iteration (ms): 4632.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.117070E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.633 | tokens per gpu per second (tgs): 1768.483 | TFLOPs: 14.23 |
g0184: [2024-08-10 16:36:04,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=26890, skipped=40, lr=[0.00019991289068840737, 0.00019991289068840737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26890 loss: 0.8232 iter time (s): 4.439 samples/sec: 28.834
g0198:  iteration    26890/10000000 | consumed samples:      3441920 | consumed tokens:   7049052160 | elapsed time per iteration (ms): 4472.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.978683E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.622 | tokens per gpu per second (tgs): 1831.806 | TFLOPs: 14.74 |
g0184: [2024-08-10 16:37:05,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=26900, skipped=40, lr=[0.00019991277786912204, 0.00019991277786912204], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26900 loss: 0.8092 iter time (s): 6.025 samples/sec: 21.243
g0198:  iteration    26900/10000000 | consumed samples:      3443200 | consumed tokens:   7051673600 | elapsed time per iteration (ms): 6059.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.920263E-01 | loss scale: 8192.0 | grad norm: 0.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.123 | tokens per gpu per second (tgs): 1351.869 | TFLOPs: 10.88 |
g0184: [2024-08-10 16:37:57,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=26910, skipped=40, lr=[0.00019991266497685885, 0.00019991266497685885], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26910 loss: 0.7523 iter time (s): 5.143 samples/sec: 24.887
g0198:  iteration    26910/10000000 | consumed samples:      3444480 | consumed tokens:   7054295040 | elapsed time per iteration (ms): 5176.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.997936E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.725 | tokens per gpu per second (tgs): 1582.415 | TFLOPs: 12.73 |
g0184: [2024-08-10 16:38:45,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=26920, skipped=40, lr=[0.0001999125520116178, 0.0001999125520116178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26920 loss: 0.8056 iter time (s): 4.842 samples/sec: 26.434
g0198:  iteration    26920/10000000 | consumed samples:      3445760 | consumed tokens:   7056916480 | elapsed time per iteration (ms): 4875.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.913434E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.254 | tokens per gpu per second (tgs): 1680.250 | TFLOPs: 13.52 |
g0184: [2024-08-10 16:39:34,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=26930, skipped=40, lr=[0.00019991243897339906, 0.00019991243897339906], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26930 loss: 0.7786 iter time (s): 4.847 samples/sec: 26.408
g0198:  iteration    26930/10000000 | consumed samples:      3447040 | consumed tokens:   7059537920 | elapsed time per iteration (ms): 4881.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.926473E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.224 | tokens per gpu per second (tgs): 1678.336 | TFLOPs: 13.51 |
g0184: [2024-08-10 16:40:22,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=26940, skipped=40, lr=[0.00019991232586220265, 0.00019991232586220265], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26940 loss: 0.8136 iter time (s): 4.776 samples/sec: 26.801
g0198:  iteration    26940/10000000 | consumed samples:      3448320 | consumed tokens:   7062159360 | elapsed time per iteration (ms): 4809.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.832756E-01 | loss scale: 8192.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.615 | tokens per gpu per second (tgs): 1703.342 | TFLOPs: 13.71 |
g0184: [2024-08-10 16:41:10,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=26950, skipped=40, lr=[0.0001999122126780287, 0.0001999122126780287], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26950 loss: 0.7722 iter time (s): 4.721 samples/sec: 27.115
g0198:  iteration    26950/10000000 | consumed samples:      3449600 | consumed tokens:   7064780800 | elapsed time per iteration (ms): 4753.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.831295E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.928 | tokens per gpu per second (tgs): 1723.368 | TFLOPs: 13.87 |
g0184: [2024-08-10 16:42:16,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=26960, skipped=40, lr=[0.00019991209942087727, 0.00019991209942087727], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26960 loss: 0.7904 iter time (s): 6.541 samples/sec: 19.568
g0198:  iteration    26960/10000000 | consumed samples:      3450880 | consumed tokens:   7067402240 | elapsed time per iteration (ms): 6575.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.905422E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.468 | tokens per gpu per second (tgs): 1245.921 | TFLOPs: 10.03 |
g0184: [2024-08-10 16:43:14,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=26970, skipped=40, lr=[0.00019991198609074846, 0.00019991198609074846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26970 loss: 0.7793 iter time (s): 5.769 samples/sec: 22.187
g0198:  iteration    26970/10000000 | consumed samples:      3452160 | consumed tokens:   7070023680 | elapsed time per iteration (ms): 5802.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.984677E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.060 | tokens per gpu per second (tgs): 1411.846 | TFLOPs: 11.36 |
g0184: [2024-08-10 16:44:00,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=26980, skipped=40, lr=[0.00019991187268764236, 0.00019991187268764236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26980 loss: 0.7608 iter time (s): 4.611 samples/sec: 27.760
g0198:  iteration    26980/10000000 | consumed samples:      3453440 | consumed tokens:   7072645120 | elapsed time per iteration (ms): 4643.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.817766E-01 | loss scale: 8192.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.566 | tokens per gpu per second (tgs): 1764.244 | TFLOPs: 14.20 |
g0184: [2024-08-10 16:44:49,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=26990, skipped=40, lr=[0.00019991175921155906, 0.00019991175921155906], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 26990 loss: 0.8003 iter time (s): 4.820 samples/sec: 26.556
g0198:  iteration    26990/10000000 | consumed samples:      3454720 | consumed tokens:   7075266560 | elapsed time per iteration (ms): 4863.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.880922E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.318 | tokens per gpu per second (tgs): 1684.377 | TFLOPs: 13.55 |
g0184: [2024-08-10 16:45:42,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=40, lr=[0.00019991164566249864, 0.00019991164566249864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27000 loss: 0.8053 iter time (s): 5.269 samples/sec: 24.294
g0198:  iteration    27000/10000000 | consumed samples:      3456000 | consumed tokens:   7077888000 | elapsed time per iteration (ms): 5304.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.875120E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.131 | tokens per gpu per second (tgs): 1544.395 | TFLOPs: 12.43 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 27000 | lm loss value: 7.913073E-01 | lm loss PPL: 2.206279E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   27000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 16:52:35,829] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step27000 is about to be saved!
g0184: [2024-08-10 16:52:35,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0184: [2024-08-10 16:52:35,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0198: [2024-08-10 16:52:35,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0198: [2024-08-10 16:52:35,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0198: [2024-08-10 16:52:35,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0184: [2024-08-10 16:52:35,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0185: [2024-08-10 16:52:35,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0185: [2024-08-10 16:52:35,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0185: [2024-08-10 16:52:35,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0197: [2024-08-10 16:52:35,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0197: [2024-08-10 16:52:35,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0197: [2024-08-10 16:52:35,837] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0194: [2024-08-10 16:52:35,837] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0194: [2024-08-10 16:52:35,837] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0194: [2024-08-10 16:52:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0195: [2024-08-10 16:52:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0195: [2024-08-10 16:52:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0195: [2024-08-10 16:52:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0188: [2024-08-10 16:52:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0188: [2024-08-10 16:52:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0188: [2024-08-10 16:52:35,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0187: [2024-08-10 16:52:35,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0187: [2024-08-10 16:52:35,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0187: [2024-08-10 16:52:35,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0198: [2024-08-10 16:52:35,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_23-model_00-model_states.pt...
g0197: [2024-08-10 16:52:35,874] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_20-model_00-model_states.pt...
g0185: [2024-08-10 16:52:35,874] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_05-model_00-model_states.pt...
g0194: [2024-08-10 16:52:35,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_14-model_00-model_states.pt...
g0187: [2024-08-10 16:52:35,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_08-model_00-model_states.pt...
g0188: [2024-08-10 16:52:35,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_11-model_00-model_states.pt...
g0195: [2024-08-10 16:52:35,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_17-model_00-model_states.pt...
g0184: [2024-08-10 16:52:35,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_01-model_00-model_states.pt...
g0198: [2024-08-10 16:52:35,968] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 16:52:35,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 16:52:35,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 16:52:36,016] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_25-model_00-model_states.pt...
g0188: [2024-08-10 16:52:36,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 16:52:36,080] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_12-model_00-model_states.pt...
g0197: [2024-08-10 16:52:36,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_20-model_00-model_states.pt.
g0185: [2024-08-10 16:52:36,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_05-model_00-model_states.pt.
g0197: [2024-08-10 16:52:36,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_21-model_00-model_states.pt...
g0185: [2024-08-10 16:52:36,125] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_06-model_00-model_states.pt...
g0195: [2024-08-10 16:52:36,140] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_17-model_00-model_states.pt.
g0198: [2024-08-10 16:52:36,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 16:52:36,145] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_07_model_states.pt...
g0195: [2024-08-10 16:52:36,180] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_18-model_00-model_states.pt...
g0185: [2024-08-10 16:52:36,234] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_06-model_00-model_states.pt.
g0194: [2024-08-10 16:52:36,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_14-model_00-model_states.pt.
g0188: [2024-08-10 16:52:36,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_12-model_00-model_states.pt.
g0184: [2024-08-10 16:52:36,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_01-model_00-model_states.pt.
g0185: [2024-08-10 16:52:36,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_07-model_00-model_states.pt...
g0188: [2024-08-10 16:52:36,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_13-model_00-model_states.pt...
g0184: [2024-08-10 16:52:36,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_02-model_00-model_states.pt...
g0194: [2024-08-10 16:52:36,282] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_15-model_00-model_states.pt...
g0197: [2024-08-10 16:52:36,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_21-model_00-model_states.pt.
g0197: [2024-08-10 16:52:36,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_22-model_00-model_states.pt...
g0195: [2024-08-10 16:52:36,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_18-model_00-model_states.pt.
g0195: [2024-08-10 16:52:36,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_19-model_00-model_states.pt...
g0184: [2024-08-10 16:52:36,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 16:52:36,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_03-model_00-model_states.pt...
g0197: [2024-08-10 16:52:36,426] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 16:52:36,428] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_06_model_states.pt...
g0188: [2024-08-10 16:52:36,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 16:52:36,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_03_model_states.pt...
g0185: [2024-08-10 16:52:36,491] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 16:52:36,492] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_01_model_states.pt...
g0195: [2024-08-10 16:52:36,502] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 16:52:36,504] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_05_model_states.pt...
g0194: [2024-08-10 16:52:36,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_15-model_00-model_states.pt.
g0184: [2024-08-10 16:52:36,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_03-model_00-model_states.pt.
g0194: [2024-08-10 16:52:36,541] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_16-model_00-model_states.pt...
g0184: [2024-08-10 16:52:36,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_04-model_00-model_states.pt...
g0187: [2024-08-10 16:52:37,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_08-model_00-model_states.pt.
g0187: [2024-08-10 16:52:37,407] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_09-model_00-model_states.pt...
g0187: [2024-08-10 16:52:37,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 16:52:37,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_10-model_00-model_states.pt...
g0187: [2024-08-10 16:52:37,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 16:52:37,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_02_model_states.pt...
g0184: [2024-08-10 16:52:37,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 16:52:37,972] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_00_model_states.pt
g0184: [2024-08-10 16:52:37,972] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_00_model_states.pt...
g0194: [2024-08-10 16:52:37,978] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 16:52:37,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_04_model_states.pt...
g0198: [2024-08-10 16:52:38,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 16:52:38,206] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0197: [2024-08-10 16:52:38,757] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 16:52:38,757] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0188: [2024-08-10 16:52:38,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 16:52:38,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0195: [2024-08-10 16:52:39,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 16:52:39,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0185: [2024-08-10 16:52:39,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 16:52:39,270] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0187: [2024-08-10 16:52:40,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 16:52:40,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0194: [2024-08-10 16:52:40,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 16:52:40,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0184: [2024-08-10 16:52:42,153] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step27000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 16:52:42,155] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27000 is ready now!
g0184:   successfully saved checkpoint at iteration   27000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 3.55, Latency(second): 6.351
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (6350.71, 6351.96)
g0184: [2024-08-10 16:53:39,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=27010, skipped=40, lr=[0.00019991153204046118, 0.00019991153204046118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27010 loss: 0.8091 iter time (s): 5.719 samples/sec: 22.383
g0198:  iteration    27010/10000000 | consumed samples:      3457280 | consumed tokens:   7080509440 | elapsed time per iteration (ms): 47748.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.765650E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.681 | tokens per gpu per second (tgs): 171.568 | TFLOPs: 1.38 |
g0184: [2024-08-10 16:54:32,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=27020, skipped=40, lr=[0.0001999114183454468, 0.0001999114183454468], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27020 loss: 0.7788 iter time (s): 5.227 samples/sec: 24.489
g0198:  iteration    27020/10000000 | consumed samples:      3458560 | consumed tokens:   7083130880 | elapsed time per iteration (ms): 5259.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.773355E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.337 | tokens per gpu per second (tgs): 1557.595 | TFLOPs: 12.53 |
g0184: [2024-08-10 16:55:17,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=27030, skipped=40, lr=[0.00019991130457745553, 0.00019991130457745553], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27030 loss: 0.7914 iter time (s): 4.516 samples/sec: 28.341
g0198:  iteration    27030/10000000 | consumed samples:      3459840 | consumed tokens:   7085752320 | elapsed time per iteration (ms): 4549.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.867232E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.137 | tokens per gpu per second (tgs): 1800.778 | TFLOPs: 14.49 |
g0184: [2024-08-10 16:56:02,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=27040, skipped=40, lr=[0.0001999111907364875, 0.0001999111907364875], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27040 loss: 0.7886 iter time (s): 4.476 samples/sec: 28.596
g0198:  iteration    27040/10000000 | consumed samples:      3461120 | consumed tokens:   7088373760 | elapsed time per iteration (ms): 4508.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.902815E-01 | loss scale: 8192.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.390 | tokens per gpu per second (tgs): 1816.956 | TFLOPs: 14.62 |
g0184: [2024-08-10 16:56:48,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=27050, skipped=40, lr=[0.0001999110768225428, 0.0001999110768225428], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27050 loss: 0.7806 iter time (s): 4.574 samples/sec: 27.985
g0198:  iteration    27050/10000000 | consumed samples:      3462400 | consumed tokens:   7090995200 | elapsed time per iteration (ms): 4607.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.874815E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.779 | tokens per gpu per second (tgs): 1777.879 | TFLOPs: 14.31 |
g0184: [2024-08-10 16:57:56,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=27060, skipped=40, lr=[0.0001999109628356215, 0.0001999109628356215], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27060 loss: 0.7541 iter time (s): 6.697 samples/sec: 19.114
g0198:  iteration    27060/10000000 | consumed samples:      3463680 | consumed tokens:   7093616640 | elapsed time per iteration (ms): 6729.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.877132E-01 | loss scale: 8192.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.020 | tokens per gpu per second (tgs): 1217.306 | TFLOPs: 9.80 |
g0184: [2024-08-10 16:58:51,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=27070, skipped=40, lr=[0.0001999108487757237, 0.0001999108487757237], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27070 loss: 0.7934 iter time (s): 5.518 samples/sec: 23.197
g0198:  iteration    27070/10000000 | consumed samples:      3464960 | consumed tokens:   7096238080 | elapsed time per iteration (ms): 5553.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.878732E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.050 | tokens per gpu per second (tgs): 1475.183 | TFLOPs: 11.87 |
g0195: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 16:59:23,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 16:59:23,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 16:59:41,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=27080, skipped=40, lr=[0.00019991073464284946, 0.00019991073464284946], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27080 loss: 0.7977 iter time (s): 4.959 samples/sec: 25.813
g0198:  iteration    27080/10000000 | consumed samples:      3466240 | consumed tokens:   7098859520 | elapsed time per iteration (ms): 4993.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.866530E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.632 | tokens per gpu per second (tgs): 1640.476 | TFLOPs: 13.20 |
g0184: [2024-08-10 17:00:26,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=27090, skipped=40, lr=[0.0001999106204369989, 0.0001999106204369989], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27090 loss: 0.8119 iter time (s): 4.440 samples/sec: 28.827
g0198:  iteration    27090/10000000 | consumed samples:      3467520 | consumed tokens:   7101480960 | elapsed time per iteration (ms): 4472.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.997870E-01 | loss scale: 16384.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.618 | tokens per gpu per second (tgs): 1831.567 | TFLOPs: 14.74 |
g0184: [2024-08-10 17:01:14,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=27100, skipped=40, lr=[0.0001999105061581721, 0.0001999105061581721], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27100 loss: 0.7889 iter time (s): 4.799 samples/sec: 26.670
g0198:  iteration    27100/10000000 | consumed samples:      3468800 | consumed tokens:   7104102400 | elapsed time per iteration (ms): 4832.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.815640E-01 | loss scale: 16384.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.486 | tokens per gpu per second (tgs): 1695.111 | TFLOPs: 13.64 |
g0184: [2024-08-10 17:02:15,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=27110, skipped=40, lr=[0.00019991039180636914, 0.00019991039180636914], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27110 loss: 0.7741 iter time (s): 6.029 samples/sec: 21.232
g0198:  iteration    27110/10000000 | consumed samples:      3470080 | consumed tokens:   7106723840 | elapsed time per iteration (ms): 6061.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.931302E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.118 | tokens per gpu per second (tgs): 1351.531 | TFLOPs: 10.88 |
g0184: [2024-08-10 17:03:11,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=27120, skipped=40, lr=[0.00019991027738159014, 0.00019991027738159014], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27120 loss: 0.7697 iter time (s): 5.619 samples/sec: 22.780
g0198:  iteration    27120/10000000 | consumed samples:      3471360 | consumed tokens:   7109345280 | elapsed time per iteration (ms): 5652.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.842156E-01 | loss scale: 16384.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.645 | tokens per gpu per second (tgs): 1449.309 | TFLOPs: 11.66 |
g0184: [2024-08-10 17:04:00,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=27130, skipped=40, lr=[0.0001999101628838351, 0.0001999101628838351], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27130 loss: 0.7745 iter time (s): 4.820 samples/sec: 26.558
g0198:  iteration    27130/10000000 | consumed samples:      3472640 | consumed tokens:   7111966720 | elapsed time per iteration (ms): 4853.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.932480E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.374 | tokens per gpu per second (tgs): 1687.941 | TFLOPs: 13.58 |
g0184: [2024-08-10 17:04:45,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=27140, skipped=40, lr=[0.00019991004831310422, 0.00019991004831310422], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27140 loss: 0.7847 iter time (s): 4.480 samples/sec: 28.569
g0198:  iteration    27140/10000000 | consumed samples:      3473920 | consumed tokens:   7114588160 | elapsed time per iteration (ms): 4513.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.968413E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.362 | tokens per gpu per second (tgs): 1815.195 | TFLOPs: 14.61 |
g0184: [2024-08-10 17:05:29,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=27150, skipped=40, lr=[0.00019990993366939753, 0.00019990993366939753], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27150 loss: 0.8291 iter time (s): 4.400 samples/sec: 29.092
g0198:  iteration    27150/10000000 | consumed samples:      3475200 | consumed tokens:   7117209600 | elapsed time per iteration (ms): 4432.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.967449E-01 | loss scale: 16384.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.879 | tokens per gpu per second (tgs): 1848.235 | TFLOPs: 14.87 |
g0184: [2024-08-10 17:06:15,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=27160, skipped=40, lr=[0.00019990981895271512, 0.00019990981895271512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27160 loss: 0.7941 iter time (s): 4.541 samples/sec: 28.189
g0198:  iteration    27160/10000000 | consumed samples:      3476480 | consumed tokens:   7119831040 | elapsed time per iteration (ms): 4586.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.842998E-01 | loss scale: 16384.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.907 | tokens per gpu per second (tgs): 1786.027 | TFLOPs: 14.37 |
g0184: [2024-08-10 17:06:59,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=27170, skipped=40, lr=[0.00019990970416305706, 0.00019990970416305706], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27170 loss: 0.7573 iter time (s): 4.362 samples/sec: 29.343
g0198:  iteration    27170/10000000 | consumed samples:      3477760 | consumed tokens:   7122452480 | elapsed time per iteration (ms): 4394.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.898975E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.126 | tokens per gpu per second (tgs): 1864.056 | TFLOPs: 15.00 |
g0184: [2024-08-10 17:08:08,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=27180, skipped=40, lr=[0.00019990958930042347, 0.00019990958930042347], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27180 loss: 0.7878 iter time (s): 6.840 samples/sec: 18.715
g0198:  iteration    27180/10000000 | consumed samples:      3479040 | consumed tokens:   7125073920 | elapsed time per iteration (ms): 6872.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.819985E-01 | loss scale: 16384.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.625 | tokens per gpu per second (tgs): 1191.970 | TFLOPs: 9.59 |
g0184: [2024-08-10 17:09:01,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=27190, skipped=40, lr=[0.00019990947436481445, 0.00019990947436481445], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27190 loss: 0.7765 iter time (s): 5.273 samples/sec: 24.276
g0198:  iteration    27190/10000000 | consumed samples:      3480320 | consumed tokens:   7127695360 | elapsed time per iteration (ms): 5306.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.841352E-01 | loss scale: 16384.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.123 | tokens per gpu per second (tgs): 1543.903 | TFLOPs: 12.42 |
g0184: [2024-08-10 17:09:52,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=27200, skipped=40, lr=[0.0001999093593562301, 0.0001999093593562301], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27200 loss: 0.7972 iter time (s): 5.095 samples/sec: 25.121
g0198:  iteration    27200/10000000 | consumed samples:      3481600 | consumed tokens:   7130316800 | elapsed time per iteration (ms): 5128.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.882976E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.957 | tokens per gpu per second (tgs): 1597.234 | TFLOPs: 12.85 |
g0184: [2024-08-10 17:10:41,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=27210, skipped=40, lr=[0.0001999092442746704, 0.0001999092442746704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27210 loss: 0.7686 iter time (s): 4.794 samples/sec: 26.701
g0198:  iteration    27210/10000000 | consumed samples:      3482880 | consumed tokens:   7132938240 | elapsed time per iteration (ms): 4826.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.844260E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.520 | tokens per gpu per second (tgs): 1697.299 | TFLOPs: 13.66 |
g0184: [2024-08-10 17:11:27,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=27220, skipped=40, lr=[0.00019990912912013555, 0.00019990912912013555], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27220 loss: 0.8085 iter time (s): 4.578 samples/sec: 27.962
g0198:  iteration    27220/10000000 | consumed samples:      3484160 | consumed tokens:   7135559680 | elapsed time per iteration (ms): 4610.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.012671E-01 | loss scale: 16384.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.761 | tokens per gpu per second (tgs): 1776.682 | TFLOPs: 14.30 |
g0184: [2024-08-10 17:12:10,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=27230, skipped=40, lr=[0.0001999090138926256, 0.0001999090138926256], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27230 loss: 0.7697 iter time (s): 4.298 samples/sec: 29.779
g0198:  iteration    27230/10000000 | consumed samples:      3485440 | consumed tokens:   7138181120 | elapsed time per iteration (ms): 4331.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.921460E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.553 | tokens per gpu per second (tgs): 1891.362 | TFLOPs: 15.22 |
g0184: [2024-08-10 17:13:16,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=27240, skipped=40, lr=[0.00019990889859214064, 0.00019990889859214064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27240 loss: 0.7543 iter time (s): 6.563 samples/sec: 19.504
g0198:  iteration    27240/10000000 | consumed samples:      3486720 | consumed tokens:   7140802560 | elapsed time per iteration (ms): 6600.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.796825E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.393 | tokens per gpu per second (tgs): 1241.122 | TFLOPs: 9.99 |
g0184: [2024-08-10 17:14:09,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=27250, skipped=40, lr=[0.00019990878321868077, 0.00019990878321868077], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27250 loss: 0.7826 iter time (s): 5.250 samples/sec: 24.379
g0198:  iteration    27250/10000000 | consumed samples:      3488000 | consumed tokens:   7143424000 | elapsed time per iteration (ms): 5283.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.749077E-01 | loss scale: 16384.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.225 | tokens per gpu per second (tgs): 1550.426 | TFLOPs: 12.48 |
g0184: [2024-08-10 17:14:58,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=27260, skipped=40, lr=[0.00019990866777224608, 0.00019990866777224608], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27260 loss: 0.7846 iter time (s): 4.922 samples/sec: 26.006
g0198:  iteration    27260/10000000 | consumed samples:      3489280 | consumed tokens:   7146045440 | elapsed time per iteration (ms): 4954.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.868932E-01 | loss scale: 16384.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.836 | tokens per gpu per second (tgs): 1653.534 | TFLOPs: 13.31 |
g0184: [2024-08-10 17:15:50,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=27270, skipped=40, lr=[0.0001999085522528366, 0.0001999085522528366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27270 loss: 0.7739 iter time (s): 5.095 samples/sec: 25.123
g0198:  iteration    27270/10000000 | consumed samples:      3490560 | consumed tokens:   7148666880 | elapsed time per iteration (ms): 5136.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.915972E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.921 | tokens per gpu per second (tgs): 1594.921 | TFLOPs: 12.83 |
g0184: [2024-08-10 17:16:33,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=27280, skipped=40, lr=[0.0001999084366604525, 0.0001999084366604525], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27280 loss: 0.7816 iter time (s): 4.256 samples/sec: 30.079
g0198:  iteration    27280/10000000 | consumed samples:      3491840 | consumed tokens:   7151288320 | elapsed time per iteration (ms): 4288.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.818234E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.851 | tokens per gpu per second (tgs): 1910.466 | TFLOPs: 15.37 |
g0184: [2024-08-10 17:17:17,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=27290, skipped=40, lr=[0.00019990832099509384, 0.00019990832099509384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27290 loss: 0.7958 iter time (s): 4.397 samples/sec: 29.113
g0198:  iteration    27290/10000000 | consumed samples:      3493120 | consumed tokens:   7153909760 | elapsed time per iteration (ms): 4431.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.881130E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.885 | tokens per gpu per second (tgs): 1848.642 | TFLOPs: 14.88 |
g0184: [2024-08-10 17:18:02,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=27300, skipped=40, lr=[0.0001999082052567607, 0.0001999082052567607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27300 loss: 0.8034 iter time (s): 4.499 samples/sec: 28.450
g0198:  iteration    27300/10000000 | consumed samples:      3494400 | consumed tokens:   7156531200 | elapsed time per iteration (ms): 4531.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.886342E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.245 | tokens per gpu per second (tgs): 1807.682 | TFLOPs: 14.55 |
g0184: [2024-08-10 17:18:55,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=27310, skipped=40, lr=[0.00019990808944545318, 0.00019990808944545318], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27310 loss: 0.7836 iter time (s): 5.204 samples/sec: 24.594
g0198:  iteration    27310/10000000 | consumed samples:      3495680 | consumed tokens:   7159152640 | elapsed time per iteration (ms): 5237.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.883126E-01 | loss scale: 16384.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.439 | tokens per gpu per second (tgs): 1564.084 | TFLOPs: 12.59 |
g0184: [2024-08-10 17:19:42,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=27320, skipped=40, lr=[0.00019990797356117136, 0.00019990797356117136], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27320 loss: 0.8006 iter time (s): 4.684 samples/sec: 27.329
g0198:  iteration    27320/10000000 | consumed samples:      3496960 | consumed tokens:   7161774080 | elapsed time per iteration (ms): 4716.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.908215E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.138 | tokens per gpu per second (tgs): 1736.827 | TFLOPs: 13.98 |
g0184: [2024-08-10 17:20:24,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=27330, skipped=40, lr=[0.00019990785760391533, 0.00019990785760391533], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27330 loss: 0.7565 iter time (s): 4.181 samples/sec: 30.616
g0198:  iteration    27330/10000000 | consumed samples:      3498240 | consumed tokens:   7164395520 | elapsed time per iteration (ms): 4213.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.790661E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.376 | tokens per gpu per second (tgs): 1944.070 | TFLOPs: 15.64 |
g0184: [2024-08-10 17:21:08,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=27340, skipped=40, lr=[0.00019990774157368517, 0.00019990774157368517], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27340 loss: 0.7746 iter time (s): 4.385 samples/sec: 29.192
g0198:  iteration    27340/10000000 | consumed samples:      3499520 | consumed tokens:   7167016960 | elapsed time per iteration (ms): 4417.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.816844E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.973 | tokens per gpu per second (tgs): 1854.295 | TFLOPs: 14.92 |
g0184: [2024-08-10 17:21:52,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=27350, skipped=40, lr=[0.00019990762547048097, 0.00019990762547048097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27350 loss: 0.7842 iter time (s): 4.402 samples/sec: 29.081
g0198:  iteration    27350/10000000 | consumed samples:      3500800 | consumed tokens:   7169638400 | elapsed time per iteration (ms): 4434.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.888590E-01 | loss scale: 16384.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.864 | tokens per gpu per second (tgs): 1847.309 | TFLOPs: 14.87 |
g0184: [2024-08-10 17:22:37,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=27360, skipped=40, lr=[0.00019990750929430287, 0.00019990750929430287], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27360 loss: 0.7844 iter time (s): 4.434 samples/sec: 28.868
g0198:  iteration    27360/10000000 | consumed samples:      3502080 | consumed tokens:   7172259840 | elapsed time per iteration (ms): 4466.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.906363E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.657 | tokens per gpu per second (tgs): 1834.071 | TFLOPs: 14.76 |
g0184: [2024-08-10 17:23:19,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=27370, skipped=40, lr=[0.00019990739304515088, 0.00019990739304515088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27370 loss: 0.7852 iter time (s): 4.112 samples/sec: 31.126
g0198:  iteration    27370/10000000 | consumed samples:      3503360 | consumed tokens:   7174881280 | elapsed time per iteration (ms): 4146.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.915377E-01 | loss scale: 16384.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.872 | tokens per gpu per second (tgs): 1975.821 | TFLOPs: 15.90 |
g0184: [2024-08-10 17:24:01,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=27380, skipped=40, lr=[0.00019990727672302515, 0.00019990727672302515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27380 loss: 0.7584 iter time (s): 4.188 samples/sec: 30.565
g0198:  iteration    27380/10000000 | consumed samples:      3504640 | consumed tokens:   7177502720 | elapsed time per iteration (ms): 4221.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.822272E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.320 | tokens per gpu per second (tgs): 1940.461 | TFLOPs: 15.62 |
g0184: [2024-08-10 17:24:47,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=27390, skipped=40, lr=[0.00019990716032792575, 0.00019990716032792575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27390 loss: 0.7933 iter time (s): 4.600 samples/sec: 27.828
g0198:  iteration    27390/10000000 | consumed samples:      3505920 | consumed tokens:   7180124160 | elapsed time per iteration (ms): 4633.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.838569E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.626 | tokens per gpu per second (tgs): 1768.095 | TFLOPs: 14.23 |
g0184: [2024-08-10 17:25:30,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=27400, skipped=40, lr=[0.00019990704385985277, 0.00019990704385985277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27400 loss: 0.8085 iter time (s): 4.285 samples/sec: 29.869
g0198:  iteration    27400/10000000 | consumed samples:      3507200 | consumed tokens:   7182745600 | elapsed time per iteration (ms): 4318.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.892384E-01 | loss scale: 16384.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.637 | tokens per gpu per second (tgs): 1896.773 | TFLOPs: 15.26 |
g0184: [2024-08-10 17:26:15,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=27410, skipped=40, lr=[0.00019990692731880626, 0.00019990692731880626], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27410 loss: 0.7990 iter time (s): 4.390 samples/sec: 29.158
g0198:  iteration    27410/10000000 | consumed samples:      3508480 | consumed tokens:   7185367040 | elapsed time per iteration (ms): 4423.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.790991E-01 | loss scale: 16384.0 | grad norm: 0.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.939 | tokens per gpu per second (tgs): 1852.089 | TFLOPs: 14.90 |
g0184: [2024-08-10 17:26:59,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=27420, skipped=40, lr=[0.0001999068107047864, 0.0001999068107047864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27420 loss: 0.8007 iter time (s): 4.436 samples/sec: 28.857
g0198:  iteration    27420/10000000 | consumed samples:      3509760 | consumed tokens:   7187988480 | elapsed time per iteration (ms): 4468.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.835660E-01 | loss scale: 16384.0 | grad norm: 0.287 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.646 | tokens per gpu per second (tgs): 1833.374 | TFLOPs: 14.75 |
g0184: [2024-08-10 17:27:44,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=27430, skipped=40, lr=[0.0001999066940177932, 0.0001999066940177932], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27430 loss: 0.7489 iter time (s): 4.441 samples/sec: 28.820
g0198:  iteration    27430/10000000 | consumed samples:      3511040 | consumed tokens:   7190609920 | elapsed time per iteration (ms): 4473.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.782067E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.611 | tokens per gpu per second (tgs): 1831.095 | TFLOPs: 14.74 |
g0184: [2024-08-10 17:28:29,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=27440, skipped=40, lr=[0.00019990657725782676, 0.00019990657725782676], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27440 loss: 0.7726 iter time (s): 4.451 samples/sec: 28.756
g0198:  iteration    27440/10000000 | consumed samples:      3512320 | consumed tokens:   7193231360 | elapsed time per iteration (ms): 4483.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.819332E-01 | loss scale: 16384.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.548 | tokens per gpu per second (tgs): 1827.043 | TFLOPs: 14.70 |
g0184: [2024-08-10 17:29:12,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=27450, skipped=40, lr=[0.0001999064604248872, 0.0001999064604248872], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27450 loss: 0.7788 iter time (s): 4.302 samples/sec: 29.751
g0198:  iteration    27450/10000000 | consumed samples:      3513600 | consumed tokens:   7195852800 | elapsed time per iteration (ms): 4335.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.872096E-01 | loss scale: 16384.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.523 | tokens per gpu per second (tgs): 1889.476 | TFLOPs: 15.20 |
g0184: [2024-08-10 17:29:54,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=27460, skipped=40, lr=[0.0001999063435189746, 0.0001999063435189746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27460 loss: 0.7879 iter time (s): 4.134 samples/sec: 30.963
g0198:  iteration    27460/10000000 | consumed samples:      3514880 | consumed tokens:   7198474240 | elapsed time per iteration (ms): 4166.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.978487E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.721 | tokens per gpu per second (tgs): 1966.142 | TFLOPs: 15.82 |
g0184: [2024-08-10 17:30:37,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=27470, skipped=40, lr=[0.00019990622654008906, 0.00019990622654008906], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27470 loss: 0.7776 iter time (s): 4.259 samples/sec: 30.057
g0198:  iteration    27470/10000000 | consumed samples:      3516160 | consumed tokens:   7201095680 | elapsed time per iteration (ms): 4291.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.863120E-01 | loss scale: 16384.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.830 | tokens per gpu per second (tgs): 1909.133 | TFLOPs: 15.36 |
g0184: [2024-08-10 17:31:20,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=27480, skipped=40, lr=[0.0001999061094882306, 0.0001999061094882306], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27480 loss: 0.7925 iter time (s): 4.328 samples/sec: 29.574
g0198:  iteration    27480/10000000 | consumed samples:      3517440 | consumed tokens:   7203717120 | elapsed time per iteration (ms): 4361.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.875009E-01 | loss scale: 16384.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.351 | tokens per gpu per second (tgs): 1878.453 | TFLOPs: 15.12 |
g0184: [2024-08-10 17:32:04,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=27490, skipped=40, lr=[0.0001999059923633994, 0.0001999059923633994], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27490 loss: 0.8171 iter time (s): 4.317 samples/sec: 29.649
g0198:  iteration    27490/10000000 | consumed samples:      3518720 | consumed tokens:   7206338560 | elapsed time per iteration (ms): 4351.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.835353E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.415 | tokens per gpu per second (tgs): 1882.541 | TFLOPs: 15.15 |
g0184: [2024-08-10 17:32:48,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=27500, skipped=40, lr=[0.00019990587516559552, 0.00019990587516559552], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27500 loss: 0.7814 iter time (s): 4.393 samples/sec: 29.139
g0198:  iteration    27500/10000000 | consumed samples:      3520000 | consumed tokens:   7208960000 | elapsed time per iteration (ms): 4424.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.760862E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.927 | tokens per gpu per second (tgs): 1851.334 | TFLOPs: 14.90 |
g0184: [2024-08-10 17:33:33,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=27510, skipped=40, lr=[0.00019990575789481907, 0.00019990575789481907], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27510 loss: 0.7633 iter time (s): 4.419 samples/sec: 28.969
g0198:  iteration    27510/10000000 | consumed samples:      3521280 | consumed tokens:   7211581440 | elapsed time per iteration (ms): 4452.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.800395E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.749 | tokens per gpu per second (tgs): 1839.909 | TFLOPs: 14.81 |
g0184: [2024-08-10 17:34:18,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=27520, skipped=40, lr=[0.0001999056405510701, 0.0001999056405510701], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27520 loss: 0.7712 iter time (s): 4.499 samples/sec: 28.453
g0198:  iteration    27520/10000000 | consumed samples:      3522560 | consumed tokens:   7214202880 | elapsed time per iteration (ms): 4541.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.879177E-01 | loss scale: 16384.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.184 | tokens per gpu per second (tgs): 1803.757 | TFLOPs: 14.52 |
g0184: [2024-08-10 17:35:02,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=27530, skipped=40, lr=[0.00019990552313434867, 0.00019990552313434867], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27530 loss: 0.7612 iter time (s): 4.348 samples/sec: 29.442
g0198:  iteration    27530/10000000 | consumed samples:      3523840 | consumed tokens:   7216824320 | elapsed time per iteration (ms): 4380.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.837210E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.222 | tokens per gpu per second (tgs): 1870.178 | TFLOPs: 15.05 |
g0184: [2024-08-10 17:35:46,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=27540, skipped=40, lr=[0.00019990540564465494, 0.00019990540564465494], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27540 loss: 0.7969 iter time (s): 4.343 samples/sec: 29.475
g0198:  iteration    27540/10000000 | consumed samples:      3525120 | consumed tokens:   7219445760 | elapsed time per iteration (ms): 4376.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.846788E-01 | loss scale: 16384.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.246 | tokens per gpu per second (tgs): 1871.737 | TFLOPs: 15.06 |
g0184: [2024-08-10 17:36:32,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=27550, skipped=40, lr=[0.000199905288081989, 0.000199905288081989], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27550 loss: 0.7648 iter time (s): 4.580 samples/sec: 27.946
g0198:  iteration    27550/10000000 | consumed samples:      3526400 | consumed tokens:   7222067200 | elapsed time per iteration (ms): 4612.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.855378E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.749 | tokens per gpu per second (tgs): 1775.962 | TFLOPs: 14.29 |
g0184: [2024-08-10 17:37:17,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=27560, skipped=40, lr=[0.00019990517044635089, 0.00019990517044635089], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27560 loss: 0.7772 iter time (s): 4.444 samples/sec: 28.804
g0198:  iteration    27560/10000000 | consumed samples:      3527680 | consumed tokens:   7224688640 | elapsed time per iteration (ms): 4476.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.863480E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.595 | tokens per gpu per second (tgs): 1830.102 | TFLOPs: 14.73 |
g0184: [2024-08-10 17:38:02,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=27570, skipped=40, lr=[0.00019990505273774075, 0.00019990505273774075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27570 loss: 0.7702 iter time (s): 4.506 samples/sec: 28.408
g0198:  iteration    27570/10000000 | consumed samples:      3528960 | consumed tokens:   7227310080 | elapsed time per iteration (ms): 4538.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.821527E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.202 | tokens per gpu per second (tgs): 1804.957 | TFLOPs: 14.52 |
g0185: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 17:38:27,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-10 17:38:27,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 17:38:27,414] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-10 17:38:44,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=27580, skipped=40, lr=[0.00019990493495615862, 0.00019990493495615862], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27580 loss: 0.7667 iter time (s): 4.159 samples/sec: 30.774
g0198:  iteration    27580/10000000 | consumed samples:      3530240 | consumed tokens:   7229931520 | elapsed time per iteration (ms): 4192.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.716103E-01 | loss scale: 32768.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.533 | tokens per gpu per second (tgs): 1954.092 | TFLOPs: 15.72 |
g0184: [2024-08-10 17:39:26,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=27590, skipped=40, lr=[0.00019990481710160463, 0.00019990481710160463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27590 loss: 0.7700 iter time (s): 4.196 samples/sec: 30.507
g0198:  iteration    27590/10000000 | consumed samples:      3531520 | consumed tokens:   7232552960 | elapsed time per iteration (ms): 4228.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.785647E-01 | loss scale: 32768.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.269 | tokens per gpu per second (tgs): 1937.205 | TFLOPs: 15.59 |
g0184: [2024-08-10 17:40:12,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=27600, skipped=40, lr=[0.00019990469917407886, 0.00019990469917407886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27600 loss: 0.7639 iter time (s): 4.537 samples/sec: 28.214
g0198:  iteration    27600/10000000 | consumed samples:      3532800 | consumed tokens:   7235174400 | elapsed time per iteration (ms): 4569.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.906577E-01 | loss scale: 32768.0 | grad norm: 0.293 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.010 | tokens per gpu per second (tgs): 1792.650 | TFLOPs: 14.43 |
g0184: [2024-08-10 17:40:57,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=27610, skipped=40, lr=[0.00019990458117358142, 0.00019990458117358142], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27610 loss: 0.7608 iter time (s): 4.458 samples/sec: 28.711
g0198:  iteration    27610/10000000 | consumed samples:      3534080 | consumed tokens:   7237795840 | elapsed time per iteration (ms): 4490.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.848373E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.502 | tokens per gpu per second (tgs): 1824.134 | TFLOPs: 14.68 |
g0184: [2024-08-10 17:41:43,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=27620, skipped=40, lr=[0.00019990446310011236, 0.00019990446310011236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27620 loss: 0.7776 iter time (s): 4.468 samples/sec: 28.645
g0198:  iteration    27620/10000000 | consumed samples:      3535360 | consumed tokens:   7240417280 | elapsed time per iteration (ms): 4600.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.935791E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.823 | tokens per gpu per second (tgs): 1780.692 | TFLOPs: 14.33 |
g0184: [2024-08-10 17:42:27,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=27630, skipped=40, lr=[0.0001999043449536718, 0.0001999043449536718], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27630 loss: 0.7988 iter time (s): 4.431 samples/sec: 28.889
g0198:  iteration    27630/10000000 | consumed samples:      3536640 | consumed tokens:   7243038720 | elapsed time per iteration (ms): 4463.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.839508E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.675 | tokens per gpu per second (tgs): 1835.206 | TFLOPs: 14.77 |
g0184: [2024-08-10 17:43:11,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=27640, skipped=40, lr=[0.0001999042267342598, 0.0001999042267342598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27640 loss: 0.7944 iter time (s): 4.307 samples/sec: 29.720
g0198:  iteration    27640/10000000 | consumed samples:      3537920 | consumed tokens:   7245660160 | elapsed time per iteration (ms): 4385.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.774720E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.190 | tokens per gpu per second (tgs): 1868.188 | TFLOPs: 15.03 |
g0184: [2024-08-10 17:43:53,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=27650, skipped=40, lr=[0.00019990410844187648, 0.00019990410844187648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27650 loss: 0.7754 iter time (s): 4.192 samples/sec: 30.534
g0198:  iteration    27650/10000000 | consumed samples:      3539200 | consumed tokens:   7248281600 | elapsed time per iteration (ms): 4224.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.925542E-01 | loss scale: 32768.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.299 | tokens per gpu per second (tgs): 1939.147 | TFLOPs: 15.60 |
g0184: [2024-08-10 17:44:37,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=27660, skipped=40, lr=[0.00019990399007652193, 0.00019990399007652193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27660 loss: 0.7599 iter time (s): 4.302 samples/sec: 29.756
g0198:  iteration    27660/10000000 | consumed samples:      3540480 | consumed tokens:   7250903040 | elapsed time per iteration (ms): 4334.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.786833E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.530 | tokens per gpu per second (tgs): 1889.914 | TFLOPs: 15.21 |
g0184: [2024-08-10 17:45:21,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=27670, skipped=40, lr=[0.00019990387163819625, 0.00019990387163819625], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27670 loss: 0.8091 iter time (s): 4.412 samples/sec: 29.012
g0198:  iteration    27670/10000000 | consumed samples:      3541760 | consumed tokens:   7253524480 | elapsed time per iteration (ms): 4445.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.840768E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.794 | tokens per gpu per second (tgs): 1842.848 | TFLOPs: 14.83 |
g0184: [2024-08-10 17:46:06,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=27680, skipped=40, lr=[0.0001999037531268995, 0.0001999037531268995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27680 loss: 0.7633 iter time (s): 4.456 samples/sec: 28.726
g0198:  iteration    27680/10000000 | consumed samples:      3543040 | consumed tokens:   7256145920 | elapsed time per iteration (ms): 4488.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.891730E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.516 | tokens per gpu per second (tgs): 1825.034 | TFLOPs: 14.69 |
g0184: [2024-08-10 17:46:50,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=27690, skipped=40, lr=[0.0001999036345426318, 0.0001999036345426318], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27690 loss: 0.8048 iter time (s): 4.372 samples/sec: 29.275
g0198:  iteration    27690/10000000 | consumed samples:      3544320 | consumed tokens:   7258767360 | elapsed time per iteration (ms): 4405.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.810533E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.054 | tokens per gpu per second (tgs): 1859.460 | TFLOPs: 14.96 |
g0184: [2024-08-10 17:47:33,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=27700, skipped=40, lr=[0.00019990351588539322, 0.00019990351588539322], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27700 loss: 0.7951 iter time (s): 4.275 samples/sec: 29.940
g0198:  iteration    27700/10000000 | consumed samples:      3545600 | consumed tokens:   7261388800 | elapsed time per iteration (ms): 4310.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.863053E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.697 | tokens per gpu per second (tgs): 1900.618 | TFLOPs: 15.29 |
g0184: [2024-08-10 17:48:17,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=27710, skipped=40, lr=[0.00019990339715518385, 0.00019990339715518385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27710 loss: 0.7953 iter time (s): 4.347 samples/sec: 29.446
g0198:  iteration    27710/10000000 | consumed samples:      3546880 | consumed tokens:   7264010240 | elapsed time per iteration (ms): 4380.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.854918E-01 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.224 | tokens per gpu per second (tgs): 1870.304 | TFLOPs: 15.05 |
g0184: [2024-08-10 17:49:00,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=27720, skipped=40, lr=[0.0001999032783520038, 0.0001999032783520038], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27720 loss: 0.7906 iter time (s): 4.282 samples/sec: 29.893
g0198:  iteration    27720/10000000 | consumed samples:      3548160 | consumed tokens:   7266631680 | elapsed time per iteration (ms): 4331.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.956603E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.550 | tokens per gpu per second (tgs): 1891.222 | TFLOPs: 15.22 |
g0184: [2024-08-10 17:49:43,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=27730, skipped=40, lr=[0.00019990315947585312, 0.00019990315947585312], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27730 loss: 0.7821 iter time (s): 4.233 samples/sec: 30.236
g0198:  iteration    27730/10000000 | consumed samples:      3549440 | consumed tokens:   7269253120 | elapsed time per iteration (ms): 4276.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.011572E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.933 | tokens per gpu per second (tgs): 1915.683 | TFLOPs: 15.42 |
g0184: [2024-08-10 17:50:30,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=27740, skipped=40, lr=[0.00019990304052673198, 0.00019990304052673198], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27740 loss: 0.7733 iter time (s): 4.665 samples/sec: 27.441
g0198:  iteration    27740/10000000 | consumed samples:      3550720 | consumed tokens:   7271874560 | elapsed time per iteration (ms): 4699.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.929038E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.239 | tokens per gpu per second (tgs): 1743.288 | TFLOPs: 14.03 |
g0184: [2024-08-10 17:51:19,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=27750, skipped=40, lr=[0.0001999029215046404, 0.0001999029215046404], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27750 loss: 0.8037 iter time (s): 4.837 samples/sec: 26.464
g0198:  iteration    27750/10000000 | consumed samples:      3552000 | consumed tokens:   7274496000 | elapsed time per iteration (ms): 4870.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.844539E-01 | loss scale: 32768.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.281 | tokens per gpu per second (tgs): 1681.980 | TFLOPs: 13.54 |
g0184: [2024-08-10 17:52:04,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=27760, skipped=40, lr=[0.00019990280240957852, 0.00019990280240957852], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27760 loss: 0.7781 iter time (s): 4.448 samples/sec: 28.779
g0198:  iteration    27760/10000000 | consumed samples:      3553280 | consumed tokens:   7277117440 | elapsed time per iteration (ms): 4481.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.775892E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.563 | tokens per gpu per second (tgs): 1828.034 | TFLOPs: 14.71 |
g0184: [2024-08-10 17:52:50,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=27770, skipped=40, lr=[0.00019990268324154639, 0.00019990268324154639], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27770 loss: 0.7644 iter time (s): 4.559 samples/sec: 28.079
g0198:  iteration    27770/10000000 | consumed samples:      3554560 | consumed tokens:   7279738880 | elapsed time per iteration (ms): 4592.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.837504E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.872 | tokens per gpu per second (tgs): 1783.807 | TFLOPs: 14.35 |
g0184: [2024-08-10 17:53:33,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=27780, skipped=40, lr=[0.00019990256400054411, 0.00019990256400054411], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27780 loss: 0.7768 iter time (s): 4.315 samples/sec: 29.664
g0198:  iteration    27780/10000000 | consumed samples:      3555840 | consumed tokens:   7282360320 | elapsed time per iteration (ms): 4348.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.925991E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.438 | tokens per gpu per second (tgs): 1884.037 | TFLOPs: 15.16 |
g0184: [2024-08-10 17:54:16,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=27790, skipped=40, lr=[0.0001999024446865718, 0.0001999024446865718], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27790 loss: 0.7949 iter time (s): 4.246 samples/sec: 30.144
g0198:  iteration    27790/10000000 | consumed samples:      3557120 | consumed tokens:   7284981760 | elapsed time per iteration (ms): 4280.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.922002E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.905 | tokens per gpu per second (tgs): 1913.930 | TFLOPs: 15.40 |
g0184: [2024-08-10 17:55:00,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=27800, skipped=40, lr=[0.00019990232529962953, 0.00019990232529962953], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27800 loss: 0.7412 iter time (s): 4.421 samples/sec: 28.952
g0198:  iteration    27800/10000000 | consumed samples:      3558400 | consumed tokens:   7287603200 | elapsed time per iteration (ms): 4454.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.841006E-01 | loss scale: 32768.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.735 | tokens per gpu per second (tgs): 1839.063 | TFLOPs: 14.80 |
g0184: [2024-08-10 17:55:44,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=27810, skipped=40, lr=[0.00019990220583971738, 0.00019990220583971738], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27810 loss: 0.8130 iter time (s): 4.337 samples/sec: 29.513
g0198:  iteration    27810/10000000 | consumed samples:      3559680 | consumed tokens:   7290224640 | elapsed time per iteration (ms): 4370.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.929008E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.285 | tokens per gpu per second (tgs): 1874.221 | TFLOPs: 15.08 |
g0184: [2024-08-10 17:56:28,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=27820, skipped=40, lr=[0.00019990208630683547, 0.00019990208630683547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27820 loss: 0.7925 iter time (s): 4.359 samples/sec: 29.365
g0198:  iteration    27820/10000000 | consumed samples:      3560960 | consumed tokens:   7292846080 | elapsed time per iteration (ms): 4391.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.886655E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.145 | tokens per gpu per second (tgs): 1865.289 | TFLOPs: 15.01 |
g0184: [2024-08-10 17:57:11,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=27830, skipped=40, lr=[0.00019990196670098385, 0.00019990196670098385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27830 loss: 0.8051 iter time (s): 4.301 samples/sec: 29.759
g0198:  iteration    27830/10000000 | consumed samples:      3562240 | consumed tokens:   7295467520 | elapsed time per iteration (ms): 4334.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.828696E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.532 | tokens per gpu per second (tgs): 1890.067 | TFLOPs: 15.21 |
g0184: [2024-08-10 17:57:56,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=27840, skipped=40, lr=[0.0001999018470221627, 0.0001999018470221627], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27840 loss: 0.7966 iter time (s): 4.452 samples/sec: 28.754
g0198:  iteration    27840/10000000 | consumed samples:      3563520 | consumed tokens:   7298088960 | elapsed time per iteration (ms): 4484.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.781174E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.544 | tokens per gpu per second (tgs): 1826.829 | TFLOPs: 14.70 |
g0184: [2024-08-10 17:58:40,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=27850, skipped=40, lr=[0.000199901727270372, 0.000199901727270372], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27850 loss: 0.8251 iter time (s): 4.329 samples/sec: 29.571
g0198:  iteration    27850/10000000 | consumed samples:      3564800 | consumed tokens:   7300710400 | elapsed time per iteration (ms): 4362.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.919387E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.344 | tokens per gpu per second (tgs): 1878.030 | TFLOPs: 15.11 |
g0184: [2024-08-10 17:59:24,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=27860, skipped=40, lr=[0.00019990160744561193, 0.00019990160744561193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27860 loss: 0.7653 iter time (s): 4.405 samples/sec: 29.058
g0198:  iteration    27860/10000000 | consumed samples:      3566080 | consumed tokens:   7303331840 | elapsed time per iteration (ms): 4437.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.795138E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.845 | tokens per gpu per second (tgs): 1846.073 | TFLOPs: 14.86 |
g0184: [2024-08-10 18:00:09,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=27870, skipped=40, lr=[0.00019990148754788251, 0.00019990148754788251], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27870 loss: 0.8061 iter time (s): 4.484 samples/sec: 28.547
g0198:  iteration    27870/10000000 | consumed samples:      3567360 | consumed tokens:   7305953280 | elapsed time per iteration (ms): 4517.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.823414E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.336 | tokens per gpu per second (tgs): 1813.493 | TFLOPs: 14.59 |
g0184: [2024-08-10 18:00:54,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=27880, skipped=40, lr=[0.0001999013675771839, 0.0001999013675771839], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27880 loss: 0.7653 iter time (s): 4.448 samples/sec: 28.776
g0198:  iteration    27880/10000000 | consumed samples:      3568640 | consumed tokens:   7308574720 | elapsed time per iteration (ms): 4480.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.823118E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.566 | tokens per gpu per second (tgs): 1828.213 | TFLOPs: 14.71 |
g0184: [2024-08-10 18:01:37,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=27890, skipped=40, lr=[0.00019990124753351614, 0.00019990124753351614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27890 loss: 0.7675 iter time (s): 4.238 samples/sec: 30.206
g0198:  iteration    27890/10000000 | consumed samples:      3569920 | consumed tokens:   7311196160 | elapsed time per iteration (ms): 4269.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.812779E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.978 | tokens per gpu per second (tgs): 1918.567 | TFLOPs: 15.44 |
g0184: [2024-08-10 18:02:22,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=27900, skipped=40, lr=[0.00019990112741687934, 0.00019990112741687934], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27900 loss: 0.7868 iter time (s): 4.476 samples/sec: 28.600
g0198:  iteration    27900/10000000 | consumed samples:      3571200 | consumed tokens:   7313817600 | elapsed time per iteration (ms): 4508.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.733630E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.393 | tokens per gpu per second (tgs): 1817.176 | TFLOPs: 14.62 |
g0184: [2024-08-10 18:03:05,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=27910, skipped=40, lr=[0.00019990100722727363, 0.00019990100722727363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27910 loss: 0.7944 iter time (s): 4.255 samples/sec: 30.082
g0198:  iteration    27910/10000000 | consumed samples:      3572480 | consumed tokens:   7316439040 | elapsed time per iteration (ms): 4287.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.853481E-01 | loss scale: 32768.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.852 | tokens per gpu per second (tgs): 1910.522 | TFLOPs: 15.37 |
g0184: [2024-08-10 18:03:49,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=27920, skipped=40, lr=[0.00019990088696469904, 0.00019990088696469904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27920 loss: 0.7699 iter time (s): 4.328 samples/sec: 29.578
g0198:  iteration    27920/10000000 | consumed samples:      3573760 | consumed tokens:   7319060480 | elapsed time per iteration (ms): 4385.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.773384E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.187 | tokens per gpu per second (tgs): 1867.984 | TFLOPs: 15.03 |
g0184: [2024-08-10 18:04:32,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=27930, skipped=40, lr=[0.0001999007666291557, 0.0001999007666291557], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27930 loss: 0.7558 iter time (s): 4.309 samples/sec: 29.708
g0198:  iteration    27930/10000000 | consumed samples:      3575040 | consumed tokens:   7321681920 | elapsed time per iteration (ms): 4348.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.794560E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.437 | tokens per gpu per second (tgs): 1883.975 | TFLOPs: 15.16 |
g0184: [2024-08-10 18:05:18,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=27940, skipped=40, lr=[0.0001999006462206437, 0.0001999006462206437], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27940 loss: 0.7878 iter time (s): 4.507 samples/sec: 28.398
g0198:  iteration    27940/10000000 | consumed samples:      3576320 | consumed tokens:   7324303360 | elapsed time per iteration (ms): 4540.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.798865E-01 | loss scale: 32768.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.194 | tokens per gpu per second (tgs): 1804.411 | TFLOPs: 14.52 |
g0184: [2024-08-10 18:06:06,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=27950, skipped=40, lr=[0.0001999005257391631, 0.0001999005257391631], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27950 loss: 0.8051 iter time (s): 4.771 samples/sec: 26.827
g0198:  iteration    27950/10000000 | consumed samples:      3577600 | consumed tokens:   7326924800 | elapsed time per iteration (ms): 4804.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.632277E-01 | loss scale: 32768.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.641 | tokens per gpu per second (tgs): 1705.045 | TFLOPs: 13.72 |
g0184: [2024-08-10 18:06:51,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=27960, skipped=40, lr=[0.00019990040518471405, 0.00019990040518471405], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27960 loss: 0.7783 iter time (s): 4.535 samples/sec: 28.225
g0198:  iteration    27960/10000000 | consumed samples:      3578880 | consumed tokens:   7329546240 | elapsed time per iteration (ms): 4575.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.829319E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.976 | tokens per gpu per second (tgs): 1790.495 | TFLOPs: 14.41 |
g0184: [2024-08-10 18:07:36,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=27970, skipped=40, lr=[0.0001999002845572966, 0.0001999002845572966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27970 loss: 0.7692 iter time (s): 4.341 samples/sec: 29.486
g0198:  iteration    27970/10000000 | consumed samples:      3580160 | consumed tokens:   7332167680 | elapsed time per iteration (ms): 4423.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.834793E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.934 | tokens per gpu per second (tgs): 1851.773 | TFLOPs: 14.90 |
g0184: [2024-08-10 18:08:20,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=27980, skipped=40, lr=[0.00019990016385691082, 0.00019990016385691082], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27980 loss: 0.7987 iter time (s): 4.441 samples/sec: 28.825
g0198:  iteration    27980/10000000 | consumed samples:      3581440 | consumed tokens:   7334789120 | elapsed time per iteration (ms): 4474.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.769487E-01 | loss scale: 32768.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.609 | tokens per gpu per second (tgs): 1830.958 | TFLOPs: 14.73 |
g0184: [2024-08-10 18:09:04,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=27990, skipped=40, lr=[0.00019990004308355688, 0.00019990004308355688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 27990 loss: 0.7873 iter time (s): 4.290 samples/sec: 29.839
g0198:  iteration    27990/10000000 | consumed samples:      3582720 | consumed tokens:   7337410560 | elapsed time per iteration (ms): 4324.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.896310E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.597 | tokens per gpu per second (tgs): 1894.212 | TFLOPs: 15.24 |
g0184: [2024-08-10 18:09:48,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=40, lr=[0.0001998999222372348, 0.0001998999222372348], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28000 loss: 0.7863 iter time (s): 4.407 samples/sec: 29.047
g0198:  iteration    28000/10000000 | consumed samples:      3584000 | consumed tokens:   7340032000 | elapsed time per iteration (ms): 4439.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.805570E-01 | loss scale: 32768.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.834 | tokens per gpu per second (tgs): 1845.361 | TFLOPs: 14.85 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 28000 | lm loss value: 7.850143E-01 | lm loss PPL: 2.192438E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   28000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 18:16:55,273] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step28000 is about to be saved!
g0188: [2024-08-10 18:16:55,279] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0188: [2024-08-10 18:16:55,279] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0184: [2024-08-10 18:16:55,279] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0188: [2024-08-10 18:16:55,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0198: [2024-08-10 18:16:55,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0198: [2024-08-10 18:16:55,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0198: [2024-08-10 18:16:55,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0184: [2024-08-10 18:16:55,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0184: [2024-08-10 18:16:55,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0194: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0194: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0194: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0185: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0185: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0197: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0197: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0197: [2024-08-10 18:16:55,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0185: [2024-08-10 18:16:55,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0187: [2024-08-10 18:16:55,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0187: [2024-08-10 18:16:55,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0187: [2024-08-10 18:16:55,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0195: [2024-08-10 18:16:55,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0195: [2024-08-10 18:16:55,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0195: [2024-08-10 18:16:55,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0198: [2024-08-10 18:16:55,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_23-model_00-model_states.pt...
g0188: [2024-08-10 18:16:55,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_11-model_00-model_states.pt...
g0187: [2024-08-10 18:16:55,316] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_08-model_00-model_states.pt...
g0197: [2024-08-10 18:16:55,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_20-model_00-model_states.pt...
g0194: [2024-08-10 18:16:55,319] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_14-model_00-model_states.pt...
g0185: [2024-08-10 18:16:55,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_05-model_00-model_states.pt...
g0195: [2024-08-10 18:16:55,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_17-model_00-model_states.pt...
g0184: [2024-08-10 18:16:55,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_01-model_00-model_states.pt...
g0188: [2024-08-10 18:16:55,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_11-model_00-model_states.pt.
g0188: [2024-08-10 18:16:55,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_12-model_00-model_states.pt...
g0185: [2024-08-10 18:16:55,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_05-model_00-model_states.pt.
g0198: [2024-08-10 18:16:55,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 18:16:55,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 18:16:55,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_24-model_00-model_states.pt.
g0195: [2024-08-10 18:16:55,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_17-model_00-model_states.pt.
g0197: [2024-08-10 18:16:55,503] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_20-model_00-model_states.pt.
g0185: [2024-08-10 18:16:55,503] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_06-model_00-model_states.pt...
g0198: [2024-08-10 18:16:55,523] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_25-model_00-model_states.pt...
g0195: [2024-08-10 18:16:55,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_18-model_00-model_states.pt...
g0187: [2024-08-10 18:16:55,531] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_08-model_00-model_states.pt.
g0197: [2024-08-10 18:16:55,542] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_21-model_00-model_states.pt...
g0187: [2024-08-10 18:16:55,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_09-model_00-model_states.pt...
g0194: [2024-08-10 18:16:55,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_14-model_00-model_states.pt.
g0185: [2024-08-10 18:16:55,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_06-model_00-model_states.pt.
g0194: [2024-08-10 18:16:55,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_15-model_00-model_states.pt...
g0185: [2024-08-10 18:16:55,626] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_07-model_00-model_states.pt...
g0187: [2024-08-10 18:16:55,699] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_09-model_00-model_states.pt.
g0184: [2024-08-10 18:16:55,699] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_01-model_00-model_states.pt.
g0188: [2024-08-10 18:16:55,706] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_12-model_00-model_states.pt.
g0197: [2024-08-10 18:16:55,719] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_21-model_00-model_states.pt.
g0184: [2024-08-10 18:16:55,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_02-model_00-model_states.pt...
g0187: [2024-08-10 18:16:55,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_10-model_00-model_states.pt...
g0188: [2024-08-10 18:16:55,740] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_13-model_00-model_states.pt...
g0197: [2024-08-10 18:16:55,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_22-model_00-model_states.pt...
g0195: [2024-08-10 18:16:55,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_18-model_00-model_states.pt.
g0198: [2024-08-10 18:16:55,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 18:16:55,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_07_model_states.pt...
g0195: [2024-08-10 18:16:55,800] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_19-model_00-model_states.pt...
g0185: [2024-08-10 18:16:55,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 18:16:55,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_01_model_states.pt...
g0184: [2024-08-10 18:16:55,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_02-model_00-model_states.pt.
g0194: [2024-08-10 18:16:55,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_15-model_00-model_states.pt.
g0187: [2024-08-10 18:16:55,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_10-model_00-model_states.pt.
g0197: [2024-08-10 18:16:55,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_22-model_00-model_states.pt.
g0187: [2024-08-10 18:16:55,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_02_model_states.pt...
g0197: [2024-08-10 18:16:55,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_06_model_states.pt...
g0188: [2024-08-10 18:16:55,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 18:16:55,880] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_03_model_states.pt...
g0184: [2024-08-10 18:16:55,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_03-model_00-model_states.pt...
g0194: [2024-08-10 18:16:55,895] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_16-model_00-model_states.pt...
g0184: [2024-08-10 18:16:56,051] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_03-model_00-model_states.pt.
g0194: [2024-08-10 18:16:56,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 18:16:56,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_04_model_states.pt...
g0184: [2024-08-10 18:16:56,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_04-model_00-model_states.pt...
g0195: [2024-08-10 18:16:56,156] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 18:16:56,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_05_model_states.pt...
g0184: [2024-08-10 18:16:56,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 18:16:56,300] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_00_model_states.pt
g0184: [2024-08-10 18:16:56,300] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_00_model_states.pt...
g0198: [2024-08-10 18:16:58,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 18:16:58,254] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0197: [2024-08-10 18:16:58,309] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 18:16:58,310] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0187: [2024-08-10 18:16:58,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 18:16:58,366] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0185: [2024-08-10 18:16:58,489] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 18:16:58,489] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0194: [2024-08-10 18:16:58,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 18:16:58,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0195: [2024-08-10 18:16:58,597] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 18:16:58,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0188: [2024-08-10 18:16:59,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 18:16:59,345] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0184: [2024-08-10 18:17:00,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step28000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 18:17:00,361] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28000 is ready now!
g0184:   successfully saved checkpoint at iteration   28000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.41, Latency(second): 5.103
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (5102.84, 5103.03)
g0184: [2024-08-10 18:17:46,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=28010, skipped=40, lr=[0.00019989980131794474, 0.00019989980131794474], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28010 loss: 0.7802 iter time (s): 4.574 samples/sec: 27.986
g0198:  iteration    28010/10000000 | consumed samples:      3585280 | consumed tokens:   7342653440 | elapsed time per iteration (ms): 47794.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.836076E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.678 | tokens per gpu per second (tgs): 171.402 | TFLOPs: 1.38 |
g0184: [2024-08-10 18:18:29,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=28020, skipped=40, lr=[0.00019989968032568673, 0.00019989968032568673], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28020 loss: 0.7619 iter time (s): 4.221 samples/sec: 30.325
g0198:  iteration    28020/10000000 | consumed samples:      3586560 | consumed tokens:   7345274880 | elapsed time per iteration (ms): 4253.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.885711E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.094 | tokens per gpu per second (tgs): 1926.040 | TFLOPs: 15.50 |
g0184: [2024-08-10 18:19:14,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=28030, skipped=40, lr=[0.00019989955926046088, 0.00019989955926046088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28030 loss: 0.7911 iter time (s): 4.470 samples/sec: 28.633
g0198:  iteration    28030/10000000 | consumed samples:      3587840 | consumed tokens:   7347896320 | elapsed time per iteration (ms): 4502.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.896696E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.426 | tokens per gpu per second (tgs): 1819.261 | TFLOPs: 14.64 |
g0184: [2024-08-10 18:19:58,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=28040, skipped=40, lr=[0.00019989943812226733, 0.00019989943812226733], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28040 loss: 0.7998 iter time (s): 4.430 samples/sec: 28.893
g0198:  iteration    28040/10000000 | consumed samples:      3589120 | consumed tokens:   7350517760 | elapsed time per iteration (ms): 4462.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.778931E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.681 | tokens per gpu per second (tgs): 1835.606 | TFLOPs: 14.77 |
g0184: [2024-08-10 18:20:46,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=28050, skipped=40, lr=[0.0001998993169111061, 0.0001998993169111061], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28050 loss: 0.7578 iter time (s): 4.623 samples/sec: 27.686
g0198:  iteration    28050/10000000 | consumed samples:      3590400 | consumed tokens:   7353139200 | elapsed time per iteration (ms): 4744.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.897478E-01 | loss scale: 32768.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.978 | tokens per gpu per second (tgs): 1726.599 | TFLOPs: 13.89 |
g0184: [2024-08-10 18:21:33,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=28060, skipped=40, lr=[0.0001998991956269773, 0.0001998991956269773], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28060 loss: 0.7809 iter time (s): 4.682 samples/sec: 27.337
g0198:  iteration    28060/10000000 | consumed samples:      3591680 | consumed tokens:   7355760640 | elapsed time per iteration (ms): 4745.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.769273E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.973 | tokens per gpu per second (tgs): 1726.255 | TFLOPs: 13.89 |
g0184: [2024-08-10 18:22:19,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=28070, skipped=40, lr=[0.0001998990742698811, 0.0001998990742698811], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28070 loss: 0.8052 iter time (s): 4.578 samples/sec: 27.958
g0198:  iteration    28070/10000000 | consumed samples:      3592960 | consumed tokens:   7358382080 | elapsed time per iteration (ms): 4613.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.868278E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.745 | tokens per gpu per second (tgs): 1775.662 | TFLOPs: 14.29 |
g0184: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 18:22:45,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-10 18:22:45,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-10 18:23:02,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=28080, skipped=40, lr=[0.0001998989528398175, 0.0001998989528398175], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28080 loss: 0.7738 iter time (s): 4.247 samples/sec: 30.139
g0198:  iteration    28080/10000000 | consumed samples:      3594240 | consumed tokens:   7361003520 | elapsed time per iteration (ms): 4279.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.842134E-01 | loss scale: 65536.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.910 | tokens per gpu per second (tgs): 1914.239 | TFLOPs: 15.40 |
g0184: [2024-08-10 18:23:47,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=28090, skipped=40, lr=[0.0001998988313367866, 0.0001998988313367866], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28090 loss: 0.8224 iter time (s): 4.434 samples/sec: 28.867
g0198:  iteration    28090/10000000 | consumed samples:      3595520 | consumed tokens:   7363624960 | elapsed time per iteration (ms): 4467.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.837562E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.654 | tokens per gpu per second (tgs): 1833.826 | TFLOPs: 14.76 |
g0184: [2024-08-10 18:24:32,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=28100, skipped=40, lr=[0.00019989870976078858, 0.00019989870976078858], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28100 loss: 0.7674 iter time (s): 4.465 samples/sec: 28.666
g0198:  iteration    28100/10000000 | consumed samples:      3596800 | consumed tokens:   7366246400 | elapsed time per iteration (ms): 4497.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.771308E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.459 | tokens per gpu per second (tgs): 1821.355 | TFLOPs: 14.66 |
g0184: [2024-08-10 18:25:17,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=28110, skipped=40, lr=[0.00019989858811182343, 0.00019989858811182343], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28110 loss: 0.7608 iter time (s): 4.519 samples/sec: 28.323
g0198:  iteration    28110/10000000 | consumed samples:      3598080 | consumed tokens:   7368867840 | elapsed time per iteration (ms): 4553.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.846803E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.113 | tokens per gpu per second (tgs): 1799.260 | TFLOPs: 14.48 |
g0184: [2024-08-10 18:26:02,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=28120, skipped=40, lr=[0.00019989846638989128, 0.00019989846638989128], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28120 loss: 0.7757 iter time (s): 4.490 samples/sec: 28.506
g0198:  iteration    28120/10000000 | consumed samples:      3599360 | consumed tokens:   7371489280 | elapsed time per iteration (ms): 4522.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.861544E-01 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.301 | tokens per gpu per second (tgs): 1811.285 | TFLOPs: 14.58 |
g0184: [2024-08-10 18:26:48,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=28130, skipped=40, lr=[0.0001998983445949923, 0.0001998983445949923], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28130 loss: 0.7707 iter time (s): 4.564 samples/sec: 28.043
g0198:  iteration    28130/10000000 | consumed samples:      3600640 | consumed tokens:   7374110720 | elapsed time per iteration (ms): 4598.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.845854E-01 | loss scale: 65536.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.837 | tokens per gpu per second (tgs): 1781.551 | TFLOPs: 14.34 |
g0184: [2024-08-10 18:27:36,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=28140, skipped=40, lr=[0.00019989822272712646, 0.00019989822272712646], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28140 loss: 0.7704 iter time (s): 4.688 samples/sec: 27.305
g0198:  iteration    28140/10000000 | consumed samples:      3601920 | consumed tokens:   7376732160 | elapsed time per iteration (ms): 4721.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.874926E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.113 | tokens per gpu per second (tgs): 1735.214 | TFLOPs: 13.96 |
g0184: [2024-08-10 18:28:21,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=28150, skipped=40, lr=[0.00019989810078629392, 0.00019989810078629392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28150 loss: 0.7809 iter time (s): 4.480 samples/sec: 28.570
g0198:  iteration    28150/10000000 | consumed samples:      3603200 | consumed tokens:   7379353600 | elapsed time per iteration (ms): 4512.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.769259E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.364 | tokens per gpu per second (tgs): 1815.324 | TFLOPs: 14.61 |
g0184: [2024-08-10 18:29:05,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=28160, skipped=40, lr=[0.00019989797877249476, 0.00019989797877249476], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28160 loss: 0.7862 iter time (s): 4.355 samples/sec: 29.390
g0198:  iteration    28160/10000000 | consumed samples:      3604480 | consumed tokens:   7381975040 | elapsed time per iteration (ms): 4388.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.909784E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.169 | tokens per gpu per second (tgs): 1866.789 | TFLOPs: 15.02 |
g0184: [2024-08-10 18:29:48,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=28170, skipped=40, lr=[0.0001998978566857291, 0.0001998978566857291], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28170 loss: 0.7652 iter time (s): 4.316 samples/sec: 29.657
g0198:  iteration    28170/10000000 | consumed samples:      3605760 | consumed tokens:   7384596480 | elapsed time per iteration (ms): 4349.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.817627E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.432 | tokens per gpu per second (tgs): 1883.663 | TFLOPs: 15.16 |
g0184: [2024-08-10 18:30:32,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=28180, skipped=40, lr=[0.00019989773452599698, 0.00019989773452599698], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28180 loss: 0.7841 iter time (s): 4.381 samples/sec: 29.217
g0198:  iteration    28180/10000000 | consumed samples:      3607040 | consumed tokens:   7387217920 | elapsed time per iteration (ms): 4413.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.795936E-01 | loss scale: 65536.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.000 | tokens per gpu per second (tgs): 1855.994 | TFLOPs: 14.94 |
g0184: [2024-08-10 18:31:20,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=28190, skipped=40, lr=[0.00019989761229329852, 0.00019989761229329852], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28190 loss: 0.7672 iter time (s): 4.776 samples/sec: 26.799
g0198:  iteration    28190/10000000 | consumed samples:      3608320 | consumed tokens:   7389839360 | elapsed time per iteration (ms): 4809.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.798929E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.615 | tokens per gpu per second (tgs): 1703.383 | TFLOPs: 13.71 |
g0184: [2024-08-10 18:32:10,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=28200, skipped=40, lr=[0.00019989748998763384, 0.00019989748998763384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28200 loss: 0.7892 iter time (s): 4.929 samples/sec: 25.967
g0198:  iteration    28200/10000000 | consumed samples:      3609600 | consumed tokens:   7392460800 | elapsed time per iteration (ms): 4962.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.871588E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.795 | tokens per gpu per second (tgs): 1650.874 | TFLOPs: 13.28 |
g0184: [2024-08-10 18:32:55,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=28210, skipped=40, lr=[0.000199897367609003, 0.000199897367609003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28210 loss: 0.8010 iter time (s): 4.424 samples/sec: 28.931
g0198:  iteration    28210/10000000 | consumed samples:      3610880 | consumed tokens:   7395082240 | elapsed time per iteration (ms): 4459.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.809661E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.700 | tokens per gpu per second (tgs): 1836.805 | TFLOPs: 14.78 |
g0184: [2024-08-10 18:33:40,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=28220, skipped=40, lr=[0.00019989724515740614, 0.00019989724515740614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28220 loss: 0.7576 iter time (s): 4.462 samples/sec: 28.690
g0198:  iteration    28220/10000000 | consumed samples:      3612160 | consumed tokens:   7397703680 | elapsed time per iteration (ms): 4494.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.863517E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.478 | tokens per gpu per second (tgs): 1822.564 | TFLOPs: 14.67 |
g0184: [2024-08-10 18:34:26,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=28230, skipped=40, lr=[0.00019989712263284327, 0.00019989712263284327], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28230 loss: 0.7921 iter time (s): 4.612 samples/sec: 27.755
g0198:  iteration    28230/10000000 | consumed samples:      3613440 | consumed tokens:   7400325120 | elapsed time per iteration (ms): 4645.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.786095E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.552 | tokens per gpu per second (tgs): 1763.312 | TFLOPs: 14.19 |
g0184: [2024-08-10 18:35:19,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=28240, skipped=40, lr=[0.00019989700003531457, 0.00019989700003531457], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28240 loss: 0.7781 iter time (s): 5.301 samples/sec: 24.145
g0198:  iteration    28240/10000000 | consumed samples:      3614720 | consumed tokens:   7402946560 | elapsed time per iteration (ms): 5333.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.810987E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.997 | tokens per gpu per second (tgs): 1535.835 | TFLOPs: 12.36 |
g0184: [2024-08-10 18:36:07,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=28250, skipped=40, lr=[0.0001998968773648201, 0.0001998968773648201], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28250 loss: 0.7654 iter time (s): 4.731 samples/sec: 27.054
g0198:  iteration    28250/10000000 | consumed samples:      3616000 | consumed tokens:   7405568000 | elapsed time per iteration (ms): 4763.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.808666E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.870 | tokens per gpu per second (tgs): 1719.681 | TFLOPs: 13.84 |
g0184: [2024-08-10 18:36:55,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=28260, skipped=40, lr=[0.00019989675462135992, 0.00019989675462135992], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28260 loss: 0.8110 iter time (s): 4.757 samples/sec: 26.906
g0198:  iteration    28260/10000000 | consumed samples:      3617280 | consumed tokens:   7408189440 | elapsed time per iteration (ms): 4790.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.915648E-01 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.722 | tokens per gpu per second (tgs): 1710.214 | TFLOPs: 13.76 |
g0184: [2024-08-10 18:37:44,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=28270, skipped=40, lr=[0.00019989663180493416, 0.00019989663180493416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28270 loss: 0.7718 iter time (s): 4.900 samples/sec: 26.124
g0198:  iteration    28270/10000000 | consumed samples:      3618560 | consumed tokens:   7410810880 | elapsed time per iteration (ms): 4932.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.795123E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.950 | tokens per gpu per second (tgs): 1660.808 | TFLOPs: 13.36 |
g0184: [2024-08-10 18:38:32,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=28280, skipped=40, lr=[0.00019989650891554292, 0.00019989650891554292], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28280 loss: 0.7705 iter time (s): 4.726 samples/sec: 27.083
g0198:  iteration    28280/10000000 | consumed samples:      3619840 | consumed tokens:   7413432320 | elapsed time per iteration (ms): 4759.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.752735E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.892 | tokens per gpu per second (tgs): 1721.095 | TFLOPs: 13.85 |
g0184: [2024-08-10 18:39:26,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=28290, skipped=40, lr=[0.00019989638595318635, 0.00019989638595318635], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28290 loss: 0.8062 iter time (s): 5.342 samples/sec: 23.963
g0198:  iteration    28290/10000000 | consumed samples:      3621120 | consumed tokens:   7416053760 | elapsed time per iteration (ms): 5400.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.852752E-01 | loss scale: 65536.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.703 | tokens per gpu per second (tgs): 1516.984 | TFLOPs: 12.21 |
g0184: [2024-08-10 18:40:15,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=28300, skipped=40, lr=[0.00019989626291786443, 0.00019989626291786443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28300 loss: 0.7876 iter time (s): 4.866 samples/sec: 26.307
g0198:  iteration    28300/10000000 | consumed samples:      3622400 | consumed tokens:   7418675200 | elapsed time per iteration (ms): 4898.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.868564E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.133 | tokens per gpu per second (tgs): 1672.502 | TFLOPs: 13.46 |
g0184: [2024-08-10 18:41:05,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=28310, skipped=40, lr=[0.00019989613980957727, 0.00019989613980957727], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28310 loss: 0.7759 iter time (s): 4.972 samples/sec: 25.745
g0198:  iteration    28310/10000000 | consumed samples:      3623680 | consumed tokens:   7421296640 | elapsed time per iteration (ms): 5004.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.798591E-01 | loss scale: 65536.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.576 | tokens per gpu per second (tgs): 1636.876 | TFLOPs: 13.17 |
g0184: [2024-08-10 18:41:53,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=28320, skipped=40, lr=[0.00019989601662832504, 0.00019989601662832504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28320 loss: 0.7743 iter time (s): 4.737 samples/sec: 27.024
g0198:  iteration    28320/10000000 | consumed samples:      3624960 | consumed tokens:   7423918080 | elapsed time per iteration (ms): 4770.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.754063E-01 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.832 | tokens per gpu per second (tgs): 1717.264 | TFLOPs: 13.82 |
g0184: [2024-08-10 18:42:38,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=28330, skipped=40, lr=[0.00019989589337410783, 0.00019989589337410783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28330 loss: 0.8003 iter time (s): 4.554 samples/sec: 28.109
g0198:  iteration    28330/10000000 | consumed samples:      3626240 | consumed tokens:   7426539520 | elapsed time per iteration (ms): 4586.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.797981E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.910 | tokens per gpu per second (tgs): 1786.225 | TFLOPs: 14.37 |
g0184: [2024-08-10 18:43:29,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=28340, skipped=40, lr=[0.00019989577004692565, 0.00019989577004692565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28340 loss: 0.7805 iter time (s): 5.051 samples/sec: 25.339
g0198:  iteration    28340/10000000 | consumed samples:      3627520 | consumed tokens:   7429160960 | elapsed time per iteration (ms): 5088.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.833262E-01 | loss scale: 65536.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.157 | tokens per gpu per second (tgs): 1610.027 | TFLOPs: 12.96 |
g0184: [2024-08-10 18:44:21,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=28350, skipped=40, lr=[0.00019989564664677864, 0.00019989564664677864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28350 loss: 0.8272 iter time (s): 5.175 samples/sec: 24.732
g0198:  iteration    28350/10000000 | consumed samples:      3628800 | consumed tokens:   7431782400 | elapsed time per iteration (ms): 5208.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.987618E-01 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.577 | tokens per gpu per second (tgs): 1572.932 | TFLOPs: 12.66 |
g0184: [2024-08-10 18:45:12,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=28360, skipped=40, lr=[0.00019989552317366694, 0.00019989552317366694], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28360 loss: 0.7835 iter time (s): 4.998 samples/sec: 25.609
g0198:  iteration    28360/10000000 | consumed samples:      3630080 | consumed tokens:   7434403840 | elapsed time per iteration (ms): 5030.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.824243E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.443 | tokens per gpu per second (tgs): 1628.363 | TFLOPs: 13.10 |
g0184: [2024-08-10 18:46:03,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=28370, skipped=40, lr=[0.00019989539962759058, 0.00019989539962759058], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28370 loss: 0.7996 iter time (s): 5.093 samples/sec: 25.133
g0198:  iteration    28370/10000000 | consumed samples:      3631360 | consumed tokens:   7437025280 | elapsed time per iteration (ms): 5125.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.818933E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.971 | tokens per gpu per second (tgs): 1598.174 | TFLOPs: 12.86 |
g0184: [2024-08-10 18:46:51,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=28380, skipped=40, lr=[0.00019989527600854966, 0.00019989527600854966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28380 loss: 0.7542 iter time (s): 4.735 samples/sec: 27.033
g0198:  iteration    28380/10000000 | consumed samples:      3632640 | consumed tokens:   7439646720 | elapsed time per iteration (ms): 4767.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.816214E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.846 | tokens per gpu per second (tgs): 1718.159 | TFLOPs: 13.83 |
g0184: [2024-08-10 18:47:44,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=28390, skipped=40, lr=[0.00019989515231654434, 0.00019989515231654434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28390 loss: 0.7958 iter time (s): 5.293 samples/sec: 24.183
g0198:  iteration    28390/10000000 | consumed samples:      3633920 | consumed tokens:   7442268160 | elapsed time per iteration (ms): 5325.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.780386E-01 | loss scale: 65536.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.036 | tokens per gpu per second (tgs): 1538.282 | TFLOPs: 12.38 |
g0184: [2024-08-10 18:48:34,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=28400, skipped=40, lr=[0.00019989502855157463, 0.00019989502855157463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28400 loss: 0.7936 iter time (s): 4.984 samples/sec: 25.683
g0198:  iteration    28400/10000000 | consumed samples:      3635200 | consumed tokens:   7444889600 | elapsed time per iteration (ms): 5016.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.816825E-01 | loss scale: 65536.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.515 | tokens per gpu per second (tgs): 1632.935 | TFLOPs: 13.14 |
g0184: [2024-08-10 18:49:34,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=28410, skipped=40, lr=[0.0001998949047136407, 0.0001998949047136407], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28410 loss: 0.7621 iter time (s): 5.982 samples/sec: 21.399
g0198:  iteration    28410/10000000 | consumed samples:      3636480 | consumed tokens:   7447511040 | elapsed time per iteration (ms): 6015.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.779326E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.280 | tokens per gpu per second (tgs): 1361.914 | TFLOPs: 10.96 |
g0184: [2024-08-10 18:50:26,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=28420, skipped=40, lr=[0.00019989478080274258, 0.00019989478080274258], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28420 loss: 0.7637 iter time (s): 5.120 samples/sec: 25.002
g0198:  iteration    28420/10000000 | consumed samples:      3637760 | consumed tokens:   7450132480 | elapsed time per iteration (ms): 5152.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.773396E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.844 | tokens per gpu per second (tgs): 1590.021 | TFLOPs: 12.80 |
g0184: [2024-08-10 18:51:17,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=28430, skipped=40, lr=[0.0001998946568188804, 0.0001998946568188804], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28430 loss: 0.7706 iter time (s): 5.057 samples/sec: 25.314
g0198:  iteration    28430/10000000 | consumed samples:      3639040 | consumed tokens:   7452753920 | elapsed time per iteration (ms): 5090.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.869581E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.144 | tokens per gpu per second (tgs): 1609.211 | TFLOPs: 12.95 |
g0184: [2024-08-10 18:52:04,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=28440, skipped=40, lr=[0.0001998945327620543, 0.0001998945327620543], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28440 loss: 0.8149 iter time (s): 4.677 samples/sec: 27.366
g0198:  iteration    28440/10000000 | consumed samples:      3640320 | consumed tokens:   7455375360 | elapsed time per iteration (ms): 4711.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.820089E-01 | loss scale: 65536.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.166 | tokens per gpu per second (tgs): 1738.595 | TFLOPs: 13.99 |
g0184: [2024-08-10 18:52:54,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=28450, skipped=40, lr=[0.00019989440863226425, 0.00019989440863226425], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28450 loss: 0.7918 iter time (s): 5.013 samples/sec: 25.533
g0198:  iteration    28450/10000000 | consumed samples:      3641600 | consumed tokens:   7457996800 | elapsed time per iteration (ms): 5047.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.815423E-01 | loss scale: 65536.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.362 | tokens per gpu per second (tgs): 1623.151 | TFLOPs: 13.06 |
g0184: [2024-08-10 18:53:41,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=28460, skipped=40, lr=[0.00019989428442951048, 0.00019989428442951048], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28460 loss: 0.7822 iter time (s): 4.612 samples/sec: 27.755
g0198:  iteration    28460/10000000 | consumed samples:      3642880 | consumed tokens:   7460618240 | elapsed time per iteration (ms): 4652.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.838257E-01 | loss scale: 65536.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.514 | tokens per gpu per second (tgs): 1760.877 | TFLOPs: 14.17 |
g0184: [2024-08-10 18:54:28,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=28470, skipped=40, lr=[0.000199894160153793, 0.000199894160153793], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28470 loss: 0.8208 iter time (s): 4.743 samples/sec: 26.986
g0198:  iteration    28470/10000000 | consumed samples:      3644160 | consumed tokens:   7463239680 | elapsed time per iteration (ms): 4775.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.955277E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.802 | tokens per gpu per second (tgs): 1715.349 | TFLOPs: 13.80 |
g0184: [2024-08-10 18:55:20,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=28480, skipped=40, lr=[0.00019989403580511193, 0.00019989403580511193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28480 loss: 0.7759 iter time (s): 5.079 samples/sec: 25.204
g0198:  iteration    28480/10000000 | consumed samples:      3645440 | consumed tokens:   7465861120 | elapsed time per iteration (ms): 5118.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.790277E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.006 | tokens per gpu per second (tgs): 1600.377 | TFLOPs: 12.88 |
g0184: [2024-08-10 18:56:07,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=28490, skipped=40, lr=[0.0001998939113834674, 0.0001998939113834674], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28490 loss: 0.7452 iter time (s): 4.663 samples/sec: 27.448
g0198:  iteration    28490/10000000 | consumed samples:      3646720 | consumed tokens:   7468482560 | elapsed time per iteration (ms): 4696.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.830015E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.254 | tokens per gpu per second (tgs): 1744.257 | TFLOPs: 14.04 |
g0184: [2024-08-10 18:56:53,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=28500, skipped=40, lr=[0.00019989378688885944, 0.00019989378688885944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28500 loss: 0.7821 iter time (s): 4.569 samples/sec: 28.015
g0198:  iteration    28500/10000000 | consumed samples:      3648000 | consumed tokens:   7471104000 | elapsed time per iteration (ms): 4601.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.911448E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.818 | tokens per gpu per second (tgs): 1780.337 | TFLOPs: 14.33 |
g0184: [2024-08-10 18:57:39,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=28510, skipped=40, lr=[0.00019989366232128818, 0.00019989366232128818], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28510 loss: 0.7866 iter time (s): 4.568 samples/sec: 28.023
g0198:  iteration    28510/10000000 | consumed samples:      3649280 | consumed tokens:   7473725440 | elapsed time per iteration (ms): 4599.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.777497E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.827 | tokens per gpu per second (tgs): 1780.923 | TFLOPs: 14.33 |
g0184: [2024-08-10 18:58:28,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=28520, skipped=40, lr=[0.0001998935376807537, 0.0001998935376807537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28520 loss: 0.7706 iter time (s): 4.904 samples/sec: 26.099
g0198:  iteration    28520/10000000 | consumed samples:      3650560 | consumed tokens:   7476346880 | elapsed time per iteration (ms): 4936.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.811265E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.928 | tokens per gpu per second (tgs): 1659.399 | TFLOPs: 13.35 |
g0184: [2024-08-10 18:59:15,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=28530, skipped=40, lr=[0.00019989341296725615, 0.00019989341296725615], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28530 loss: 0.7271 iter time (s): 4.657 samples/sec: 27.485
g0198:  iteration    28530/10000000 | consumed samples:      3651840 | consumed tokens:   7478968320 | elapsed time per iteration (ms): 4689.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.839168E-01 | loss scale: 65536.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.297 | tokens per gpu per second (tgs): 1746.993 | TFLOPs: 14.06 |
g0184: [2024-08-10 19:00:02,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=28540, skipped=40, lr=[0.00019989328818079558, 0.00019989328818079558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28540 loss: 0.7815 iter time (s): 4.655 samples/sec: 27.499
g0198:  iteration    28540/10000000 | consumed samples:      3653120 | consumed tokens:   7481589760 | elapsed time per iteration (ms): 4700.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.762176E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.231 | tokens per gpu per second (tgs): 1742.785 | TFLOPs: 14.02 |
g0184: [2024-08-10 19:00:49,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=28550, skipped=40, lr=[0.0001998931633213721, 0.0001998931633213721], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28550 loss: 0.7831 iter time (s): 4.648 samples/sec: 27.536
g0198:  iteration    28550/10000000 | consumed samples:      3654400 | consumed tokens:   7484211200 | elapsed time per iteration (ms): 4680.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.852713E-01 | loss scale: 65536.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.346 | tokens per gpu per second (tgs): 1750.113 | TFLOPs: 14.08 |
g0184: [2024-08-10 19:01:35,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=28560, skipped=40, lr=[0.0001998930383889858, 0.0001998930383889858], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28560 loss: 0.8097 iter time (s): 4.635 samples/sec: 27.619
g0198:  iteration    28560/10000000 | consumed samples:      3655680 | consumed tokens:   7486832640 | elapsed time per iteration (ms): 4667.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.837036E-01 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.425 | tokens per gpu per second (tgs): 1755.229 | TFLOPs: 14.12 |
g0184: [2024-08-10 19:02:21,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=28570, skipped=40, lr=[0.00019989291338363675, 0.00019989291338363675], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28570 loss: 0.7906 iter time (s): 4.544 samples/sec: 28.169
g0198:  iteration    28570/10000000 | consumed samples:      3656960 | consumed tokens:   7489454080 | elapsed time per iteration (ms): 4577.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.798122E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.964 | tokens per gpu per second (tgs): 1789.677 | TFLOPs: 14.40 |
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 19:02:49,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 19:02:49,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 19:02:49,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-10 19:03:10,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=28580, skipped=40, lr=[0.00019989278830532509, 0.00019989278830532509], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28580 loss: 0.7772 iter time (s): 4.856 samples/sec: 26.357
g0198:  iteration    28580/10000000 | consumed samples:      3658240 | consumed tokens:   7492075520 | elapsed time per iteration (ms): 4889.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.756818E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.179 | tokens per gpu per second (tgs): 1675.455 | TFLOPs: 13.48 |
g0184: [2024-08-10 19:04:02,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=28590, skipped=40, lr=[0.0001998926631540509, 0.0001998926631540509], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28590 loss: 0.7766 iter time (s): 5.113 samples/sec: 25.033
g0198:  iteration    28590/10000000 | consumed samples:      3659520 | consumed tokens:   7494696960 | elapsed time per iteration (ms): 5145.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.776509E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.876 | tokens per gpu per second (tgs): 1592.048 | TFLOPs: 12.81 |
g0184: [2024-08-10 19:04:49,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=28600, skipped=40, lr=[0.00019989253792981423, 0.00019989253792981423], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28600 loss: 0.7555 iter time (s): 4.712 samples/sec: 27.166
g0198:  iteration    28600/10000000 | consumed samples:      3660800 | consumed tokens:   7497318400 | elapsed time per iteration (ms): 4746.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.794180E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.968 | tokens per gpu per second (tgs): 1725.976 | TFLOPs: 13.89 |
g0184: [2024-08-10 19:05:36,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=28610, skipped=40, lr=[0.0001998924126326153, 0.0001998924126326153], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28610 loss: 0.7490 iter time (s): 4.719 samples/sec: 27.126
g0198:  iteration    28610/10000000 | consumed samples:      3662080 | consumed tokens:   7499939840 | elapsed time per iteration (ms): 4752.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.744783E-01 | loss scale: 131072.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.936 | tokens per gpu per second (tgs): 1723.905 | TFLOPs: 13.87 |
g0184: [2024-08-10 19:06:26,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=28620, skipped=40, lr=[0.00019989228726245405, 0.00019989228726245405], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28620 loss: 0.7448 iter time (s): 4.884 samples/sec: 26.210
g0198:  iteration    28620/10000000 | consumed samples:      3663360 | consumed tokens:   7502561280 | elapsed time per iteration (ms): 4916.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.773031E-01 | loss scale: 131072.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.037 | tokens per gpu per second (tgs): 1666.343 | TFLOPs: 13.41 |
g0184: [2024-08-10 19:07:16,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=28630, skipped=40, lr=[0.0001998921618193307, 0.0001998921618193307], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28630 loss: 0.7336 iter time (s): 5.018 samples/sec: 25.508
g0198:  iteration    28630/10000000 | consumed samples:      3664640 | consumed tokens:   7505182720 | elapsed time per iteration (ms): 5050.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.805655E-01 | loss scale: 131072.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.343 | tokens per gpu per second (tgs): 1621.939 | TFLOPs: 13.05 |
g0184: [2024-08-10 19:08:07,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=28640, skipped=40, lr=[0.0001998920363032453, 0.0001998920363032453], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28640 loss: 0.7913 iter time (s): 5.057 samples/sec: 25.311
g0198:  iteration    28640/10000000 | consumed samples:      3665920 | consumed tokens:   7507804160 | elapsed time per iteration (ms): 5089.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.775605E-01 | loss scale: 131072.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.149 | tokens per gpu per second (tgs): 1609.542 | TFLOPs: 12.95 |
g0184: [2024-08-10 19:08:55,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=28650, skipped=40, lr=[0.00019989191071419793, 0.00019989191071419793], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28650 loss: 0.7719 iter time (s): 4.782 samples/sec: 26.767
g0198:  iteration    28650/10000000 | consumed samples:      3667200 | consumed tokens:   7510425600 | elapsed time per iteration (ms): 4816.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.835721E-01 | loss scale: 131072.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.574 | tokens per gpu per second (tgs): 1700.729 | TFLOPs: 13.69 |
g0184: [2024-08-10 19:09:41,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=28660, skipped=40, lr=[0.0001998917850521887, 0.0001998917850521887], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28660 loss: 0.7707 iter time (s): 4.533 samples/sec: 28.237
g0198:  iteration    28660/10000000 | consumed samples:      3668480 | consumed tokens:   7513047040 | elapsed time per iteration (ms): 4567.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.910090E-01 | loss scale: 131072.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.023 | tokens per gpu per second (tgs): 1793.472 | TFLOPs: 14.43 |
g0184: [2024-08-10 19:10:30,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=28670, skipped=40, lr=[0.0001998916593172177, 0.0001998916593172177], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28670 loss: 0.7869 iter time (s): 4.849 samples/sec: 26.399
g0198:  iteration    28670/10000000 | consumed samples:      3669760 | consumed tokens:   7515668480 | elapsed time per iteration (ms): 4881.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.859845E-01 | loss scale: 131072.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.224 | tokens per gpu per second (tgs): 1678.324 | TFLOPs: 13.51 |
g0184: [2024-08-10 19:11:15,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=28680, skipped=40, lr=[0.00019989153350928506, 0.00019989153350928506], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28680 loss: 0.7735 iter time (s): 4.449 samples/sec: 28.771
g0198:  iteration    28680/10000000 | consumed samples:      3671040 | consumed tokens:   7518289920 | elapsed time per iteration (ms): 4481.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.791758E-01 | loss scale: 131072.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.564 | tokens per gpu per second (tgs): 1828.086 | TFLOPs: 14.71 |
g0184: [2024-08-10 19:12:02,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=28690, skipped=40, lr=[0.00019989140762839084, 0.00019989140762839084], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28690 loss: 0.7757 iter time (s): 4.760 samples/sec: 26.892
g0198:  iteration    28690/10000000 | consumed samples:      3672320 | consumed tokens:   7520911360 | elapsed time per iteration (ms): 4792.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.798600E-01 | loss scale: 131072.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.710 | tokens per gpu per second (tgs): 1709.448 | TFLOPs: 13.76 |
g0184: [2024-08-10 19:12:51,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=28700, skipped=40, lr=[0.00019989128167453515, 0.00019989128167453515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28700 loss: 0.7861 iter time (s): 4.779 samples/sec: 26.786
g0198:  iteration    28700/10000000 | consumed samples:      3673600 | consumed tokens:   7523532800 | elapsed time per iteration (ms): 4811.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.810064E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.605 | tokens per gpu per second (tgs): 1702.730 | TFLOPs: 13.70 |
g0184: [2024-08-10 19:13:41,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=28710, skipped=40, lr=[0.0001998911556477181, 0.0001998911556477181], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28710 loss: 0.7992 iter time (s): 5.010 samples/sec: 25.550
g0198:  iteration    28710/10000000 | consumed samples:      3674880 | consumed tokens:   7526154240 | elapsed time per iteration (ms): 5042.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.834129E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.383 | tokens per gpu per second (tgs): 1624.516 | TFLOPs: 13.07 |
g0184: [2024-08-10 19:14:34,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=28720, skipped=40, lr=[0.00019989102954793975, 0.00019989102954793975], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28720 loss: 0.7613 iter time (s): 5.293 samples/sec: 24.184
g0198:  iteration    28720/10000000 | consumed samples:      3676160 | consumed tokens:   7528775680 | elapsed time per iteration (ms): 5325.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.773997E-01 | loss scale: 131072.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.037 | tokens per gpu per second (tgs): 1538.399 | TFLOPs: 12.38 |
g0184: [2024-08-10 19:15:22,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=28730, skipped=40, lr=[0.00019989090337520023, 0.00019989090337520023], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28730 loss: 0.7609 iter time (s): 4.734 samples/sec: 27.037
g0198:  iteration    28730/10000000 | consumed samples:      3677440 | consumed tokens:   7531397120 | elapsed time per iteration (ms): 4766.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.838743E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.854 | tokens per gpu per second (tgs): 1718.656 | TFLOPs: 13.83 |
g0184: [2024-08-10 19:16:13,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=28740, skipped=40, lr=[0.00019989077712949963, 0.00019989077712949963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28740 loss: 0.7899 iter time (s): 5.060 samples/sec: 25.297
g0198:  iteration    28740/10000000 | consumed samples:      3678720 | consumed tokens:   7534018560 | elapsed time per iteration (ms): 5105.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.814846E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.069 | tokens per gpu per second (tgs): 1604.430 | TFLOPs: 12.91 |
g0184: [2024-08-10 19:17:09,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=28750, skipped=40, lr=[0.00019989065081083804, 0.00019989065081083804], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28750 loss: 0.7896 iter time (s): 5.539 samples/sec: 23.109
g0198:  iteration    28750/10000000 | consumed samples:      3680000 | consumed tokens:   7536640000 | elapsed time per iteration (ms): 5571.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.717210E-01 | loss scale: 131072.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.973 | tokens per gpu per second (tgs): 1470.266 | TFLOPs: 11.83 |
g0184: [2024-08-10 19:18:00,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=28760, skipped=40, lr=[0.00019989052441921556, 0.00019989052441921556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28760 loss: 0.7658 iter time (s): 5.089 samples/sec: 25.150
g0198:  iteration    28760/10000000 | consumed samples:      3681280 | consumed tokens:   7539261440 | elapsed time per iteration (ms): 5122.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.813574E-01 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.987 | tokens per gpu per second (tgs): 1599.187 | TFLOPs: 12.87 |
g0184: [2024-08-10 19:18:55,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=28770, skipped=40, lr=[0.00019989039795463229, 0.00019989039795463229], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28770 loss: 0.8028 iter time (s): 5.499 samples/sec: 23.276
g0198:  iteration    28770/10000000 | consumed samples:      3682560 | consumed tokens:   7541882880 | elapsed time per iteration (ms): 5531.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.961269E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.140 | tokens per gpu per second (tgs): 1480.946 | TFLOPs: 11.92 |
g0184: [2024-08-10 19:19:47,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=28780, skipped=40, lr=[0.0001998902714170883, 0.0001998902714170883], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28780 loss: 0.7807 iter time (s): 5.158 samples/sec: 24.816
g0198:  iteration    28780/10000000 | consumed samples:      3683840 | consumed tokens:   7544504320 | elapsed time per iteration (ms): 5190.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.672431E-01 | loss scale: 131072.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.660 | tokens per gpu per second (tgs): 1578.229 | TFLOPs: 12.70 |
g0184: [2024-08-10 19:20:37,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=28790, skipped=40, lr=[0.00019989014480658378, 0.00019989014480658378], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28790 loss: 0.7897 iter time (s): 4.908 samples/sec: 26.079
g0198:  iteration    28790/10000000 | consumed samples:      3685120 | consumed tokens:   7547125760 | elapsed time per iteration (ms): 4940.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.851527E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.906 | tokens per gpu per second (tgs): 1658.001 | TFLOPs: 13.34 |
g0184: [2024-08-10 19:21:29,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=28800, skipped=40, lr=[0.0001998900181231187, 0.0001998900181231187], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28800 loss: 0.8248 iter time (s): 5.180 samples/sec: 24.712
g0198:  iteration    28800/10000000 | consumed samples:      3686400 | consumed tokens:   7549747200 | elapsed time per iteration (ms): 5212.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.817036E-01 | loss scale: 131072.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.558 | tokens per gpu per second (tgs): 1571.696 | TFLOPs: 12.65 |
g0184: [2024-08-10 19:22:21,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=28810, skipped=40, lr=[0.00019988989136669324, 0.00019988989136669324], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28810 loss: 0.8345 iter time (s): 5.162 samples/sec: 24.799
g0198:  iteration    28810/10000000 | consumed samples:      3687680 | consumed tokens:   7552368640 | elapsed time per iteration (ms): 5194.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.900555E-01 | loss scale: 131072.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.642 | tokens per gpu per second (tgs): 1577.077 | TFLOPs: 12.69 |
g0184: [2024-08-10 19:23:09,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=28820, skipped=40, lr=[0.00019988976453730746, 0.00019988976453730746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28820 loss: 0.7724 iter time (s): 4.759 samples/sec: 26.899
g0198:  iteration    28820/10000000 | consumed samples:      3688960 | consumed tokens:   7554990080 | elapsed time per iteration (ms): 4791.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.799274E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.716 | tokens per gpu per second (tgs): 1709.803 | TFLOPs: 13.76 |
g0184: [2024-08-10 19:23:58,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=28830, skipped=40, lr=[0.00019988963763496148, 0.00019988963763496148], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28830 loss: 0.7782 iter time (s): 4.938 samples/sec: 25.923
g0198:  iteration    28830/10000000 | consumed samples:      3690240 | consumed tokens:   7557611520 | elapsed time per iteration (ms): 4970.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.808240E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.753 | tokens per gpu per second (tgs): 1648.170 | TFLOPs: 13.26 |
g0184: [2024-08-10 19:24:48,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=28840, skipped=40, lr=[0.0001998895106596554, 0.0001998895106596554], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28840 loss: 0.7933 iter time (s): 4.972 samples/sec: 25.743
g0198:  iteration    28840/10000000 | consumed samples:      3691520 | consumed tokens:   7560232960 | elapsed time per iteration (ms): 5005.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.840828E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.572 | tokens per gpu per second (tgs): 1636.592 | TFLOPs: 13.17 |
g0184: [2024-08-10 19:25:38,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=28850, skipped=40, lr=[0.00019988938361138931, 0.00019988938361138931], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28850 loss: 0.8019 iter time (s): 4.919 samples/sec: 26.019
g0198:  iteration    28850/10000000 | consumed samples:      3692800 | consumed tokens:   7562854400 | elapsed time per iteration (ms): 4951.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.830016E-01 | loss scale: 131072.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.850 | tokens per gpu per second (tgs): 1654.426 | TFLOPs: 13.31 |
g0184: [2024-08-10 19:26:27,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=28860, skipped=40, lr=[0.00019988925649016328, 0.00019988925649016328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28860 loss: 0.7756 iter time (s): 4.873 samples/sec: 26.269
g0198:  iteration    28860/10000000 | consumed samples:      3694080 | consumed tokens:   7565475840 | elapsed time per iteration (ms): 4905.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.730285E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.095 | tokens per gpu per second (tgs): 1670.096 | TFLOPs: 13.44 |
g0184: [2024-08-10 19:27:14,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=28870, skipped=40, lr=[0.00019988912929597749, 0.00019988912929597749], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28870 loss: 0.7986 iter time (s): 4.694 samples/sec: 27.269
g0198:  iteration    28870/10000000 | consumed samples:      3695360 | consumed tokens:   7568097280 | elapsed time per iteration (ms): 4728.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.828018E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.071 | tokens per gpu per second (tgs): 1732.529 | TFLOPs: 13.94 |
g0184: [2024-08-10 19:28:04,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=28880, skipped=40, lr=[0.00019988900202883193, 0.00019988900202883193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28880 loss: 0.8099 iter time (s): 4.927 samples/sec: 25.978
g0198:  iteration    28880/10000000 | consumed samples:      3696640 | consumed tokens:   7570718720 | elapsed time per iteration (ms): 4959.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.819314E-01 | loss scale: 131072.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.809 | tokens per gpu per second (tgs): 1651.754 | TFLOPs: 13.29 |
g0184: [2024-08-10 19:28:51,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=28890, skipped=40, lr=[0.00019988887468872676, 0.00019988887468872676], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28890 loss: 0.7515 iter time (s): 4.664 samples/sec: 27.443
g0198:  iteration    28890/10000000 | consumed samples:      3697920 | consumed tokens:   7573340160 | elapsed time per iteration (ms): 4697.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.772782E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.251 | tokens per gpu per second (tgs): 1744.072 | TFLOPs: 14.03 |
g0184: [2024-08-10 19:29:37,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=28900, skipped=40, lr=[0.00019988874727566207, 0.00019988874727566207], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28900 loss: 0.7431 iter time (s): 4.646 samples/sec: 27.551
g0198:  iteration    28900/10000000 | consumed samples:      3699200 | consumed tokens:   7575961600 | elapsed time per iteration (ms): 4678.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.730662E-01 | loss scale: 131072.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.361 | tokens per gpu per second (tgs): 1751.104 | TFLOPs: 14.09 |
g0184: [2024-08-10 19:30:26,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=28910, skipped=40, lr=[0.00019988861978963797, 0.00019988861978963797], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28910 loss: 0.7691 iter time (s): 4.832 samples/sec: 26.490
g0198:  iteration    28910/10000000 | consumed samples:      3700480 | consumed tokens:   7578583040 | elapsed time per iteration (ms): 4864.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.888488E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.314 | tokens per gpu per second (tgs): 1684.115 | TFLOPs: 13.55 |
g0184: [2024-08-10 19:31:15,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=28920, skipped=40, lr=[0.00019988849223065452, 0.00019988849223065452], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28920 loss: 0.7677 iter time (s): 4.809 samples/sec: 26.619
g0198:  iteration    28920/10000000 | consumed samples:      3701760 | consumed tokens:   7581204480 | elapsed time per iteration (ms): 4840.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.842366E-01 | loss scale: 131072.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.442 | tokens per gpu per second (tgs): 1692.269 | TFLOPs: 13.62 |
g0184: [2024-08-10 19:32:01,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=28930, skipped=40, lr=[0.00019988836459871185, 0.00019988836459871185], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28930 loss: 0.7991 iter time (s): 4.603 samples/sec: 27.808
g0198:  iteration    28930/10000000 | consumed samples:      3703040 | consumed tokens:   7583825920 | elapsed time per iteration (ms): 4635.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.789714E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.611 | tokens per gpu per second (tgs): 1767.085 | TFLOPs: 14.22 |
g0184: [2024-08-10 19:32:49,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=28940, skipped=40, lr=[0.00019988823689381002, 0.00019988823689381002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28940 loss: 0.7781 iter time (s): 4.745 samples/sec: 26.973
g0198:  iteration    28940/10000000 | consumed samples:      3704320 | consumed tokens:   7586447360 | elapsed time per iteration (ms): 4777.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.782220E-01 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.791 | tokens per gpu per second (tgs): 1714.612 | TFLOPs: 13.80 |
g0184: [2024-08-10 19:33:33,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=28950, skipped=40, lr=[0.0001998881091159492, 0.0001998881091159492], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28950 loss: 0.7643 iter time (s): 4.437 samples/sec: 28.849
g0198:  iteration    28950/10000000 | consumed samples:      3705600 | consumed tokens:   7589068800 | elapsed time per iteration (ms): 4469.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.721582E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.641 | tokens per gpu per second (tgs): 1833.018 | TFLOPs: 14.75 |
g0184: [2024-08-10 19:34:23,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=28960, skipped=40, lr=[0.00019988798126512942, 0.00019988798126512942], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28960 loss: 0.7760 iter time (s): 4.968 samples/sec: 25.763
g0198:  iteration    28960/10000000 | consumed samples:      3706880 | consumed tokens:   7591690240 | elapsed time per iteration (ms): 5001.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.710764E-01 | loss scale: 131072.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.595 | tokens per gpu per second (tgs): 1638.064 | TFLOPs: 13.18 |
g0184: [2024-08-10 19:35:11,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=28970, skipped=40, lr=[0.0001998878533413508, 0.0001998878533413508], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28970 loss: 0.7796 iter time (s): 4.723 samples/sec: 27.100
g0198:  iteration    28970/10000000 | consumed samples:      3708160 | consumed tokens:   7594311680 | elapsed time per iteration (ms): 4756.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.809595E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.913 | tokens per gpu per second (tgs): 1722.444 | TFLOPs: 13.86 |
g0184: [2024-08-10 19:35:56,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=28980, skipped=40, lr=[0.0001998877253446135, 0.0001998877253446135], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28980 loss: 0.7796 iter time (s): 4.494 samples/sec: 28.483
g0198:  iteration    28980/10000000 | consumed samples:      3709440 | consumed tokens:   7596933120 | elapsed time per iteration (ms): 4525.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.849077E-01 | loss scale: 131072.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.281 | tokens per gpu per second (tgs): 1810.009 | TFLOPs: 14.57 |
g0184: [2024-08-10 19:36:43,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=28990, skipped=40, lr=[0.0001998875972749175, 0.0001998875972749175], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 28990 loss: 0.8029 iter time (s): 4.674 samples/sec: 27.386
g0198:  iteration    28990/10000000 | consumed samples:      3710720 | consumed tokens:   7599554560 | elapsed time per iteration (ms): 4706.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.721411E-01 | loss scale: 131072.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.199 | tokens per gpu per second (tgs): 1740.706 | TFLOPs: 14.01 |
g0184: [2024-08-10 19:37:44,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=40, lr=[0.00019988746913226298, 0.00019988746913226298], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29000 loss: 0.7949 iter time (s): 6.021 samples/sec: 21.260
g0198:  iteration    29000/10000000 | consumed samples:      3712000 | consumed tokens:   7602176000 | elapsed time per iteration (ms): 6056.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.870817E-01 | loss scale: 131072.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.136 | tokens per gpu per second (tgs): 1352.692 | TFLOPs: 10.89 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 29000 | lm loss value: 7.824879E-01 | lm loss PPL: 2.186906E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   29000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 19:45:04,839] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step29000 is about to be saved!
g0198: [2024-08-10 19:45:04,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0198: [2024-08-10 19:45:04,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0198: [2024-08-10 19:45:04,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0184: [2024-08-10 19:45:04,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0184: [2024-08-10 19:45:04,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0184: [2024-08-10 19:45:04,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0185: [2024-08-10 19:45:04,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0185: [2024-08-10 19:45:04,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0185: [2024-08-10 19:45:04,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0188: [2024-08-10 19:45:04,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0188: [2024-08-10 19:45:04,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0188: [2024-08-10 19:45:04,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0197: [2024-08-10 19:45:04,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0197: [2024-08-10 19:45:04,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0197: [2024-08-10 19:45:04,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0194: [2024-08-10 19:45:04,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0194: [2024-08-10 19:45:04,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0194: [2024-08-10 19:45:04,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0187: [2024-08-10 19:45:04,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0195: [2024-08-10 19:45:04,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0195: [2024-08-10 19:45:04,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0195: [2024-08-10 19:45:04,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0187: [2024-08-10 19:45:04,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0187: [2024-08-10 19:45:04,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0198: [2024-08-10 19:45:04,866] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_23-model_00-model_states.pt...
g0187: [2024-08-10 19:45:04,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_08-model_00-model_states.pt...
g0197: [2024-08-10 19:45:04,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_20-model_00-model_states.pt...
g0185: [2024-08-10 19:45:04,886] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_05-model_00-model_states.pt...
g0194: [2024-08-10 19:45:04,886] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_14-model_00-model_states.pt...
g0195: [2024-08-10 19:45:04,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_17-model_00-model_states.pt...
g0188: [2024-08-10 19:45:04,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_11-model_00-model_states.pt...
g0184: [2024-08-10 19:45:04,894] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_01-model_00-model_states.pt...
g0198: [2024-08-10 19:45:04,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 19:45:04,968] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 19:45:04,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 19:45:05,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_25-model_00-model_states.pt...
g0197: [2024-08-10 19:45:05,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_20-model_00-model_states.pt.
g0185: [2024-08-10 19:45:05,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_05-model_00-model_states.pt.
g0197: [2024-08-10 19:45:05,077] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_21-model_00-model_states.pt...
g0195: [2024-08-10 19:45:05,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_17-model_00-model_states.pt.
g0194: [2024-08-10 19:45:05,083] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_14-model_00-model_states.pt.
g0185: [2024-08-10 19:45:05,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_06-model_00-model_states.pt...
g0184: [2024-08-10 19:45:05,113] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_01-model_00-model_states.pt.
g0188: [2024-08-10 19:45:05,118] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_11-model_00-model_states.pt.
g0195: [2024-08-10 19:45:05,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_18-model_00-model_states.pt...
g0194: [2024-08-10 19:45:05,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_15-model_00-model_states.pt...
g0184: [2024-08-10 19:45:05,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_02-model_00-model_states.pt...
g0188: [2024-08-10 19:45:05,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_12-model_00-model_states.pt...
g0198: [2024-08-10 19:45:05,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 19:45:05,198] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_07_model_states.pt...
g0195: [2024-08-10 19:45:05,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_18-model_00-model_states.pt.
g0187: [2024-08-10 19:45:05,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_08-model_00-model_states.pt.
g0197: [2024-08-10 19:45:05,224] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_21-model_00-model_states.pt.
g0185: [2024-08-10 19:45:05,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_06-model_00-model_states.pt.
g0184: [2024-08-10 19:45:05,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_02-model_00-model_states.pt.
g0197: [2024-08-10 19:45:05,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_22-model_00-model_states.pt...
g0195: [2024-08-10 19:45:05,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_19-model_00-model_states.pt...
g0187: [2024-08-10 19:45:05,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_09-model_00-model_states.pt...
g0184: [2024-08-10 19:45:05,268] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_03-model_00-model_states.pt...
g0188: [2024-08-10 19:45:05,272] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_12-model_00-model_states.pt.
g0185: [2024-08-10 19:45:05,272] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_07-model_00-model_states.pt...
g0188: [2024-08-10 19:45:05,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_13-model_00-model_states.pt...
g0194: [2024-08-10 19:45:05,340] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 19:45:05,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_16-model_00-model_states.pt...
g0185: [2024-08-10 19:45:05,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 19:45:05,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_01_model_states.pt...
g0197: [2024-08-10 19:45:05,376] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 19:45:05,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_06_model_states.pt...
g0195: [2024-08-10 19:45:05,394] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 19:45:05,396] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_05_model_states.pt...
g0187: [2024-08-10 19:45:05,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 19:45:05,451] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_10-model_00-model_states.pt...
g0184: [2024-08-10 19:45:05,455] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 19:45:05,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_04-model_00-model_states.pt...
g0188: [2024-08-10 19:45:05,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 19:45:05,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_03_model_states.pt...
g0194: [2024-08-10 19:45:05,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 19:45:05,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_04_model_states.pt...
g0184: [2024-08-10 19:45:05,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 19:45:05,609] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_00_model_states.pt
g0184: [2024-08-10 19:45:05,609] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_00_model_states.pt...
g0187: [2024-08-10 19:45:05,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 19:45:05,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_02_model_states.pt...
g0198: [2024-08-10 19:45:07,637] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 19:45:07,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0195: [2024-08-10 19:45:07,757] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 19:45:07,757] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0185: [2024-08-10 19:45:07,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 19:45:07,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0197: [2024-08-10 19:45:07,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 19:45:07,879] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0188: [2024-08-10 19:45:07,937] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 19:45:07,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0187: [2024-08-10 19:45:08,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 19:45:08,054] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0194: [2024-08-10 19:45:08,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 19:45:08,088] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0184: [2024-08-10 19:45:09,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step29000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 19:45:09,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29000 is ready now!
g0184:   successfully saved checkpoint at iteration   29000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.34, Latency(second): 5.184
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (5183.85, 5184.09)
g0184: [2024-08-10 19:45:56,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=29010, skipped=40, lr=[0.00019988734091665, 0.00019988734091665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29010 loss: 0.7750 iter time (s): 4.645 samples/sec: 27.556
g0198:  iteration    29010/10000000 | consumed samples:      3713280 | consumed tokens:   7604797440 | elapsed time per iteration (ms): 49244.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.807235E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.599 | tokens per gpu per second (tgs): 166.352 | TFLOPs: 1.34 |
g0184: [2024-08-10 19:46:41,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=29020, skipped=40, lr=[0.00019988721262807868, 0.00019988721262807868], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29020 loss: 0.8033 iter time (s): 4.481 samples/sec: 28.562
g0198:  iteration    29020/10000000 | consumed samples:      3714560 | consumed tokens:   7607418880 | elapsed time per iteration (ms): 4513.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.760653E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.358 | tokens per gpu per second (tgs): 1814.935 | TFLOPs: 14.61 |
g0184: [2024-08-10 19:47:30,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=29030, skipped=40, lr=[0.00019988708426654913, 0.00019988708426654913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29030 loss: 0.7768 iter time (s): 4.787 samples/sec: 26.741
g0198:  iteration    29030/10000000 | consumed samples:      3715840 | consumed tokens:   7610040320 | elapsed time per iteration (ms): 4819.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.687628E-01 | loss scale: 131072.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.560 | tokens per gpu per second (tgs): 1699.870 | TFLOPs: 13.68 |
g0184: [2024-08-10 19:48:14,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=29040, skipped=40, lr=[0.00019988695583206143, 0.00019988695583206143], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29040 loss: 0.7669 iter time (s): 4.443 samples/sec: 28.811
g0198:  iteration    29040/10000000 | consumed samples:      3717120 | consumed tokens:   7612661760 | elapsed time per iteration (ms): 4475.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.742715E-01 | loss scale: 131072.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.604 | tokens per gpu per second (tgs): 1830.631 | TFLOPs: 14.73 |
g0184: [2024-08-10 19:48:58,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=29050, skipped=40, lr=[0.00019988682732461567, 0.00019988682732461567], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29050 loss: 0.7675 iter time (s): 4.339 samples/sec: 29.502
g0198:  iteration    29050/10000000 | consumed samples:      3718400 | consumed tokens:   7615283200 | elapsed time per iteration (ms): 4392.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.730002E-01 | loss scale: 131072.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.141 | tokens per gpu per second (tgs): 1865.016 | TFLOPs: 15.01 |
g0184: [2024-08-10 19:49:43,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=29060, skipped=40, lr=[0.00019988669874421196, 0.00019988669874421196], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29060 loss: 0.7814 iter time (s): 4.446 samples/sec: 28.792
g0198:  iteration    29060/10000000 | consumed samples:      3719680 | consumed tokens:   7617904640 | elapsed time per iteration (ms): 4478.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.786768E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.584 | tokens per gpu per second (tgs): 1829.349 | TFLOPs: 14.72 |
g0184: [2024-08-10 19:50:27,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=29070, skipped=40, lr=[0.0001998865700908504, 0.0001998865700908504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29070 loss: 0.7919 iter time (s): 4.383 samples/sec: 29.203
g0198:  iteration    29070/10000000 | consumed samples:      3720960 | consumed tokens:   7620526080 | elapsed time per iteration (ms): 4415.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.770276E-01 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.986 | tokens per gpu per second (tgs): 1855.111 | TFLOPs: 14.93 |
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0187: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0194: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0185: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0195: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0198: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0188: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0197: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 19:50:53,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0184: [2024-08-10 19:51:12,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=29080, skipped=40, lr=[0.0001998864413645311, 0.0001998864413645311], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29080 loss: 0.7888 iter time (s): 4.419 samples/sec: 28.965
g0198:  iteration    29080/10000000 | consumed samples:      3722240 | consumed tokens:   7623147520 | elapsed time per iteration (ms): 4451.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.751812E-01 | loss scale: 262144.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.752 | tokens per gpu per second (tgs): 1840.116 | TFLOPs: 14.81 |
g0184: [2024-08-10 19:51:54,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=29090, skipped=40, lr=[0.00019988631256525417, 0.00019988631256525417], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29090 loss: 0.7892 iter time (s): 4.215 samples/sec: 30.369
g0198:  iteration    29090/10000000 | consumed samples:      3723520 | consumed tokens:   7625768960 | elapsed time per iteration (ms): 4248.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.767496E-01 | loss scale: 262144.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.128 | tokens per gpu per second (tgs): 1928.212 | TFLOPs: 15.52 |
g0184: [2024-08-10 19:52:41,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=29100, skipped=40, lr=[0.00019988618369301965, 0.00019988618369301965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29100 loss: 0.7757 iter time (s): 4.695 samples/sec: 27.263
g0198:  iteration    29100/10000000 | consumed samples:      3724800 | consumed tokens:   7628390400 | elapsed time per iteration (ms): 4727.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.827822E-01 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.073 | tokens per gpu per second (tgs): 1732.699 | TFLOPs: 13.94 |
g0184: [2024-08-10 19:53:26,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=29110, skipped=40, lr=[0.0001998860547478277, 0.0001998860547478277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29110 loss: 0.7863 iter time (s): 4.408 samples/sec: 29.038
g0198:  iteration    29110/10000000 | consumed samples:      3726080 | consumed tokens:   7631011840 | elapsed time per iteration (ms): 4441.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.763809E-01 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.823 | tokens per gpu per second (tgs): 1844.641 | TFLOPs: 14.84 |
g0184: [2024-08-10 19:54:13,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=29120, skipped=40, lr=[0.0001998859257296784, 0.0001998859257296784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29120 loss: 0.7803 iter time (s): 4.645 samples/sec: 27.554
g0198:  iteration    29120/10000000 | consumed samples:      3727360 | consumed tokens:   7633633280 | elapsed time per iteration (ms): 4689.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.791793E-01 | loss scale: 262144.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.297 | tokens per gpu per second (tgs): 1747.016 | TFLOPs: 14.06 |
g0184: [2024-08-10 19:54:59,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=29130, skipped=40, lr=[0.0001998857966385718, 0.0001998857966385718], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29130 loss: 0.7920 iter time (s): 4.549 samples/sec: 28.140
g0198:  iteration    29130/10000000 | consumed samples:      3728640 | consumed tokens:   7636254720 | elapsed time per iteration (ms): 4582.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.717183E-01 | loss scale: 262144.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.935 | tokens per gpu per second (tgs): 1787.821 | TFLOPs: 14.39 |
g0184: [2024-08-10 19:55:45,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=29140, skipped=40, lr=[0.0001998856674745081, 0.0001998856674745081], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29140 loss: 0.7629 iter time (s): 4.592 samples/sec: 27.872
g0198:  iteration    29140/10000000 | consumed samples:      3729920 | consumed tokens:   7638876160 | elapsed time per iteration (ms): 4626.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.925882E-01 | loss scale: 262144.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.668 | tokens per gpu per second (tgs): 1770.775 | TFLOPs: 14.25 |
g0184: [2024-08-10 19:56:30,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=29150, skipped=40, lr=[0.0001998855382374873, 0.0001998855382374873], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29150 loss: 0.8036 iter time (s): 4.483 samples/sec: 28.551
g0198:  iteration    29150/10000000 | consumed samples:      3731200 | consumed tokens:   7641497600 | elapsed time per iteration (ms): 4517.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.878692E-01 | loss scale: 262144.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.338 | tokens per gpu per second (tgs): 1813.604 | TFLOPs: 14.59 |
g0184: [2024-08-10 19:57:15,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=29160, skipped=40, lr=[0.00019988540892750954, 0.00019988540892750954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29160 loss: 0.7965 iter time (s): 4.433 samples/sec: 28.878
g0198:  iteration    29160/10000000 | consumed samples:      3732480 | consumed tokens:   7644119040 | elapsed time per iteration (ms): 4467.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.724814E-01 | loss scale: 262144.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.649 | tokens per gpu per second (tgs): 1833.566 | TFLOPs: 14.76 |
g0184: [2024-08-10 19:58:01,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=29170, skipped=40, lr=[0.00019988527954457495, 0.00019988527954457495], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29170 loss: 0.7407 iter time (s): 4.636 samples/sec: 27.612
g0198:  iteration    29170/10000000 | consumed samples:      3733760 | consumed tokens:   7646740480 | elapsed time per iteration (ms): 4669.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.655457E-01 | loss scale: 262144.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.409 | tokens per gpu per second (tgs): 1754.197 | TFLOPs: 14.12 |
g0184: [2024-08-10 19:58:48,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=29180, skipped=40, lr=[0.0001998851500886836, 0.0001998851500886836], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29180 loss: 0.7657 iter time (s): 4.591 samples/sec: 27.878
g0198:  iteration    29180/10000000 | consumed samples:      3735040 | consumed tokens:   7649361920 | elapsed time per iteration (ms): 4624.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.806654E-01 | loss scale: 262144.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.679 | tokens per gpu per second (tgs): 1771.475 | TFLOPs: 14.26 |
g0184: [2024-08-10 19:59:33,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=29190, skipped=40, lr=[0.00019988502055983556, 0.00019988502055983556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29190 loss: 0.7865 iter time (s): 4.501 samples/sec: 28.438
g0198:  iteration    29190/10000000 | consumed samples:      3736320 | consumed tokens:   7651983360 | elapsed time per iteration (ms): 4538.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.714818E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.205 | tokens per gpu per second (tgs): 1805.100 | TFLOPs: 14.53 |
g0184: [2024-08-10 20:00:19,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=29200, skipped=40, lr=[0.00019988489095803098, 0.00019988489095803098], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29200 loss: 0.7835 iter time (s): 4.586 samples/sec: 27.913
g0198:  iteration    29200/10000000 | consumed samples:      3737600 | consumed tokens:   7654604800 | elapsed time per iteration (ms): 4622.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.747933E-01 | loss scale: 262144.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.691 | tokens per gpu per second (tgs): 1772.201 | TFLOPs: 14.26 |
g0184: [2024-08-10 20:01:05,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=29210, skipped=40, lr=[0.00019988476128326996, 0.00019988476128326996], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29210 loss: 0.7902 iter time (s): 4.561 samples/sec: 28.061
g0198:  iteration    29210/10000000 | consumed samples:      3738880 | consumed tokens:   7657226240 | elapsed time per iteration (ms): 4594.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.862132E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.859 | tokens per gpu per second (tgs): 1782.988 | TFLOPs: 14.35 |
g0184: [2024-08-10 20:01:50,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=29220, skipped=40, lr=[0.00019988463153555256, 0.00019988463153555256], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29220 loss: 0.8087 iter time (s): 4.483 samples/sec: 28.549
g0198:  iteration    29220/10000000 | consumed samples:      3740160 | consumed tokens:   7659847680 | elapsed time per iteration (ms): 4516.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.833785E-01 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.342 | tokens per gpu per second (tgs): 1813.916 | TFLOPs: 14.60 |
g0184: [2024-08-10 20:02:35,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=29230, skipped=40, lr=[0.0001998845017148789, 0.0001998845017148789], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29230 loss: 0.7835 iter time (s): 4.471 samples/sec: 28.628
g0198:  iteration    29230/10000000 | consumed samples:      3741440 | consumed tokens:   7662469120 | elapsed time per iteration (ms): 4503.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.722320E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.425 | tokens per gpu per second (tgs): 1819.202 | TFLOPs: 14.64 |
g0184: [2024-08-10 20:03:21,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=29240, skipped=40, lr=[0.00019988437182124908, 0.00019988437182124908], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29240 loss: 0.8123 iter time (s): 4.528 samples/sec: 28.268
g0198:  iteration    29240/10000000 | consumed samples:      3742720 | consumed tokens:   7665090560 | elapsed time per iteration (ms): 4560.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.711957E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.068 | tokens per gpu per second (tgs): 1796.371 | TFLOPs: 14.46 |
g0184: [2024-08-10 20:04:05,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=29250, skipped=40, lr=[0.00019988424185466317, 0.00019988424185466317], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29250 loss: 0.7638 iter time (s): 4.369 samples/sec: 29.300
g0198:  iteration    29250/10000000 | consumed samples:      3744000 | consumed tokens:   7667712000 | elapsed time per iteration (ms): 4401.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.737762E-01 | loss scale: 262144.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.079 | tokens per gpu per second (tgs): 1861.059 | TFLOPs: 14.98 |
g0184: [2024-08-10 20:04:52,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=29260, skipped=40, lr=[0.00019988411181512134, 0.00019988411181512134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29260 loss: 0.7734 iter time (s): 4.645 samples/sec: 27.558
g0198:  iteration    29260/10000000 | consumed samples:      3745280 | consumed tokens:   7670333440 | elapsed time per iteration (ms): 4676.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.675883E-01 | loss scale: 262144.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.368 | tokens per gpu per second (tgs): 1751.582 | TFLOPs: 14.10 |
g0184: [2024-08-10 20:05:35,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=29270, skipped=40, lr=[0.00019988398170262364, 0.00019988398170262364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29270 loss: 0.7631 iter time (s): 4.318 samples/sec: 29.643
g0198:  iteration    29270/10000000 | consumed samples:      3746560 | consumed tokens:   7672954880 | elapsed time per iteration (ms): 4350.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.733465E-01 | loss scale: 262144.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.423 | tokens per gpu per second (tgs): 1883.101 | TFLOPs: 15.15 |
g0184: [2024-08-10 20:06:19,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=29280, skipped=40, lr=[0.00019988385151717018, 0.00019988385151717018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29280 loss: 0.7668 iter time (s): 4.351 samples/sec: 29.417
g0198:  iteration    29280/10000000 | consumed samples:      3747840 | consumed tokens:   7675576320 | elapsed time per iteration (ms): 4383.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.733237E-01 | loss scale: 262144.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.200 | tokens per gpu per second (tgs): 1868.789 | TFLOPs: 15.04 |
g0184: [2024-08-10 20:07:02,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=29290, skipped=40, lr=[0.00019988372125876106, 0.00019988372125876106], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29290 loss: 0.7747 iter time (s): 4.228 samples/sec: 30.274
g0198:  iteration    29290/10000000 | consumed samples:      3749120 | consumed tokens:   7678197760 | elapsed time per iteration (ms): 4260.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.734129E-01 | loss scale: 262144.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.043 | tokens per gpu per second (tgs): 1922.760 | TFLOPs: 15.47 |
g0184: [2024-08-10 20:07:45,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=29300, skipped=40, lr=[0.00019988359092739638, 0.00019988359092739638], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29300 loss: 0.7597 iter time (s): 4.314 samples/sec: 29.670
g0198:  iteration    29300/10000000 | consumed samples:      3750400 | consumed tokens:   7680819200 | elapsed time per iteration (ms): 4346.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.717503E-01 | loss scale: 262144.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.449 | tokens per gpu per second (tgs): 1884.732 | TFLOPs: 15.17 |
g0184: [2024-08-10 20:08:31,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=29310, skipped=40, lr=[0.00019988346052307623, 0.00019988346052307623], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29310 loss: 0.7767 iter time (s): 4.581 samples/sec: 27.939
g0198:  iteration    29310/10000000 | consumed samples:      3751680 | consumed tokens:   7683440640 | elapsed time per iteration (ms): 4618.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.807459E-01 | loss scale: 262144.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.715 | tokens per gpu per second (tgs): 1773.772 | TFLOPs: 14.27 |
g0184: [2024-08-10 20:09:18,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=29320, skipped=40, lr=[0.0001998833300458007, 0.0001998833300458007], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29320 loss: 0.7852 iter time (s): 4.662 samples/sec: 27.457
g0198:  iteration    29320/10000000 | consumed samples:      3752960 | consumed tokens:   7686062080 | elapsed time per iteration (ms): 4694.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.830678E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.266 | tokens per gpu per second (tgs): 1745.029 | TFLOPs: 14.04 |
g0184: [2024-08-10 20:10:01,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=29330, skipped=40, lr=[0.00019988319949556995, 0.00019988319949556995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29330 loss: 0.7472 iter time (s): 4.259 samples/sec: 30.053
g0198:  iteration    29330/10000000 | consumed samples:      3754240 | consumed tokens:   7688683520 | elapsed time per iteration (ms): 4292.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.716821E-01 | loss scale: 262144.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.822 | tokens per gpu per second (tgs): 1908.610 | TFLOPs: 15.36 |
g0184: [2024-08-10 20:10:47,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=29340, skipped=40, lr=[0.00019988306887238402, 0.00019988306887238402], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29340 loss: 0.7975 iter time (s): 4.507 samples/sec: 28.403
g0198:  iteration    29340/10000000 | consumed samples:      3755520 | consumed tokens:   7691304960 | elapsed time per iteration (ms): 4539.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.712313E-01 | loss scale: 262144.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.197 | tokens per gpu per second (tgs): 1804.603 | TFLOPs: 14.52 |
g0184: [2024-08-10 20:11:32,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=29350, skipped=40, lr=[0.00019988293817624304, 0.00019988293817624304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29350 loss: 0.7604 iter time (s): 4.541 samples/sec: 28.190
g0198:  iteration    29350/10000000 | consumed samples:      3756800 | consumed tokens:   7693926400 | elapsed time per iteration (ms): 4574.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.715631E-01 | loss scale: 262144.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.983 | tokens per gpu per second (tgs): 1790.927 | TFLOPs: 14.41 |
g0184: [2024-08-10 20:12:17,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=29360, skipped=40, lr=[0.0001998828074071471, 0.0001998828074071471], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29360 loss: 0.7625 iter time (s): 4.424 samples/sec: 28.935
g0198:  iteration    29360/10000000 | consumed samples:      3758080 | consumed tokens:   7696547840 | elapsed time per iteration (ms): 4455.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.890092E-01 | loss scale: 262144.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.726 | tokens per gpu per second (tgs): 1838.453 | TFLOPs: 14.79 |
g0184: [2024-08-10 20:13:05,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=29370, skipped=40, lr=[0.0001998826765650963, 0.0001998826765650963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29370 loss: 0.7743 iter time (s): 4.738 samples/sec: 27.013
g0198:  iteration    29370/10000000 | consumed samples:      3759360 | consumed tokens:   7699169280 | elapsed time per iteration (ms): 4770.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.724885E-01 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.830 | tokens per gpu per second (tgs): 1717.139 | TFLOPs: 13.82 |
g0184: [2024-08-10 20:13:51,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=29380, skipped=40, lr=[0.00019988254565009073, 0.00019988254565009073], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29380 loss: 0.7499 iter time (s): 4.570 samples/sec: 28.010
g0198:  iteration    29380/10000000 | consumed samples:      3760640 | consumed tokens:   7701790720 | elapsed time per iteration (ms): 4603.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.735060E-01 | loss scale: 262144.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.806 | tokens per gpu per second (tgs): 1779.595 | TFLOPs: 14.32 |
g0184: [2024-08-10 20:14:37,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=29390, skipped=40, lr=[0.0001998824146621305, 0.0001998824146621305], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29390 loss: 0.7818 iter time (s): 4.570 samples/sec: 28.007
g0198:  iteration    29390/10000000 | consumed samples:      3761920 | consumed tokens:   7704412160 | elapsed time per iteration (ms): 4606.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.768099E-01 | loss scale: 262144.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.785 | tokens per gpu per second (tgs): 1778.218 | TFLOPs: 14.31 |
g0184: [2024-08-10 20:15:24,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=29400, skipped=40, lr=[0.00019988228360121575, 0.00019988228360121575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29400 loss: 0.7676 iter time (s): 4.658 samples/sec: 27.481
g0198:  iteration    29400/10000000 | consumed samples:      3763200 | consumed tokens:   7707033600 | elapsed time per iteration (ms): 4691.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.859140E-01 | loss scale: 262144.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.283 | tokens per gpu per second (tgs): 1746.137 | TFLOPs: 14.05 |
g0184: [2024-08-10 20:16:11,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=29410, skipped=40, lr=[0.00019988215246734652, 0.00019988215246734652], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29410 loss: 0.8055 iter time (s): 4.683 samples/sec: 27.332
g0198:  iteration    29410/10000000 | consumed samples:      3764480 | consumed tokens:   7709655040 | elapsed time per iteration (ms): 4716.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.774562E-01 | loss scale: 262144.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.138 | tokens per gpu per second (tgs): 1736.859 | TFLOPs: 13.98 |
g0184: [2024-08-10 20:16:55,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=29420, skipped=40, lr=[0.00019988202126052293, 0.00019988202126052293], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29420 loss: 0.7795 iter time (s): 4.369 samples/sec: 29.300
g0198:  iteration    29420/10000000 | consumed samples:      3765760 | consumed tokens:   7712276480 | elapsed time per iteration (ms): 4401.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.709244E-01 | loss scale: 262144.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.079 | tokens per gpu per second (tgs): 1861.077 | TFLOPs: 14.98 |
g0184: [2024-08-10 20:17:40,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=29430, skipped=40, lr=[0.00019988188998074509, 0.00019988188998074509], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29430 loss: 0.7776 iter time (s): 4.531 samples/sec: 28.251
g0198:  iteration    29430/10000000 | consumed samples:      3767040 | consumed tokens:   7714897920 | elapsed time per iteration (ms): 4563.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.750950E-01 | loss scale: 262144.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.050 | tokens per gpu per second (tgs): 1795.217 | TFLOPs: 14.45 |
g0184: [2024-08-10 20:18:26,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=29440, skipped=40, lr=[0.00019988175862801312, 0.00019988175862801312], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29440 loss: 0.7684 iter time (s): 4.505 samples/sec: 28.416
g0198:  iteration    29440/10000000 | consumed samples:      3768320 | consumed tokens:   7717519360 | elapsed time per iteration (ms): 4537.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.776900E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.211 | tokens per gpu per second (tgs): 1805.506 | TFLOPs: 14.53 |
g0184: [2024-08-10 20:19:10,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=29450, skipped=40, lr=[0.00019988162720232705, 0.00019988162720232705], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29450 loss: 0.7526 iter time (s): 4.420 samples/sec: 28.957
g0198:  iteration    29450/10000000 | consumed samples:      3769600 | consumed tokens:   7720140800 | elapsed time per iteration (ms): 4452.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.726067E-01 | loss scale: 262144.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.749 | tokens per gpu per second (tgs): 1839.938 | TFLOPs: 14.81 |
g0184: [2024-08-10 20:19:54,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=29460, skipped=40, lr=[0.00019988149570368706, 0.00019988149570368706], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29460 loss: 0.7931 iter time (s): 4.279 samples/sec: 29.910
g0198:  iteration    29460/10000000 | consumed samples:      3770880 | consumed tokens:   7722762240 | elapsed time per iteration (ms): 4311.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.724696E-01 | loss scale: 262144.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.686 | tokens per gpu per second (tgs): 1899.895 | TFLOPs: 15.29 |
g0184: [2024-08-10 20:20:38,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=29470, skipped=40, lr=[0.00019988136413209322, 0.00019988136413209322], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29470 loss: 0.7972 iter time (s): 4.439 samples/sec: 28.837
g0198:  iteration    29470/10000000 | consumed samples:      3772160 | consumed tokens:   7725383680 | elapsed time per iteration (ms): 4470.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.796696E-01 | loss scale: 262144.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.630 | tokens per gpu per second (tgs): 1832.295 | TFLOPs: 14.74 |
g0184: [2024-08-10 20:21:21,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=29480, skipped=40, lr=[0.00019988123248754566, 0.00019988123248754566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29480 loss: 0.7805 iter time (s): 4.288 samples/sec: 29.849
g0198:  iteration    29480/10000000 | consumed samples:      3773440 | consumed tokens:   7728005120 | elapsed time per iteration (ms): 4320.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.761945E-01 | loss scale: 262144.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.627 | tokens per gpu per second (tgs): 1896.096 | TFLOPs: 15.26 |
g0184: [2024-08-10 20:22:08,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=29490, skipped=40, lr=[0.00019988110077004441, 0.00019988110077004441], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29490 loss: 0.7971 iter time (s): 4.604 samples/sec: 27.804
g0198:  iteration    29490/10000000 | consumed samples:      3774720 | consumed tokens:   7730626560 | elapsed time per iteration (ms): 4635.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.860137E-01 | loss scale: 262144.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.612 | tokens per gpu per second (tgs): 1767.157 | TFLOPs: 14.22 |
g0184: [2024-08-10 20:22:49,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=29500, skipped=40, lr=[0.00019988096897958962, 0.00019988096897958962], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29500 loss: 0.7946 iter time (s): 4.132 samples/sec: 30.980
g0198:  iteration    29500/10000000 | consumed samples:      3776000 | consumed tokens:   7733248000 | elapsed time per iteration (ms): 4163.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.792100E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.740 | tokens per gpu per second (tgs): 1967.383 | TFLOPs: 15.83 |
g0184: [2024-08-10 20:23:36,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=29510, skipped=40, lr=[0.0001998808371161814, 0.0001998808371161814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29510 loss: 0.7750 iter time (s): 4.590 samples/sec: 27.886
g0198:  iteration    29510/10000000 | consumed samples:      3777280 | consumed tokens:   7735869440 | elapsed time per iteration (ms): 4623.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.793301E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.685 | tokens per gpu per second (tgs): 1771.852 | TFLOPs: 14.26 |
g0184: [2024-08-10 20:24:20,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=29520, skipped=40, lr=[0.00019988070517981983, 0.00019988070517981983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29520 loss: 0.7502 iter time (s): 4.376 samples/sec: 29.247
g0198:  iteration    29520/10000000 | consumed samples:      3778560 | consumed tokens:   7738490880 | elapsed time per iteration (ms): 4408.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.768793E-01 | loss scale: 262144.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.032 | tokens per gpu per second (tgs): 1858.062 | TFLOPs: 14.95 |
g0184: [2024-08-10 20:25:05,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=29530, skipped=40, lr=[0.000199880573170505, 0.000199880573170505], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29530 loss: 0.7707 iter time (s): 4.503 samples/sec: 28.425
g0198:  iteration    29530/10000000 | consumed samples:      3779840 | consumed tokens:   7741112320 | elapsed time per iteration (ms): 4535.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.741000E-01 | loss scale: 262144.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.221 | tokens per gpu per second (tgs): 1806.142 | TFLOPs: 14.53 |
g0184: [2024-08-10 20:25:50,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=29540, skipped=40, lr=[0.00019988044108823708, 0.00019988044108823708], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29540 loss: 0.7780 iter time (s): 4.439 samples/sec: 28.835
g0198:  iteration    29540/10000000 | consumed samples:      3781120 | consumed tokens:   7743733760 | elapsed time per iteration (ms): 4471.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.787906E-01 | loss scale: 262144.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.625 | tokens per gpu per second (tgs): 1832.028 | TFLOPs: 14.74 |
g0184: [2024-08-10 20:26:36,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=29550, skipped=40, lr=[0.0001998803089330161, 0.0001998803089330161], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29550 loss: 0.7868 iter time (s): 4.609 samples/sec: 27.773
g0198:  iteration    29550/10000000 | consumed samples:      3782400 | consumed tokens:   7746355200 | elapsed time per iteration (ms): 4642.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.721036E-01 | loss scale: 262144.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.573 | tokens per gpu per second (tgs): 1764.682 | TFLOPs: 14.20 |
g0184: [2024-08-10 20:27:20,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=29560, skipped=40, lr=[0.00019988017670484216, 0.00019988017670484216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29560 loss: 0.7708 iter time (s): 4.350 samples/sec: 29.423
g0198:  iteration    29560/10000000 | consumed samples:      3783680 | consumed tokens:   7748976640 | elapsed time per iteration (ms): 4384.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.817666E-01 | loss scale: 262144.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.196 | tokens per gpu per second (tgs): 1868.551 | TFLOPs: 15.04 |
g0184: [2024-08-10 20:28:04,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=29570, skipped=40, lr=[0.00019988004440371537, 0.00019988004440371537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29570 loss: 0.7707 iter time (s): 4.345 samples/sec: 29.458
g0198:  iteration    29570/10000000 | consumed samples:      3784960 | consumed tokens:   7751598080 | elapsed time per iteration (ms): 4377.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.830475E-01 | loss scale: 262144.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.239 | tokens per gpu per second (tgs): 1871.272 | TFLOPs: 15.06 |
g0188: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0188: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0197: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0197: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0197: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0188: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 20:28:32,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 20:28:32,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 20:28:49,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=29580, skipped=40, lr=[0.00019987991202963587, 0.00019987991202963587], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29580 loss: 0.7541 iter time (s): 4.503 samples/sec: 28.425
g0198:  iteration    29580/10000000 | consumed samples:      3786240 | consumed tokens:   7754219520 | elapsed time per iteration (ms): 4535.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.656613E-01 | loss scale: 524288.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.220 | tokens per gpu per second (tgs): 1806.076 | TFLOPs: 14.53 |
g0184: [2024-08-10 20:29:33,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=29590, skipped=40, lr=[0.00019987977958260377, 0.00019987977958260377], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29590 loss: 0.7514 iter time (s): 4.347 samples/sec: 29.444
g0198:  iteration    29590/10000000 | consumed samples:      3787520 | consumed tokens:   7756840960 | elapsed time per iteration (ms): 4379.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.776918E-01 | loss scale: 524288.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.227 | tokens per gpu per second (tgs): 1870.508 | TFLOPs: 15.05 |
g0184: [2024-08-10 20:30:18,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=29600, skipped=40, lr=[0.0001998796470626191, 0.0001998796470626191], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29600 loss: 0.7709 iter time (s): 4.437 samples/sec: 28.851
g0198:  iteration    29600/10000000 | consumed samples:      3788800 | consumed tokens:   7759462400 | elapsed time per iteration (ms): 4469.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.779518E-01 | loss scale: 524288.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.640 | tokens per gpu per second (tgs): 1832.974 | TFLOPs: 14.75 |
g0184: [2024-08-10 20:31:02,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=29610, skipped=40, lr=[0.00019987951446968203, 0.00019987951446968203], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29610 loss: 0.7456 iter time (s): 4.406 samples/sec: 29.048
g0198:  iteration    29610/10000000 | consumed samples:      3790080 | consumed tokens:   7762083840 | elapsed time per iteration (ms): 4439.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.764722E-01 | loss scale: 524288.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.833 | tokens per gpu per second (tgs): 1845.315 | TFLOPs: 14.85 |
g0184: [2024-08-10 20:31:47,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=29620, skipped=40, lr=[0.0001998793818037926, 0.0001998793818037926], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29620 loss: 0.7528 iter time (s): 4.437 samples/sec: 28.850
g0198:  iteration    29620/10000000 | consumed samples:      3791360 | consumed tokens:   7764705280 | elapsed time per iteration (ms): 4471.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.766954E-01 | loss scale: 524288.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.627 | tokens per gpu per second (tgs): 1832.115 | TFLOPs: 14.74 |
g0184: [2024-08-10 20:32:30,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=29630, skipped=40, lr=[0.00019987924906495096, 0.00019987924906495096], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29630 loss: 0.7669 iter time (s): 4.294 samples/sec: 29.812
g0198:  iteration    29630/10000000 | consumed samples:      3792640 | consumed tokens:   7767326720 | elapsed time per iteration (ms): 4326.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.832469E-01 | loss scale: 524288.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.588 | tokens per gpu per second (tgs): 1893.652 | TFLOPs: 15.24 |
g0184: [2024-08-10 20:33:15,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=29640, skipped=40, lr=[0.00019987911625315722, 0.00019987911625315722], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29640 loss: 0.7701 iter time (s): 4.446 samples/sec: 28.791
g0198:  iteration    29640/10000000 | consumed samples:      3793920 | consumed tokens:   7769948160 | elapsed time per iteration (ms): 4478.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.629678E-01 | loss scale: 524288.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.580 | tokens per gpu per second (tgs): 1829.150 | TFLOPs: 14.72 |
g0184: [2024-08-10 20:33:56,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=29650, skipped=40, lr=[0.00019987898336841145, 0.00019987898336841145], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29650 loss: 0.7726 iter time (s): 4.094 samples/sec: 31.263
g0198:  iteration    29650/10000000 | consumed samples:      3795200 | consumed tokens:   7772569600 | elapsed time per iteration (ms): 4126.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.729725E-01 | loss scale: 524288.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.019 | tokens per gpu per second (tgs): 1985.186 | TFLOPs: 15.98 |
g0184: [2024-08-10 20:34:41,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=29660, skipped=40, lr=[0.00019987885041071373, 0.00019987885041071373], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29660 loss: 0.7592 iter time (s): 4.432 samples/sec: 28.883
g0198:  iteration    29660/10000000 | consumed samples:      3796480 | consumed tokens:   7775191040 | elapsed time per iteration (ms): 4465.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.677843E-01 | loss scale: 524288.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.667 | tokens per gpu per second (tgs): 1834.686 | TFLOPs: 14.76 |
g0184: [2024-08-10 20:35:29,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=29670, skipped=40, lr=[0.00019987871738006422, 0.00019987871738006422], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29670 loss: 0.7599 iter time (s): 4.761 samples/sec: 26.886
g0198:  iteration    29670/10000000 | consumed samples:      3797760 | consumed tokens:   7777812480 | elapsed time per iteration (ms): 4793.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.782829E-01 | loss scale: 524288.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.702 | tokens per gpu per second (tgs): 1708.939 | TFLOPs: 13.75 |
g0184: [2024-08-10 20:36:16,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=29680, skipped=40, lr=[0.000199878584276463, 0.000199878584276463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29680 loss: 0.7711 iter time (s): 4.654 samples/sec: 27.506
g0198:  iteration    29680/10000000 | consumed samples:      3799040 | consumed tokens:   7780433920 | elapsed time per iteration (ms): 4685.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.723765E-01 | loss scale: 524288.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.316 | tokens per gpu per second (tgs): 1748.251 | TFLOPs: 14.07 |
g0184: [2024-08-10 20:37:01,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=29690, skipped=40, lr=[0.00019987845109991018, 0.00019987845109991018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29690 loss: 0.7602 iter time (s): 4.544 samples/sec: 28.169
g0198:  iteration    29690/10000000 | consumed samples:      3800320 | consumed tokens:   7783055360 | elapsed time per iteration (ms): 4577.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.840816E-01 | loss scale: 524288.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.961 | tokens per gpu per second (tgs): 1789.523 | TFLOPs: 14.40 |
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 29699
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 29699
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 29699
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 29699
g0188: Grad overflow on iteration 29699
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 29699
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0184: Grad overflow on iteration 29699
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 29699
g0194: Grad overflow on iteration 29699
g0195: Grad overflow on iteration 29699
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0188: Grad overflow on iteration 29699
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 29699
g0198: Grad overflow on iteration 29699
g0194: Grad overflow on iteration 29699
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: Grad overflow on iteration 29699
g0197: Grad overflow on iteration 29699
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 29699
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 29699
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0185: Grad overflow on iteration 29699
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 29699
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: Grad overflow on iteration 29699
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 29699
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 29699
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 29699
g0194: Grad overflow on iteration 29699
g0184: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0184: [2024-08-10 20:37:48,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0198: Grad overflow on iteration 29699
g0195: Grad overflow on iteration 29699
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 29699
g0187: Grad overflow on iteration 29699
g0185: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: Grad overflow on iteration 29699
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 29699
g0188: Grad overflow on iteration 29699
g0187: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0188: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0198: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0184: [2024-08-10 20:37:48,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=29700, skipped=41, lr=[0.00019987833117863908, 0.00019987833117863908], mom=[(0.9, 0.95), (0.9, 0.95)]
g0195: [2024-08-10 20:37:48,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0184: steps: 29700 loss: 0.7955 iter time (s): 4.671 samples/sec: 27.403
g0198:  iteration    29700/10000000 | consumed samples:      3801600 | consumed tokens:   7785676800 | elapsed time per iteration (ms): 4704.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.753339E-01 | loss scale: 262144.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.207 | tokens per gpu per second (tgs): 1741.243 | TFLOPs: 14.01 |
g0184: [2024-08-10 20:38:32,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=29710, skipped=41, lr=[0.0001998781845279501, 0.0001998781845279501], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29710 loss: 0.7676 iter time (s): 4.369 samples/sec: 29.300
g0198:  iteration    29710/10000000 | consumed samples:      3802880 | consumed tokens:   7788298240 | elapsed time per iteration (ms): 4401.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.727414E-01 | loss scale: 262144.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.081 | tokens per gpu per second (tgs): 1861.195 | TFLOPs: 14.98 |
g0184: [2024-08-10 20:39:18,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=29720, skipped=41, lr=[0.00019987805113254304, 0.00019987805113254304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29720 loss: 0.7854 iter time (s): 4.554 samples/sec: 28.107
g0198:  iteration    29720/10000000 | consumed samples:      3804160 | consumed tokens:   7790919680 | elapsed time per iteration (ms): 4587.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.697576E-01 | loss scale: 262144.0 | grad norm: 0.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.905 | tokens per gpu per second (tgs): 1785.930 | TFLOPs: 14.37 |
g0184: [2024-08-10 20:40:02,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=29730, skipped=41, lr=[0.00019987791766418476, 0.00019987791766418476], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29730 loss: 0.7506 iter time (s): 4.382 samples/sec: 29.213
g0198:  iteration    29730/10000000 | consumed samples:      3805440 | consumed tokens:   7793541120 | elapsed time per iteration (ms): 4414.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.704533E-01 | loss scale: 262144.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.995 | tokens per gpu per second (tgs): 1855.652 | TFLOPs: 14.93 |
g0184: [2024-08-10 20:40:45,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=29740, skipped=41, lr=[0.00019987778412287543, 0.00019987778412287543], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29740 loss: 0.7854 iter time (s): 4.233 samples/sec: 30.237
g0198:  iteration    29740/10000000 | consumed samples:      3806720 | consumed tokens:   7796162560 | elapsed time per iteration (ms): 4265.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.755053E-01 | loss scale: 262144.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.005 | tokens per gpu per second (tgs): 1920.346 | TFLOPs: 15.45 |
g0184: [2024-08-10 20:41:30,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=29750, skipped=41, lr=[0.00019987765050861507, 0.00019987765050861507], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29750 loss: 0.7420 iter time (s): 4.474 samples/sec: 28.611
g0198:  iteration    29750/10000000 | consumed samples:      3808000 | consumed tokens:   7798784000 | elapsed time per iteration (ms): 4506.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.765270E-01 | loss scale: 262144.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.401 | tokens per gpu per second (tgs): 1817.695 | TFLOPs: 14.63 |
g0184: [2024-08-10 20:42:14,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=29760, skipped=41, lr=[0.00019987751682140384, 0.00019987751682140384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29760 loss: 0.7901 iter time (s): 4.400 samples/sec: 29.090
g0198:  iteration    29760/10000000 | consumed samples:      3809280 | consumed tokens:   7801405440 | elapsed time per iteration (ms): 4433.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.874187E-01 | loss scale: 262144.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.871 | tokens per gpu per second (tgs): 1847.730 | TFLOPs: 14.87 |
g0184: [2024-08-10 20:43:00,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=29770, skipped=41, lr=[0.0001998773830612418, 0.0001998773830612418], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29770 loss: 0.7523 iter time (s): 4.547 samples/sec: 28.153
g0198:  iteration    29770/10000000 | consumed samples:      3810560 | consumed tokens:   7804026880 | elapsed time per iteration (ms): 4579.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.609459E-01 | loss scale: 262144.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.952 | tokens per gpu per second (tgs): 1788.955 | TFLOPs: 14.40 |
g0184: [2024-08-10 20:43:47,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=29780, skipped=41, lr=[0.00019987724922812907, 0.00019987724922812907], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29780 loss: 0.7789 iter time (s): 4.604 samples/sec: 27.799
g0198:  iteration    29780/10000000 | consumed samples:      3811840 | consumed tokens:   7806648320 | elapsed time per iteration (ms): 4636.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.665158E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.605 | tokens per gpu per second (tgs): 1766.694 | TFLOPs: 14.22 |
g0184: [2024-08-10 20:44:30,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=29790, skipped=41, lr=[0.00019987711532206575, 0.00019987711532206575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29790 loss: 0.7828 iter time (s): 4.342 samples/sec: 29.479
g0198:  iteration    29790/10000000 | consumed samples:      3813120 | consumed tokens:   7809269760 | elapsed time per iteration (ms): 4375.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.758544E-01 | loss scale: 262144.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.257 | tokens per gpu per second (tgs): 1872.473 | TFLOPs: 15.07 |
g0184: [2024-08-10 20:45:17,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=29800, skipped=41, lr=[0.000199876981343052, 0.000199876981343052], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29800 loss: 0.7854 iter time (s): 4.674 samples/sec: 27.385
g0198:  iteration    29800/10000000 | consumed samples:      3814400 | consumed tokens:   7811891200 | elapsed time per iteration (ms): 4708.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.737470E-01 | loss scale: 262144.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.185 | tokens per gpu per second (tgs): 1739.822 | TFLOPs: 14.00 |
g0184: [2024-08-10 20:46:01,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=29810, skipped=41, lr=[0.00019987684729108783, 0.00019987684729108783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29810 loss: 0.7655 iter time (s): 4.349 samples/sec: 29.435
g0198:  iteration    29810/10000000 | consumed samples:      3815680 | consumed tokens:   7814512640 | elapsed time per iteration (ms): 4382.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.787264E-01 | loss scale: 262144.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.208 | tokens per gpu per second (tgs): 1869.343 | TFLOPs: 15.04 |
g0184: [2024-08-10 20:46:45,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=29820, skipped=41, lr=[0.0001998767131661734, 0.0001998767131661734], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29820 loss: 0.7881 iter time (s): 4.307 samples/sec: 29.721
g0198:  iteration    29820/10000000 | consumed samples:      3816960 | consumed tokens:   7817134080 | elapsed time per iteration (ms): 4339.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.797951E-01 | loss scale: 262144.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.497 | tokens per gpu per second (tgs): 1887.832 | TFLOPs: 15.19 |
g0184: [2024-08-10 20:47:29,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=29830, skipped=41, lr=[0.0001998765789683088, 0.0001998765789683088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29830 loss: 0.7924 iter time (s): 4.349 samples/sec: 29.432
g0198:  iteration    29830/10000000 | consumed samples:      3818240 | consumed tokens:   7819755520 | elapsed time per iteration (ms): 4382.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.795238E-01 | loss scale: 262144.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.209 | tokens per gpu per second (tgs): 1869.359 | TFLOPs: 15.04 |
g0184: [2024-08-10 20:48:12,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=29840, skipped=41, lr=[0.0001998764446974941, 0.0001998764446974941], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29840 loss: 0.7748 iter time (s): 4.335 samples/sec: 29.527
g0198:  iteration    29840/10000000 | consumed samples:      3819520 | consumed tokens:   7822376960 | elapsed time per iteration (ms): 4367.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.801943E-01 | loss scale: 262144.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.308 | tokens per gpu per second (tgs): 1875.723 | TFLOPs: 15.09 |
g0184: [2024-08-10 20:48:59,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=29850, skipped=41, lr=[0.00019987631035372946, 0.00019987631035372946], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29850 loss: 0.8161 iter time (s): 4.644 samples/sec: 27.565
g0198:  iteration    29850/10000000 | consumed samples:      3820800 | consumed tokens:   7824998400 | elapsed time per iteration (ms): 4683.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.921028E-01 | loss scale: 262144.0 | grad norm: 0.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.330 | tokens per gpu per second (tgs): 1749.140 | TFLOPs: 14.08 |
g0184: [2024-08-10 20:49:48,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=29860, skipped=41, lr=[0.00019987617593701494, 0.00019987617593701494], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29860 loss: 0.7965 iter time (s): 4.882 samples/sec: 26.218
g0198:  iteration    29860/10000000 | consumed samples:      3822080 | consumed tokens:   7827619840 | elapsed time per iteration (ms): 4914.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.797042E-01 | loss scale: 262144.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.043 | tokens per gpu per second (tgs): 1666.777 | TFLOPs: 13.41 |
g0184: [2024-08-10 20:50:33,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=29870, skipped=41, lr=[0.00019987604144735066, 0.00019987604144735066], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29870 loss: 0.7922 iter time (s): 4.424 samples/sec: 28.934
g0198:  iteration    29870/10000000 | consumed samples:      3823360 | consumed tokens:   7830241280 | elapsed time per iteration (ms): 4456.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.753340E-01 | loss scale: 262144.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.724 | tokens per gpu per second (tgs): 1838.322 | TFLOPs: 14.79 |
g0184: [2024-08-10 20:51:15,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=29880, skipped=41, lr=[0.00019987590688473673, 0.00019987590688473673], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29880 loss: 0.7632 iter time (s): 4.210 samples/sec: 30.406
g0198:  iteration    29880/10000000 | consumed samples:      3824640 | consumed tokens:   7832862720 | elapsed time per iteration (ms): 4242.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.762076E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.170 | tokens per gpu per second (tgs): 1930.865 | TFLOPs: 15.54 |
g0184: [2024-08-10 20:51:58,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=29890, skipped=41, lr=[0.00019987577224917327, 0.00019987577224917327], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29890 loss: 0.7994 iter time (s): 4.254 samples/sec: 30.090
g0198:  iteration    29890/10000000 | consumed samples:      3825920 | consumed tokens:   7835484160 | elapsed time per iteration (ms): 4287.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.809963E-01 | loss scale: 262144.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.856 | tokens per gpu per second (tgs): 1910.780 | TFLOPs: 15.38 |
g0184: [2024-08-10 20:52:42,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=29900, skipped=41, lr=[0.00019987563754066037, 0.00019987563754066037], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29900 loss: 0.7610 iter time (s): 4.340 samples/sec: 29.496
g0198:  iteration    29900/10000000 | consumed samples:      3827200 | consumed tokens:   7838105600 | elapsed time per iteration (ms): 4372.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.756577E-01 | loss scale: 262144.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.272 | tokens per gpu per second (tgs): 1873.385 | TFLOPs: 15.08 |
g0184: [2024-08-10 20:53:30,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=29910, skipped=41, lr=[0.0001998755027591981, 0.0001998755027591981], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29910 loss: 0.7679 iter time (s): 4.800 samples/sec: 26.666
g0198:  iteration    29910/10000000 | consumed samples:      3828480 | consumed tokens:   7840727040 | elapsed time per iteration (ms): 4832.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.751800E-01 | loss scale: 262144.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.488 | tokens per gpu per second (tgs): 1695.213 | TFLOPs: 13.64 |
g0184: [2024-08-10 20:54:14,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=29920, skipped=41, lr=[0.00019987536790478657, 0.00019987536790478657], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29920 loss: 0.7612 iter time (s): 4.344 samples/sec: 29.464
g0198:  iteration    29920/10000000 | consumed samples:      3829760 | consumed tokens:   7843348480 | elapsed time per iteration (ms): 4377.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.761131E-01 | loss scale: 262144.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.241 | tokens per gpu per second (tgs): 1871.455 | TFLOPs: 15.06 |
g0184: [2024-08-10 20:55:00,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=29930, skipped=41, lr=[0.00019987523297742592, 0.00019987523297742592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29930 loss: 0.7578 iter time (s): 4.539 samples/sec: 28.202
g0198:  iteration    29930/10000000 | consumed samples:      3831040 | consumed tokens:   7845969920 | elapsed time per iteration (ms): 4571.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.759348E-01 | loss scale: 262144.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.001 | tokens per gpu per second (tgs): 1792.087 | TFLOPs: 14.42 |
g0184: [2024-08-10 20:55:44,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=29940, skipped=41, lr=[0.00019987509797711624, 0.00019987509797711624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29940 loss: 0.7540 iter time (s): 4.366 samples/sec: 29.318
g0198:  iteration    29940/10000000 | consumed samples:      3832320 | consumed tokens:   7848591360 | elapsed time per iteration (ms): 4399.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.751980E-01 | loss scale: 262144.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.092 | tokens per gpu per second (tgs): 1861.872 | TFLOPs: 14.98 |
g0184: [2024-08-10 20:56:28,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=29950, skipped=41, lr=[0.00019987496290385762, 0.00019987496290385762], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29950 loss: 0.8149 iter time (s): 4.360 samples/sec: 29.357
g0198:  iteration    29950/10000000 | consumed samples:      3833600 | consumed tokens:   7851212800 | elapsed time per iteration (ms): 4393.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.851832E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.136 | tokens per gpu per second (tgs): 1864.719 | TFLOPs: 15.01 |
g0184: [2024-08-10 20:57:13,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=29960, skipped=41, lr=[0.0001998748277576502, 0.0001998748277576502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29960 loss: 0.7638 iter time (s): 4.507 samples/sec: 28.401
g0198:  iteration    29960/10000000 | consumed samples:      3834880 | consumed tokens:   7853834240 | elapsed time per iteration (ms): 4539.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.676943E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.199 | tokens per gpu per second (tgs): 1804.705 | TFLOPs: 14.52 |
g0184: [2024-08-10 20:57:57,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=29970, skipped=41, lr=[0.00019987469253849403, 0.00019987469253849403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29970 loss: 0.7384 iter time (s): 4.353 samples/sec: 29.404
g0198:  iteration    29970/10000000 | consumed samples:      3836160 | consumed tokens:   7856455680 | elapsed time per iteration (ms): 4386.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.747644E-01 | loss scale: 262144.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.178 | tokens per gpu per second (tgs): 1867.380 | TFLOPs: 15.03 |
g0184: [2024-08-10 20:58:42,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=29980, skipped=41, lr=[0.00019987455724638925, 0.00019987455724638925], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29980 loss: 0.7569 iter time (s): 4.463 samples/sec: 28.680
g0198:  iteration    29980/10000000 | consumed samples:      3837440 | consumed tokens:   7859077120 | elapsed time per iteration (ms): 4496.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.706305E-01 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.465 | tokens per gpu per second (tgs): 1821.744 | TFLOPs: 14.66 |
g0184: [2024-08-10 20:59:27,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=29990, skipped=41, lr=[0.00019987442188133594, 0.00019987442188133594], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 29990 loss: 0.7926 iter time (s): 4.448 samples/sec: 28.776
g0198:  iteration    29990/10000000 | consumed samples:      3838720 | consumed tokens:   7861698560 | elapsed time per iteration (ms): 4481.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.806942E-01 | loss scale: 262144.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.565 | tokens per gpu per second (tgs): 1828.149 | TFLOPs: 14.71 |
g0184: [2024-08-10 21:00:11,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=41, lr=[0.00019987428644333425, 0.00019987428644333425], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30000 loss: 0.7758 iter time (s): 4.425 samples/sec: 28.925
g0198:  iteration    30000/10000000 | consumed samples:      3840000 | consumed tokens:   7864320000 | elapsed time per iteration (ms): 4458.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.822352E-01 | loss scale: 262144.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.710 | tokens per gpu per second (tgs): 1837.440 | TFLOPs: 14.79 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 30000 | lm loss value: 7.769197E-01 | lm loss PPL: 2.174763E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   30000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 21:07:16,550] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30000 is about to be saved!
g0198: [2024-08-10 21:07:16,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0184: [2024-08-10 21:07:16,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0184: [2024-08-10 21:07:16,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0198: [2024-08-10 21:07:16,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0198: [2024-08-10 21:07:16,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0197: [2024-08-10 21:07:16,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0197: [2024-08-10 21:07:16,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0197: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0194: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0194: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0194: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0195: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0195: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0195: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0184: [2024-08-10 21:07:16,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0187: [2024-08-10 21:07:16,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0187: [2024-08-10 21:07:16,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0187: [2024-08-10 21:07:16,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0188: [2024-08-10 21:07:16,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0188: [2024-08-10 21:07:16,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0188: [2024-08-10 21:07:16,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0185: [2024-08-10 21:07:16,570] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0185: [2024-08-10 21:07:16,570] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0185: [2024-08-10 21:07:16,570] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0198: [2024-08-10 21:07:16,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_23-model_00-model_states.pt...
g0187: [2024-08-10 21:07:16,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_08-model_00-model_states.pt...
g0194: [2024-08-10 21:07:16,598] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_14-model_00-model_states.pt...
g0195: [2024-08-10 21:07:16,599] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_17-model_00-model_states.pt...
g0197: [2024-08-10 21:07:16,599] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_20-model_00-model_states.pt...
g0188: [2024-08-10 21:07:16,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_11-model_00-model_states.pt...
g0184: [2024-08-10 21:07:16,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_01-model_00-model_states.pt...
g0185: [2024-08-10 21:07:16,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_05-model_00-model_states.pt...
g0195: [2024-08-10 21:07:16,706] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_17-model_00-model_states.pt.
g0197: [2024-08-10 21:07:16,711] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_20-model_00-model_states.pt.
g0188: [2024-08-10 21:07:16,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_11-model_00-model_states.pt.
g0198: [2024-08-10 21:07:16,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 21:07:16,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 21:07:16,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_24-model_00-model_states.pt.
g0195: [2024-08-10 21:07:16,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_18-model_00-model_states.pt...
g0197: [2024-08-10 21:07:16,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_21-model_00-model_states.pt...
g0188: [2024-08-10 21:07:16,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_12-model_00-model_states.pt...
g0194: [2024-08-10 21:07:16,770] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_14-model_00-model_states.pt.
g0185: [2024-08-10 21:07:16,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_05-model_00-model_states.pt.
g0198: [2024-08-10 21:07:16,783] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_25-model_00-model_states.pt...
g0194: [2024-08-10 21:07:16,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_15-model_00-model_states.pt...
g0185: [2024-08-10 21:07:16,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_06-model_00-model_states.pt...
g0195: [2024-08-10 21:07:16,852] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_18-model_00-model_states.pt.
g0187: [2024-08-10 21:07:16,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_08-model_00-model_states.pt.
g0195: [2024-08-10 21:07:16,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_19-model_00-model_states.pt...
g0197: [2024-08-10 21:07:16,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_21-model_00-model_states.pt.
g0187: [2024-08-10 21:07:16,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_09-model_00-model_states.pt...
g0194: [2024-08-10 21:07:16,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_15-model_00-model_states.pt.
g0185: [2024-08-10 21:07:16,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_06-model_00-model_states.pt.
g0188: [2024-08-10 21:07:16,927] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_12-model_00-model_states.pt.
g0197: [2024-08-10 21:07:16,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_22-model_00-model_states.pt...
g0194: [2024-08-10 21:07:16,935] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_16-model_00-model_states.pt...
g0185: [2024-08-10 21:07:16,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_07-model_00-model_states.pt...
g0188: [2024-08-10 21:07:16,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_13-model_00-model_states.pt...
g0187: [2024-08-10 21:07:17,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_09-model_00-model_states.pt.
g0187: [2024-08-10 21:07:17,031] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_10-model_00-model_states.pt...
g0198: [2024-08-10 21:07:17,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_25-model_00-model_states.pt.
g0195: [2024-08-10 21:07:17,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_19-model_00-model_states.pt.
g0198: [2024-08-10 21:07:17,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_07_model_states.pt...
g0195: [2024-08-10 21:07:17,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_05_model_states.pt...
g0197: [2024-08-10 21:07:17,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 21:07:17,076] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_06_model_states.pt...
g0185: [2024-08-10 21:07:17,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 21:07:17,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_01_model_states.pt...
g0188: [2024-08-10 21:07:17,113] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 21:07:17,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_03_model_states.pt...
g0184: [2024-08-10 21:07:17,118] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_01-model_00-model_states.pt.
g0194: [2024-08-10 21:07:17,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 21:07:17,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_04_model_states.pt...
g0187: [2024-08-10 21:07:17,135] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_10-model_00-model_states.pt.
g0184: [2024-08-10 21:07:17,135] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_02-model_00-model_states.pt...
g0187: [2024-08-10 21:07:17,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_02_model_states.pt...
g0184: [2024-08-10 21:07:17,256] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_02-model_00-model_states.pt.
g0184: [2024-08-10 21:07:17,272] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_03-model_00-model_states.pt...
g0184: [2024-08-10 21:07:17,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 21:07:17,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 21:07:17,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 21:07:17,547] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_00_model_states.pt
g0184: [2024-08-10 21:07:17,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_00_model_states.pt...
g0195: [2024-08-10 21:07:19,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 21:07:19,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0197: [2024-08-10 21:07:19,425] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 21:07:19,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0185: [2024-08-10 21:07:19,552] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 21:07:19,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0187: [2024-08-10 21:07:19,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 21:07:19,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0194: [2024-08-10 21:07:19,694] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 21:07:19,695] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0198: [2024-08-10 21:07:19,834] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 21:07:19,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0188: [2024-08-10 21:07:19,890] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 21:07:19,890] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0184: [2024-08-10 21:07:21,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step30000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 21:07:21,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!
g0184:   successfully saved checkpoint at iteration   30000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.51, Latency(second): 4.992
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4991.76, 4992.94)
g0184: [2024-08-10 21:08:08,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=30010, skipped=41, lr=[0.00019987415093238424, 0.00019987415093238424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30010 loss: 0.7848 iter time (s): 4.618 samples/sec: 27.719
g0198:  iteration    30010/10000000 | consumed samples:      3841280 | consumed tokens:   7866941440 | elapsed time per iteration (ms): 47641.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.700939E-01 | loss scale: 262144.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.687 | tokens per gpu per second (tgs): 171.950 | TFLOPs: 1.38 |
g0184: [2024-08-10 21:08:51,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=30020, skipped=41, lr=[0.00019987401534848602, 0.00019987401534848602], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30020 loss: 0.7372 iter time (s): 4.316 samples/sec: 29.658
g0198:  iteration    30020/10000000 | consumed samples:      3842560 | consumed tokens:   7869562880 | elapsed time per iteration (ms): 4349.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.673245E-01 | loss scale: 262144.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.428 | tokens per gpu per second (tgs): 1883.413 | TFLOPs: 15.16 |
g0184: [2024-08-10 21:09:34,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=30030, skipped=41, lr=[0.0001998738796916397, 0.0001998738796916397], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30030 loss: 0.7698 iter time (s): 4.272 samples/sec: 29.963
g0198:  iteration    30030/10000000 | consumed samples:      3843840 | consumed tokens:   7872184320 | elapsed time per iteration (ms): 4304.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.752880E-01 | loss scale: 262144.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.738 | tokens per gpu per second (tgs): 1903.241 | TFLOPs: 15.32 |
g0184: [2024-08-10 21:10:18,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=30040, skipped=41, lr=[0.0001998737439618454, 0.0001998737439618454], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30040 loss: 0.7431 iter time (s): 4.355 samples/sec: 29.390
g0198:  iteration    30040/10000000 | consumed samples:      3845120 | consumed tokens:   7874805760 | elapsed time per iteration (ms): 4387.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.706804E-01 | loss scale: 262144.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.175 | tokens per gpu per second (tgs): 1867.180 | TFLOPs: 15.03 |
g0184: [2024-08-10 21:11:02,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=30050, skipped=41, lr=[0.0001998736081591032, 0.0001998736081591032], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30050 loss: 0.7606 iter time (s): 4.349 samples/sec: 29.433
g0198:  iteration    30050/10000000 | consumed samples:      3846400 | consumed tokens:   7877427200 | elapsed time per iteration (ms): 4381.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.721323E-01 | loss scale: 262144.0 | grad norm: 0.153 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.215 | tokens per gpu per second (tgs): 1869.758 | TFLOPs: 15.05 |
g0184: [2024-08-10 21:11:47,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=30060, skipped=41, lr=[0.0001998734722834132, 0.0001998734722834132], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30060 loss: 0.7805 iter time (s): 4.526 samples/sec: 28.282
g0198:  iteration    30060/10000000 | consumed samples:      3847680 | consumed tokens:   7880048640 | elapsed time per iteration (ms): 4558.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.700237E-01 | loss scale: 262144.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.080 | tokens per gpu per second (tgs): 1797.134 | TFLOPs: 14.46 |
g0184: [2024-08-10 21:12:32,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=30070, skipped=41, lr=[0.00019987333633477558, 0.00019987333633477558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30070 loss: 0.7589 iter time (s): 4.429 samples/sec: 28.903
g0198:  iteration    30070/10000000 | consumed samples:      3848960 | consumed tokens:   7882670080 | elapsed time per iteration (ms): 4461.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.739697E-01 | loss scale: 262144.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.691 | tokens per gpu per second (tgs): 1836.244 | TFLOPs: 14.78 |
g0184: [2024-08-10 21:13:20,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=30080, skipped=41, lr=[0.00019987320031319034, 0.00019987320031319034], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30080 loss: 0.7768 iter time (s): 4.732 samples/sec: 27.047
g0198:  iteration    30080/10000000 | consumed samples:      3850240 | consumed tokens:   7885291520 | elapsed time per iteration (ms): 4766.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.680696E-01 | loss scale: 262144.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.855 | tokens per gpu per second (tgs): 1718.740 | TFLOPs: 13.83 |
g0184: [2024-08-10 21:14:05,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=30090, skipped=41, lr=[0.00019987306421865762, 0.00019987306421865762], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30090 loss: 0.7470 iter time (s): 4.463 samples/sec: 28.683
g0198:  iteration    30090/10000000 | consumed samples:      3851520 | consumed tokens:   7887912960 | elapsed time per iteration (ms): 4494.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.717547E-01 | loss scale: 262144.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.477 | tokens per gpu per second (tgs): 1822.516 | TFLOPs: 14.67 |
g0184: [2024-08-10 21:14:50,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=30100, skipped=41, lr=[0.00019987292805117755, 0.00019987292805117755], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30100 loss: 0.7853 iter time (s): 4.521 samples/sec: 28.312
g0198:  iteration    30100/10000000 | consumed samples:      3852800 | consumed tokens:   7890534400 | elapsed time per iteration (ms): 4553.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.784357E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.109 | tokens per gpu per second (tgs): 1798.995 | TFLOPs: 14.48 |
g0184: [2024-08-10 21:15:35,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=30110, skipped=41, lr=[0.0001998727918107502, 0.0001998727918107502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30110 loss: 0.7608 iter time (s): 4.434 samples/sec: 28.865
g0198:  iteration    30110/10000000 | consumed samples:      3854080 | consumed tokens:   7893155840 | elapsed time per iteration (ms): 4466.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.796057E-01 | loss scale: 262144.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.656 | tokens per gpu per second (tgs): 1833.990 | TFLOPs: 14.76 |
g0184: [2024-08-10 21:16:20,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=30120, skipped=41, lr=[0.00019987265549737573, 0.00019987265549737573], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30120 loss: 0.7707 iter time (s): 4.451 samples/sec: 28.760
g0198:  iteration    30120/10000000 | consumed samples:      3855360 | consumed tokens:   7895777280 | elapsed time per iteration (ms): 4483.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.737567E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.551 | tokens per gpu per second (tgs): 1827.282 | TFLOPs: 14.70 |
g0184: [2024-08-10 21:17:07,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=30130, skipped=41, lr=[0.00019987251911105416, 0.00019987251911105416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30130 loss: 0.7669 iter time (s): 4.729 samples/sec: 27.066
g0198:  iteration    30130/10000000 | consumed samples:      3856640 | consumed tokens:   7898398720 | elapsed time per iteration (ms): 4762.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.770613E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.879 | tokens per gpu per second (tgs): 1720.285 | TFLOPs: 13.84 |
g0184: [2024-08-10 21:17:57,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=30140, skipped=41, lr=[0.00019987238265178566, 0.00019987238265178566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30140 loss: 0.7890 iter time (s): 4.902 samples/sec: 26.112
g0198:  iteration    30140/10000000 | consumed samples:      3857920 | consumed tokens:   7901020160 | elapsed time per iteration (ms): 4934.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.678977E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.940 | tokens per gpu per second (tgs): 1660.182 | TFLOPs: 13.36 |
g0184: [2024-08-10 21:18:42,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=30150, skipped=41, lr=[0.00019987224611957033, 0.00019987224611957033], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30150 loss: 0.7853 iter time (s): 4.556 samples/sec: 28.093
g0198:  iteration    30150/10000000 | consumed samples:      3859200 | consumed tokens:   7903641600 | elapsed time per iteration (ms): 4589.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.695967E-01 | loss scale: 262144.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.893 | tokens per gpu per second (tgs): 1785.126 | TFLOPs: 14.37 |
g0184: [2024-08-10 21:19:28,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=30160, skipped=41, lr=[0.00019987210951440826, 0.00019987210951440826], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30160 loss: 0.7933 iter time (s): 4.486 samples/sec: 28.536
g0198:  iteration    30160/10000000 | consumed samples:      3860480 | consumed tokens:   7906263040 | elapsed time per iteration (ms): 4522.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.765163E-01 | loss scale: 262144.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.301 | tokens per gpu per second (tgs): 1811.243 | TFLOPs: 14.58 |
g0184: [2024-08-10 21:20:13,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=30170, skipped=41, lr=[0.00019987197283629957, 0.00019987197283629957], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30170 loss: 0.7720 iter time (s): 4.499 samples/sec: 28.452
g0198:  iteration    30170/10000000 | consumed samples:      3861760 | consumed tokens:   7908884480 | elapsed time per iteration (ms): 4531.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.846115E-01 | loss scale: 262144.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.245 | tokens per gpu per second (tgs): 1807.655 | TFLOPs: 14.55 |
g0184: [2024-08-10 21:20:58,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=30180, skipped=41, lr=[0.00019987183608524434, 0.00019987183608524434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30180 loss: 0.7865 iter time (s): 4.435 samples/sec: 28.862
g0198:  iteration    30180/10000000 | consumed samples:      3863040 | consumed tokens:   7911505920 | elapsed time per iteration (ms): 4468.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.810763E-01 | loss scale: 262144.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.646 | tokens per gpu per second (tgs): 1833.354 | TFLOPs: 14.75 |
g0184: [2024-08-10 21:21:42,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=30190, skipped=41, lr=[0.00019987169926124267, 0.00019987169926124267], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30190 loss: 0.7955 iter time (s): 4.409 samples/sec: 29.032
g0198:  iteration    30190/10000000 | consumed samples:      3864320 | consumed tokens:   7914127360 | elapsed time per iteration (ms): 4442.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.782331E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.810 | tokens per gpu per second (tgs): 1843.822 | TFLOPs: 14.84 |
g0184: [2024-08-10 21:22:29,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=30200, skipped=41, lr=[0.0001998715623642947, 0.0001998715623642947], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30200 loss: 0.7510 iter time (s): 4.666 samples/sec: 27.432
g0198:  iteration    30200/10000000 | consumed samples:      3865600 | consumed tokens:   7916748800 | elapsed time per iteration (ms): 4699.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.746522E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.238 | tokens per gpu per second (tgs): 1743.230 | TFLOPs: 14.03 |
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0197: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0197: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0188: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0188: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 21:22:34,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0188: [2024-08-10 21:22:34,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0195: [2024-08-10 21:22:34,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0187: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0194: [2024-08-10 21:22:34,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0185: [2024-08-10 21:22:34,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0198: [2024-08-10 21:22:34,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 21:22:34,066] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0184: [2024-08-10 21:23:12,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=30210, skipped=41, lr=[0.00019987142539440052, 0.00019987142539440052], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30210 loss: 0.7772 iter time (s): 4.287 samples/sec: 29.860
g0198:  iteration    30210/10000000 | consumed samples:      3866880 | consumed tokens:   7919370240 | elapsed time per iteration (ms): 4319.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.790499E-01 | loss scale: 524288.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.634 | tokens per gpu per second (tgs): 1896.601 | TFLOPs: 15.26 |
g0184: [2024-08-10 21:23:56,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=30220, skipped=41, lr=[0.00019987128835156022, 0.00019987128835156022], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30220 loss: 0.7893 iter time (s): 4.360 samples/sec: 29.359
g0198:  iteration    30220/10000000 | consumed samples:      3868160 | consumed tokens:   7921991680 | elapsed time per iteration (ms): 4394.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.719530E-01 | loss scale: 524288.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.125 | tokens per gpu per second (tgs): 1864.031 | TFLOPs: 15.00 |
g0184: [2024-08-10 21:24:40,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=30230, skipped=41, lr=[0.00019987115123577394, 0.00019987115123577394], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30230 loss: 0.7676 iter time (s): 4.360 samples/sec: 29.360
g0198:  iteration    30230/10000000 | consumed samples:      3869440 | consumed tokens:   7924613120 | elapsed time per iteration (ms): 4392.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.801889E-01 | loss scale: 524288.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.140 | tokens per gpu per second (tgs): 1864.944 | TFLOPs: 15.01 |
g0184: [2024-08-10 21:25:25,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=30240, skipped=41, lr=[0.00019987101404704178, 0.00019987101404704178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30240 loss: 0.8017 iter time (s): 4.373 samples/sec: 29.271
g0198:  iteration    30240/10000000 | consumed samples:      3870720 | consumed tokens:   7927234560 | elapsed time per iteration (ms): 4444.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.796727E-01 | loss scale: 524288.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.799 | tokens per gpu per second (tgs): 1843.157 | TFLOPs: 14.83 |
g0184: [2024-08-10 21:26:10,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=30250, skipped=41, lr=[0.0001998708767853638, 0.0001998708767853638], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30250 loss: 0.7597 iter time (s): 4.461 samples/sec: 28.691
g0198:  iteration    30250/10000000 | consumed samples:      3872000 | consumed tokens:   7929856000 | elapsed time per iteration (ms): 4494.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.750724E-01 | loss scale: 524288.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.482 | tokens per gpu per second (tgs): 1822.842 | TFLOPs: 14.67 |
g0184: [2024-08-10 21:26:55,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=30260, skipped=41, lr=[0.0001998707394507401, 0.0001998707394507401], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30260 loss: 0.7726 iter time (s): 4.497 samples/sec: 28.465
g0198:  iteration    30260/10000000 | consumed samples:      3873280 | consumed tokens:   7932477440 | elapsed time per iteration (ms): 4543.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.728022E-01 | loss scale: 524288.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.171 | tokens per gpu per second (tgs): 1802.946 | TFLOPs: 14.51 |
g0184: [2024-08-10 21:27:38,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=30270, skipped=41, lr=[0.00019987060204317087, 0.00019987060204317087], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30270 loss: 0.7994 iter time (s): 4.269 samples/sec: 29.985
g0198:  iteration    30270/10000000 | consumed samples:      3874560 | consumed tokens:   7935098880 | elapsed time per iteration (ms): 4301.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.829096E-01 | loss scale: 524288.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.756 | tokens per gpu per second (tgs): 1904.385 | TFLOPs: 15.32 |
g0184: [2024-08-10 21:28:21,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=30280, skipped=41, lr=[0.00019987046456265616, 0.00019987046456265616], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30280 loss: 0.8127 iter time (s): 4.286 samples/sec: 29.868
g0198:  iteration    30280/10000000 | consumed samples:      3875840 | consumed tokens:   7937720320 | elapsed time per iteration (ms): 4318.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.814882E-01 | loss scale: 524288.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.640 | tokens per gpu per second (tgs): 1896.983 | TFLOPs: 15.27 |
g0184: [2024-08-10 21:29:05,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=30290, skipped=41, lr=[0.00019987032700919608, 0.00019987032700919608], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30290 loss: 0.7900 iter time (s): 4.374 samples/sec: 29.266
g0198:  iteration    30290/10000000 | consumed samples:      3877120 | consumed tokens:   7940341760 | elapsed time per iteration (ms): 4406.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.746951E-01 | loss scale: 524288.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.049 | tokens per gpu per second (tgs): 1859.126 | TFLOPs: 14.96 |
g0184: [2024-08-10 21:29:51,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=30300, skipped=41, lr=[0.00019987018938279075, 0.00019987018938279075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30300 loss: 0.7861 iter time (s): 4.512 samples/sec: 28.371
g0198:  iteration    30300/10000000 | consumed samples:      3878400 | consumed tokens:   7942963200 | elapsed time per iteration (ms): 4544.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.768922E-01 | loss scale: 524288.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.167 | tokens per gpu per second (tgs): 1802.692 | TFLOPs: 14.51 |
g0184: [2024-08-10 21:30:35,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=30310, skipped=41, lr=[0.00019987005168344024, 0.00019987005168344024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30310 loss: 0.7590 iter time (s): 4.376 samples/sec: 29.251
g0198:  iteration    30310/10000000 | consumed samples:      3879680 | consumed tokens:   7945584640 | elapsed time per iteration (ms): 4408.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.775164E-01 | loss scale: 524288.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.032 | tokens per gpu per second (tgs): 1858.042 | TFLOPs: 14.95 |
g0184: [2024-08-10 21:31:18,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=30320, skipped=41, lr=[0.00019986991391114469, 0.00019986991391114469], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30320 loss: 0.7748 iter time (s): 4.287 samples/sec: 29.856
g0198:  iteration    30320/10000000 | consumed samples:      3880960 | consumed tokens:   7948206080 | elapsed time per iteration (ms): 4319.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.716773E-01 | loss scale: 524288.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.633 | tokens per gpu per second (tgs): 1896.526 | TFLOPs: 15.26 |
g0184: [2024-08-10 21:32:02,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=30330, skipped=41, lr=[0.00019986977606590417, 0.00019986977606590417], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30330 loss: 0.7782 iter time (s): 4.327 samples/sec: 29.579
g0198:  iteration    30330/10000000 | consumed samples:      3882240 | consumed tokens:   7950827520 | elapsed time per iteration (ms): 4360.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.854478E-01 | loss scale: 524288.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.356 | tokens per gpu per second (tgs): 1878.798 | TFLOPs: 15.12 |
g0184: [2024-08-10 21:32:43,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=30340, skipped=41, lr=[0.00019986963814771884, 0.00019986963814771884], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30340 loss: 0.7777 iter time (s): 4.098 samples/sec: 31.236
g0198:  iteration    30340/10000000 | consumed samples:      3883520 | consumed tokens:   7953448960 | elapsed time per iteration (ms): 4130.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.700918E-01 | loss scale: 524288.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.990 | tokens per gpu per second (tgs): 1983.386 | TFLOPs: 15.96 |
g0184: [2024-08-10 21:33:26,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=30350, skipped=41, lr=[0.00019986950015658881, 0.00019986950015658881], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30350 loss: 0.7547 iter time (s): 4.315 samples/sec: 29.662
g0198:  iteration    30350/10000000 | consumed samples:      3884800 | consumed tokens:   7956070400 | elapsed time per iteration (ms): 4347.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.701867E-01 | loss scale: 524288.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.441 | tokens per gpu per second (tgs): 1884.206 | TFLOPs: 15.16 |
g0184: [2024-08-10 21:34:12,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=30360, skipped=41, lr=[0.00019986936209251413, 0.00019986936209251413], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30360 loss: 0.7629 iter time (s): 4.515 samples/sec: 28.350
g0198:  iteration    30360/10000000 | consumed samples:      3886080 | consumed tokens:   7958691840 | elapsed time per iteration (ms): 4557.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.842196E-01 | loss scale: 524288.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.088 | tokens per gpu per second (tgs): 1797.652 | TFLOPs: 14.47 |
g0184: [2024-08-10 21:34:57,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=30370, skipped=41, lr=[0.0001998692239554949, 0.0001998692239554949], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30370 loss: 0.7757 iter time (s): 4.475 samples/sec: 28.601
g0198:  iteration    30370/10000000 | consumed samples:      3887360 | consumed tokens:   7961313280 | elapsed time per iteration (ms): 4508.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.724744E-01 | loss scale: 524288.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.391 | tokens per gpu per second (tgs): 1817.041 | TFLOPs: 14.62 |
g0184: [2024-08-10 21:35:43,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=30380, skipped=41, lr=[0.0001998690857455313, 0.0001998690857455313], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30380 loss: 0.8021 iter time (s): 4.561 samples/sec: 28.065
g0198:  iteration    30380/10000000 | consumed samples:      3888640 | consumed tokens:   7963934720 | elapsed time per iteration (ms): 4593.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.783297E-01 | loss scale: 524288.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.864 | tokens per gpu per second (tgs): 1783.285 | TFLOPs: 14.35 |
g0184: [2024-08-10 21:36:27,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=30390, skipped=41, lr=[0.00019986894746262332, 0.00019986894746262332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30390 loss: 0.7710 iter time (s): 4.353 samples/sec: 29.406
g0198:  iteration    30390/10000000 | consumed samples:      3889920 | consumed tokens:   7966556160 | elapsed time per iteration (ms): 4385.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.772398E-01 | loss scale: 524288.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.185 | tokens per gpu per second (tgs): 1867.815 | TFLOPs: 15.03 |
g0184: [2024-08-10 21:37:10,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=30400, skipped=41, lr=[0.00019986880910677118, 0.00019986880910677118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30400 loss: 0.7422 iter time (s): 4.262 samples/sec: 30.030
g0198:  iteration    30400/10000000 | consumed samples:      3891200 | consumed tokens:   7969177600 | elapsed time per iteration (ms): 4297.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.692090E-01 | loss scale: 524288.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.786 | tokens per gpu per second (tgs): 1906.314 | TFLOPs: 15.34 |
g0184: [2024-08-10 21:37:57,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=30410, skipped=41, lr=[0.00019986867067797498, 0.00019986867067797498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30410 loss: 0.7655 iter time (s): 4.669 samples/sec: 27.416
g0198:  iteration    30410/10000000 | consumed samples:      3892480 | consumed tokens:   7971799040 | elapsed time per iteration (ms): 4701.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.742838E-01 | loss scale: 524288.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.227 | tokens per gpu per second (tgs): 1742.511 | TFLOPs: 14.02 |
g0184: [2024-08-10 21:38:42,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=30420, skipped=41, lr=[0.00019986853217623477, 0.00019986853217623477], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30420 loss: 0.8098 iter time (s): 4.516 samples/sec: 28.346
g0198:  iteration    30420/10000000 | consumed samples:      3893760 | consumed tokens:   7974420480 | elapsed time per iteration (ms): 4548.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.762886E-01 | loss scale: 524288.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.140 | tokens per gpu per second (tgs): 1800.970 | TFLOPs: 14.49 |
g0184: [2024-08-10 21:39:26,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=30430, skipped=41, lr=[0.00019986839360155068, 0.00019986839360155068], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30430 loss: 0.7904 iter time (s): 4.378 samples/sec: 29.237
g0198:  iteration    30430/10000000 | consumed samples:      3895040 | consumed tokens:   7977041920 | elapsed time per iteration (ms): 4410.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.796202E-01 | loss scale: 524288.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.021 | tokens per gpu per second (tgs): 1857.328 | TFLOPs: 14.95 |
g0184: [2024-08-10 21:40:09,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=30440, skipped=41, lr=[0.0001998682549539228, 0.0001998682549539228], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30440 loss: 0.7741 iter time (s): 4.226 samples/sec: 30.289
g0198:  iteration    30440/10000000 | consumed samples:      3896320 | consumed tokens:   7979663360 | elapsed time per iteration (ms): 4259.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.739233E-01 | loss scale: 524288.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.053 | tokens per gpu per second (tgs): 1923.376 | TFLOPs: 15.48 |
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30441
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30441
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30441
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30441
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30441
g0188: Grad overflow on iteration 30441
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30441
g0184: Grad overflow on iteration 30441
g0195: Grad overflow on iteration 30441
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30441
g0195: Grad overflow on iteration 30441
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: Grad overflow on iteration 30441
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0184: Grad overflow on iteration 30441
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: Grad overflow on iteration 30441
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0185: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30441
g0197: Grad overflow on iteration 30441
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30441
g0188: Grad overflow on iteration 30441
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0184: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: Grad overflow on iteration 30441
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30441
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30441
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30441
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0197: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: Grad overflow on iteration 30441
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30441
g0198: Grad overflow on iteration 30441
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30441
g0195: Grad overflow on iteration 30441
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30441
g0195: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: Grad overflow on iteration 30441
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: Grad overflow on iteration 30441
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30441
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: Grad overflow on iteration 30441
g0184: [2024-08-10 21:40:18,677] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0194: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0187: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0188: [2024-08-10 21:40:18,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0195: [2024-08-10 21:40:18,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0194: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30442
g0194: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0194: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30442
g0198: Grad overflow on iteration 30442
g0195: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30442
g0198: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30442
g0194: Grad overflow on iteration 30442
g0194: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30442
g0197: Grad overflow on iteration 30442
g0194: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30442
g0195: Grad overflow on iteration 30442
g0188: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30442
g0188: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: Grad overflow on iteration 30442
g0188: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30442
g0188: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: Grad overflow on iteration 30442
g0198: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0188: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: Grad overflow on iteration 30442
g0194: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: Grad overflow on iteration 30442
g0195: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: Grad overflow on iteration 30442
g0187: Grad overflow on iteration 30442
g0198: Grad overflow on iteration 30442
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30442
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0195: Grad overflow on iteration 30442
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0197: Grad overflow on iteration 30442
g0184: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30442
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: Grad overflow on iteration 30442
g0184: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: Grad overflow on iteration 30442
g0197: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: Grad overflow on iteration 30442
g0198: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30442
g0184: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: Grad overflow on iteration 30442
g0184: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:23,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0185: Grad overflow on iteration 30442
g0187: Grad overflow on iteration 30442
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30442
g0187: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30442
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0185: [2024-08-10 21:40:23,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0198: [2024-08-10 21:40:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0184: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30447
g0184: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30447
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: Grad overflow on iteration 30447
g0188: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30447
g0194: Grad overflow on iteration 30447
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30447
g0194: Grad overflow on iteration 30447
g0188: Grad overflow on iteration 30447
g0184: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30447
g0195: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: Grad overflow on iteration 30447
g0195: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: Grad overflow on iteration 30447
g0195: Grad overflow on iteration 30447
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: Grad overflow on iteration 30447
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30447
g0184: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30447
g0188: Grad overflow on iteration 30447
g0188: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30447
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30447
g0194: Grad overflow on iteration 30447
g0188: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: Grad overflow on iteration 30447
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30447
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30447
g0195: Grad overflow on iteration 30447
g0184: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 21:40:45,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30447
g0195: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30447
g0195: Grad overflow on iteration 30447
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30447
g0198: [2024-08-10 21:40:45,040] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30447
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: Grad overflow on iteration 30447
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30447
g0187: Grad overflow on iteration 30447
g0197: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: Grad overflow on iteration 30447
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 21:40:45,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-10 21:40:45,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-10 21:40:45,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-10 21:40:45,040] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0195: [2024-08-10 21:40:45,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 21:40:45,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30448
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30448
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30448
g0194: Grad overflow on iteration 30448
g0185: Grad overflow on iteration 30448
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: Grad overflow on iteration 30448
g0197: Grad overflow on iteration 30448
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30448
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30448
g0188: Grad overflow on iteration 30448
g0187: Grad overflow on iteration 30448
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30448
g0187: Grad overflow on iteration 30448
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30448
g0188: Grad overflow on iteration 30448
g0195: Grad overflow on iteration 30448
g0194: Grad overflow on iteration 30448
g0185: Grad overflow on iteration 30448
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30448
g0194: Grad overflow on iteration 30448
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: Grad overflow on iteration 30448
g0187: Grad overflow on iteration 30448
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30448
g0184: Grad overflow on iteration 30448
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: Grad overflow on iteration 30448
g0184: Grad overflow on iteration 30448
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: Grad overflow on iteration 30448
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30448
g0184: Grad overflow on iteration 30448
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: Grad overflow on iteration 30448
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: Grad overflow on iteration 30448
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30448
g0184: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-10 21:40:48,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-10 21:40:48,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0194: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30449
g0194: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30449
g0194: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30449
g0195: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30449
g0194: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: Grad overflow on iteration 30449
g0194: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: Grad overflow on iteration 30449
g0185: Grad overflow on iteration 30449
g0195: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30449
g0197: Grad overflow on iteration 30449
g0188: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30449
g0195: Grad overflow on iteration 30449
g0188: Grad overflow on iteration 30449
g0195: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: Grad overflow on iteration 30449
g0197: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: Grad overflow on iteration 30449
g0197: Grad overflow on iteration 30449
g0187: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30449
g0197: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30449
g0187: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: Grad overflow on iteration 30449
g0184: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30449
g0198: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30449
g0184: Grad overflow on iteration 30449
g0198: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30449
g0188: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30449
g0184: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30449
g0184: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 21:40:53,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30449
g0194: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30449
g0185: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30449
g0187: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30449
g0195: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30449
g0188: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30449
g0197: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30449
g0187: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30449
g0184: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-10 21:40:53,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-10 21:40:53,329] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0184: [2024-08-10 21:40:53,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=30450, skipped=46, lr=[0.00019986815785718177, 0.00019986815785718177], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30450 loss: 0.8857 iter time (s): 4.346 samples/sec: 29.449
g0198:  iteration    30450/10000000 | consumed samples:      3897600 | consumed tokens:   7982284800 | elapsed time per iteration (ms): 4379.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.266838E-01 | loss scale: 16384.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.226 | tokens per gpu per second (tgs): 1870.469 | TFLOPs: 15.05 |
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30450
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30450
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30450
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 30450
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30450
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30450
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: Grad overflow on iteration 30450
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30450
g0194: Grad overflow on iteration 30450
g0197: Grad overflow on iteration 30450
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 30450
g0194: Grad overflow on iteration 30450
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30450
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30450
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 30450
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 30450
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 30450
g0184: Grad overflow on iteration 30450
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30450
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 30450
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 30450
g0188: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 30450
g0188: Grad overflow on iteration 30450
g0188: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 30450
g0187: Grad overflow on iteration 30450
g0188: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 30450
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 21:40:57,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 30450
g0198: Grad overflow on iteration 30450
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 30450
g0198: Grad overflow on iteration 30450
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 30450
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 30450
g0198: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 21:40:57,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 21:40:57,679] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 21:41:37,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=30460, skipped=47, lr=[0.00019986797743983613, 0.00019986797743983613], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30460 loss: 0.7871 iter time (s): 4.390 samples/sec: 29.156
g0198:  iteration    30460/10000000 | consumed samples:      3898880 | consumed tokens:   7984906240 | elapsed time per iteration (ms): 4423.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 8.114955E-01 | loss scale: 8192.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.938 | tokens per gpu per second (tgs): 1852.021 | TFLOPs: 14.90 |
g0184: [2024-08-10 21:42:23,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=30470, skipped=47, lr=[0.0001998678385733776, 0.0001998678385733776], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30470 loss: 0.7818 iter time (s): 4.511 samples/sec: 28.372
g0198:  iteration    30470/10000000 | consumed samples:      3900160 | consumed tokens:   7987527680 | elapsed time per iteration (ms): 4545.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.779304E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.158 | tokens per gpu per second (tgs): 1802.089 | TFLOPs: 14.50 |
g0184: [2024-08-10 21:43:09,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=30480, skipped=47, lr=[0.00019986769963397567, 0.00019986769963397567], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30480 loss: 0.7918 iter time (s): 4.617 samples/sec: 27.723
g0198:  iteration    30480/10000000 | consumed samples:      3901440 | consumed tokens:   7990149120 | elapsed time per iteration (ms): 4664.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.703090E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.439 | tokens per gpu per second (tgs): 1756.084 | TFLOPs: 14.13 |
g0184: [2024-08-10 21:43:56,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=30490, skipped=47, lr=[0.0001998675606216305, 0.0001998675606216305], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30490 loss: 0.7722 iter time (s): 4.643 samples/sec: 27.567
g0198:  iteration    30490/10000000 | consumed samples:      3902720 | consumed tokens:   7992770560 | elapsed time per iteration (ms): 4676.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.793415E-01 | loss scale: 8192.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.373 | tokens per gpu per second (tgs): 1751.878 | TFLOPs: 14.10 |
g0184: [2024-08-10 21:44:41,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=30500, skipped=47, lr=[0.00019986742153634224, 0.00019986742153634224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30500 loss: 0.7650 iter time (s): 4.429 samples/sec: 28.901
g0198:  iteration    30500/10000000 | consumed samples:      3904000 | consumed tokens:   7995392000 | elapsed time per iteration (ms): 4461.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.697804E-01 | loss scale: 8192.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.688 | tokens per gpu per second (tgs): 1836.029 | TFLOPs: 14.77 |
g0184: [2024-08-10 21:45:25,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=30510, skipped=47, lr=[0.00019986728237811091, 0.00019986728237811091], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30510 loss: 0.7342 iter time (s): 4.402 samples/sec: 29.076
g0198:  iteration    30510/10000000 | consumed samples:      3905280 | consumed tokens:   7998013440 | elapsed time per iteration (ms): 4435.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.696503E-01 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.858 | tokens per gpu per second (tgs): 1846.911 | TFLOPs: 14.86 |
g0184: [2024-08-10 21:46:10,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=30520, skipped=47, lr=[0.0001998671431469367, 0.0001998671431469367], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30520 loss: 0.7973 iter time (s): 4.449 samples/sec: 28.768
g0198:  iteration    30520/10000000 | consumed samples:      3906560 | consumed tokens:   8000634880 | elapsed time per iteration (ms): 4482.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.847351E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.555 | tokens per gpu per second (tgs): 1827.523 | TFLOPs: 14.71 |
g0184: [2024-08-10 21:46:53,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=30530, skipped=47, lr=[0.00019986700384281962, 0.00019986700384281962], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30530 loss: 0.7419 iter time (s): 4.276 samples/sec: 29.932
g0198:  iteration    30530/10000000 | consumed samples:      3907840 | consumed tokens:   8003256320 | elapsed time per iteration (ms): 4309.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.739102E-01 | loss scale: 8192.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.706 | tokens per gpu per second (tgs): 1901.155 | TFLOPs: 15.30 |
g0184: [2024-08-10 21:47:37,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=30540, skipped=47, lr=[0.00019986686446575987, 0.00019986686446575987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30540 loss: 0.7536 iter time (s): 4.403 samples/sec: 29.072
g0198:  iteration    30540/10000000 | consumed samples:      3909120 | consumed tokens:   8005877760 | elapsed time per iteration (ms): 4435.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.666479E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.857 | tokens per gpu per second (tgs): 1846.874 | TFLOPs: 14.86 |
g0184: [2024-08-10 21:48:22,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=30550, skipped=47, lr=[0.0001998667250157575, 0.0001998667250157575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30550 loss: 0.7599 iter time (s): 4.477 samples/sec: 28.591
g0198:  iteration    30550/10000000 | consumed samples:      3910400 | consumed tokens:   8008499200 | elapsed time per iteration (ms): 4509.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.748675E-01 | loss scale: 8192.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.383 | tokens per gpu per second (tgs): 1816.526 | TFLOPs: 14.62 |
g0184: [2024-08-10 21:49:05,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=30560, skipped=47, lr=[0.00019986658549281267, 0.00019986658549281267], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30560 loss: 0.7754 iter time (s): 4.267 samples/sec: 29.996
g0198:  iteration    30560/10000000 | consumed samples:      3911680 | consumed tokens:   8011120640 | elapsed time per iteration (ms): 4299.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.718996E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.769 | tokens per gpu per second (tgs): 1905.190 | TFLOPs: 15.33 |
g0184: [2024-08-10 21:49:50,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=30570, skipped=47, lr=[0.00019986644589692544, 0.00019986644589692544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30570 loss: 0.7734 iter time (s): 4.447 samples/sec: 28.783
g0198:  iteration    30570/10000000 | consumed samples:      3912960 | consumed tokens:   8013742080 | elapsed time per iteration (ms): 4479.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.795034E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.575 | tokens per gpu per second (tgs): 1828.779 | TFLOPs: 14.72 |
g0184: [2024-08-10 21:50:36,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=30580, skipped=47, lr=[0.00019986630622809593, 0.00019986630622809593], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30580 loss: 0.7880 iter time (s): 4.599 samples/sec: 27.830
g0198:  iteration    30580/10000000 | consumed samples:      3914240 | consumed tokens:   8016363520 | elapsed time per iteration (ms): 4632.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.766759E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.630 | tokens per gpu per second (tgs): 1768.330 | TFLOPs: 14.23 |
g0184: [2024-08-10 21:51:22,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=30590, skipped=47, lr=[0.00019986616648632423, 0.00019986616648632423], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30590 loss: 0.8055 iter time (s): 4.483 samples/sec: 28.554
g0198:  iteration    30590/10000000 | consumed samples:      3915520 | consumed tokens:   8018984960 | elapsed time per iteration (ms): 4515.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.689232E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.348 | tokens per gpu per second (tgs): 1814.276 | TFLOPs: 14.60 |
g0184: [2024-08-10 21:52:06,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=30600, skipped=47, lr=[0.0001998660266716105, 0.0001998660266716105], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30600 loss: 0.7464 iter time (s): 4.382 samples/sec: 29.209
g0198:  iteration    30600/10000000 | consumed samples:      3916800 | consumed tokens:   8021606400 | elapsed time per iteration (ms): 4414.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.668738E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.993 | tokens per gpu per second (tgs): 1855.578 | TFLOPs: 14.93 |
g0184: [2024-08-10 21:52:51,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=30610, skipped=47, lr=[0.0001998658867839548, 0.0001998658867839548], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30610 loss: 0.7807 iter time (s): 4.452 samples/sec: 28.752
g0198:  iteration    30610/10000000 | consumed samples:      3918080 | consumed tokens:   8024227840 | elapsed time per iteration (ms): 4484.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.665660E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.542 | tokens per gpu per second (tgs): 1826.709 | TFLOPs: 14.70 |
g0184: [2024-08-10 21:53:36,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=30620, skipped=47, lr=[0.00019986574682335725, 0.00019986574682335725], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30620 loss: 0.7445 iter time (s): 4.509 samples/sec: 28.389
g0198:  iteration    30620/10000000 | consumed samples:      3919360 | consumed tokens:   8026849280 | elapsed time per iteration (ms): 4542.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.732708E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.182 | tokens per gpu per second (tgs): 1803.623 | TFLOPs: 14.51 |
g0184: [2024-08-10 21:54:18,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=30630, skipped=47, lr=[0.00019986560678981796, 0.00019986560678981796], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30630 loss: 0.7922 iter time (s): 4.130 samples/sec: 30.996
g0198:  iteration    30630/10000000 | consumed samples:      3920640 | consumed tokens:   8029470720 | elapsed time per iteration (ms): 4162.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.841972E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.748 | tokens per gpu per second (tgs): 1967.879 | TFLOPs: 15.84 |
g0184: [2024-08-10 21:54:59,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=30640, skipped=47, lr=[0.00019986546668333705, 0.00019986546668333705], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30640 loss: 0.7739 iter time (s): 4.141 samples/sec: 30.913
g0198:  iteration    30640/10000000 | consumed samples:      3921920 | consumed tokens:   8032092160 | elapsed time per iteration (ms): 4173.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.640884E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.673 | tokens per gpu per second (tgs): 1963.094 | TFLOPs: 15.80 |
g0184: [2024-08-10 21:55:43,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=30650, skipped=47, lr=[0.0001998653265039146, 0.0001998653265039146], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30650 loss: 0.7627 iter time (s): 4.377 samples/sec: 29.247
g0198:  iteration    30650/10000000 | consumed samples:      3923200 | consumed tokens:   8034713600 | elapsed time per iteration (ms): 4409.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.748365E-01 | loss scale: 8192.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.030 | tokens per gpu per second (tgs): 1857.930 | TFLOPs: 14.95 |
g0184: [2024-08-10 21:56:27,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=30660, skipped=47, lr=[0.00019986518625155075, 0.00019986518625155075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30660 loss: 0.7624 iter time (s): 4.296 samples/sec: 29.795
g0198:  iteration    30660/10000000 | consumed samples:      3924480 | consumed tokens:   8037335040 | elapsed time per iteration (ms): 4376.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.814955E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.251 | tokens per gpu per second (tgs): 1872.044 | TFLOPs: 15.06 |
g0184: [2024-08-10 21:57:11,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=30670, skipped=47, lr=[0.00019986504592624558, 0.00019986504592624558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30670 loss: 0.7624 iter time (s): 4.327 samples/sec: 29.583
g0198:  iteration    30670/10000000 | consumed samples:      3925760 | consumed tokens:   8039956480 | elapsed time per iteration (ms): 4359.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.818426E-01 | loss scale: 8192.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.360 | tokens per gpu per second (tgs): 1879.014 | TFLOPs: 15.12 |
g0184: [2024-08-10 21:58:01,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=30680, skipped=47, lr=[0.0001998649055279992, 0.0001998649055279992], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30680 loss: 0.7921 iter time (s): 5.035 samples/sec: 25.424
g0198:  iteration    30680/10000000 | consumed samples:      3927040 | consumed tokens:   8042577920 | elapsed time per iteration (ms): 5067.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.748536E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.259 | tokens per gpu per second (tgs): 1616.580 | TFLOPs: 13.01 |
g0184: [2024-08-10 21:58:45,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=30690, skipped=47, lr=[0.00019986476505681172, 0.00019986476505681172], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30690 loss: 0.7507 iter time (s): 4.278 samples/sec: 29.918
g0198:  iteration    30690/10000000 | consumed samples:      3928320 | consumed tokens:   8045199360 | elapsed time per iteration (ms): 4310.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.684666E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.692 | tokens per gpu per second (tgs): 1900.299 | TFLOPs: 15.29 |
g0184: [2024-08-10 21:59:30,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=30700, skipped=47, lr=[0.00019986462451268328, 0.00019986462451268328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30700 loss: 0.7924 iter time (s): 4.519 samples/sec: 28.327
g0198:  iteration    30700/10000000 | consumed samples:      3929600 | consumed tokens:   8047820800 | elapsed time per iteration (ms): 4551.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.639759E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.124 | tokens per gpu per second (tgs): 1799.951 | TFLOPs: 14.48 |
g0184: [2024-08-10 22:00:18,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=30710, skipped=47, lr=[0.00019986448389561396, 0.00019986448389561396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30710 loss: 0.7678 iter time (s): 4.718 samples/sec: 27.131
g0198:  iteration    30710/10000000 | consumed samples:      3930880 | consumed tokens:   8050442240 | elapsed time per iteration (ms): 4750.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.818036E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.945 | tokens per gpu per second (tgs): 1724.486 | TFLOPs: 13.88 |
g0184: [2024-08-10 22:01:00,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=30720, skipped=47, lr=[0.00019986434320560384, 0.00019986434320560384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30720 loss: 0.7921 iter time (s): 4.247 samples/sec: 30.142
g0198:  iteration    30720/10000000 | consumed samples:      3932160 | consumed tokens:   8053063680 | elapsed time per iteration (ms): 4278.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.707962E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.914 | tokens per gpu per second (tgs): 1914.519 | TFLOPs: 15.41 |
g0184: [2024-08-10 22:01:44,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=30730, skipped=47, lr=[0.00019986420244265312, 0.00019986420244265312], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30730 loss: 0.7673 iter time (s): 4.332 samples/sec: 29.550
g0198:  iteration    30730/10000000 | consumed samples:      3933440 | consumed tokens:   8055685120 | elapsed time per iteration (ms): 4364.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.818229E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.329 | tokens per gpu per second (tgs): 1877.078 | TFLOPs: 15.11 |
g0184: [2024-08-10 22:02:28,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=30740, skipped=47, lr=[0.00019986406160676178, 0.00019986406160676178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30740 loss: 0.7775 iter time (s): 4.379 samples/sec: 29.229
g0198:  iteration    30740/10000000 | consumed samples:      3934720 | consumed tokens:   8058306560 | elapsed time per iteration (ms): 4413.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.770855E-01 | loss scale: 8192.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.000 | tokens per gpu per second (tgs): 1856.014 | TFLOPs: 14.94 |
g0184: [2024-08-10 22:03:12,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=30750, skipped=47, lr=[0.00019986392069793003, 0.00019986392069793003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30750 loss: 0.7519 iter time (s): 4.343 samples/sec: 29.471
g0198:  iteration    30750/10000000 | consumed samples:      3936000 | consumed tokens:   8060928000 | elapsed time per iteration (ms): 4382.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.734128E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.206 | tokens per gpu per second (tgs): 1869.164 | TFLOPs: 15.04 |
g0184: [2024-08-10 22:03:55,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=30760, skipped=47, lr=[0.00019986377971615795, 0.00019986377971615795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30760 loss: 0.7705 iter time (s): 4.229 samples/sec: 30.268
g0198:  iteration    30760/10000000 | consumed samples:      3937280 | consumed tokens:   8063549440 | elapsed time per iteration (ms): 4263.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.692898E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.025 | tokens per gpu per second (tgs): 1921.601 | TFLOPs: 15.46 |
g0184: [2024-08-10 22:04:39,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=30770, skipped=47, lr=[0.0001998636386614456, 0.0001998636386614456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30770 loss: 0.7687 iter time (s): 4.377 samples/sec: 29.245
g0198:  iteration    30770/10000000 | consumed samples:      3938560 | consumed tokens:   8066170880 | elapsed time per iteration (ms): 4410.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.649093E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.024 | tokens per gpu per second (tgs): 1857.546 | TFLOPs: 14.95 |
g0184: [2024-08-10 22:05:25,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=30780, skipped=47, lr=[0.00019986349753379316, 0.00019986349753379316], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30780 loss: 0.7833 iter time (s): 4.602 samples/sec: 27.814
g0198:  iteration    30780/10000000 | consumed samples:      3939840 | consumed tokens:   8068792320 | elapsed time per iteration (ms): 4635.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.770588E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.616 | tokens per gpu per second (tgs): 1767.419 | TFLOPs: 14.22 |
g0184: [2024-08-10 22:06:16,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=30790, skipped=47, lr=[0.0001998633563332007, 0.0001998633563332007], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30790 loss: 0.7715 iter time (s): 5.026 samples/sec: 25.468
g0198:  iteration    30790/10000000 | consumed samples:      3941120 | consumed tokens:   8071413760 | elapsed time per iteration (ms): 5058.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.875269E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.306 | tokens per gpu per second (tgs): 1619.561 | TFLOPs: 13.03 |
g0184: [2024-08-10 22:07:01,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=30800, skipped=47, lr=[0.0001998632150596684, 0.0001998632150596684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30800 loss: 0.7974 iter time (s): 4.455 samples/sec: 28.732
g0198:  iteration    30800/10000000 | consumed samples:      3942400 | consumed tokens:   8074035200 | elapsed time per iteration (ms): 4488.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.735196E-01 | loss scale: 8192.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.519 | tokens per gpu per second (tgs): 1825.234 | TFLOPs: 14.69 |
g0184: [2024-08-10 22:07:44,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=30810, skipped=47, lr=[0.00019986307371319624, 0.00019986307371319624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30810 loss: 0.7426 iter time (s): 4.316 samples/sec: 29.659
g0198:  iteration    30810/10000000 | consumed samples:      3943680 | consumed tokens:   8076656640 | elapsed time per iteration (ms): 4349.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.633148E-01 | loss scale: 8192.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.429 | tokens per gpu per second (tgs): 1883.469 | TFLOPs: 15.16 |
g0184: [2024-08-10 22:08:29,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=30820, skipped=47, lr=[0.0001998629322937844, 0.0001998629322937844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30820 loss: 0.7981 iter time (s): 4.458 samples/sec: 28.709
g0198:  iteration    30820/10000000 | consumed samples:      3944960 | consumed tokens:   8079278080 | elapsed time per iteration (ms): 4499.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.845780E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.450 | tokens per gpu per second (tgs): 1820.796 | TFLOPs: 14.65 |
g0184: [2024-08-10 22:09:13,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=30830, skipped=47, lr=[0.00019986279080143297, 0.00019986279080143297], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30830 loss: 0.7864 iter time (s): 4.393 samples/sec: 29.141
g0198:  iteration    30830/10000000 | consumed samples:      3946240 | consumed tokens:   8081899520 | elapsed time per iteration (ms): 4425.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.689055E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.927 | tokens per gpu per second (tgs): 1851.309 | TFLOPs: 14.90 |
g0184: [2024-08-10 22:09:59,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=30840, skipped=47, lr=[0.00019986264923614212, 0.00019986264923614212], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30840 loss: 0.7639 iter time (s): 4.512 samples/sec: 28.366
g0198:  iteration    30840/10000000 | consumed samples:      3947520 | consumed tokens:   8084520960 | elapsed time per iteration (ms): 4558.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.635293E-01 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.079 | tokens per gpu per second (tgs): 1797.027 | TFLOPs: 14.46 |
g0184: [2024-08-10 22:10:47,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=30850, skipped=47, lr=[0.00019986250759791187, 0.00019986250759791187], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30850 loss: 0.7819 iter time (s): 4.815 samples/sec: 26.585
g0198:  iteration    30850/10000000 | consumed samples:      3948800 | consumed tokens:   8087142400 | elapsed time per iteration (ms): 4849.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.718005E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.393 | tokens per gpu per second (tgs): 1689.182 | TFLOPs: 13.59 |
g0184: [2024-08-10 22:11:32,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=30860, skipped=47, lr=[0.0001998623658867424, 0.0001998623658867424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30860 loss: 0.7797 iter time (s): 4.414 samples/sec: 28.997
g0198:  iteration    30860/10000000 | consumed samples:      3950080 | consumed tokens:   8089763840 | elapsed time per iteration (ms): 4447.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.683783E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.778 | tokens per gpu per second (tgs): 1841.805 | TFLOPs: 14.82 |
g0184: [2024-08-10 22:12:15,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=30870, skipped=47, lr=[0.00019986222410263375, 0.00019986222410263375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30870 loss: 0.7697 iter time (s): 4.256 samples/sec: 30.077
g0198:  iteration    30870/10000000 | consumed samples:      3951360 | consumed tokens:   8092385280 | elapsed time per iteration (ms): 4321.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.750030E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.620 | tokens per gpu per second (tgs): 1895.705 | TFLOPs: 15.26 |
g0184: [2024-08-10 22:12:59,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=30880, skipped=47, lr=[0.0001998620822455861, 0.0001998620822455861], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30880 loss: 0.7971 iter time (s): 4.343 samples/sec: 29.470
g0198:  iteration    30880/10000000 | consumed samples:      3952640 | consumed tokens:   8095006720 | elapsed time per iteration (ms): 4375.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.672548E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.252 | tokens per gpu per second (tgs): 1872.102 | TFLOPs: 15.07 |
g0184: [2024-08-10 22:13:44,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=30890, skipped=47, lr=[0.00019986194031559954, 0.00019986194031559954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30890 loss: 0.7699 iter time (s): 4.496 samples/sec: 28.470
g0198:  iteration    30890/10000000 | consumed samples:      3953920 | consumed tokens:   8097628160 | elapsed time per iteration (ms): 4528.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.753656E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.266 | tokens per gpu per second (tgs): 1809.002 | TFLOPs: 14.56 |
g0184: [2024-08-10 22:14:29,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=30900, skipped=47, lr=[0.00019986179831267415, 0.00019986179831267415], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30900 loss: 0.7687 iter time (s): 4.466 samples/sec: 28.664
g0198:  iteration    30900/10000000 | consumed samples:      3955200 | consumed tokens:   8100249600 | elapsed time per iteration (ms): 4498.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.758411E-01 | loss scale: 8192.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.456 | tokens per gpu per second (tgs): 1821.183 | TFLOPs: 14.66 |
g0184: [2024-08-10 22:15:12,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=30910, skipped=47, lr=[0.00019986165623681005, 0.00019986165623681005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30910 loss: 0.7715 iter time (s): 4.274 samples/sec: 29.950
g0198:  iteration    30910/10000000 | consumed samples:      3956480 | consumed tokens:   8102871040 | elapsed time per iteration (ms): 4323.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.669154E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.609 | tokens per gpu per second (tgs): 1894.970 | TFLOPs: 15.25 |
g0184: [2024-08-10 22:15:55,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=30920, skipped=47, lr=[0.00019986151408800734, 0.00019986151408800734], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30920 loss: 0.7803 iter time (s): 4.261 samples/sec: 30.037
g0198:  iteration    30920/10000000 | consumed samples:      3957760 | consumed tokens:   8105492480 | elapsed time per iteration (ms): 4294.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.780815E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.807 | tokens per gpu per second (tgs): 1907.638 | TFLOPs: 15.35 |
g0184: [2024-08-10 22:16:39,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=30930, skipped=47, lr=[0.00019986137186626618, 0.00019986137186626618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30930 loss: 0.7702 iter time (s): 4.373 samples/sec: 29.272
g0198:  iteration    30930/10000000 | consumed samples:      3959040 | consumed tokens:   8108113920 | elapsed time per iteration (ms): 4405.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.717732E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.055 | tokens per gpu per second (tgs): 1859.509 | TFLOPs: 14.96 |
g0184: [2024-08-10 22:17:29,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=30940, skipped=47, lr=[0.00019986122957158664, 0.00019986122957158664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30940 loss: 0.8172 iter time (s): 4.900 samples/sec: 26.123
g0198:  iteration    30940/10000000 | consumed samples:      3960320 | consumed tokens:   8110735360 | elapsed time per iteration (ms): 4932.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.831519E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.948 | tokens per gpu per second (tgs): 1660.672 | TFLOPs: 13.36 |
g0184: [2024-08-10 22:18:17,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=30950, skipped=47, lr=[0.0001998610872039688, 0.0001998610872039688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30950 loss: 0.7655 iter time (s): 4.768 samples/sec: 26.843
g0198:  iteration    30950/10000000 | consumed samples:      3961600 | consumed tokens:   8113356800 | elapsed time per iteration (ms): 4801.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.651733E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.660 | tokens per gpu per second (tgs): 1706.226 | TFLOPs: 13.73 |
g0184: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 22:18:25,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 22:18:25,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 22:18:25,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 22:18:25,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 22:18:25,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 22:18:25,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 22:18:25,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 22:18:25,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 22:18:25,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 22:18:25,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 22:18:25,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 22:18:25,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 22:19:00,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=30960, skipped=47, lr=[0.00019986094476341285, 0.00019986094476341285], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30960 loss: 0.7577 iter time (s): 4.319 samples/sec: 29.635
g0198:  iteration    30960/10000000 | consumed samples:      3962880 | consumed tokens:   8115978240 | elapsed time per iteration (ms): 4352.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.691800E-01 | loss scale: 16384.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.407 | tokens per gpu per second (tgs): 1882.031 | TFLOPs: 15.15 |
g0184: [2024-08-10 22:19:45,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=30970, skipped=47, lr=[0.00019986080224991881, 0.00019986080224991881], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30970 loss: 0.7602 iter time (s): 4.429 samples/sec: 28.900
g0198:  iteration    30970/10000000 | consumed samples:      3964160 | consumed tokens:   8118599680 | elapsed time per iteration (ms): 4461.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.684896E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.688 | tokens per gpu per second (tgs): 1836.002 | TFLOPs: 14.77 |
g0184: [2024-08-10 22:20:28,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=30980, skipped=47, lr=[0.00019986065966348684, 0.00019986065966348684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30980 loss: 0.7586 iter time (s): 4.325 samples/sec: 29.595
g0198:  iteration    30980/10000000 | consumed samples:      3965440 | consumed tokens:   8121221120 | elapsed time per iteration (ms): 4357.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.733152E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.374 | tokens per gpu per second (tgs): 1879.935 | TFLOPs: 15.13 |
g0184: [2024-08-10 22:21:14,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=30990, skipped=47, lr=[0.00019986051700411706, 0.00019986051700411706], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 30990 loss: 0.7489 iter time (s): 4.491 samples/sec: 28.502
g0198:  iteration    30990/10000000 | consumed samples:      3966720 | consumed tokens:   8123842560 | elapsed time per iteration (ms): 4523.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.678071E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.298 | tokens per gpu per second (tgs): 1811.043 | TFLOPs: 14.57 |
g0184: [2024-08-10 22:21:58,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=47, lr=[0.00019986037427180956, 0.00019986037427180956], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31000 loss: 0.7600 iter time (s): 4.453 samples/sec: 28.742
g0198:  iteration    31000/10000000 | consumed samples:      3968000 | consumed tokens:   8126464000 | elapsed time per iteration (ms): 4487.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.746276E-01 | loss scale: 16384.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.526 | tokens per gpu per second (tgs): 1825.647 | TFLOPs: 14.69 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 31000 | lm loss value: 7.730532E-01 | lm loss PPL: 2.166370E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   31000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 22:29:15,766] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step31000 is about to be saved!
g0184: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0184: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0198: [2024-08-10 22:29:15,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0184: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0198: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0185: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0185: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0185: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0198: [2024-08-10 22:29:15,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0188: [2024-08-10 22:29:15,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0188: [2024-08-10 22:29:15,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0188: [2024-08-10 22:29:15,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0187: [2024-08-10 22:29:15,775] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0187: [2024-08-10 22:29:15,775] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0187: [2024-08-10 22:29:15,775] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0195: [2024-08-10 22:29:15,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0195: [2024-08-10 22:29:15,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0195: [2024-08-10 22:29:15,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0197: [2024-08-10 22:29:15,779] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0197: [2024-08-10 22:29:15,779] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0197: [2024-08-10 22:29:15,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0194: [2024-08-10 22:29:15,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0194: [2024-08-10 22:29:15,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0194: [2024-08-10 22:29:15,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0198: [2024-08-10 22:29:15,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_23-model_00-model_states.pt...
g0187: [2024-08-10 22:29:15,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_08-model_00-model_states.pt...
g0188: [2024-08-10 22:29:15,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_11-model_00-model_states.pt...
g0185: [2024-08-10 22:29:15,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_05-model_00-model_states.pt...
g0195: [2024-08-10 22:29:15,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_17-model_00-model_states.pt...
g0197: [2024-08-10 22:29:15,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_20-model_00-model_states.pt...
g0194: [2024-08-10 22:29:15,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_14-model_00-model_states.pt...
g0184: [2024-08-10 22:29:15,822] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_01-model_00-model_states.pt...
g0188: [2024-08-10 22:29:15,935] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_11-model_00-model_states.pt.
g0194: [2024-08-10 22:29:15,940] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_14-model_00-model_states.pt.
g0197: [2024-08-10 22:29:15,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_20-model_00-model_states.pt.
g0187: [2024-08-10 22:29:15,965] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_08-model_00-model_states.pt.
g0188: [2024-08-10 22:29:15,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_12-model_00-model_states.pt...
g0194: [2024-08-10 22:29:15,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_15-model_00-model_states.pt...
g0197: [2024-08-10 22:29:16,000] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_21-model_00-model_states.pt...
g0187: [2024-08-10 22:29:16,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_09-model_00-model_states.pt...
g0195: [2024-08-10 22:29:16,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_17-model_00-model_states.pt.
g0185: [2024-08-10 22:29:16,028] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_05-model_00-model_states.pt.
g0184: [2024-08-10 22:29:16,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_01-model_00-model_states.pt.
g0184: [2024-08-10 22:29:16,053] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_02-model_00-model_states.pt...
g0195: [2024-08-10 22:29:16,056] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_18-model_00-model_states.pt...
g0198: [2024-08-10 22:29:16,065] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_23-model_00-model_states.pt.
g0185: [2024-08-10 22:29:16,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_06-model_00-model_states.pt...
g0198: [2024-08-10 22:29:16,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 22:29:16,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_24-model_00-model_states.pt.
g0198: [2024-08-10 22:29:16,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_25-model_00-model_states.pt...
g0187: [2024-08-10 22:29:16,117] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_09-model_00-model_states.pt.
g0194: [2024-08-10 22:29:16,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_15-model_00-model_states.pt.
g0187: [2024-08-10 22:29:16,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_10-model_00-model_states.pt...
g0194: [2024-08-10 22:29:16,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_16-model_00-model_states.pt...
g0185: [2024-08-10 22:29:16,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 22:29:16,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_07-model_00-model_states.pt...
g0197: [2024-08-10 22:29:16,202] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_21-model_00-model_states.pt.
g0195: [2024-08-10 22:29:16,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_18-model_00-model_states.pt.
g0188: [2024-08-10 22:29:16,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_12-model_00-model_states.pt.
g0197: [2024-08-10 22:29:16,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_22-model_00-model_states.pt...
g0195: [2024-08-10 22:29:16,261] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_19-model_00-model_states.pt...
g0188: [2024-08-10 22:29:16,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_13-model_00-model_states.pt...
g0194: [2024-08-10 22:29:16,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 22:29:16,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_04_model_states.pt...
g0187: [2024-08-10 22:29:16,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 22:29:16,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_02_model_states.pt...
g0198: [2024-08-10 22:29:16,326] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 22:29:16,328] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_07_model_states.pt...
g0184: [2024-08-10 22:29:16,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_02-model_00-model_states.pt.
g0185: [2024-08-10 22:29:16,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 22:29:16,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_01_model_states.pt...
g0184: [2024-08-10 22:29:16,356] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_03-model_00-model_states.pt...
g0195: [2024-08-10 22:29:16,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 22:29:16,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_05_model_states.pt...
g0188: [2024-08-10 22:29:16,436] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 22:29:16,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_03_model_states.pt...
g0184: [2024-08-10 22:29:16,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_03-model_00-model_states.pt.
g0184: [2024-08-10 22:29:16,511] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_04-model_00-model_states.pt...
g0184: [2024-08-10 22:29:16,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 22:29:16,658] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_00_model_states.pt
g0184: [2024-08-10 22:29:16,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_00_model_states.pt...
g0197: [2024-08-10 22:29:16,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 22:29:16,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_06_model_states.pt...
g0187: [2024-08-10 22:29:18,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 22:29:18,658] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0194: [2024-08-10 22:29:18,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 22:29:18,666] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0185: [2024-08-10 22:29:18,737] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 22:29:18,738] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0195: [2024-08-10 22:29:18,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 22:29:18,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0188: [2024-08-10 22:29:18,896] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 22:29:18,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0198: [2024-08-10 22:29:18,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 22:29:18,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0197: [2024-08-10 22:29:19,274] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 22:29:19,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0184: [2024-08-10 22:29:20,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step31000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 22:29:20,575] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31000 is ready now!
g0184:   successfully saved checkpoint at iteration   31000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.66, Latency(second): 4.836
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4834.09, 4836.22)
g0184: [2024-08-10 22:30:08,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=31010, skipped=47, lr=[0.00019986023146656442, 0.00019986023146656442], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31010 loss: 0.7664 iter time (s): 4.745 samples/sec: 26.977
g0198:  iteration    31010/10000000 | consumed samples:      3969280 | consumed tokens:   8129085440 | elapsed time per iteration (ms): 48947.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.707661E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.615 | tokens per gpu per second (tgs): 167.364 | TFLOPs: 1.35 |
g0184: [2024-08-10 22:30:50,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=31020, skipped=47, lr=[0.0001998600885883818, 0.0001998600885883818], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31020 loss: 0.7487 iter time (s): 4.169 samples/sec: 30.706
g0198:  iteration    31020/10000000 | consumed samples:      3970560 | consumed tokens:   8131706880 | elapsed time per iteration (ms): 4205.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.679293E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.439 | tokens per gpu per second (tgs): 1948.102 | TFLOPs: 15.68 |
g0184: [2024-08-10 22:31:34,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=31030, skipped=47, lr=[0.00019985994563726179, 0.00019985994563726179], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31030 loss: 0.7498 iter time (s): 4.348 samples/sec: 29.441
g0198:  iteration    31030/10000000 | consumed samples:      3971840 | consumed tokens:   8134328320 | elapsed time per iteration (ms): 4393.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.670404E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.132 | tokens per gpu per second (tgs): 1864.462 | TFLOPs: 15.00 |
g0184: [2024-08-10 22:32:18,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=31040, skipped=47, lr=[0.0001998598026132045, 0.0001998598026132045], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31040 loss: 0.7122 iter time (s): 4.350 samples/sec: 29.425
g0198:  iteration    31040/10000000 | consumed samples:      3973120 | consumed tokens:   8136949760 | elapsed time per iteration (ms): 4382.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.705151E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.208 | tokens per gpu per second (tgs): 1869.296 | TFLOPs: 15.04 |
g0184: [2024-08-10 22:33:04,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=31050, skipped=47, lr=[0.00019985965951621005, 0.00019985965951621005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31050 loss: 0.7589 iter time (s): 4.585 samples/sec: 27.915
g0198:  iteration    31050/10000000 | consumed samples:      3974400 | consumed tokens:   8139571200 | elapsed time per iteration (ms): 4617.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.595751E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.721 | tokens per gpu per second (tgs): 1774.145 | TFLOPs: 14.28 |
g0184: [2024-08-10 22:33:52,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=31060, skipped=47, lr=[0.00019985951634627854, 0.00019985951634627854], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31060 loss: 0.7221 iter time (s): 4.810 samples/sec: 26.611
g0198:  iteration    31060/10000000 | consumed samples:      3975680 | consumed tokens:   8142192640 | elapsed time per iteration (ms): 4842.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.633528E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.434 | tokens per gpu per second (tgs): 1691.796 | TFLOPs: 13.61 |
g0184: [2024-08-10 22:34:40,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=31070, skipped=47, lr=[0.00019985937310341005, 0.00019985937310341005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31070 loss: 0.7760 iter time (s): 4.774 samples/sec: 26.810
g0198:  iteration    31070/10000000 | consumed samples:      3976960 | consumed tokens:   8144814080 | elapsed time per iteration (ms): 4806.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.713476E-01 | loss scale: 16384.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.632 | tokens per gpu per second (tgs): 1704.440 | TFLOPs: 13.72 |
g0184: [2024-08-10 22:35:27,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=31080, skipped=47, lr=[0.00019985922978760475, 0.00019985922978760475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31080 loss: 0.7948 iter time (s): 4.594 samples/sec: 27.865
g0198:  iteration    31080/10000000 | consumed samples:      3978240 | consumed tokens:   8147435520 | elapsed time per iteration (ms): 4625.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.778278E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.672 | tokens per gpu per second (tgs): 1771.001 | TFLOPs: 14.25 |
g0184: [2024-08-10 22:36:11,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=31090, skipped=47, lr=[0.0001998590863988627, 0.0001998590863988627], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31090 loss: 0.8022 iter time (s): 4.418 samples/sec: 28.971
g0198:  iteration    31090/10000000 | consumed samples:      3979520 | consumed tokens:   8150056960 | elapsed time per iteration (ms): 4450.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.742803E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.759 | tokens per gpu per second (tgs): 1840.596 | TFLOPs: 14.81 |
g0184: [2024-08-10 22:36:56,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=31100, skipped=47, lr=[0.00019985894293718406, 0.00019985894293718406], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31100 loss: 0.7571 iter time (s): 4.485 samples/sec: 28.540
g0198:  iteration    31100/10000000 | consumed samples:      3980800 | consumed tokens:   8152678400 | elapsed time per iteration (ms): 4517.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.749732E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.335 | tokens per gpu per second (tgs): 1813.462 | TFLOPs: 14.59 |
g0184: [2024-08-10 22:37:42,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=31110, skipped=47, lr=[0.00019985879940256888, 0.00019985879940256888], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31110 loss: 0.7844 iter time (s): 4.501 samples/sec: 28.436
g0198:  iteration    31110/10000000 | consumed samples:      3982080 | consumed tokens:   8155299840 | elapsed time per iteration (ms): 4576.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.685273E-01 | loss scale: 16384.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.969 | tokens per gpu per second (tgs): 1790.003 | TFLOPs: 14.40 |
g0184: [2024-08-10 22:38:29,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=31120, skipped=47, lr=[0.00019985865579501733, 0.00019985865579501733], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31120 loss: 0.7885 iter time (s): 4.637 samples/sec: 27.604
g0198:  iteration    31120/10000000 | consumed samples:      3983360 | consumed tokens:   8157921280 | elapsed time per iteration (ms): 4669.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.770844E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.414 | tokens per gpu per second (tgs): 1754.472 | TFLOPs: 14.12 |
g0184: [2024-08-10 22:39:15,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=31130, skipped=47, lr=[0.00019985851211452946, 0.00019985851211452946], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31130 loss: 0.7644 iter time (s): 4.556 samples/sec: 28.095
g0198:  iteration    31130/10000000 | consumed samples:      3984640 | consumed tokens:   8160542720 | elapsed time per iteration (ms): 4588.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.679993E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.896 | tokens per gpu per second (tgs): 1785.339 | TFLOPs: 14.37 |
g0184: [2024-08-10 22:39:58,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=31140, skipped=47, lr=[0.00019985836836110545, 0.00019985836836110545], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31140 loss: 0.7898 iter time (s): 4.330 samples/sec: 29.562
g0198:  iteration    31140/10000000 | consumed samples:      3985920 | consumed tokens:   8163164160 | elapsed time per iteration (ms): 4362.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.813412E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.342 | tokens per gpu per second (tgs): 1877.889 | TFLOPs: 15.11 |
g0184: [2024-08-10 22:40:50,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=31150, skipped=47, lr=[0.00019985822453474534, 0.00019985822453474534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31150 loss: 0.7413 iter time (s): 5.181 samples/sec: 24.704
g0198:  iteration    31150/10000000 | consumed samples:      3987200 | consumed tokens:   8165785600 | elapsed time per iteration (ms): 5214.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.688759E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.549 | tokens per gpu per second (tgs): 1571.112 | TFLOPs: 12.64 |
g0184: [2024-08-10 22:41:42,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=31160, skipped=47, lr=[0.0001998580806354493, 0.0001998580806354493], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31160 loss: 0.7723 iter time (s): 5.108 samples/sec: 25.057
g0198:  iteration    31160/10000000 | consumed samples:      3988480 | consumed tokens:   8168407040 | elapsed time per iteration (ms): 5143.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.712175E-01 | loss scale: 16384.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.888 | tokens per gpu per second (tgs): 1592.820 | TFLOPs: 12.82 |
g0184: [2024-08-10 22:42:29,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=31170, skipped=47, lr=[0.00019985793666321737, 0.00019985793666321737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31170 loss: 0.7758 iter time (s): 4.648 samples/sec: 27.537
g0198:  iteration    31170/10000000 | consumed samples:      3989760 | consumed tokens:   8171028480 | elapsed time per iteration (ms): 4683.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.676451E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.331 | tokens per gpu per second (tgs): 1749.168 | TFLOPs: 14.08 |
g0184: [2024-08-10 22:43:14,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=31180, skipped=47, lr=[0.00019985779261804973, 0.00019985779261804973], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31180 loss: 0.7785 iter time (s): 4.499 samples/sec: 28.454
g0198:  iteration    31180/10000000 | consumed samples:      3991040 | consumed tokens:   8173649920 | elapsed time per iteration (ms): 4530.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.677648E-01 | loss scale: 16384.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.252 | tokens per gpu per second (tgs): 1808.107 | TFLOPs: 14.55 |
g0184: [2024-08-10 22:43:59,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=31190, skipped=47, lr=[0.0001998576484999465, 0.0001998576484999465], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31190 loss: 0.7592 iter time (s): 4.427 samples/sec: 28.912
g0198:  iteration    31190/10000000 | consumed samples:      3992320 | consumed tokens:   8176271360 | elapsed time per iteration (ms): 4459.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.729123E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.704 | tokens per gpu per second (tgs): 1837.037 | TFLOPs: 14.78 |
g0184: [2024-08-10 22:44:43,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=31200, skipped=47, lr=[0.0001998575043089077, 0.0001998575043089077], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31200 loss: 0.7649 iter time (s): 4.408 samples/sec: 29.039
g0198:  iteration    31200/10000000 | consumed samples:      3993600 | consumed tokens:   8178892800 | elapsed time per iteration (ms): 4440.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.755809E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.828 | tokens per gpu per second (tgs): 1845.003 | TFLOPs: 14.85 |
g0184: [2024-08-10 22:45:26,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=31210, skipped=47, lr=[0.00019985736004493355, 0.00019985736004493355], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31210 loss: 0.7586 iter time (s): 4.258 samples/sec: 30.060
g0198:  iteration    31210/10000000 | consumed samples:      3994880 | consumed tokens:   8181514240 | elapsed time per iteration (ms): 4290.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.747888E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.836 | tokens per gpu per second (tgs): 1909.490 | TFLOPs: 15.37 |
g0184: [2024-08-10 22:46:12,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=31220, skipped=47, lr=[0.00019985721570802407, 0.00019985721570802407], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31220 loss: 0.7681 iter time (s): 4.582 samples/sec: 27.934
g0198:  iteration    31220/10000000 | consumed samples:      3996160 | consumed tokens:   8184135680 | elapsed time per iteration (ms): 4614.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.731971E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.740 | tokens per gpu per second (tgs): 1775.364 | TFLOPs: 14.29 |
g0184: [2024-08-10 22:46:56,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=31230, skipped=47, lr=[0.00019985707129817944, 0.00019985707129817944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31230 loss: 0.7721 iter time (s): 4.344 samples/sec: 29.464
g0198:  iteration    31230/10000000 | consumed samples:      3997440 | consumed tokens:   8186757120 | elapsed time per iteration (ms): 4376.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.764428E-01 | loss scale: 16384.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.247 | tokens per gpu per second (tgs): 1871.834 | TFLOPs: 15.06 |
g0184: [2024-08-10 22:47:39,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=31240, skipped=47, lr=[0.00019985692681539973, 0.00019985692681539973], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31240 loss: 0.7589 iter time (s): 4.329 samples/sec: 29.565
g0198:  iteration    31240/10000000 | consumed samples:      3998720 | consumed tokens:   8189378560 | elapsed time per iteration (ms): 4361.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.628846E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.348 | tokens per gpu per second (tgs): 1878.248 | TFLOPs: 15.11 |
g0184: [2024-08-10 22:48:27,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=31250, skipped=47, lr=[0.00019985678225968505, 0.00019985678225968505], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31250 loss: 0.7624 iter time (s): 4.724 samples/sec: 27.094
g0198:  iteration    31250/10000000 | consumed samples:      4000000 | consumed tokens:   8192000000 | elapsed time per iteration (ms): 4756.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.665928E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.911 | tokens per gpu per second (tgs): 1722.306 | TFLOPs: 13.86 |
g0184: [2024-08-10 22:49:10,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=31260, skipped=47, lr=[0.00019985663763103553, 0.00019985663763103553], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31260 loss: 0.7531 iter time (s): 4.302 samples/sec: 29.753
g0198:  iteration    31260/10000000 | consumed samples:      4001280 | consumed tokens:   8194621440 | elapsed time per iteration (ms): 4335.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.662604E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.524 | tokens per gpu per second (tgs): 1889.513 | TFLOPs: 15.21 |
g0184: [2024-08-10 22:49:55,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=31270, skipped=47, lr=[0.00019985649292945126, 0.00019985649292945126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31270 loss: 0.7664 iter time (s): 4.384 samples/sec: 29.197
g0198:  iteration    31270/10000000 | consumed samples:      4002560 | consumed tokens:   8197242880 | elapsed time per iteration (ms): 4417.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.816341E-01 | loss scale: 16384.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.974 | tokens per gpu per second (tgs): 1854.353 | TFLOPs: 14.92 |
g0184: [2024-08-10 22:50:39,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=31280, skipped=47, lr=[0.0001998563481549324, 0.0001998563481549324], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31280 loss: 0.7401 iter time (s): 4.420 samples/sec: 28.962
g0198:  iteration    31280/10000000 | consumed samples:      4003840 | consumed tokens:   8199864320 | elapsed time per iteration (ms): 4452.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.663433E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.750 | tokens per gpu per second (tgs): 1839.997 | TFLOPs: 14.81 |
g0184: [2024-08-10 22:51:22,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=31290, skipped=47, lr=[0.00019985620330747897, 0.00019985620330747897], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31290 loss: 0.7809 iter time (s): 4.230 samples/sec: 30.257
g0198:  iteration    31290/10000000 | consumed samples:      4005120 | consumed tokens:   8202485760 | elapsed time per iteration (ms): 4270.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.629205E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.972 | tokens per gpu per second (tgs): 1918.223 | TFLOPs: 15.44 |
g0184: [2024-08-10 22:52:07,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=31300, skipped=47, lr=[0.00019985605838709116, 0.00019985605838709116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31300 loss: 0.7968 iter time (s): 4.443 samples/sec: 28.808
g0198:  iteration    31300/10000000 | consumed samples:      4006400 | consumed tokens:   8205107200 | elapsed time per iteration (ms): 4475.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.777956E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.599 | tokens per gpu per second (tgs): 1830.355 | TFLOPs: 14.73 |
g0184: [2024-08-10 22:52:52,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=31310, skipped=47, lr=[0.0001998559133937691, 0.0001998559133937691], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31310 loss: 0.7510 iter time (s): 4.526 samples/sec: 28.283
g0198:  iteration    31310/10000000 | consumed samples:      4007680 | consumed tokens:   8207728640 | elapsed time per iteration (ms): 4560.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.624900E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.069 | tokens per gpu per second (tgs): 1796.440 | TFLOPs: 14.46 |
g0184: [2024-08-10 22:53:36,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=31320, skipped=47, lr=[0.0001998557683275128, 0.0001998557683275128], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31320 loss: 0.7631 iter time (s): 4.350 samples/sec: 29.425
g0198:  iteration    31320/10000000 | consumed samples:      4008960 | consumed tokens:   8210350080 | elapsed time per iteration (ms): 4382.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.649108E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.207 | tokens per gpu per second (tgs): 1869.222 | TFLOPs: 15.04 |
g0184: [2024-08-10 22:54:19,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=31330, skipped=47, lr=[0.00019985562318832248, 0.00019985562318832248], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31330 loss: 0.7803 iter time (s): 4.233 samples/sec: 30.242
g0198:  iteration    31330/10000000 | consumed samples:      4010240 | consumed tokens:   8212971520 | elapsed time per iteration (ms): 4265.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.785838E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.009 | tokens per gpu per second (tgs): 1920.566 | TFLOPs: 15.46 |
g0184: [2024-08-10 22:55:02,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=31340, skipped=47, lr=[0.00019985547797619818, 0.00019985547797619818], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31340 loss: 0.7547 iter time (s): 4.353 samples/sec: 29.402
g0198:  iteration    31340/10000000 | consumed samples:      4011520 | consumed tokens:   8215592960 | elapsed time per iteration (ms): 4386.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.761072E-01 | loss scale: 16384.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.183 | tokens per gpu per second (tgs): 1867.728 | TFLOPs: 15.03 |
g0184: [2024-08-10 22:55:51,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=31350, skipped=47, lr=[0.00019985533269114002, 0.00019985533269114002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31350 loss: 0.7596 iter time (s): 4.782 samples/sec: 26.764
g0198:  iteration    31350/10000000 | consumed samples:      4012800 | consumed tokens:   8218214400 | elapsed time per iteration (ms): 4815.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.697159E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.583 | tokens per gpu per second (tgs): 1701.321 | TFLOPs: 13.69 |
g0184: [2024-08-10 22:56:41,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=31360, skipped=47, lr=[0.00019985518733314818, 0.00019985518733314818], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31360 loss: 0.7602 iter time (s): 4.993 samples/sec: 25.634
g0198:  iteration    31360/10000000 | consumed samples:      4014080 | consumed tokens:   8220835840 | elapsed time per iteration (ms): 5026.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.691637E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.464 | tokens per gpu per second (tgs): 1629.670 | TFLOPs: 13.11 |
g0184: [2024-08-10 22:57:34,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=31370, skipped=47, lr=[0.00019985504190222266, 0.00019985504190222266], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31370 loss: 0.7654 iter time (s): 5.231 samples/sec: 24.471
g0198:  iteration    31370/10000000 | consumed samples:      4015360 | consumed tokens:   8223457280 | elapsed time per iteration (ms): 5264.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.707222E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.314 | tokens per gpu per second (tgs): 1556.114 | TFLOPs: 12.52 |
g0184: [2024-08-10 22:58:20,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=31380, skipped=47, lr=[0.00019985489639836367, 0.00019985489639836367], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31380 loss: 0.7855 iter time (s): 4.641 samples/sec: 27.581
g0198:  iteration    31380/10000000 | consumed samples:      4016640 | consumed tokens:   8226078720 | elapsed time per iteration (ms): 4673.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.709997E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.387 | tokens per gpu per second (tgs): 1752.752 | TFLOPs: 14.10 |
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 31389
g0198: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 31389
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 31389
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 31389
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: Grad overflow on iteration 31389
g0184: Grad overflow on iteration 31389
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 31389
g0198: Grad overflow on iteration 31389
g0185: Grad overflow on iteration 31389
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 31389
g0197: Grad overflow on iteration 31389
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 31389
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 31389
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 31389
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 31389
g0187: Grad overflow on iteration 31389
g0184: Grad overflow on iteration 31389
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 31389
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: Grad overflow on iteration 31389
g0198: Grad overflow on iteration 31389
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 31389
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: Grad overflow on iteration 31389
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 31389
g0195: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 31389
g0198: Grad overflow on iteration 31389
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 31389
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: Grad overflow on iteration 31389
g0198: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 31389
g0188: Grad overflow on iteration 31389
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: Grad overflow on iteration 31389
g0198: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 31389
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 31389
g0194: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 22:59:10,848] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0187: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 22:59:10,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-10 22:59:10,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=31390, skipped=48, lr=[0.00019985476538253251, 0.00019985476538253251], mom=[(0.9, 0.95), (0.9, 0.95)]
g0188: [2024-08-10 22:59:10,848] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-10 22:59:10,848] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: steps: 31390 loss: nan iter time (s): 4.976 samples/sec: 25.723
g0198:  iteration    31390/10000000 | consumed samples:      4017920 | consumed tokens:   8228700160 | elapsed time per iteration (ms): 5009.2 | learning rate: 1.999E-04 | global batch size:   128 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.553 | tokens per gpu per second (tgs): 1635.380 | TFLOPs: 13.16 |
g0184: [2024-08-10 22:59:55,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=31400, skipped=48, lr=[0.0001998546051718456, 0.0001998546051718456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31400 loss: 0.7597 iter time (s): 4.426 samples/sec: 28.918
g0198:  iteration    31400/10000000 | consumed samples:      4019200 | consumed tokens:   8231321600 | elapsed time per iteration (ms): 4463.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.729212E-01 | loss scale: 8192.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.675 | tokens per gpu per second (tgs): 1835.211 | TFLOPs: 14.77 |
g0184: [2024-08-10 23:00:41,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=31410, skipped=48, lr=[0.00019985445944918672, 0.00019985445944918672], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31410 loss: 0.7679 iter time (s): 4.619 samples/sec: 27.713
g0198:  iteration    31410/10000000 | consumed samples:      4020480 | consumed tokens:   8233943040 | elapsed time per iteration (ms): 4651.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.673377E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.521 | tokens per gpu per second (tgs): 1761.313 | TFLOPs: 14.17 |
g0184: [2024-08-10 23:01:29,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=31420, skipped=48, lr=[0.0001998543136535948, 0.0001998543136535948], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31420 loss: 0.7840 iter time (s): 4.714 samples/sec: 27.152
g0198:  iteration    31420/10000000 | consumed samples:      4021760 | consumed tokens:   8236564480 | elapsed time per iteration (ms): 4746.2 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.740209E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.969 | tokens per gpu per second (tgs): 1725.994 | TFLOPs: 13.89 |
g0184: [2024-08-10 23:02:12,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=31430, skipped=48, lr=[0.00019985416778506995, 0.00019985416778506995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31430 loss: 0.7782 iter time (s): 4.289 samples/sec: 29.845
g0198:  iteration    31430/10000000 | consumed samples:      4023040 | consumed tokens:   8239185920 | elapsed time per iteration (ms): 4321.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.731625E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.620 | tokens per gpu per second (tgs): 1895.666 | TFLOPs: 15.25 |
g0184: [2024-08-10 23:02:58,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=31440, skipped=48, lr=[0.00019985402184361225, 0.00019985402184361225], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31440 loss: 0.7894 iter time (s): 4.551 samples/sec: 28.128
g0198:  iteration    31440/10000000 | consumed samples:      4024320 | consumed tokens:   8241807360 | elapsed time per iteration (ms): 4582.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.764939E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.930 | tokens per gpu per second (tgs): 1787.523 | TFLOPs: 14.38 |
g0184: [2024-08-10 23:03:44,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=31450, skipped=48, lr=[0.0001998538758292218, 0.0001998538758292218], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31450 loss: 0.7647 iter time (s): 4.587 samples/sec: 27.904
g0198:  iteration    31450/10000000 | consumed samples:      4025600 | consumed tokens:   8244428800 | elapsed time per iteration (ms): 4619.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.789289E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.708 | tokens per gpu per second (tgs): 1773.301 | TFLOPs: 14.27 |
g0184: [2024-08-10 23:04:28,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=31460, skipped=48, lr=[0.00019985372974189877, 0.00019985372974189877], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31460 loss: 0.7769 iter time (s): 4.357 samples/sec: 29.379
g0198:  iteration    31460/10000000 | consumed samples:      4026880 | consumed tokens:   8247050240 | elapsed time per iteration (ms): 4390.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.725778E-01 | loss scale: 8192.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.154 | tokens per gpu per second (tgs): 1865.874 | TFLOPs: 15.01 |
g0184: [2024-08-10 23:05:15,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=31470, skipped=48, lr=[0.00019985358358164321, 0.00019985358358164321], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31470 loss: 0.7508 iter time (s): 4.670 samples/sec: 27.410
g0198:  iteration    31470/10000000 | consumed samples:      4028160 | consumed tokens:   8249671680 | elapsed time per iteration (ms): 4702.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.692550E-01 | loss scale: 8192.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.219 | tokens per gpu per second (tgs): 1742.041 | TFLOPs: 14.02 |
g0184: [2024-08-10 23:06:03,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=31480, skipped=48, lr=[0.00019985343734845527, 0.00019985343734845527], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31480 loss: 0.7680 iter time (s): 4.730 samples/sec: 27.062
g0198:  iteration    31480/10000000 | consumed samples:      4029440 | consumed tokens:   8252293120 | elapsed time per iteration (ms): 4778.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.704482E-01 | loss scale: 8192.0 | grad norm: 3.449 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.785 | tokens per gpu per second (tgs): 1714.257 | TFLOPs: 13.79 |
g0184: [2024-08-10 23:06:50,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=31490, skipped=48, lr=[0.00019985329104233506, 0.00019985329104233506], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31490 loss: 0.7723 iter time (s): 4.661 samples/sec: 27.464
g0198:  iteration    31490/10000000 | consumed samples:      4030720 | consumed tokens:   8254914560 | elapsed time per iteration (ms): 4692.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.758753E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.275 | tokens per gpu per second (tgs): 1745.599 | TFLOPs: 14.05 |
g0184: [2024-08-10 23:07:45,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=31500, skipped=48, lr=[0.0001998531446632827, 0.0001998531446632827], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31500 loss: 0.7497 iter time (s): 5.514 samples/sec: 23.215
g0198:  iteration    31500/10000000 | consumed samples:      4032000 | consumed tokens:   8257536000 | elapsed time per iteration (ms): 5546.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.746621E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.076 | tokens per gpu per second (tgs): 1476.888 | TFLOPs: 11.88 |
g0184: [2024-08-10 23:08:38,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=31510, skipped=48, lr=[0.00019985299821129827, 0.00019985299821129827], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31510 loss: 0.7953 iter time (s): 5.196 samples/sec: 24.635
g0198:  iteration    31510/10000000 | consumed samples:      4033280 | consumed tokens:   8260157440 | elapsed time per iteration (ms): 5228.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.749089E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.483 | tokens per gpu per second (tgs): 1566.922 | TFLOPs: 12.61 |
g0184: [2024-08-10 23:09:29,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=31520, skipped=48, lr=[0.0001998528516863819, 0.0001998528516863819], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31520 loss: 0.7303 iter time (s): 5.117 samples/sec: 25.014
g0198:  iteration    31520/10000000 | consumed samples:      4034560 | consumed tokens:   8262778880 | elapsed time per iteration (ms): 5149.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.699220E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.856 | tokens per gpu per second (tgs): 1590.786 | TFLOPs: 12.80 |
g0184: [2024-08-10 23:10:14,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=31530, skipped=48, lr=[0.00019985270508853374, 0.00019985270508853374], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31530 loss: 0.7777 iter time (s): 4.457 samples/sec: 28.721
g0198:  iteration    31530/10000000 | consumed samples:      4035840 | consumed tokens:   8265400320 | elapsed time per iteration (ms): 4488.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.730061E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.515 | tokens per gpu per second (tgs): 1824.970 | TFLOPs: 14.69 |
g0184: [2024-08-10 23:11:00,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=31540, skipped=48, lr=[0.00019985255841775386, 0.00019985255841775386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31540 loss: 0.7693 iter time (s): 4.602 samples/sec: 27.816
g0198:  iteration    31540/10000000 | consumed samples:      4037120 | consumed tokens:   8268021760 | elapsed time per iteration (ms): 4635.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.611585E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.616 | tokens per gpu per second (tgs): 1767.429 | TFLOPs: 14.22 |
g0184: [2024-08-10 23:11:44,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=31550, skipped=48, lr=[0.00019985241167404236, 0.00019985241167404236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31550 loss: 0.7678 iter time (s): 4.344 samples/sec: 29.463
g0198:  iteration    31550/10000000 | consumed samples:      4038400 | consumed tokens:   8270643200 | elapsed time per iteration (ms): 4376.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.722014E-01 | loss scale: 8192.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.246 | tokens per gpu per second (tgs): 1871.757 | TFLOPs: 15.06 |
g0184: [2024-08-10 23:12:29,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=31560, skipped=48, lr=[0.00019985226485739934, 0.00019985226485739934], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31560 loss: 0.7123 iter time (s): 4.429 samples/sec: 28.901
g0198:  iteration    31560/10000000 | consumed samples:      4039680 | consumed tokens:   8273264640 | elapsed time per iteration (ms): 4461.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.635237E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.693 | tokens per gpu per second (tgs): 1836.368 | TFLOPs: 14.78 |
g0184: [2024-08-10 23:13:16,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=31570, skipped=48, lr=[0.000199852117967825, 0.000199852117967825], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31570 loss: 0.7578 iter time (s): 4.743 samples/sec: 26.988
g0198:  iteration    31570/10000000 | consumed samples:      4040960 | consumed tokens:   8275886080 | elapsed time per iteration (ms): 4775.5 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.657386E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.803 | tokens per gpu per second (tgs): 1715.406 | TFLOPs: 13.80 |
g0184: [2024-08-10 23:14:03,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=31580, skipped=48, lr=[0.00019985197100531936, 0.00019985197100531936], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31580 loss: 0.7566 iter time (s): 4.648 samples/sec: 27.537
g0198:  iteration    31580/10000000 | consumed samples:      4042240 | consumed tokens:   8278507520 | elapsed time per iteration (ms): 4680.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.697413E-01 | loss scale: 8192.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.347 | tokens per gpu per second (tgs): 1750.187 | TFLOPs: 14.08 |
g0184: [2024-08-10 23:14:50,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=31590, skipped=48, lr=[0.0001998518239698826, 0.0001998518239698826], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31590 loss: 0.7789 iter time (s): 4.670 samples/sec: 27.409
g0198:  iteration    31590/10000000 | consumed samples:      4043520 | consumed tokens:   8281128960 | elapsed time per iteration (ms): 4712.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.700257E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.164 | tokens per gpu per second (tgs): 1738.489 | TFLOPs: 13.99 |
g0184: [2024-08-10 23:15:35,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=31600, skipped=48, lr=[0.0001998516768615148, 0.0001998516768615148], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31600 loss: 0.7768 iter time (s): 4.404 samples/sec: 29.068
g0198:  iteration    31600/10000000 | consumed samples:      4044800 | consumed tokens:   8283750400 | elapsed time per iteration (ms): 4438.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.664534E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.839 | tokens per gpu per second (tgs): 1845.703 | TFLOPs: 14.85 |
g0184: [2024-08-10 23:16:18,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=31610, skipped=48, lr=[0.00019985152968021608, 0.00019985152968021608], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31610 loss: 0.7754 iter time (s): 4.313 samples/sec: 29.680
g0198:  iteration    31610/10000000 | consumed samples:      4046080 | consumed tokens:   8286371840 | elapsed time per iteration (ms): 4345.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.582537E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.457 | tokens per gpu per second (tgs): 1885.216 | TFLOPs: 15.17 |
g0184: [2024-08-10 23:17:11,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=31620, skipped=48, lr=[0.00019985138242598654, 0.00019985138242598654], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31620 loss: 0.7791 iter time (s): 5.280 samples/sec: 24.245
g0198:  iteration    31620/10000000 | consumed samples:      4047360 | consumed tokens:   8288993280 | elapsed time per iteration (ms): 5312.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.695403E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.096 | tokens per gpu per second (tgs): 1542.167 | TFLOPs: 12.41 |
g0184: [2024-08-10 23:18:00,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=31630, skipped=48, lr=[0.0001998512350988263, 0.0001998512350988263], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31630 loss: 0.7317 iter time (s): 4.828 samples/sec: 26.513
g0198:  iteration    31630/10000000 | consumed samples:      4048640 | consumed tokens:   8291614720 | elapsed time per iteration (ms): 4860.9 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.693093E-01 | loss scale: 8192.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.333 | tokens per gpu per second (tgs): 1685.289 | TFLOPs: 13.56 |
g0184: [2024-08-10 23:18:46,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=31640, skipped=48, lr=[0.0001998510876987355, 0.0001998510876987355], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31640 loss: 0.7372 iter time (s): 4.571 samples/sec: 28.001
g0198:  iteration    31640/10000000 | consumed samples:      4049920 | consumed tokens:   8294236160 | elapsed time per iteration (ms): 4603.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.588844E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.804 | tokens per gpu per second (tgs): 1779.425 | TFLOPs: 14.32 |
g0184: [2024-08-10 23:19:34,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=31650, skipped=48, lr=[0.0001998509402257142, 0.0001998509402257142], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31650 loss: 0.7556 iter time (s): 4.770 samples/sec: 26.832
g0198:  iteration    31650/10000000 | consumed samples:      4051200 | consumed tokens:   8296857600 | elapsed time per iteration (ms): 4803.0 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.698957E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.650 | tokens per gpu per second (tgs): 1705.614 | TFLOPs: 13.73 |
g0184: [2024-08-10 23:20:21,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=31660, skipped=48, lr=[0.00019985079267976257, 0.00019985079267976257], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31660 loss: 0.7826 iter time (s): 4.690 samples/sec: 27.291
g0198:  iteration    31660/10000000 | consumed samples:      4052480 | consumed tokens:   8299479040 | elapsed time per iteration (ms): 4722.4 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.775684E-01 | loss scale: 8192.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.105 | tokens per gpu per second (tgs): 1734.697 | TFLOPs: 13.96 |
g0184: [2024-08-10 23:21:05,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=31670, skipped=48, lr=[0.0001998506450608807, 0.0001998506450608807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31670 loss: 0.7841 iter time (s): 4.383 samples/sec: 29.206
g0198:  iteration    31670/10000000 | consumed samples:      4053760 | consumed tokens:   8302100480 | elapsed time per iteration (ms): 4415.1 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.699400E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.992 | tokens per gpu per second (tgs): 1855.471 | TFLOPs: 14.93 |
g0184: [2024-08-10 23:21:49,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=31680, skipped=48, lr=[0.00019985049736906868, 0.00019985049736906868], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31680 loss: 0.7690 iter time (s): 4.357 samples/sec: 29.377
g0198:  iteration    31680/10000000 | consumed samples:      4055040 | consumed tokens:   8304721920 | elapsed time per iteration (ms): 4389.7 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.659119E-01 | loss scale: 8192.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.159 | tokens per gpu per second (tgs): 1866.186 | TFLOPs: 15.02 |
g0184: [2024-08-10 23:22:34,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=31690, skipped=48, lr=[0.00019985034960432667, 0.00019985034960432667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31690 loss: 0.7691 iter time (s): 4.392 samples/sec: 29.147
g0198:  iteration    31690/10000000 | consumed samples:      4056320 | consumed tokens:   8307343360 | elapsed time per iteration (ms): 4424.3 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.704682E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.931 | tokens per gpu per second (tgs): 1851.599 | TFLOPs: 14.90 |
g0184: [2024-08-10 23:23:18,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=31700, skipped=48, lr=[0.00019985020176665474, 0.00019985020176665474], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31700 loss: 0.7758 iter time (s): 4.424 samples/sec: 28.933
g0198:  iteration    31700/10000000 | consumed samples:      4057600 | consumed tokens:   8309964800 | elapsed time per iteration (ms): 4456.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.627668E-01 | loss scale: 8192.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.722 | tokens per gpu per second (tgs): 1838.181 | TFLOPs: 14.79 |
g0184: [2024-08-10 23:24:06,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=31710, skipped=48, lr=[0.00019985005385605303, 0.00019985005385605303], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31710 loss: 0.7854 iter time (s): 4.743 samples/sec: 26.987
g0198:  iteration    31710/10000000 | consumed samples:      4058880 | consumed tokens:   8312586240 | elapsed time per iteration (ms): 4779.6 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 7.680097E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.781 | tokens per gpu per second (tgs): 1713.962 | TFLOPs: 13.79 |
g0184: [2024-08-10 23:24:56,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=31720, skipped=48, lr=[0.00019984990587252165, 0.00019984990587252165], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31720 loss: 0.7812 iter time (s): 4.938 samples/sec: 25.921
g0198:  iteration    31720/10000000 | consumed samples:      4060160 | consumed tokens:   8315207680 | elapsed time per iteration (ms): 4978.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.770888E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.711 | tokens per gpu per second (tgs): 1645.510 | TFLOPs: 13.24 |
g0184: [2024-08-10 23:25:42,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=31730, skipped=48, lr=[0.0001998497578160607, 0.0001998497578160607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31730 loss: 0.8295 iter time (s): 4.632 samples/sec: 27.635
g0198:  iteration    31730/10000000 | consumed samples:      4061440 | consumed tokens:   8317829120 | elapsed time per iteration (ms): 4664.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.758557E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.441 | tokens per gpu per second (tgs): 1756.212 | TFLOPs: 14.13 |
g0184: [2024-08-10 23:26:34,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=31740, skipped=48, lr=[0.0001998496096866703, 0.0001998496096866703], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31740 loss: 0.7550 iter time (s): 5.122 samples/sec: 24.988
g0198:  iteration    31740/10000000 | consumed samples:      4062720 | consumed tokens:   8320450560 | elapsed time per iteration (ms): 5154.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.639708E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.833 | tokens per gpu per second (tgs): 1589.302 | TFLOPs: 12.79 |
g0184: [2024-08-10 23:27:19,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=31750, skipped=48, lr=[0.0001998494614843506, 0.0001998494614843506], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31750 loss: 0.7331 iter time (s): 4.507 samples/sec: 28.400
g0198:  iteration    31750/10000000 | consumed samples:      4064000 | consumed tokens:   8323072000 | elapsed time per iteration (ms): 4539.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.594195E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.197 | tokens per gpu per second (tgs): 1804.627 | TFLOPs: 14.52 |
g0184: [2024-08-10 23:28:04,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=31760, skipped=48, lr=[0.00019984931320910166, 0.00019984931320910166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31760 loss: 0.7758 iter time (s): 4.412 samples/sec: 29.010
g0198:  iteration    31760/10000000 | consumed samples:      4065280 | consumed tokens:   8325693440 | elapsed time per iteration (ms): 4444.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.676311E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.797 | tokens per gpu per second (tgs): 1843.023 | TFLOPs: 14.83 |
g0184: [2024-08-10 23:28:48,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=31770, skipped=48, lr=[0.0001998491648609236, 0.0001998491648609236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31770 loss: 0.7806 iter time (s): 4.436 samples/sec: 28.853
g0198:  iteration    31770/10000000 | consumed samples:      4066560 | consumed tokens:   8328314880 | elapsed time per iteration (ms): 4469.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.734909E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.639 | tokens per gpu per second (tgs): 1832.925 | TFLOPs: 14.75 |
g0184: [2024-08-10 23:29:33,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=31780, skipped=48, lr=[0.00019984901643981657, 0.00019984901643981657], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31780 loss: 0.7583 iter time (s): 4.415 samples/sec: 28.995
g0198:  iteration    31780/10000000 | consumed samples:      4067840 | consumed tokens:   8330936320 | elapsed time per iteration (ms): 4484.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.668919E-01 | loss scale: 8192.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.542 | tokens per gpu per second (tgs): 1826.663 | TFLOPs: 14.70 |
g0184: [2024-08-10 23:30:20,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=31790, skipped=48, lr=[0.00019984886794578065, 0.00019984886794578065], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31790 loss: 0.7704 iter time (s): 4.657 samples/sec: 27.488
g0198:  iteration    31790/10000000 | consumed samples:      4069120 | consumed tokens:   8333557760 | elapsed time per iteration (ms): 4689.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.657328E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.296 | tokens per gpu per second (tgs): 1746.918 | TFLOPs: 14.06 |
g0184: [2024-08-10 23:31:09,812] [INFO] [logging.py:96:log_dist] [Rank 0] step=31800, skipped=48, lr=[0.00019984871937881596, 0.00019984871937881596], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31800 loss: 0.7665 iter time (s): 4.883 samples/sec: 26.214
g0198:  iteration    31800/10000000 | consumed samples:      4070400 | consumed tokens:   8336179200 | elapsed time per iteration (ms): 4916.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.665164E-01 | loss scale: 8192.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.037 | tokens per gpu per second (tgs): 1666.365 | TFLOPs: 13.41 |
g0184: [2024-08-10 23:31:54,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=31810, skipped=48, lr=[0.00019984857073892265, 0.00019984857073892265], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31810 loss: 0.7360 iter time (s): 4.414 samples/sec: 28.997
g0198:  iteration    31810/10000000 | consumed samples:      4071680 | consumed tokens:   8338800640 | elapsed time per iteration (ms): 4446.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.737059E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.786 | tokens per gpu per second (tgs): 1842.324 | TFLOPs: 14.83 |
g0184: [2024-08-10 23:32:41,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=31820, skipped=48, lr=[0.00019984842202610078, 0.00019984842202610078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31820 loss: 0.7775 iter time (s): 4.693 samples/sec: 27.274
g0198:  iteration    31820/10000000 | consumed samples:      4072960 | consumed tokens:   8341422080 | elapsed time per iteration (ms): 4725.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.785929E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.087 | tokens per gpu per second (tgs): 1733.598 | TFLOPs: 13.95 |
g0184: [2024-08-10 23:33:26,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=31830, skipped=48, lr=[0.0001998482732403505, 0.0001998482732403505], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31830 loss: 0.7802 iter time (s): 4.512 samples/sec: 28.369
g0198:  iteration    31830/10000000 | consumed samples:      4074240 | consumed tokens:   8344043520 | elapsed time per iteration (ms): 4544.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.818236E-01 | loss scale: 8192.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.164 | tokens per gpu per second (tgs): 1802.484 | TFLOPs: 14.50 |
g0184: [2024-08-10 23:34:12,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=31840, skipped=48, lr=[0.0001998481243816719, 0.0001998481243816719], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31840 loss: 0.7761 iter time (s): 4.531 samples/sec: 28.252
g0198:  iteration    31840/10000000 | consumed samples:      4075520 | consumed tokens:   8346664960 | elapsed time per iteration (ms): 4563.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.554429E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.046 | tokens per gpu per second (tgs): 1794.954 | TFLOPs: 14.44 |
g0184: [2024-08-10 23:34:58,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=31850, skipped=48, lr=[0.00019984797545006516, 0.00019984797545006516], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31850 loss: 0.7683 iter time (s): 4.523 samples/sec: 28.297
g0198:  iteration    31850/10000000 | consumed samples:      4076800 | consumed tokens:   8349286400 | elapsed time per iteration (ms): 4555.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.676365E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.095 | tokens per gpu per second (tgs): 1798.110 | TFLOPs: 14.47 |
g0184: [2024-08-10 23:35:43,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=31860, skipped=48, lr=[0.0001998478264455303, 0.0001998478264455303], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31860 loss: 0.7895 iter time (s): 4.498 samples/sec: 28.455
g0198:  iteration    31860/10000000 | consumed samples:      4078080 | consumed tokens:   8351907840 | elapsed time per iteration (ms): 4530.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.805025E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.251 | tokens per gpu per second (tgs): 1808.076 | TFLOPs: 14.55 |
g0184: [2024-08-10 23:36:28,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=31870, skipped=48, lr=[0.0001998476773680675, 0.0001998476773680675], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31870 loss: 0.7598 iter time (s): 4.418 samples/sec: 28.971
g0198:  iteration    31870/10000000 | consumed samples:      4079360 | consumed tokens:   8354529280 | elapsed time per iteration (ms): 4464.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.741127E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.673 | tokens per gpu per second (tgs): 1835.081 | TFLOPs: 14.77 |
g0184: [2024-08-10 23:37:13,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=31880, skipped=48, lr=[0.00019984752821767686, 0.00019984752821767686], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31880 loss: 0.7612 iter time (s): 4.462 samples/sec: 28.686
g0198:  iteration    31880/10000000 | consumed samples:      4080640 | consumed tokens:   8357150720 | elapsed time per iteration (ms): 4494.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.638929E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.478 | tokens per gpu per second (tgs): 1822.588 | TFLOPs: 14.67 |
g0184: [2024-08-10 23:37:58,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=31890, skipped=48, lr=[0.00019984737899435845, 0.00019984737899435845], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31890 loss: 0.7630 iter time (s): 4.466 samples/sec: 28.659
g0198:  iteration    31890/10000000 | consumed samples:      4081920 | consumed tokens:   8359772160 | elapsed time per iteration (ms): 4498.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.650462E-01 | loss scale: 8192.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.452 | tokens per gpu per second (tgs): 1820.916 | TFLOPs: 14.65 |
g0184: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 23:38:03,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-10 23:38:03,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-10 23:38:44,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=31900, skipped=48, lr=[0.00019984722969811246, 0.00019984722969811246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31900 loss: 0.7313 iter time (s): 4.598 samples/sec: 27.839
g0198:  iteration    31900/10000000 | consumed samples:      4083200 | consumed tokens:   8362393600 | elapsed time per iteration (ms): 4630.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.733620E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.646 | tokens per gpu per second (tgs): 1769.321 | TFLOPs: 14.24 |
g0184: [2024-08-10 23:39:31,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=31910, skipped=48, lr=[0.0001998470803289389, 0.0001998470803289389], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31910 loss: 0.7771 iter time (s): 4.638 samples/sec: 27.599
g0198:  iteration    31910/10000000 | consumed samples:      4084480 | consumed tokens:   8365015040 | elapsed time per iteration (ms): 4670.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.643258E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.406 | tokens per gpu per second (tgs): 1753.986 | TFLOPs: 14.11 |
g0184: [2024-08-10 23:40:14,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=31920, skipped=48, lr=[0.00019984693088683804, 0.00019984693088683804], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31920 loss: 0.7615 iter time (s): 4.288 samples/sec: 29.851
g0198:  iteration    31920/10000000 | consumed samples:      4085760 | consumed tokens:   8367636480 | elapsed time per iteration (ms): 4320.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.747114E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.629 | tokens per gpu per second (tgs): 1896.272 | TFLOPs: 15.26 |
g0184: [2024-08-10 23:40:58,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=31930, skipped=48, lr=[0.0001998467813718099, 0.0001998467813718099], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31930 loss: 0.7712 iter time (s): 4.368 samples/sec: 29.304
g0198:  iteration    31930/10000000 | consumed samples:      4087040 | consumed tokens:   8370257920 | elapsed time per iteration (ms): 4400.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.600338E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.088 | tokens per gpu per second (tgs): 1861.656 | TFLOPs: 14.98 |
g0184: [2024-08-10 23:41:40,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=31940, skipped=48, lr=[0.00019984663178385456, 0.00019984663178385456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31940 loss: 0.7822 iter time (s): 4.223 samples/sec: 30.312
g0198:  iteration    31940/10000000 | consumed samples:      4088320 | consumed tokens:   8372879360 | elapsed time per iteration (ms): 4255.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.740376E-01 | loss scale: 16384.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.082 | tokens per gpu per second (tgs): 1925.253 | TFLOPs: 15.49 |
g0184: [2024-08-10 23:42:28,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=31950, skipped=48, lr=[0.00019984648212297218, 0.00019984648212297218], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31950 loss: 0.7446 iter time (s): 4.716 samples/sec: 27.141
g0198:  iteration    31950/10000000 | consumed samples:      4089600 | consumed tokens:   8375500800 | elapsed time per iteration (ms): 4748.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.691651E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.955 | tokens per gpu per second (tgs): 1725.102 | TFLOPs: 13.88 |
g0184: [2024-08-10 23:43:13,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=31960, skipped=48, lr=[0.00019984633238916288, 0.00019984633238916288], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31960 loss: 0.8165 iter time (s): 4.486 samples/sec: 28.532
g0198:  iteration    31960/10000000 | consumed samples:      4090880 | consumed tokens:   8378122240 | elapsed time per iteration (ms): 4518.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.718076E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.329 | tokens per gpu per second (tgs): 1813.025 | TFLOPs: 14.59 |
g0184: [2024-08-10 23:43:58,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=31970, skipped=48, lr=[0.00019984618258242677, 0.00019984618258242677], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31970 loss: 0.7718 iter time (s): 4.504 samples/sec: 28.418
g0198:  iteration    31970/10000000 | consumed samples:      4092160 | consumed tokens:   8380743680 | elapsed time per iteration (ms): 4536.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.718392E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.217 | tokens per gpu per second (tgs): 1805.887 | TFLOPs: 14.53 |
g0184: [2024-08-10 23:44:43,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=31980, skipped=48, lr=[0.00019984603270276397, 0.00019984603270276397], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31980 loss: 0.7371 iter time (s): 4.471 samples/sec: 28.631
g0198:  iteration    31980/10000000 | consumed samples:      4093440 | consumed tokens:   8383365120 | elapsed time per iteration (ms): 4502.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.646447E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.427 | tokens per gpu per second (tgs): 1819.326 | TFLOPs: 14.64 |
g0184: [2024-08-10 23:45:26,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=31990, skipped=48, lr=[0.00019984588275017456, 0.00019984588275017456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 31990 loss: 0.7452 iter time (s): 4.275 samples/sec: 29.942
g0198:  iteration    31990/10000000 | consumed samples:      4094720 | consumed tokens:   8385986560 | elapsed time per iteration (ms): 4307.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.661707E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.718 | tokens per gpu per second (tgs): 1901.966 | TFLOPs: 15.31 |
g0184: [2024-08-10 23:46:11,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=48, lr=[0.0001998457327246587, 0.0001998457327246587], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32000 loss: 0.7873 iter time (s): 4.380 samples/sec: 29.221
g0198:  iteration    32000/10000000 | consumed samples:      4096000 | consumed tokens:   8388608000 | elapsed time per iteration (ms): 4412.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.751368E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.007 | tokens per gpu per second (tgs): 1856.434 | TFLOPs: 14.94 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 32000 | lm loss value: 7.683033E-01 | lm loss PPL: 2.156105E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   32000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-10 23:53:43,999] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step32000 is about to be saved!
g0184: [2024-08-10 23:53:44,007] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0184: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0184: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0198: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0198: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0198: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0185: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0185: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0185: [2024-08-10 23:53:44,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0197: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0197: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0187: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0187: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0187: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0197: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0188: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0188: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0188: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0194: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0194: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0194: [2024-08-10 23:53:44,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0195: [2024-08-10 23:53:44,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0195: [2024-08-10 23:53:44,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0195: [2024-08-10 23:53:44,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0198: [2024-08-10 23:53:44,034] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_23-model_00-model_states.pt...
g0187: [2024-08-10 23:53:44,043] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_08-model_00-model_states.pt...
g0197: [2024-08-10 23:53:44,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_20-model_00-model_states.pt...
g0188: [2024-08-10 23:53:44,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_11-model_00-model_states.pt...
g0194: [2024-08-10 23:53:44,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_14-model_00-model_states.pt...
g0185: [2024-08-10 23:53:44,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_05-model_00-model_states.pt...
g0195: [2024-08-10 23:53:44,054] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_17-model_00-model_states.pt...
g0184: [2024-08-10 23:53:44,058] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_01-model_00-model_states.pt...
g0197: [2024-08-10 23:53:44,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_20-model_00-model_states.pt.
g0188: [2024-08-10 23:53:44,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_11-model_00-model_states.pt.
g0198: [2024-08-10 23:53:44,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_23-model_00-model_states.pt.
g0198: [2024-08-10 23:53:44,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_24-model_00-model_states.pt...
g0198: [2024-08-10 23:53:44,175] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_24-model_00-model_states.pt.
g0185: [2024-08-10 23:53:44,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_05-model_00-model_states.pt.
g0194: [2024-08-10 23:53:44,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_14-model_00-model_states.pt.
g0188: [2024-08-10 23:53:44,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_12-model_00-model_states.pt...
g0197: [2024-08-10 23:53:44,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_21-model_00-model_states.pt...
g0185: [2024-08-10 23:53:44,220] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_06-model_00-model_states.pt...
g0198: [2024-08-10 23:53:44,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_25-model_00-model_states.pt...
g0194: [2024-08-10 23:53:44,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_15-model_00-model_states.pt...
g0195: [2024-08-10 23:53:44,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_17-model_00-model_states.pt.
g0184: [2024-08-10 23:53:44,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_01-model_00-model_states.pt.
g0187: [2024-08-10 23:53:44,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_08-model_00-model_states.pt.
g0184: [2024-08-10 23:53:44,274] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_02-model_00-model_states.pt...
g0195: [2024-08-10 23:53:44,283] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_18-model_00-model_states.pt...
g0187: [2024-08-10 23:53:44,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_09-model_00-model_states.pt...
g0185: [2024-08-10 23:53:44,319] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_06-model_00-model_states.pt.
g0185: [2024-08-10 23:53:44,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_07-model_00-model_states.pt...
g0184: [2024-08-10 23:53:44,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_02-model_00-model_states.pt.
g0197: [2024-08-10 23:53:44,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_21-model_00-model_states.pt.
g0184: [2024-08-10 23:53:44,392] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_03-model_00-model_states.pt...
g0197: [2024-08-10 23:53:44,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_22-model_00-model_states.pt...
g0198: [2024-08-10 23:53:44,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_25-model_00-model_states.pt.
g0198: [2024-08-10 23:53:44,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_07_model_states.pt...
g0185: [2024-08-10 23:53:44,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_07-model_00-model_states.pt.
g0185: [2024-08-10 23:53:44,454] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_01_model_states.pt...
g0194: [2024-08-10 23:53:44,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_15-model_00-model_states.pt.
g0194: [2024-08-10 23:53:44,509] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_16-model_00-model_states.pt...
g0188: [2024-08-10 23:53:44,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_12-model_00-model_states.pt.
g0187: [2024-08-10 23:53:44,524] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_09-model_00-model_states.pt.
g0197: [2024-08-10 23:53:44,532] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_22-model_00-model_states.pt.
g0197: [2024-08-10 23:53:44,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_06_model_states.pt...
g0195: [2024-08-10 23:53:44,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_18-model_00-model_states.pt.
g0187: [2024-08-10 23:53:44,551] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_10-model_00-model_states.pt...
g0184: [2024-08-10 23:53:44,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_03-model_00-model_states.pt.
g0188: [2024-08-10 23:53:44,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_13-model_00-model_states.pt...
g0195: [2024-08-10 23:53:44,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_19-model_00-model_states.pt...
g0184: [2024-08-10 23:53:44,575] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_04-model_00-model_states.pt...
g0188: [2024-08-10 23:53:44,663] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_13-model_00-model_states.pt.
g0188: [2024-08-10 23:53:44,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_03_model_states.pt...
g0194: [2024-08-10 23:53:44,670] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_16-model_00-model_states.pt.
g0194: [2024-08-10 23:53:44,671] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_04_model_states.pt...
g0187: [2024-08-10 23:53:44,672] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_10-model_00-model_states.pt.
g0187: [2024-08-10 23:53:44,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_02_model_states.pt...
g0195: [2024-08-10 23:53:44,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_19-model_00-model_states.pt.
g0195: [2024-08-10 23:53:44,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_05_model_states.pt...
g0185: [2024-08-10 23:53:46,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_01_model_states.pt.
g0185: [2024-08-10 23:53:46,866] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0198: [2024-08-10 23:53:46,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_07_model_states.pt.
g0198: [2024-08-10 23:53:46,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0187: [2024-08-10 23:53:46,968] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_02_model_states.pt.
g0187: [2024-08-10 23:53:46,969] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0194: [2024-08-10 23:53:47,032] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_04_model_states.pt.
g0194: [2024-08-10 23:53:47,032] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0197: [2024-08-10 23:53:47,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_06_model_states.pt.
g0197: [2024-08-10 23:53:47,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0188: [2024-08-10 23:53:47,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_03_model_states.pt.
g0188: [2024-08-10 23:53:47,159] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0195: [2024-08-10 23:53:47,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_05_model_states.pt.
g0195: [2024-08-10 23:53:47,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0184: [2024-08-10 23:53:48,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/layer_04-model_00-model_states.pt.
g0184: [2024-08-10 23:53:48,557] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_00_model_states.pt
g0184: [2024-08-10 23:53:48,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_00_model_states.pt...
g0184: [2024-08-10 23:53:52,082] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step32000/mp_rank_00_model_states.pt.
g0184: [2024-08-10 23:53:52,082] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32000 is ready now!
g0184:   successfully saved checkpoint at iteration   32000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 2.78, Latency(second): 8.105
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (8103.55, 8104.85)
g0184: [2024-08-10 23:54:36,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=32010, skipped=48, lr=[0.00019984558262621649, 0.00019984558262621649], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32010 loss: 0.7446 iter time (s): 4.447 samples/sec: 28.781
g0198:  iteration    32010/10000000 | consumed samples:      4097280 | consumed tokens:   8391229440 | elapsed time per iteration (ms): 50578.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.624495E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.531 | tokens per gpu per second (tgs): 161.966 | TFLOPs: 1.30 |
g0184: [2024-08-10 23:55:24,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=32020, skipped=48, lr=[0.00019984543245484804, 0.00019984543245484804], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32020 loss: 0.7804 iter time (s): 4.733 samples/sec: 27.041
g0198:  iteration    32020/10000000 | consumed samples:      4098560 | consumed tokens:   8393850880 | elapsed time per iteration (ms): 4766.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.795193E-01 | loss scale: 16384.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.853 | tokens per gpu per second (tgs): 1718.577 | TFLOPs: 13.83 |
g0184: [2024-08-10 23:56:07,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=32030, skipped=48, lr=[0.0001998452822105535, 0.0001998452822105535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32030 loss: 0.7678 iter time (s): 4.292 samples/sec: 29.822
g0198:  iteration    32030/10000000 | consumed samples:      4099840 | consumed tokens:   8396472320 | elapsed time per iteration (ms): 4325.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.654392E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.591 | tokens per gpu per second (tgs): 1893.819 | TFLOPs: 15.24 |
g0184: [2024-08-10 23:56:49,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=32040, skipped=48, lr=[0.0001998451318933329, 0.0001998451318933329], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32040 loss: 0.7658 iter time (s): 4.181 samples/sec: 30.616
g0198:  iteration    32040/10000000 | consumed samples:      4101120 | consumed tokens:   8399093760 | elapsed time per iteration (ms): 4214.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.690985E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.369 | tokens per gpu per second (tgs): 1943.624 | TFLOPs: 15.64 |
g0184: [2024-08-10 23:57:33,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=32050, skipped=48, lr=[0.00019984498150318644, 0.00019984498150318644], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32050 loss: 0.7804 iter time (s): 4.331 samples/sec: 29.552
g0198:  iteration    32050/10000000 | consumed samples:      4102400 | consumed tokens:   8401715200 | elapsed time per iteration (ms): 4365.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.714284E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.320 | tokens per gpu per second (tgs): 1876.503 | TFLOPs: 15.10 |
g0184: [2024-08-10 23:58:19,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=32060, skipped=48, lr=[0.00019984483104011424, 0.00019984483104011424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32060 loss: 0.7738 iter time (s): 4.545 samples/sec: 28.160
g0198:  iteration    32060/10000000 | consumed samples:      4103680 | consumed tokens:   8404336640 | elapsed time per iteration (ms): 4582.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.608815E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.934 | tokens per gpu per second (tgs): 1787.785 | TFLOPs: 14.39 |
g0184: [2024-08-10 23:59:03,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=32070, skipped=48, lr=[0.00019984468050411632, 0.00019984468050411632], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32070 loss: 0.7573 iter time (s): 4.338 samples/sec: 29.504
g0198:  iteration    32070/10000000 | consumed samples:      4104960 | consumed tokens:   8406958080 | elapsed time per iteration (ms): 4371.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.731729E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.281 | tokens per gpu per second (tgs): 1873.964 | TFLOPs: 15.08 |
g0184: [2024-08-10 23:59:48,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=32080, skipped=48, lr=[0.0001998445298951929, 0.0001998445298951929], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32080 loss: 0.7639 iter time (s): 4.502 samples/sec: 28.430
g0198:  iteration    32080/10000000 | consumed samples:      4106240 | consumed tokens:   8409579520 | elapsed time per iteration (ms): 4534.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.694838E-01 | loss scale: 16384.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.226 | tokens per gpu per second (tgs): 1806.493 | TFLOPs: 14.54 |
g0184: [2024-08-11 00:00:32,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=32090, skipped=48, lr=[0.00019984437921334404, 0.00019984437921334404], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32090 loss: 0.7814 iter time (s): 4.373 samples/sec: 29.268
g0198:  iteration    32090/10000000 | consumed samples:      4107520 | consumed tokens:   8412200960 | elapsed time per iteration (ms): 4406.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.732567E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.049 | tokens per gpu per second (tgs): 1859.145 | TFLOPs: 14.96 |
g0184: [2024-08-11 00:01:14,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=32100, skipped=48, lr=[0.00019984422845856987, 0.00019984422845856987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32100 loss: 0.7709 iter time (s): 4.178 samples/sec: 30.633
g0198:  iteration    32100/10000000 | consumed samples:      4108800 | consumed tokens:   8414822400 | elapsed time per iteration (ms): 4214.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.669127E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.374 | tokens per gpu per second (tgs): 1943.957 | TFLOPs: 15.64 |
g0184: [2024-08-11 00:01:56,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=32110, skipped=48, lr=[0.0001998440776308705, 0.0001998440776308705], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32110 loss: 0.7693 iter time (s): 4.144 samples/sec: 30.892
g0198:  iteration    32110/10000000 | consumed samples:      4110080 | consumed tokens:   8417443840 | elapsed time per iteration (ms): 4176.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.702785E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.648 | tokens per gpu per second (tgs): 1961.475 | TFLOPs: 15.78 |
g0184: [2024-08-11 00:02:40,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=32120, skipped=48, lr=[0.00019984392673024604, 0.00019984392673024604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32120 loss: 0.7647 iter time (s): 4.340 samples/sec: 29.490
g0198:  iteration    32120/10000000 | consumed samples:      4111360 | consumed tokens:   8420065280 | elapsed time per iteration (ms): 4372.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.690958E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.273 | tokens per gpu per second (tgs): 1873.454 | TFLOPs: 15.08 |
g0184: [2024-08-11 00:03:25,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=32130, skipped=48, lr=[0.00019984377575669662, 0.00019984377575669662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32130 loss: 0.7600 iter time (s): 4.455 samples/sec: 28.733
g0198:  iteration    32130/10000000 | consumed samples:      4112640 | consumed tokens:   8422686720 | elapsed time per iteration (ms): 4487.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.660910E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.527 | tokens per gpu per second (tgs): 1825.724 | TFLOPs: 14.69 |
g0184: [2024-08-11 00:04:08,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=32140, skipped=48, lr=[0.00019984362471022239, 0.00019984362471022239], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32140 loss: 0.7442 iter time (s): 4.284 samples/sec: 29.876
g0198:  iteration    32140/10000000 | consumed samples:      4113920 | consumed tokens:   8425308160 | elapsed time per iteration (ms): 4316.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.577495E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.654 | tokens per gpu per second (tgs): 1897.886 | TFLOPs: 15.27 |
g0184: [2024-08-11 00:04:53,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=32150, skipped=48, lr=[0.00019984347359082338, 0.00019984347359082338], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32150 loss: 0.7531 iter time (s): 4.460 samples/sec: 28.697
g0198:  iteration    32150/10000000 | consumed samples:      4115200 | consumed tokens:   8427929600 | elapsed time per iteration (ms): 4492.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.725907E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.489 | tokens per gpu per second (tgs): 1823.320 | TFLOPs: 14.67 |
g0184: [2024-08-11 00:05:37,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=32160, skipped=48, lr=[0.0001998433223984998, 0.0001998433223984998], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32160 loss: 0.7869 iter time (s): 4.392 samples/sec: 29.146
g0198:  iteration    32160/10000000 | consumed samples:      4116480 | consumed tokens:   8430551040 | elapsed time per iteration (ms): 4424.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.728649E-01 | loss scale: 16384.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.928 | tokens per gpu per second (tgs): 1851.370 | TFLOPs: 14.90 |
g0184: [2024-08-11 00:06:21,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=32170, skipped=48, lr=[0.0001998431711332517, 0.0001998431711332517], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32170 loss: 0.7796 iter time (s): 4.359 samples/sec: 29.364
g0198:  iteration    32170/10000000 | consumed samples:      4117760 | consumed tokens:   8433172480 | elapsed time per iteration (ms): 4391.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.757250E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.149 | tokens per gpu per second (tgs): 1865.520 | TFLOPs: 15.01 |
g0184: [2024-08-11 00:07:04,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=32180, skipped=48, lr=[0.0001998430197950792, 0.0001998430197950792], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32180 loss: 0.7427 iter time (s): 4.334 samples/sec: 29.535
g0198:  iteration    32180/10000000 | consumed samples:      4119040 | consumed tokens:   8435793920 | elapsed time per iteration (ms): 4366.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.611741E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.316 | tokens per gpu per second (tgs): 1876.249 | TFLOPs: 15.10 |
g0184: [2024-08-11 00:07:47,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=32190, skipped=48, lr=[0.00019984286838398247, 0.00019984286838398247], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32190 loss: 0.7518 iter time (s): 4.244 samples/sec: 30.157
g0198:  iteration    32190/10000000 | consumed samples:      4120320 | consumed tokens:   8438415360 | elapsed time per iteration (ms): 4277.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.655500E-01 | loss scale: 16384.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.927 | tokens per gpu per second (tgs): 1915.323 | TFLOPs: 15.41 |
g0184: [2024-08-11 00:08:30,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=32200, skipped=48, lr=[0.00019984271689996156, 0.00019984271689996156], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32200 loss: 0.7942 iter time (s): 4.200 samples/sec: 30.480
g0198:  iteration    32200/10000000 | consumed samples:      4121600 | consumed tokens:   8441036800 | elapsed time per iteration (ms): 4232.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.786650E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.242 | tokens per gpu per second (tgs): 1935.474 | TFLOPs: 15.58 |
g0184: [2024-08-11 00:09:14,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=32210, skipped=48, lr=[0.00019984256534301664, 0.00019984256534301664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32210 loss: 0.7575 iter time (s): 4.431 samples/sec: 28.886
g0198:  iteration    32210/10000000 | consumed samples:      4122880 | consumed tokens:   8443658240 | elapsed time per iteration (ms): 4464.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.648119E-01 | loss scale: 16384.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.669 | tokens per gpu per second (tgs): 1834.838 | TFLOPs: 14.77 |
g0184: [2024-08-11 00:09:59,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=32220, skipped=48, lr=[0.0001998424137131478, 0.0001998424137131478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32220 loss: 0.7602 iter time (s): 4.414 samples/sec: 28.997
g0198:  iteration    32220/10000000 | consumed samples:      4124160 | consumed tokens:   8446279680 | elapsed time per iteration (ms): 4447.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.640584E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.784 | tokens per gpu per second (tgs): 1842.154 | TFLOPs: 14.82 |
g0184: [2024-08-11 00:10:43,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=32230, skipped=48, lr=[0.00019984226201035516, 0.00019984226201035516], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32230 loss: 0.7666 iter time (s): 4.391 samples/sec: 29.152
g0198:  iteration    32230/10000000 | consumed samples:      4125440 | consumed tokens:   8448901120 | elapsed time per iteration (ms): 4423.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.658089E-01 | loss scale: 16384.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.940 | tokens per gpu per second (tgs): 1852.154 | TFLOPs: 14.90 |
g0184: [2024-08-11 00:11:30,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=32240, skipped=48, lr=[0.00019984211023463882, 0.00019984211023463882], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32240 loss: 0.7391 iter time (s): 4.663 samples/sec: 27.451
g0198:  iteration    32240/10000000 | consumed samples:      4126720 | consumed tokens:   8451522560 | elapsed time per iteration (ms): 4695.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.666247E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.260 | tokens per gpu per second (tgs): 1744.662 | TFLOPs: 14.04 |
g0184: [2024-08-11 00:12:12,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=32250, skipped=48, lr=[0.00019984195838599892, 0.00019984195838599892], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32250 loss: 0.7952 iter time (s): 4.206 samples/sec: 30.430
g0198:  iteration    32250/10000000 | consumed samples:      4128000 | consumed tokens:   8454144000 | elapsed time per iteration (ms): 4238.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.637042E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.197 | tokens per gpu per second (tgs): 1932.613 | TFLOPs: 15.55 |
g0184: [2024-08-11 00:12:57,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=32260, skipped=48, lr=[0.00019984180646443557, 0.00019984180646443557], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32260 loss: 0.7791 iter time (s): 4.464 samples/sec: 28.673
g0198:  iteration    32260/10000000 | consumed samples:      4129280 | consumed tokens:   8456765440 | elapsed time per iteration (ms): 4496.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.833927E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.465 | tokens per gpu per second (tgs): 1821.762 | TFLOPs: 14.66 |
g0184: [2024-08-11 00:13:40,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=32270, skipped=48, lr=[0.0001998416544699489, 0.0001998416544699489], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32270 loss: 0.7844 iter time (s): 4.257 samples/sec: 30.067
g0198:  iteration    32270/10000000 | consumed samples:      4130560 | consumed tokens:   8459386880 | elapsed time per iteration (ms): 4308.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.677128E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.711 | tokens per gpu per second (tgs): 1901.534 | TFLOPs: 15.30 |
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32278
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32278
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32278
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32278
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32278
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32278
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32278
g0194: Grad overflow on iteration 32278
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32278
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32278
g0188: Grad overflow on iteration 32278
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32278
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32278
g0188: Grad overflow on iteration 32278
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32278
g0195: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32278
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32278
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 32278
g0187: Grad overflow on iteration 32278
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32278
g0185: Grad overflow on iteration 32278
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32278
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 32278
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32278
g0184: Grad overflow on iteration 32278
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: Grad overflow on iteration 32278
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:14:21,721] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0187: Grad overflow on iteration 32278
g0188: Grad overflow on iteration 32278
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32278
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32278
g0198: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32278
g0198: [2024-08-11 00:14:21,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 00:14:21,721] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32278
g0198: [2024-08-11 00:14:21,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 00:14:21,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 00:14:26,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=32280, skipped=49, lr=[0.00019984150240253903, 0.00019984150240253903], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32280 loss: 0.7834 iter time (s): 4.533 samples/sec: 28.238
g0198:  iteration    32280/10000000 | consumed samples:      4131840 | consumed tokens:   8462008320 | elapsed time per iteration (ms): 4567.0 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.027 | tokens per gpu per second (tgs): 1793.729 | TFLOPs: 14.43 |
g0184: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32280
g0184: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0184: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32280
g0184: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32280
g0197: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32280
g0197: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0194: Grad overflow on iteration 32280
g0194: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0197: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32280
g0187: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32280
g0188: Grad overflow on iteration 32280
g0194: Grad overflow on iteration 32280
g0187: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32280
g0185: Grad overflow on iteration 32280
g0195: Grad overflow on iteration 32280
g0197: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0195: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: Grad overflow on iteration 32280
g0184: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32280
g0198: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0194: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32280
g0194: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32280
g0194: Grad overflow on iteration 32280
g0197: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0184: Grad overflow on iteration 32280
g0187: Grad overflow on iteration 32280
g0185: Grad overflow on iteration 32280
g0187: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0197: Grad overflow on iteration 32280
g0187: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0195: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0184: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0195: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32280
g0198: Grad overflow on iteration 32280
g0188: Grad overflow on iteration 32280
g0187: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0187: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: Grad overflow on iteration 32280
g0198: Grad overflow on iteration 32280
g0188: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32280
g0184: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0184: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0197: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:14:30,633] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
g0195: Grad overflow on iteration 32280
g0198: Grad overflow on iteration 32280
g0195: [2024-08-11 00:14:30,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32280
g0195: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: Grad overflow on iteration 32280
g0195: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32280
g0188: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 00:14:30,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0184: [2024-08-11 00:15:10,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=32290, skipped=50, lr=[0.00019984135026220604, 0.00019984135026220604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32290 loss: 0.7439 iter time (s): 4.363 samples/sec: 29.336
g0198:  iteration    32290/10000000 | consumed samples:      4133120 | consumed tokens:   8464629760 | elapsed time per iteration (ms): 4396.2 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 4096.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.116 | tokens per gpu per second (tgs): 1863.429 | TFLOPs: 15.00 |
g0184: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32292
g0197: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32292
g0197: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0197: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32292
g0197: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32292
g0197: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0188: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32292
g0188: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32292
g0188: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32292
g0188: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0188: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0188: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32292
g0188: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0197: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32292
g0185: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32292
g0197: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0185: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32292
g0185: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0194: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32292
g0185: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0195: Grad overflow on iteration 32292
g0198: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0198: Grad overflow on iteration 32292
g0185: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32292
g0195: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0184: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32292
g0195: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32292
g0198: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32292
g0184: Grad overflow on iteration 32292
g0185: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0198: Grad overflow on iteration 32292
g0194: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32292
g0184: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0187: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0184: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0187: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0195: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32292
g0195: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0187: Grad overflow on iteration 32292
g0188: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0187: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32292
g0195: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0195: Grad overflow on iteration 32292
g0187: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0187: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0198: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0194: Grad overflow on iteration 32292
g0185: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0198: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:15:23,541] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32292
g0194: Grad overflow on iteration 32292
g0198: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0194: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0198: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32292
g0198: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0194: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0198: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0194: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32292
g0184: Grad overflow on iteration 32292
g0194: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0194: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0184: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0184: [2024-08-11 00:15:23,542] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
g0195: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0184: [2024-08-11 00:15:23,542] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32292
g0184: [2024-08-11 00:15:23,543] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0184: [2024-08-11 00:15:57,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=32300, skipped=51, lr=[0.00019984119804895004, 0.00019984119804895004], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32300 loss: 0.7726 iter time (s): 4.692 samples/sec: 27.282
g0198:  iteration    32300/10000000 | consumed samples:      4134400 | consumed tokens:   8467251200 | elapsed time per iteration (ms): 4724.0 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 2048.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.096 | tokens per gpu per second (tgs): 1734.130 | TFLOPs: 13.95 |
g0184: [2024-08-11 00:16:45,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=32310, skipped=51, lr=[0.00019984104576277123, 0.00019984104576277123], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32310 loss: 0.7550 iter time (s): 4.757 samples/sec: 26.909
g0198:  iteration    32310/10000000 | consumed samples:      4135680 | consumed tokens:   8469872640 | elapsed time per iteration (ms): 4788.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.666473E-01 | loss scale: 2048.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.729 | tokens per gpu per second (tgs): 1710.647 | TFLOPs: 13.77 |
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32316
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32316
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32316
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32316
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32316
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32316
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32316
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32316
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32316
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32316
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32316
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32316
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32316
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32316
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32316
g0188: Grad overflow on iteration 32316
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32316
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32316
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32316
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: Grad overflow on iteration 32316
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: Grad overflow on iteration 32316
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32316
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32316
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0197: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: Grad overflow on iteration 32316
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32316
g0185: Grad overflow on iteration 32316
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: Grad overflow on iteration 32316
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32316
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32316
g0185: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32316
g0198: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32316
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32316
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 00:17:17,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 00:17:17,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
g0184: [2024-08-11 00:17:29,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=32320, skipped=52, lr=[0.00019984089340366963, 0.00019984089340366963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32320 loss: 0.7626 iter time (s): 4.409 samples/sec: 29.033
g0198:  iteration    32320/10000000 | consumed samples:      4136960 | consumed tokens:   8472494080 | elapsed time per iteration (ms): 4441.1 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 1024.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.822 | tokens per gpu per second (tgs): 1844.588 | TFLOPs: 14.84 |
g0184: [2024-08-11 00:18:14,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=32330, skipped=52, lr=[0.00019984074097164543, 0.00019984074097164543], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32330 loss: 0.7583 iter time (s): 4.419 samples/sec: 28.966
g0198:  iteration    32330/10000000 | consumed samples:      4138240 | consumed tokens:   8475115520 | elapsed time per iteration (ms): 4451.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.636794E-01 | loss scale: 1024.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.756 | tokens per gpu per second (tgs): 1840.365 | TFLOPs: 14.81 |
g0184: [2024-08-11 00:18:57,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=32340, skipped=52, lr=[0.0001998405884666987, 0.0001998405884666987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32340 loss: 0.7645 iter time (s): 4.302 samples/sec: 29.751
g0198:  iteration    32340/10000000 | consumed samples:      4139520 | consumed tokens:   8477736960 | elapsed time per iteration (ms): 4334.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.675653E-01 | loss scale: 1024.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.530 | tokens per gpu per second (tgs): 1889.939 | TFLOPs: 15.21 |
g0184: [2024-08-11 00:19:43,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=32350, skipped=52, lr=[0.00019984043588882957, 0.00019984043588882957], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32350 loss: 0.7647 iter time (s): 4.540 samples/sec: 28.192
g0198:  iteration    32350/10000000 | consumed samples:      4140800 | consumed tokens:   8480358400 | elapsed time per iteration (ms): 4574.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.715270E-01 | loss scale: 1024.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.980 | tokens per gpu per second (tgs): 1790.698 | TFLOPs: 14.41 |
g0184: [2024-08-11 00:20:29,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=32360, skipped=52, lr=[0.00019984028323803815, 0.00019984028323803815], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32360 loss: 0.7663 iter time (s): 4.515 samples/sec: 28.353
g0198:  iteration    32360/10000000 | consumed samples:      4142080 | consumed tokens:   8482979840 | elapsed time per iteration (ms): 4557.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.591171E-01 | loss scale: 1024.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.087 | tokens per gpu per second (tgs): 1797.575 | TFLOPs: 14.47 |
g0184: [2024-08-11 00:21:13,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=32370, skipped=52, lr=[0.0001998401305143246, 0.0001998401305143246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32370 loss: 0.7718 iter time (s): 4.374 samples/sec: 29.264
g0198:  iteration    32370/10000000 | consumed samples:      4143360 | consumed tokens:   8485601280 | elapsed time per iteration (ms): 4408.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.729269E-01 | loss scale: 1024.0 | grad norm: 0.372 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.035 | tokens per gpu per second (tgs): 1858.227 | TFLOPs: 14.95 |
g0184: [2024-08-11 00:21:58,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=32380, skipped=52, lr=[0.00019983997771768898, 0.00019983997771768898], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32380 loss: 0.7714 iter time (s): 4.488 samples/sec: 28.521
g0198:  iteration    32380/10000000 | consumed samples:      4144640 | consumed tokens:   8488222720 | elapsed time per iteration (ms): 4520.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.687493E-01 | loss scale: 1024.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.318 | tokens per gpu per second (tgs): 1812.337 | TFLOPs: 14.58 |
g0184: [2024-08-11 00:22:41,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=32390, skipped=52, lr=[0.00019983982484813144, 0.00019983982484813144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32390 loss: 0.7430 iter time (s): 4.271 samples/sec: 29.968
g0198:  iteration    32390/10000000 | consumed samples:      4145920 | consumed tokens:   8490844160 | elapsed time per iteration (ms): 4303.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.564380E-01 | loss scale: 1024.0 | grad norm: 0.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.744 | tokens per gpu per second (tgs): 1903.598 | TFLOPs: 15.32 |
g0188: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32396
g0188: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32396
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0195: Grad overflow on iteration 32396
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32396
g0188: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32396
g0197: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0194: Grad overflow on iteration 32396
g0184: Grad overflow on iteration 32396
g0185: Grad overflow on iteration 32396
g0187: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32396
g0195: Grad overflow on iteration 32396
g0194: Grad overflow on iteration 32396
g0187: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32396
g0194: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0187: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32396
g0194: Grad overflow on iteration 32396
g0184: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0194: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0184: Grad overflow on iteration 32396
g0184: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32396
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0185: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0187: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0197: Grad overflow on iteration 32396
g0185: Grad overflow on iteration 32396
g0197: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0184: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0185: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0185: Grad overflow on iteration 32396
g0198: Grad overflow on iteration 32396
g0194: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0198: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32396
g0185: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0185: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0187: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0188: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0185: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0184: [2024-08-11 00:23:15,267] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
g0197: Grad overflow on iteration 32396
g0197: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0198: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32396
g0198: Grad overflow on iteration 32396
g0198: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0197: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0198: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0198: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0187: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32396
g0185: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 32396
g0194: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32396
g0187: [2024-08-11 00:23:15,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0198: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32396
g0185: [2024-08-11 00:23:15,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0194: [2024-08-11 00:23:15,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0198: [2024-08-11 00:23:15,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32396
g0195: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0197: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32396
g0197: [2024-08-11 00:23:15,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0188: Grad overflow on iteration 32396
g0188: [2024-08-11 00:23:15,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0184: [2024-08-11 00:23:15,267] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32396
g0184: [2024-08-11 00:23:15,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0184: [2024-08-11 00:23:28,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=32400, skipped=53, lr=[0.0001998396719056521, 0.0001998396719056521], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32400 loss: 0.7291 iter time (s): 4.698 samples/sec: 27.248
g0198:  iteration    32400/10000000 | consumed samples:      4147200 | consumed tokens:   8493465600 | elapsed time per iteration (ms): 4730.8 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 512.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.057 | tokens per gpu per second (tgs): 1731.622 | TFLOPs: 13.93 |
g0184: [2024-08-11 00:24:14,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=32410, skipped=53, lr=[0.00019983951889025107, 0.00019983951889025107], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32410 loss: 0.7330 iter time (s): 4.564 samples/sec: 28.046
g0198:  iteration    32410/10000000 | consumed samples:      4148480 | consumed tokens:   8496087040 | elapsed time per iteration (ms): 4598.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.661229E-01 | loss scale: 512.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.837 | tokens per gpu per second (tgs): 1781.544 | TFLOPs: 14.34 |
g0184: [2024-08-11 00:24:58,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=32420, skipped=53, lr=[0.00019983936580192846, 0.00019983936580192846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32420 loss: 0.7688 iter time (s): 4.323 samples/sec: 29.607
g0198:  iteration    32420/10000000 | consumed samples:      4149760 | consumed tokens:   8498708480 | elapsed time per iteration (ms): 4358.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.717536E-01 | loss scale: 512.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.368 | tokens per gpu per second (tgs): 1879.543 | TFLOPs: 15.12 |
g0184: [2024-08-11 00:25:40,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=32430, skipped=53, lr=[0.00019983921264068438, 0.00019983921264068438], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32430 loss: 0.7432 iter time (s): 4.195 samples/sec: 30.512
g0198:  iteration    32430/10000000 | consumed samples:      4151040 | consumed tokens:   8501329920 | elapsed time per iteration (ms): 4227.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.793541E-01 | loss scale: 512.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.277 | tokens per gpu per second (tgs): 1937.743 | TFLOPs: 15.59 |
g0184: [2024-08-11 00:26:23,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=32440, skipped=53, lr=[0.00019983905940651898, 0.00019983905940651898], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32440 loss: 0.7811 iter time (s): 4.271 samples/sec: 29.968
g0198:  iteration    32440/10000000 | consumed samples:      4152320 | consumed tokens:   8503951360 | elapsed time per iteration (ms): 4304.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.555386E-01 | loss scale: 512.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.735 | tokens per gpu per second (tgs): 1903.043 | TFLOPs: 15.31 |
g0184: [2024-08-11 00:27:07,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=32450, skipped=53, lr=[0.00019983890609943236, 0.00019983890609943236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32450 loss: 0.7597 iter time (s): 4.356 samples/sec: 29.388
g0198:  iteration    32450/10000000 | consumed samples:      4153600 | consumed tokens:   8506572800 | elapsed time per iteration (ms): 4389.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.669370E-01 | loss scale: 512.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.160 | tokens per gpu per second (tgs): 1866.230 | TFLOPs: 15.02 |
g0184: [2024-08-11 00:27:52,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=32460, skipped=53, lr=[0.0001998387527194246, 0.0001998387527194246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32460 loss: 0.7895 iter time (s): 4.498 samples/sec: 28.455
g0198:  iteration    32460/10000000 | consumed samples:      4154880 | consumed tokens:   8509194240 | elapsed time per iteration (ms): 4531.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.746982E-01 | loss scale: 512.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.245 | tokens per gpu per second (tgs): 1807.666 | TFLOPs: 14.55 |
g0184: [2024-08-11 00:28:35,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=32470, skipped=53, lr=[0.00019983859926649592, 0.00019983859926649592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32470 loss: 0.7558 iter time (s): 4.180 samples/sec: 30.621
g0198:  iteration    32470/10000000 | consumed samples:      4156160 | consumed tokens:   8511815680 | elapsed time per iteration (ms): 4212.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.650620E-01 | loss scale: 512.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.386 | tokens per gpu per second (tgs): 1944.718 | TFLOPs: 15.65 |
g0184: [2024-08-11 00:29:16,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=32480, skipped=53, lr=[0.00019983844574064633, 0.00019983844574064633], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32480 loss: 0.7631 iter time (s): 4.159 samples/sec: 30.774
g0198:  iteration    32480/10000000 | consumed samples:      4157440 | consumed tokens:   8514437120 | elapsed time per iteration (ms): 4191.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.638588E-01 | loss scale: 512.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.538 | tokens per gpu per second (tgs): 1954.416 | TFLOPs: 15.73 |
g0184: [2024-08-11 00:30:00,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=32490, skipped=53, lr=[0.00019983829214187602, 0.00019983829214187602], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32490 loss: 0.7492 iter time (s): 4.327 samples/sec: 29.581
g0198:  iteration    32490/10000000 | consumed samples:      4158720 | consumed tokens:   8517058560 | elapsed time per iteration (ms): 4360.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.720338E-01 | loss scale: 512.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.356 | tokens per gpu per second (tgs): 1878.806 | TFLOPs: 15.12 |
g0184: [2024-08-11 00:30:46,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=32500, skipped=53, lr=[0.00019983813847018506, 0.00019983813847018506], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32500 loss: 0.7430 iter time (s): 4.570 samples/sec: 28.008
g0198:  iteration    32500/10000000 | consumed samples:      4160000 | consumed tokens:   8519680000 | elapsed time per iteration (ms): 4603.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.650987E-01 | loss scale: 512.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.805 | tokens per gpu per second (tgs): 1779.518 | TFLOPs: 14.32 |
g0184: [2024-08-11 00:31:30,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=32510, skipped=53, lr=[0.0001998379847255736, 0.0001998379847255736], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32510 loss: 0.7608 iter time (s): 4.320 samples/sec: 29.629
g0198:  iteration    32510/10000000 | consumed samples:      4161280 | consumed tokens:   8522301440 | elapsed time per iteration (ms): 4353.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.664930E-01 | loss scale: 512.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.405 | tokens per gpu per second (tgs): 1881.935 | TFLOPs: 15.14 |
g0184: [2024-08-11 00:32:13,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=32520, skipped=53, lr=[0.00019983783090804172, 0.00019983783090804172], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32520 loss: 0.7655 iter time (s): 4.350 samples/sec: 29.426
g0198:  iteration    32520/10000000 | consumed samples:      4162560 | consumed tokens:   8524922880 | elapsed time per iteration (ms): 4383.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.680588E-01 | loss scale: 512.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.199 | tokens per gpu per second (tgs): 1868.747 | TFLOPs: 15.04 |
g0184: [2024-08-11 00:32:57,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=32530, skipped=53, lr=[0.00019983767701758958, 0.00019983767701758958], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32530 loss: 0.7807 iter time (s): 4.280 samples/sec: 29.909
g0198:  iteration    32530/10000000 | consumed samples:      4163840 | consumed tokens:   8527544320 | elapsed time per iteration (ms): 4322.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.609459E-01 | loss scale: 512.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.615 | tokens per gpu per second (tgs): 1895.376 | TFLOPs: 15.25 |
g0184: [2024-08-11 00:33:39,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=32540, skipped=53, lr=[0.00019983752305421728, 0.00019983752305421728], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32540 loss: 0.7510 iter time (s): 4.207 samples/sec: 30.423
g0198:  iteration    32540/10000000 | consumed samples:      4165120 | consumed tokens:   8530165760 | elapsed time per iteration (ms): 4239.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.559653E-01 | loss scale: 512.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.191 | tokens per gpu per second (tgs): 1932.200 | TFLOPs: 15.55 |
g0184: [2024-08-11 00:34:22,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=32550, skipped=53, lr=[0.00019983736901792496, 0.00019983736901792496], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32550 loss: 0.7605 iter time (s): 4.268 samples/sec: 29.988
g0198:  iteration    32550/10000000 | consumed samples:      4166400 | consumed tokens:   8532787200 | elapsed time per iteration (ms): 4304.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.666901E-01 | loss scale: 512.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.739 | tokens per gpu per second (tgs): 1903.288 | TFLOPs: 15.32 |
g0184: [2024-08-11 00:35:05,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=32560, skipped=53, lr=[0.0001998372149087127, 0.0001998372149087127], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32560 loss: 0.7420 iter time (s): 4.248 samples/sec: 30.133
g0198:  iteration    32560/10000000 | consumed samples:      4167680 | consumed tokens:   8535408640 | elapsed time per iteration (ms): 4280.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.583244E-01 | loss scale: 512.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.903 | tokens per gpu per second (tgs): 1913.774 | TFLOPs: 15.40 |
g0184: [2024-08-11 00:35:48,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=32570, skipped=53, lr=[0.0001998370607265806, 0.0001998370607265806], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32570 loss: 0.7318 iter time (s): 4.273 samples/sec: 29.958
g0198:  iteration    32570/10000000 | consumed samples:      4168960 | consumed tokens:   8538030080 | elapsed time per iteration (ms): 4306.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.638528E-01 | loss scale: 512.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.724 | tokens per gpu per second (tgs): 1902.313 | TFLOPs: 15.31 |
g0184: [2024-08-11 00:36:33,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=32580, skipped=53, lr=[0.00019983690647152886, 0.00019983690647152886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32580 loss: 0.8000 iter time (s): 4.476 samples/sec: 28.596
g0198:  iteration    32580/10000000 | consumed samples:      4170240 | consumed tokens:   8540651520 | elapsed time per iteration (ms): 4510.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.745679E-01 | loss scale: 512.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.376 | tokens per gpu per second (tgs): 1816.036 | TFLOPs: 14.61 |
g0184: [2024-08-11 00:37:16,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=32590, skipped=53, lr=[0.00019983675214355756, 0.00019983675214355756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32590 loss: 0.7945 iter time (s): 4.228 samples/sec: 30.278
g0198:  iteration    32590/10000000 | consumed samples:      4171520 | consumed tokens:   8543272960 | elapsed time per iteration (ms): 4261.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.730011E-01 | loss scale: 512.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.035 | tokens per gpu per second (tgs): 1922.222 | TFLOPs: 15.47 |
g0184: [2024-08-11 00:37:59,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=32600, skipped=53, lr=[0.00019983659774266684, 0.00019983659774266684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32600 loss: 0.8001 iter time (s): 4.297 samples/sec: 29.787
g0198:  iteration    32600/10000000 | consumed samples:      4172800 | consumed tokens:   8545894400 | elapsed time per iteration (ms): 4330.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.770327E-01 | loss scale: 512.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.556 | tokens per gpu per second (tgs): 1891.591 | TFLOPs: 15.22 |
g0184: [2024-08-11 00:38:42,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=32610, skipped=53, lr=[0.00019983644326885674, 0.00019983644326885674], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32610 loss: 0.7583 iter time (s): 4.236 samples/sec: 30.214
g0198:  iteration    32610/10000000 | consumed samples:      4174080 | consumed tokens:   8548515840 | elapsed time per iteration (ms): 4269.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.758267E-01 | loss scale: 512.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.982 | tokens per gpu per second (tgs): 1918.850 | TFLOPs: 15.44 |
g0184: [2024-08-11 00:39:25,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=32620, skipped=53, lr=[0.00019983628872212743, 0.00019983628872212743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32620 loss: 0.7610 iter time (s): 4.260 samples/sec: 30.047
g0198:  iteration    32620/10000000 | consumed samples:      4175360 | consumed tokens:   8551137280 | elapsed time per iteration (ms): 4294.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.770164E-01 | loss scale: 512.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.805 | tokens per gpu per second (tgs): 1907.537 | TFLOPs: 15.35 |
g0184: [2024-08-11 00:40:08,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=32630, skipped=53, lr=[0.00019983613410247904, 0.00019983613410247904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32630 loss: 0.7585 iter time (s): 4.347 samples/sec: 29.445
g0198:  iteration    32630/10000000 | consumed samples:      4176640 | consumed tokens:   8553758720 | elapsed time per iteration (ms): 4379.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.634749E-01 | loss scale: 512.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.228 | tokens per gpu per second (tgs): 1870.602 | TFLOPs: 15.05 |
g0184: [2024-08-11 00:40:50,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=32640, skipped=53, lr=[0.0001998359794099117, 0.0001998359794099117], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32640 loss: 0.7348 iter time (s): 4.166 samples/sec: 30.722
g0198:  iteration    32640/10000000 | consumed samples:      4177920 | consumed tokens:   8556380160 | elapsed time per iteration (ms): 4199.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.596681E-01 | loss scale: 512.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1950.947 | TFLOPs: 15.70 |
g0184: [2024-08-11 00:41:34,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=32650, skipped=53, lr=[0.0001998358246444255, 0.0001998358246444255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32650 loss: 0.7867 iter time (s): 4.271 samples/sec: 29.971
g0198:  iteration    32650/10000000 | consumed samples:      4179200 | consumed tokens:   8559001600 | elapsed time per iteration (ms): 4316.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.684500E-01 | loss scale: 512.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.654 | tokens per gpu per second (tgs): 1897.845 | TFLOPs: 15.27 |
g0184: [2024-08-11 00:42:17,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=32660, skipped=53, lr=[0.00019983566980602056, 0.00019983566980602056], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32660 loss: 0.7328 iter time (s): 4.313 samples/sec: 29.677
g0198:  iteration    32660/10000000 | consumed samples:      4180480 | consumed tokens:   8561623040 | elapsed time per iteration (ms): 4345.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.681055E-01 | loss scale: 512.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.454 | tokens per gpu per second (tgs): 1885.037 | TFLOPs: 15.17 |
g0184: [2024-08-11 00:42:59,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=32670, skipped=53, lr=[0.00019983551489469702, 0.00019983551489469702], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32670 loss: 0.7693 iter time (s): 4.128 samples/sec: 31.005
g0198:  iteration    32670/10000000 | consumed samples:      4181760 | consumed tokens:   8564244480 | elapsed time per iteration (ms): 4161.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.737303E-01 | loss scale: 512.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.758 | tokens per gpu per second (tgs): 1968.481 | TFLOPs: 15.84 |
g0184: [2024-08-11 00:43:41,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=32680, skipped=53, lr=[0.00019983535991045497, 0.00019983535991045497], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32680 loss: 0.7593 iter time (s): 4.182 samples/sec: 30.607
g0198:  iteration    32680/10000000 | consumed samples:      4183040 | consumed tokens:   8566865920 | elapsed time per iteration (ms): 4214.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.638671E-01 | loss scale: 512.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.373 | tokens per gpu per second (tgs): 1943.841 | TFLOPs: 15.64 |
g0184: [2024-08-11 00:44:26,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=32690, skipped=53, lr=[0.00019983520485329452, 0.00019983520485329452], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32690 loss: 0.7625 iter time (s): 4.537 samples/sec: 28.210
g0198:  iteration    32690/10000000 | consumed samples:      4184320 | consumed tokens:   8569487360 | elapsed time per iteration (ms): 4569.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.739035E-01 | loss scale: 512.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.012 | tokens per gpu per second (tgs): 1792.768 | TFLOPs: 14.43 |
g0184: [2024-08-11 00:45:09,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=32700, skipped=53, lr=[0.00019983504972321585, 0.00019983504972321585], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32700 loss: 0.7799 iter time (s): 4.262 samples/sec: 30.031
g0198:  iteration    32700/10000000 | consumed samples:      4185600 | consumed tokens:   8572108800 | elapsed time per iteration (ms): 4294.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.571106E-01 | loss scale: 512.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.806 | tokens per gpu per second (tgs): 1907.564 | TFLOPs: 15.35 |
g0184: [2024-08-11 00:45:52,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=32710, skipped=53, lr=[0.00019983489452021904, 0.00019983489452021904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32710 loss: 0.7698 iter time (s): 4.264 samples/sec: 30.020
g0198:  iteration    32710/10000000 | consumed samples:      4186880 | consumed tokens:   8574730240 | elapsed time per iteration (ms): 4296.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.677486E-01 | loss scale: 512.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.793 | tokens per gpu per second (tgs): 1906.768 | TFLOPs: 15.34 |
g0184: [2024-08-11 00:46:35,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=32720, skipped=53, lr=[0.0001998347392443042, 0.0001998347392443042], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32720 loss: 0.7860 iter time (s): 4.228 samples/sec: 30.278
g0198:  iteration    32720/10000000 | consumed samples:      4188160 | consumed tokens:   8577351680 | elapsed time per iteration (ms): 4261.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.662914E-01 | loss scale: 512.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.040 | tokens per gpu per second (tgs): 1922.539 | TFLOPs: 15.47 |
g0184: [2024-08-11 00:47:19,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=32730, skipped=53, lr=[0.00019983458389547147, 0.00019983458389547147], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32730 loss: 0.7324 iter time (s): 4.324 samples/sec: 29.601
g0198:  iteration    32730/10000000 | consumed samples:      4189440 | consumed tokens:   8579973120 | elapsed time per iteration (ms): 4356.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.715655E-01 | loss scale: 512.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.380 | tokens per gpu per second (tgs): 1880.313 | TFLOPs: 15.13 |
g0184: [2024-08-11 00:48:01,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=32740, skipped=53, lr=[0.00019983442847372094, 0.00019983442847372094], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32740 loss: 0.7972 iter time (s): 4.203 samples/sec: 30.453
g0198:  iteration    32740/10000000 | consumed samples:      4190720 | consumed tokens:   8582594560 | elapsed time per iteration (ms): 4235.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.714030E-01 | loss scale: 512.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.218 | tokens per gpu per second (tgs): 1933.967 | TFLOPs: 15.56 |
g0184: [2024-08-11 00:48:45,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=32750, skipped=53, lr=[0.00019983427297905277, 0.00019983427297905277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32750 loss: 0.7625 iter time (s): 4.386 samples/sec: 29.187
g0198:  iteration    32750/10000000 | consumed samples:      4192000 | consumed tokens:   8585216000 | elapsed time per iteration (ms): 4418.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.733873E-01 | loss scale: 512.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.973 | tokens per gpu per second (tgs): 1854.246 | TFLOPs: 14.92 |
g0184: [2024-08-11 00:49:27,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=32760, skipped=53, lr=[0.00019983411741146705, 0.00019983411741146705], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32760 loss: 0.7562 iter time (s): 4.191 samples/sec: 30.539
g0198:  iteration    32760/10000000 | consumed samples:      4193280 | consumed tokens:   8587837440 | elapsed time per iteration (ms): 4223.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.704449E-01 | loss scale: 512.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.305 | tokens per gpu per second (tgs): 1939.536 | TFLOPs: 15.61 |
g0184: [2024-08-11 00:50:12,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=32770, skipped=53, lr=[0.00019983396177096393, 0.00019983396177096393], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32770 loss: 0.7529 iter time (s): 4.417 samples/sec: 28.980
g0198:  iteration    32770/10000000 | consumed samples:      4194560 | consumed tokens:   8590458880 | elapsed time per iteration (ms): 4449.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.661136E-01 | loss scale: 512.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.769 | tokens per gpu per second (tgs): 1841.203 | TFLOPs: 14.82 |
g0184: [2024-08-11 00:50:55,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=32780, skipped=53, lr=[0.00019983380605754348, 0.00019983380605754348], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32780 loss: 0.7759 iter time (s): 4.330 samples/sec: 29.562
g0198:  iteration    32780/10000000 | consumed samples:      4195840 | consumed tokens:   8593080320 | elapsed time per iteration (ms): 4362.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.713154E-01 | loss scale: 512.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.342 | tokens per gpu per second (tgs): 1877.920 | TFLOPs: 15.11 |
g0184: [2024-08-11 00:51:39,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=32790, skipped=53, lr=[0.00019983365027120587, 0.00019983365027120587], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32790 loss: 0.7748 iter time (s): 4.285 samples/sec: 29.872
g0198:  iteration    32790/10000000 | consumed samples:      4197120 | consumed tokens:   8595701760 | elapsed time per iteration (ms): 4317.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.631430E-01 | loss scale: 512.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.648 | tokens per gpu per second (tgs): 1897.454 | TFLOPs: 15.27 |
g0184: [2024-08-11 00:52:23,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=32800, skipped=53, lr=[0.0001998334944119512, 0.0001998334944119512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32800 loss: 0.7594 iter time (s): 4.416 samples/sec: 28.984
g0198:  iteration    32800/10000000 | consumed samples:      4198400 | consumed tokens:   8598323200 | elapsed time per iteration (ms): 4451.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.602919E-01 | loss scale: 512.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.758 | tokens per gpu per second (tgs): 1840.490 | TFLOPs: 14.81 |
g0184: [2024-08-11 00:53:06,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=32810, skipped=53, lr=[0.00019983333847977956, 0.00019983333847977956], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32810 loss: 0.7810 iter time (s): 4.263 samples/sec: 30.026
g0198:  iteration    32810/10000000 | consumed samples:      4199680 | consumed tokens:   8600944640 | elapsed time per iteration (ms): 4295.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.677548E-01 | loss scale: 512.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.801 | tokens per gpu per second (tgs): 1907.247 | TFLOPs: 15.35 |
g0184: [2024-08-11 00:53:49,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=32820, skipped=53, lr=[0.00019983318247469113, 0.00019983318247469113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32820 loss: 0.7479 iter time (s): 4.278 samples/sec: 29.917
g0198:  iteration    32820/10000000 | consumed samples:      4200960 | consumed tokens:   8603566080 | elapsed time per iteration (ms): 4311.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.577034E-01 | loss scale: 512.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.691 | tokens per gpu per second (tgs): 1900.234 | TFLOPs: 15.29 |
g0184: [2024-08-11 00:54:33,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=32830, skipped=53, lr=[0.00019983302639668598, 0.00019983302639668598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32830 loss: 0.7669 iter time (s): 4.309 samples/sec: 29.706
g0198:  iteration    32830/10000000 | consumed samples:      4202240 | consumed tokens:   8606187520 | elapsed time per iteration (ms): 4341.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.775985E-01 | loss scale: 512.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.481 | tokens per gpu per second (tgs): 1886.813 | TFLOPs: 15.18 |
g0184: [2024-08-11 00:55:17,376] [INFO] [logging.py:96:log_dist] [Rank 0] step=32840, skipped=53, lr=[0.00019983287024576428, 0.00019983287024576428], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32840 loss: 0.7509 iter time (s): 4.392 samples/sec: 29.144
g0198:  iteration    32840/10000000 | consumed samples:      4203520 | consumed tokens:   8608808960 | elapsed time per iteration (ms): 4424.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.670873E-01 | loss scale: 512.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.932 | tokens per gpu per second (tgs): 1851.680 | TFLOPs: 14.90 |
g0184: [2024-08-11 00:56:03,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=32850, skipped=53, lr=[0.00019983271402192608, 0.00019983271402192608], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32850 loss: 0.7904 iter time (s): 4.584 samples/sec: 27.921
g0198:  iteration    32850/10000000 | consumed samples:      4204800 | consumed tokens:   8611430400 | elapsed time per iteration (ms): 4632.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.647466E-01 | loss scale: 512.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.632 | tokens per gpu per second (tgs): 1768.477 | TFLOPs: 14.23 |
g0184: [2024-08-11 00:56:45,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=32860, skipped=53, lr=[0.00019983255772517154, 0.00019983255772517154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32860 loss: 0.7800 iter time (s): 4.162 samples/sec: 30.755
g0198:  iteration    32860/10000000 | consumed samples:      4206080 | consumed tokens:   8614051840 | elapsed time per iteration (ms): 4194.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.686072E-01 | loss scale: 512.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.516 | tokens per gpu per second (tgs): 1952.994 | TFLOPs: 15.72 |
g0184: [2024-08-11 00:57:28,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=32870, skipped=53, lr=[0.0001998324013555008, 0.0001998324013555008], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32870 loss: 0.7865 iter time (s): 4.203 samples/sec: 30.454
g0198:  iteration    32870/10000000 | consumed samples:      4207360 | consumed tokens:   8616673280 | elapsed time per iteration (ms): 4235.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.702540E-01 | loss scale: 512.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.220 | tokens per gpu per second (tgs): 1934.051 | TFLOPs: 15.56 |
g0184: [2024-08-11 00:58:10,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=32880, skipped=53, lr=[0.00019983224491291394, 0.00019983224491291394], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32880 loss: 0.7935 iter time (s): 4.176 samples/sec: 30.651
g0198:  iteration    32880/10000000 | consumed samples:      4208640 | consumed tokens:   8619294720 | elapsed time per iteration (ms): 4209.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.713120E-01 | loss scale: 512.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.407 | tokens per gpu per second (tgs): 1946.043 | TFLOPs: 15.66 |
g0184: [2024-08-11 00:58:54,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=32890, skipped=53, lr=[0.00019983208839741113, 0.00019983208839741113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32890 loss: 0.7354 iter time (s): 4.378 samples/sec: 29.239
g0198:  iteration    32890/10000000 | consumed samples:      4209920 | consumed tokens:   8621916160 | elapsed time per iteration (ms): 4410.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.627028E-01 | loss scale: 512.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.021 | tokens per gpu per second (tgs): 1857.327 | TFLOPs: 14.95 |
g0184: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32896
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32896
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32896
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32896
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 32896
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0185: Grad overflow on iteration 32896
g0184: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32896
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32896
g0197: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0197: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 32896
g0198: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32896
g0197: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 32896
g0198: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0195: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0198: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32896
g0197: Grad overflow on iteration 32896
g0187: Grad overflow on iteration 32896
g0197: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32896
g0185: Grad overflow on iteration 32896
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0185: Grad overflow on iteration 32896
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0197: Grad overflow on iteration 32896
g0197: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0197: [2024-08-11 00:59:24,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0198: Grad overflow on iteration 32896
g0198: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0198: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 32896
g0198: Grad overflow on iteration 32896
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32896
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32896
g0198: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0194: Grad overflow on iteration 32896
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0198: Grad overflow on iteration 32896
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 00:59:24,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0184: Grad overflow on iteration 32896
g0184: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 32896
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 32896
g0184: [2024-08-11 00:59:24,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 512.0, reducing to 256.0
g0185: Grad overflow on iteration 32896
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0184: [2024-08-11 00:59:24,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0188: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0184: [2024-08-11 00:59:24,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0185: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0194: Grad overflow on iteration 32896
g0184: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0184: Grad overflow on iteration 32896
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 32896
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0194: [2024-08-11 00:59:24,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0184: [2024-08-11 00:59:24,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0198: [2024-08-11 00:59:24,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0184: [2024-08-11 00:59:37,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=32900, skipped=54, lr=[0.0001998319318089924, 0.0001998319318089924], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32900 loss: 0.7606 iter time (s): 4.275 samples/sec: 29.942
g0198:  iteration    32900/10000000 | consumed samples:      4211200 | consumed tokens:   8624537600 | elapsed time per iteration (ms): 4307.9 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 256.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.713 | tokens per gpu per second (tgs): 1901.617 | TFLOPs: 15.30 |
g0184: [2024-08-11 01:00:18,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=32910, skipped=54, lr=[0.00019983177514765798, 0.00019983177514765798], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32910 loss: 0.7887 iter time (s): 4.128 samples/sec: 31.007
g0198:  iteration    32910/10000000 | consumed samples:      4212480 | consumed tokens:   8627159040 | elapsed time per iteration (ms): 4161.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.634188E-01 | loss scale: 256.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.762 | tokens per gpu per second (tgs): 1968.764 | TFLOPs: 15.84 |
g0184: [2024-08-11 01:01:02,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=32920, skipped=54, lr=[0.00019983161841340794, 0.00019983161841340794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32920 loss: 0.7616 iter time (s): 4.291 samples/sec: 29.827
g0198:  iteration    32920/10000000 | consumed samples:      4213760 | consumed tokens:   8629780480 | elapsed time per iteration (ms): 4323.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.715103E-01 | loss scale: 256.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.604 | tokens per gpu per second (tgs): 1894.652 | TFLOPs: 15.25 |
g0184: [2024-08-11 01:01:44,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=32930, skipped=54, lr=[0.00019983146160624238, 0.00019983146160624238], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32930 loss: 0.7893 iter time (s): 4.245 samples/sec: 30.151
g0198:  iteration    32930/10000000 | consumed samples:      4215040 | consumed tokens:   8632401920 | elapsed time per iteration (ms): 4277.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.833085E-01 | loss scale: 256.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.922 | tokens per gpu per second (tgs): 1914.993 | TFLOPs: 15.41 |
g0184: [2024-08-11 01:02:28,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=32940, skipped=54, lr=[0.00019983130472616146, 0.00019983130472616146], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32940 loss: 0.8052 iter time (s): 4.363 samples/sec: 29.339
g0198:  iteration    32940/10000000 | consumed samples:      4216320 | consumed tokens:   8635023360 | elapsed time per iteration (ms): 4395.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.757986E-01 | loss scale: 256.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.124 | tokens per gpu per second (tgs): 1863.919 | TFLOPs: 15.00 |
g0184: [2024-08-11 01:03:11,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=32950, skipped=54, lr=[0.00019983114777316526, 0.00019983114777316526], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32950 loss: 0.7616 iter time (s): 4.269 samples/sec: 29.985
g0198:  iteration    32950/10000000 | consumed samples:      4217600 | consumed tokens:   8637644800 | elapsed time per iteration (ms): 4301.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.778002E-01 | loss scale: 256.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.758 | tokens per gpu per second (tgs): 1904.517 | TFLOPs: 15.33 |
g0184: [2024-08-11 01:03:54,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=32960, skipped=54, lr=[0.00019983099074725395, 0.00019983099074725395], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32960 loss: 0.8041 iter time (s): 4.193 samples/sec: 30.525
g0198:  iteration    32960/10000000 | consumed samples:      4218880 | consumed tokens:   8640266240 | elapsed time per iteration (ms): 4225.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.648373E-01 | loss scale: 256.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.289 | tokens per gpu per second (tgs): 1938.502 | TFLOPs: 15.60 |
g0184: [2024-08-11 01:04:36,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=32970, skipped=54, lr=[0.0001998308336484276, 0.0001998308336484276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32970 loss: 0.7637 iter time (s): 4.190 samples/sec: 30.551
g0198:  iteration    32970/10000000 | consumed samples:      4220160 | consumed tokens:   8642887680 | elapsed time per iteration (ms): 4222.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.606053E-01 | loss scale: 256.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.312 | tokens per gpu per second (tgs): 1939.974 | TFLOPs: 15.61 |
g0184: [2024-08-11 01:05:21,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=32980, skipped=54, lr=[0.00019983067647668637, 0.00019983067647668637], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32980 loss: 0.7697 iter time (s): 4.448 samples/sec: 28.775
g0198:  iteration    32980/10000000 | consumed samples:      4221440 | consumed tokens:   8645509120 | elapsed time per iteration (ms): 4480.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.693773E-01 | loss scale: 256.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.566 | tokens per gpu per second (tgs): 1828.203 | TFLOPs: 14.71 |
g0184: [2024-08-11 01:06:05,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=32990, skipped=54, lr=[0.00019983051923203035, 0.00019983051923203035], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 32990 loss: 0.7620 iter time (s): 4.372 samples/sec: 29.275
g0198:  iteration    32990/10000000 | consumed samples:      4222720 | consumed tokens:   8648130560 | elapsed time per iteration (ms): 4406.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.583930E-01 | loss scale: 256.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.048 | tokens per gpu per second (tgs): 1859.075 | TFLOPs: 14.96 |
g0184: [2024-08-11 01:06:47,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=54, lr=[0.00019983036191445968, 0.00019983036191445968], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33000 loss: 0.7250 iter time (s): 4.168 samples/sec: 30.712
g0198:  iteration    33000/10000000 | consumed samples:      4224000 | consumed tokens:   8650752000 | elapsed time per iteration (ms): 4201.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.737779E-01 | loss scale: 256.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.468 | tokens per gpu per second (tgs): 1949.957 | TFLOPs: 15.69 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 33000 | lm loss value: 7.684690E-01 | lm loss PPL: 2.156462E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   33000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 01:13:45,123] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step33000 is about to be saved!
g0184: [2024-08-11 01:13:45,128] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0184: [2024-08-11 01:13:45,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0198: [2024-08-11 01:13:45,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0198: [2024-08-11 01:13:45,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0198: [2024-08-11 01:13:45,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0185: [2024-08-11 01:13:45,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0185: [2024-08-11 01:13:45,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0185: [2024-08-11 01:13:45,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0188: [2024-08-11 01:13:45,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0188: [2024-08-11 01:13:45,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0184: [2024-08-11 01:13:45,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0188: [2024-08-11 01:13:45,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0194: [2024-08-11 01:13:45,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0194: [2024-08-11 01:13:45,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0194: [2024-08-11 01:13:45,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0195: [2024-08-11 01:13:45,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0195: [2024-08-11 01:13:45,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0195: [2024-08-11 01:13:45,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0197: [2024-08-11 01:13:45,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0197: [2024-08-11 01:13:45,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0187: [2024-08-11 01:13:45,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0187: [2024-08-11 01:13:45,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0187: [2024-08-11 01:13:45,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0197: [2024-08-11 01:13:45,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0198: [2024-08-11 01:13:45,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_23-model_00-model_states.pt...
g0184: [2024-08-11 01:13:45,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_01-model_00-model_states.pt...
g0197: [2024-08-11 01:13:45,164] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_20-model_00-model_states.pt...
g0187: [2024-08-11 01:13:45,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_08-model_00-model_states.pt...
g0188: [2024-08-11 01:13:45,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_11-model_00-model_states.pt...
g0185: [2024-08-11 01:13:45,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_05-model_00-model_states.pt...
g0194: [2024-08-11 01:13:45,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_14-model_00-model_states.pt...
g0195: [2024-08-11 01:13:45,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_17-model_00-model_states.pt...
g0198: [2024-08-11 01:13:45,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 01:13:45,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 01:13:45,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_24-model_00-model_states.pt.
g0188: [2024-08-11 01:13:45,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_11-model_00-model_states.pt.
g0195: [2024-08-11 01:13:45,308] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_17-model_00-model_states.pt.
g0187: [2024-08-11 01:13:45,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_08-model_00-model_states.pt.
g0197: [2024-08-11 01:13:45,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_20-model_00-model_states.pt.
g0198: [2024-08-11 01:13:45,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_25-model_00-model_states.pt...
g0194: [2024-08-11 01:13:45,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_14-model_00-model_states.pt.
g0188: [2024-08-11 01:13:45,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_12-model_00-model_states.pt...
g0195: [2024-08-11 01:13:45,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_18-model_00-model_states.pt...
g0187: [2024-08-11 01:13:45,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_09-model_00-model_states.pt...
g0197: [2024-08-11 01:13:45,364] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_21-model_00-model_states.pt...
g0194: [2024-08-11 01:13:45,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_15-model_00-model_states.pt...
g0195: [2024-08-11 01:13:45,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_18-model_00-model_states.pt.
g0188: [2024-08-11 01:13:45,503] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_12-model_00-model_states.pt.
g0195: [2024-08-11 01:13:45,505] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_19-model_00-model_states.pt...
g0197: [2024-08-11 01:13:45,526] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_21-model_00-model_states.pt.
g0185: [2024-08-11 01:13:45,531] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_05-model_00-model_states.pt.
g0188: [2024-08-11 01:13:45,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_13-model_00-model_states.pt...
g0194: [2024-08-11 01:13:45,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_15-model_00-model_states.pt.
g0197: [2024-08-11 01:13:45,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_22-model_00-model_states.pt...
g0185: [2024-08-11 01:13:45,569] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_06-model_00-model_states.pt...
g0194: [2024-08-11 01:13:45,578] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_16-model_00-model_states.pt...
g0187: [2024-08-11 01:13:45,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_09-model_00-model_states.pt.
g0187: [2024-08-11 01:13:45,647] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_10-model_00-model_states.pt...
g0184: [2024-08-11 01:13:45,648] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_01-model_00-model_states.pt.
g0188: [2024-08-11 01:13:45,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 01:13:45,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_03_model_states.pt...
g0184: [2024-08-11 01:13:45,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_02-model_00-model_states.pt...
g0185: [2024-08-11 01:13:45,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_06-model_00-model_states.pt.
g0198: [2024-08-11 01:13:45,697] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 01:13:45,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_07_model_states.pt...
g0194: [2024-08-11 01:13:45,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 01:13:45,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_04_model_states.pt...
g0185: [2024-08-11 01:13:45,713] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_07-model_00-model_states.pt...
g0187: [2024-08-11 01:13:45,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 01:13:45,768] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_02_model_states.pt...
g0184: [2024-08-11 01:13:45,794] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_02-model_00-model_states.pt.
g0195: [2024-08-11 01:13:45,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 01:13:45,807] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 01:13:45,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_03-model_00-model_states.pt...
g0197: [2024-08-11 01:13:45,825] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 01:13:45,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_06_model_states.pt...
g0185: [2024-08-11 01:13:45,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 01:13:45,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_01_model_states.pt...
g0184: [2024-08-11 01:13:45,948] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 01:13:45,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 01:13:46,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 01:13:46,067] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_00_model_states.pt
g0184: [2024-08-11 01:13:46,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 01:13:48,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 01:13:48,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0187: [2024-08-11 01:13:48,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 01:13:48,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0194: [2024-08-11 01:13:48,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 01:13:48,112] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0188: [2024-08-11 01:13:48,179] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 01:13:48,180] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0195: [2024-08-11 01:13:48,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 01:13:48,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0197: [2024-08-11 01:13:48,211] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 01:13:48,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0185: [2024-08-11 01:13:48,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 01:13:48,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0184: [2024-08-11 01:13:49,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step33000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 01:13:49,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33000 is ready now!
g0184:   successfully saved checkpoint at iteration   33000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.03, Latency(second): 4.477
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4477.10, 4477.27)
g0184: [2024-08-11 01:14:34,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=33010, skipped=54, lr=[0.0001998302045239745, 0.0001998302045239745], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33010 loss: 0.7402 iter time (s): 4.475 samples/sec: 28.603
g0198:  iteration    33010/10000000 | consumed samples:      4225280 | consumed tokens:   8653373440 | elapsed time per iteration (ms): 46739.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.640493E-01 | loss scale: 256.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.739 | tokens per gpu per second (tgs): 175.271 | TFLOPs: 1.41 |
g0184: [2024-08-11 01:15:18,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=33020, skipped=54, lr=[0.00019983004706057488, 0.00019983004706057488], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33020 loss: 0.7779 iter time (s): 4.370 samples/sec: 29.292
g0198:  iteration    33020/10000000 | consumed samples:      4226560 | consumed tokens:   8655994880 | elapsed time per iteration (ms): 4402.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.631490E-01 | loss scale: 256.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.075 | tokens per gpu per second (tgs): 1860.809 | TFLOPs: 14.97 |
g0184: [2024-08-11 01:16:01,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=33030, skipped=54, lr=[0.00019982988952426097, 0.00019982988952426097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33030 loss: 0.7650 iter time (s): 4.256 samples/sec: 30.076
g0198:  iteration    33030/10000000 | consumed samples:      4227840 | consumed tokens:   8658616320 | elapsed time per iteration (ms): 4288.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.714554E-01 | loss scale: 256.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.848 | tokens per gpu per second (tgs): 1910.285 | TFLOPs: 15.37 |
g0184: [2024-08-11 01:16:44,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=33040, skipped=54, lr=[0.0001998297319150329, 0.0001998297319150329], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33040 loss: 0.7478 iter time (s): 4.215 samples/sec: 30.367
g0198:  iteration    33040/10000000 | consumed samples:      4229120 | consumed tokens:   8661237760 | elapsed time per iteration (ms): 4247.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.669343E-01 | loss scale: 256.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.134 | tokens per gpu per second (tgs): 1928.567 | TFLOPs: 15.52 |
g0184: [2024-08-11 01:17:27,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=33050, skipped=54, lr=[0.00019982957423289078, 0.00019982957423289078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33050 loss: 0.7929 iter time (s): 4.302 samples/sec: 29.757
g0198:  iteration    33050/10000000 | consumed samples:      4230400 | consumed tokens:   8663859200 | elapsed time per iteration (ms): 4334.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.713894E-01 | loss scale: 256.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.533 | tokens per gpu per second (tgs): 1890.117 | TFLOPs: 15.21 |
g0184: [2024-08-11 01:18:11,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=33060, skipped=54, lr=[0.00019982941647783475, 0.00019982941647783475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33060 loss: 0.7782 iter time (s): 4.346 samples/sec: 29.453
g0198:  iteration    33060/10000000 | consumed samples:      4231680 | consumed tokens:   8666480640 | elapsed time per iteration (ms): 4379.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.640307E-01 | loss scale: 256.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.227 | tokens per gpu per second (tgs): 1870.518 | TFLOPs: 15.05 |
g0184: [2024-08-11 01:18:54,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=33070, skipped=54, lr=[0.0001998292586498649, 0.0001998292586498649], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33070 loss: 0.7708 iter time (s): 4.282 samples/sec: 29.896
g0198:  iteration    33070/10000000 | consumed samples:      4232960 | consumed tokens:   8669102080 | elapsed time per iteration (ms): 4314.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.696501E-01 | loss scale: 256.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.665 | tokens per gpu per second (tgs): 1898.538 | TFLOPs: 15.28 |
g0184: [2024-08-11 01:19:36,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=33080, skipped=54, lr=[0.00019982910074898134, 0.00019982910074898134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33080 loss: 0.7503 iter time (s): 4.168 samples/sec: 30.712
g0198:  iteration    33080/10000000 | consumed samples:      4234240 | consumed tokens:   8671723520 | elapsed time per iteration (ms): 4202.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.593093E-01 | loss scale: 256.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.459 | tokens per gpu per second (tgs): 1949.368 | TFLOPs: 15.69 |
g0184: [2024-08-11 01:20:18,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=33090, skipped=54, lr=[0.00019982894277518424, 0.00019982894277518424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33090 loss: 0.7559 iter time (s): 4.168 samples/sec: 30.711
g0198:  iteration    33090/10000000 | consumed samples:      4235520 | consumed tokens:   8674344960 | elapsed time per iteration (ms): 4200.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.704823E-01 | loss scale: 256.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.473 | tokens per gpu per second (tgs): 1950.249 | TFLOPs: 15.69 |
g0184: [2024-08-11 01:21:01,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=33100, skipped=54, lr=[0.0001998287847284737, 0.0001998287847284737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33100 loss: 0.7516 iter time (s): 4.316 samples/sec: 29.655
g0198:  iteration    33100/10000000 | consumed samples:      4236800 | consumed tokens:   8676966400 | elapsed time per iteration (ms): 4348.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.693500E-01 | loss scale: 256.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.433 | tokens per gpu per second (tgs): 1883.727 | TFLOPs: 15.16 |
g0184: [2024-08-11 01:21:46,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=33110, skipped=54, lr=[0.00019982862660884983, 0.00019982862660884983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33110 loss: 0.7629 iter time (s): 4.426 samples/sec: 28.917
g0198:  iteration    33110/10000000 | consumed samples:      4238080 | consumed tokens:   8679587840 | elapsed time per iteration (ms): 4459.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.582952E-01 | loss scale: 256.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.705 | tokens per gpu per second (tgs): 1837.093 | TFLOPs: 14.78 |
g0184: [2024-08-11 01:22:30,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=33120, skipped=54, lr=[0.00019982846841631275, 0.00019982846841631275], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33120 loss: 0.7617 iter time (s): 4.344 samples/sec: 29.466
g0198:  iteration    33120/10000000 | consumed samples:      4239360 | consumed tokens:   8682209280 | elapsed time per iteration (ms): 4376.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.633283E-01 | loss scale: 256.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.246 | tokens per gpu per second (tgs): 1871.764 | TFLOPs: 15.06 |
g0184: [2024-08-11 01:23:14,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=33130, skipped=54, lr=[0.00019982831015086263, 0.00019982831015086263], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33130 loss: 0.7557 iter time (s): 4.382 samples/sec: 29.213
g0198:  iteration    33130/10000000 | consumed samples:      4240640 | consumed tokens:   8684830720 | elapsed time per iteration (ms): 4419.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.713157E-01 | loss scale: 256.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.965 | tokens per gpu per second (tgs): 1853.783 | TFLOPs: 14.92 |
g0184: [2024-08-11 01:23:57,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=33140, skipped=54, lr=[0.00019982815181249954, 0.00019982815181249954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33140 loss: 0.7918 iter time (s): 4.326 samples/sec: 29.586
g0198:  iteration    33140/10000000 | consumed samples:      4241920 | consumed tokens:   8687452160 | elapsed time per iteration (ms): 4359.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.802099E-01 | loss scale: 256.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.358 | tokens per gpu per second (tgs): 1878.933 | TFLOPs: 15.12 |
g0184: [2024-08-11 01:24:41,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=33150, skipped=54, lr=[0.0001998279934012236, 0.0001998279934012236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33150 loss: 0.7686 iter time (s): 4.269 samples/sec: 29.986
g0198:  iteration    33150/10000000 | consumed samples:      4243200 | consumed tokens:   8690073600 | elapsed time per iteration (ms): 4308.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.767698E-01 | loss scale: 256.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.709 | tokens per gpu per second (tgs): 1901.358 | TFLOPs: 15.30 |
g0184: [2024-08-11 01:25:22,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=33160, skipped=54, lr=[0.00019982783491703497, 0.00019982783491703497], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33160 loss: 0.7948 iter time (s): 4.160 samples/sec: 30.767
g0198:  iteration    33160/10000000 | consumed samples:      4244480 | consumed tokens:   8692695040 | elapsed time per iteration (ms): 4193.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.694790E-01 | loss scale: 256.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.525 | tokens per gpu per second (tgs): 1953.585 | TFLOPs: 15.72 |
g0184: [2024-08-11 01:26:05,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=33170, skipped=54, lr=[0.00019982767635993374, 0.00019982767635993374], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33170 loss: 0.7696 iter time (s): 4.209 samples/sec: 30.413
g0198:  iteration    33170/10000000 | consumed samples:      4245760 | consumed tokens:   8695316480 | elapsed time per iteration (ms): 4243.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.643951E-01 | loss scale: 256.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.161 | tokens per gpu per second (tgs): 1930.300 | TFLOPs: 15.53 |
g0184: [2024-08-11 01:26:49,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=33180, skipped=54, lr=[0.00019982751772992005, 0.00019982751772992005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33180 loss: 0.7871 iter time (s): 4.369 samples/sec: 29.295
g0198:  iteration    33180/10000000 | consumed samples:      4247040 | consumed tokens:   8697937920 | elapsed time per iteration (ms): 4414.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.731254E-01 | loss scale: 256.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.993 | tokens per gpu per second (tgs): 1855.533 | TFLOPs: 14.93 |
g0184: [2024-08-11 01:27:32,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=33190, skipped=54, lr=[0.00019982735902699398, 0.00019982735902699398], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33190 loss: 0.7683 iter time (s): 4.306 samples/sec: 29.726
g0198:  iteration    33190/10000000 | consumed samples:      4248320 | consumed tokens:   8700559360 | elapsed time per iteration (ms): 4338.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.754877E-01 | loss scale: 256.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.503 | tokens per gpu per second (tgs): 1888.175 | TFLOPs: 15.19 |
g0184: [2024-08-11 01:28:15,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=33200, skipped=54, lr=[0.00019982720025115571, 0.00019982720025115571], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33200 loss: 0.7779 iter time (s): 4.185 samples/sec: 30.585
g0198:  iteration    33200/10000000 | consumed samples:      4249600 | consumed tokens:   8703180800 | elapsed time per iteration (ms): 4218.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.814042E-01 | loss scale: 256.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.342 | tokens per gpu per second (tgs): 1941.857 | TFLOPs: 15.63 |
g0184: [2024-08-11 01:28:58,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=33210, skipped=54, lr=[0.00019982704140240537, 0.00019982704140240537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33210 loss: 0.7823 iter time (s): 4.292 samples/sec: 29.826
g0198:  iteration    33210/10000000 | consumed samples:      4250880 | consumed tokens:   8705802240 | elapsed time per iteration (ms): 4324.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.667301E-01 | loss scale: 256.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.596 | tokens per gpu per second (tgs): 1894.131 | TFLOPs: 15.24 |
g0184: [2024-08-11 01:29:43,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=33220, skipped=54, lr=[0.000199826882480743, 0.000199826882480743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33220 loss: 0.7504 iter time (s): 4.462 samples/sec: 28.688
g0198:  iteration    33220/10000000 | consumed samples:      4252160 | consumed tokens:   8708423680 | elapsed time per iteration (ms): 4494.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.647839E-01 | loss scale: 256.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.480 | tokens per gpu per second (tgs): 1822.693 | TFLOPs: 14.67 |
g0184: [2024-08-11 01:30:24,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=33230, skipped=54, lr=[0.0001998267234861688, 0.0001998267234861688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33230 loss: 0.7377 iter time (s): 4.131 samples/sec: 30.986
g0198:  iteration    33230/10000000 | consumed samples:      4253440 | consumed tokens:   8711045120 | elapsed time per iteration (ms): 4163.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.683899E-01 | loss scale: 256.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.745 | tokens per gpu per second (tgs): 1967.660 | TFLOPs: 15.83 |
g0184: [2024-08-11 01:31:09,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=33240, skipped=54, lr=[0.00019982656441868283, 0.00019982656441868283], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33240 loss: 0.7612 iter time (s): 4.463 samples/sec: 28.681
g0198:  iteration    33240/10000000 | consumed samples:      4254720 | consumed tokens:   8713666560 | elapsed time per iteration (ms): 4495.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.704727E-01 | loss scale: 256.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.473 | tokens per gpu per second (tgs): 1822.277 | TFLOPs: 14.66 |
g0184: [2024-08-11 01:31:52,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=33250, skipped=54, lr=[0.0001998264052782853, 0.0001998264052782853], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33250 loss: 0.7819 iter time (s): 4.231 samples/sec: 30.254
g0198:  iteration    33250/10000000 | consumed samples:      4256000 | consumed tokens:   8716288000 | elapsed time per iteration (ms): 4264.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.645632E-01 | loss scale: 256.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.018 | tokens per gpu per second (tgs): 1921.140 | TFLOPs: 15.46 |
g0184: [2024-08-11 01:32:35,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=33260, skipped=54, lr=[0.00019982624606497624, 0.00019982624606497624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33260 loss: 0.7629 iter time (s): 4.287 samples/sec: 29.860
g0198:  iteration    33260/10000000 | consumed samples:      4257280 | consumed tokens:   8718909440 | elapsed time per iteration (ms): 4319.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.744551E-01 | loss scale: 256.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.632 | tokens per gpu per second (tgs): 1896.461 | TFLOPs: 15.26 |
g0184: [2024-08-11 01:33:19,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=33270, skipped=54, lr=[0.00019982608677875585, 0.00019982608677875585], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33270 loss: 0.7552 iter time (s): 4.299 samples/sec: 29.773
g0198:  iteration    33270/10000000 | consumed samples:      4258560 | consumed tokens:   8721530880 | elapsed time per iteration (ms): 4332.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.709136E-01 | loss scale: 256.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.542 | tokens per gpu per second (tgs): 1890.704 | TFLOPs: 15.21 |
g0184: [2024-08-11 01:34:04,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=33280, skipped=54, lr=[0.0001998259274196242, 0.0001998259274196242], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33280 loss: 0.7921 iter time (s): 4.470 samples/sec: 28.632
g0198:  iteration    33280/10000000 | consumed samples:      4259840 | consumed tokens:   8724152320 | elapsed time per iteration (ms): 4503.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.679629E-01 | loss scale: 256.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.426 | tokens per gpu per second (tgs): 1819.239 | TFLOPs: 14.64 |
g0184: [2024-08-11 01:34:46,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=33290, skipped=54, lr=[0.0001998257679875814, 0.0001998257679875814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33290 loss: 0.7714 iter time (s): 4.208 samples/sec: 30.421
g0198:  iteration    33290/10000000 | consumed samples:      4261120 | consumed tokens:   8726773760 | elapsed time per iteration (ms): 4240.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.612759E-01 | loss scale: 256.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.188 | tokens per gpu per second (tgs): 1932.001 | TFLOPs: 15.55 |
g0184: [2024-08-11 01:35:28,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=33300, skipped=54, lr=[0.00019982560848262758, 0.00019982560848262758], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33300 loss: 0.7513 iter time (s): 4.188 samples/sec: 30.565
g0198:  iteration    33300/10000000 | consumed samples:      4262400 | consumed tokens:   8729395200 | elapsed time per iteration (ms): 4220.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.680186E-01 | loss scale: 256.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.327 | tokens per gpu per second (tgs): 1940.933 | TFLOPs: 15.62 |
g0184: [2024-08-11 01:36:09,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=33310, skipped=54, lr=[0.0001998254489047629, 0.0001998254489047629], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33310 loss: 0.7866 iter time (s): 4.055 samples/sec: 31.570
g0198:  iteration    33310/10000000 | consumed samples:      4263680 | consumed tokens:   8732016640 | elapsed time per iteration (ms): 4087.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.776927E-01 | loss scale: 256.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.314 | tokens per gpu per second (tgs): 2004.091 | TFLOPs: 16.13 |
g0184: [2024-08-11 01:36:52,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=33320, skipped=54, lr=[0.0001998252892539875, 0.0001998252892539875], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33320 loss: 0.7437 iter time (s): 4.280 samples/sec: 29.909
g0198:  iteration    33320/10000000 | consumed samples:      4264960 | consumed tokens:   8734638080 | elapsed time per iteration (ms): 4312.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.716397E-01 | loss scale: 256.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.681 | tokens per gpu per second (tgs): 1899.592 | TFLOPs: 15.29 |
g0184: [2024-08-11 01:37:36,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=33330, skipped=54, lr=[0.00019982512953030146, 0.00019982512953030146], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33330 loss: 0.7338 iter time (s): 4.303 samples/sec: 29.744
g0198:  iteration    33330/10000000 | consumed samples:      4266240 | consumed tokens:   8737259520 | elapsed time per iteration (ms): 4337.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.674953E-01 | loss scale: 256.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.513 | tokens per gpu per second (tgs): 1888.859 | TFLOPs: 15.20 |
g0184: [2024-08-11 01:38:18,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=33340, skipped=54, lr=[0.0001998249697337049, 0.0001998249697337049], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33340 loss: 0.7952 iter time (s): 4.246 samples/sec: 30.148
g0198:  iteration    33340/10000000 | consumed samples:      4267520 | consumed tokens:   8739880960 | elapsed time per iteration (ms): 4278.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.642639E-01 | loss scale: 256.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.915 | tokens per gpu per second (tgs): 1914.549 | TFLOPs: 15.41 |
g0184: [2024-08-11 01:39:02,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=33350, skipped=54, lr=[0.00019982480986419795, 0.00019982480986419795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33350 loss: 0.7535 iter time (s): 4.282 samples/sec: 29.891
g0198:  iteration    33350/10000000 | consumed samples:      4268800 | consumed tokens:   8742502400 | elapsed time per iteration (ms): 4315.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.730746E-01 | loss scale: 256.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.664 | tokens per gpu per second (tgs): 1898.502 | TFLOPs: 15.28 |
g0184: [2024-08-11 01:39:44,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=33360, skipped=54, lr=[0.00019982464992178073, 0.00019982464992178073], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33360 loss: 0.7769 iter time (s): 4.247 samples/sec: 30.141
g0198:  iteration    33360/10000000 | consumed samples:      4270080 | consumed tokens:   8745123840 | elapsed time per iteration (ms): 4281.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.666802E-01 | loss scale: 256.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.896 | tokens per gpu per second (tgs): 1913.356 | TFLOPs: 15.40 |
g0184: [2024-08-11 01:40:29,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=33370, skipped=54, lr=[0.00019982448990645338, 0.00019982448990645338], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33370 loss: 0.7806 iter time (s): 4.424 samples/sec: 28.933
g0198:  iteration    33370/10000000 | consumed samples:      4271360 | consumed tokens:   8747745280 | elapsed time per iteration (ms): 4456.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.659034E-01 | loss scale: 256.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.722 | tokens per gpu per second (tgs): 1838.196 | TFLOPs: 14.79 |
g0184: [2024-08-11 01:41:13,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=33380, skipped=54, lr=[0.00019982432981821598, 0.00019982432981821598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33380 loss: 0.7670 iter time (s): 4.392 samples/sec: 29.146
g0198:  iteration    33380/10000000 | consumed samples:      4272640 | consumed tokens:   8750366720 | elapsed time per iteration (ms): 4424.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.705873E-01 | loss scale: 256.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.928 | tokens per gpu per second (tgs): 1851.393 | TFLOPs: 14.90 |
g0184: [2024-08-11 01:41:58,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=33390, skipped=54, lr=[0.00019982416965706872, 0.00019982416965706872], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33390 loss: 0.7801 iter time (s): 4.428 samples/sec: 28.910
g0198:  iteration    33390/10000000 | consumed samples:      4273920 | consumed tokens:   8752988160 | elapsed time per iteration (ms): 4460.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.722390E-01 | loss scale: 256.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.697 | tokens per gpu per second (tgs): 1836.579 | TFLOPs: 14.78 |
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 01:42:33,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 01:42:33,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0187: [2024-08-11 01:42:33,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 01:42:33,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 01:42:33,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0195: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0194: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0195: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0197: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 01:42:33,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0187: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0197: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0187: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0194: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0194: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0188: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0194: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0188: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0187: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0195: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0195: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0195: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0184: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0185: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0198: [2024-08-11 01:42:33,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
g0184: [2024-08-11 01:42:42,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=33400, skipped=54, lr=[0.0001998240094230117, 0.0001998240094230117], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33400 loss: 0.7527 iter time (s): 4.347 samples/sec: 29.445
g0198:  iteration    33400/10000000 | consumed samples:      4275200 | consumed tokens:   8755609600 | elapsed time per iteration (ms): 4379.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.722647E-01 | loss scale: 512.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.228 | tokens per gpu per second (tgs): 1870.603 | TFLOPs: 15.05 |
g0184: [2024-08-11 01:43:24,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=33410, skipped=54, lr=[0.00019982384911604501, 0.00019982384911604501], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33410 loss: 0.7157 iter time (s): 4.161 samples/sec: 30.759
g0198:  iteration    33410/10000000 | consumed samples:      4276480 | consumed tokens:   8758231040 | elapsed time per iteration (ms): 4194.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.703174E-01 | loss scale: 512.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.513 | tokens per gpu per second (tgs): 1952.832 | TFLOPs: 15.71 |
g0184: [2024-08-11 01:44:07,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=33420, skipped=54, lr=[0.00019982368873616878, 0.00019982368873616878], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33420 loss: 0.7733 iter time (s): 4.266 samples/sec: 30.002
g0198:  iteration    33420/10000000 | consumed samples:      4277760 | consumed tokens:   8760852480 | elapsed time per iteration (ms): 4299.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.769578E-01 | loss scale: 512.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.770 | tokens per gpu per second (tgs): 1905.301 | TFLOPs: 15.33 |
g0184: [2024-08-11 01:44:51,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=33430, skipped=54, lr=[0.0001998235282833832, 0.0001998235282833832], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33430 loss: 0.7353 iter time (s): 4.371 samples/sec: 29.284
g0198:  iteration    33430/10000000 | consumed samples:      4279040 | consumed tokens:   8763473920 | elapsed time per iteration (ms): 4404.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.677256E-01 | loss scale: 512.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.063 | tokens per gpu per second (tgs): 1860.013 | TFLOPs: 14.97 |
g0184: [2024-08-11 01:45:32,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=33440, skipped=54, lr=[0.0001998233677576883, 0.0001998233677576883], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33440 loss: 0.7492 iter time (s): 4.142 samples/sec: 30.904
g0198:  iteration    33440/10000000 | consumed samples:      4280320 | consumed tokens:   8766095360 | elapsed time per iteration (ms): 4174.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.839554E-01 | loss scale: 512.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.660 | tokens per gpu per second (tgs): 1962.234 | TFLOPs: 15.79 |
g0184: [2024-08-11 01:46:15,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=33450, skipped=54, lr=[0.00019982320715908425, 0.00019982320715908425], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33450 loss: 0.7799 iter time (s): 4.231 samples/sec: 30.251
g0198:  iteration    33450/10000000 | consumed samples:      4281600 | consumed tokens:   8768716800 | elapsed time per iteration (ms): 4264.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.693380E-01 | loss scale: 512.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.018 | tokens per gpu per second (tgs): 1921.132 | TFLOPs: 15.46 |
g0184: [2024-08-11 01:46:58,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=33460, skipped=54, lr=[0.00019982304648757116, 0.00019982304648757116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33460 loss: 0.7616 iter time (s): 4.308 samples/sec: 29.711
g0198:  iteration    33460/10000000 | consumed samples:      4282880 | consumed tokens:   8771338240 | elapsed time per iteration (ms): 4351.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.659325E-01 | loss scale: 512.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.413 | tokens per gpu per second (tgs): 1882.454 | TFLOPs: 15.15 |
g0184: [2024-08-11 01:47:43,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=33470, skipped=54, lr=[0.00019982288574314918, 0.00019982288574314918], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33470 loss: 0.7566 iter time (s): 4.386 samples/sec: 29.187
g0198:  iteration    33470/10000000 | consumed samples:      4284160 | consumed tokens:   8773959680 | elapsed time per iteration (ms): 4418.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.656165E-01 | loss scale: 512.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.972 | tokens per gpu per second (tgs): 1854.183 | TFLOPs: 14.92 |
g0184: [2024-08-11 01:48:25,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=33480, skipped=54, lr=[0.00019982272492581837, 0.00019982272492581837], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33480 loss: 0.7890 iter time (s): 4.239 samples/sec: 30.197
g0198:  iteration    33480/10000000 | consumed samples:      4285440 | consumed tokens:   8776581120 | elapsed time per iteration (ms): 4271.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.676210E-01 | loss scale: 512.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.964 | tokens per gpu per second (tgs): 1917.706 | TFLOPs: 15.43 |
g0184: [2024-08-11 01:49:07,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=33490, skipped=54, lr=[0.00019982256403557896, 0.00019982256403557896], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33490 loss: 0.7598 iter time (s): 4.180 samples/sec: 30.624
g0198:  iteration    33490/10000000 | consumed samples:      4286720 | consumed tokens:   8779202560 | elapsed time per iteration (ms): 4212.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.631088E-01 | loss scale: 512.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.387 | tokens per gpu per second (tgs): 1944.768 | TFLOPs: 15.65 |
g0184: [2024-08-11 01:49:50,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=33500, skipped=54, lr=[0.000199822403072431, 0.000199822403072431], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33500 loss: 0.7983 iter time (s): 4.243 samples/sec: 30.167
g0198:  iteration    33500/10000000 | consumed samples:      4288000 | consumed tokens:   8781824000 | elapsed time per iteration (ms): 4277.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.720487E-01 | loss scale: 512.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.925 | tokens per gpu per second (tgs): 1915.218 | TFLOPs: 15.41 |
g0184: [2024-08-11 01:50:34,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=33510, skipped=54, lr=[0.00019982224203637458, 0.00019982224203637458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33510 loss: 0.7793 iter time (s): 4.323 samples/sec: 29.610
g0198:  iteration    33510/10000000 | consumed samples:      4289280 | consumed tokens:   8784445440 | elapsed time per iteration (ms): 4355.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.808478E-01 | loss scale: 512.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.386 | tokens per gpu per second (tgs): 1880.734 | TFLOPs: 15.13 |
g0184: [2024-08-11 01:51:18,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=33520, skipped=54, lr=[0.0001998220809274099, 0.0001998220809274099], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33520 loss: 0.7945 iter time (s): 4.340 samples/sec: 29.493
g0198:  iteration    33520/10000000 | consumed samples:      4290560 | consumed tokens:   8787066880 | elapsed time per iteration (ms): 4373.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.802057E-01 | loss scale: 512.0 | grad norm: 0.300 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.270 | tokens per gpu per second (tgs): 1873.260 | TFLOPs: 15.07 |
g0184: [2024-08-11 01:52:03,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=33530, skipped=54, lr=[0.0001998219197455371, 0.0001998219197455371], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33530 loss: 0.7954 iter time (s): 4.463 samples/sec: 28.680
g0198:  iteration    33530/10000000 | consumed samples:      4291840 | consumed tokens:   8789688320 | elapsed time per iteration (ms): 4495.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.691330E-01 | loss scale: 512.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.472 | tokens per gpu per second (tgs): 1822.189 | TFLOPs: 14.66 |
g0184: [2024-08-11 01:52:45,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=33540, skipped=54, lr=[0.00019982175849075622, 0.00019982175849075622], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33540 loss: 0.7528 iter time (s): 4.210 samples/sec: 30.406
g0198:  iteration    33540/10000000 | consumed samples:      4293120 | consumed tokens:   8792309760 | elapsed time per iteration (ms): 4243.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.603878E-01 | loss scale: 512.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.167 | tokens per gpu per second (tgs): 1930.710 | TFLOPs: 15.54 |
g0184: [2024-08-11 01:53:29,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=33550, skipped=54, lr=[0.0001998215971630674, 0.0001998215971630674], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33550 loss: 0.7774 iter time (s): 4.335 samples/sec: 29.527
g0198:  iteration    33550/10000000 | consumed samples:      4294400 | consumed tokens:   8794931200 | elapsed time per iteration (ms): 4368.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.779900E-01 | loss scale: 512.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.301 | tokens per gpu per second (tgs): 1875.244 | TFLOPs: 15.09 |
g0184: [2024-08-11 01:54:12,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=33560, skipped=54, lr=[0.0001998214357624708, 0.0001998214357624708], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33560 loss: 0.7490 iter time (s): 4.309 samples/sec: 29.704
g0198:  iteration    33560/10000000 | consumed samples:      4295680 | consumed tokens:   8797552640 | elapsed time per iteration (ms): 4342.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.671387E-01 | loss scale: 512.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.479 | tokens per gpu per second (tgs): 1886.634 | TFLOPs: 15.18 |
g0184: [2024-08-11 01:54:55,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=33570, skipped=54, lr=[0.00019982127428896655, 0.00019982127428896655], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33570 loss: 0.7737 iter time (s): 4.243 samples/sec: 30.170
g0198:  iteration    33570/10000000 | consumed samples:      4296960 | consumed tokens:   8800174080 | elapsed time per iteration (ms): 4275.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.747314E-01 | loss scale: 512.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.940 | tokens per gpu per second (tgs): 1916.133 | TFLOPs: 15.42 |
g0184: [2024-08-11 01:55:38,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=33580, skipped=54, lr=[0.00019982111274255472, 0.00019982111274255472], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33580 loss: 0.7329 iter time (s): 4.265 samples/sec: 30.013
g0198:  iteration    33580/10000000 | consumed samples:      4298240 | consumed tokens:   8802795520 | elapsed time per iteration (ms): 4297.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.643497E-01 | loss scale: 512.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.786 | tokens per gpu per second (tgs): 1906.319 | TFLOPs: 15.34 |
g0184: [2024-08-11 01:56:21,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=33590, skipped=54, lr=[0.0001998209511232355, 0.0001998209511232355], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33590 loss: 0.7760 iter time (s): 4.239 samples/sec: 30.193
g0198:  iteration    33590/10000000 | consumed samples:      4299520 | consumed tokens:   8805416960 | elapsed time per iteration (ms): 4272.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.659308E-01 | loss scale: 512.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.962 | tokens per gpu per second (tgs): 1917.579 | TFLOPs: 15.43 |
g0184: [2024-08-11 01:57:05,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=33600, skipped=54, lr=[0.00019982078943100897, 0.00019982078943100897], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33600 loss: 0.7775 iter time (s): 4.417 samples/sec: 28.982
g0198:  iteration    33600/10000000 | consumed samples:      4300800 | consumed tokens:   8808038400 | elapsed time per iteration (ms): 4449.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.688720E-01 | loss scale: 512.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.770 | tokens per gpu per second (tgs): 1841.279 | TFLOPs: 14.82 |
g0184: [2024-08-11 01:57:48,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=33610, skipped=54, lr=[0.00019982062766587527, 0.00019982062766587527], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33610 loss: 0.7703 iter time (s): 4.220 samples/sec: 30.329
g0198:  iteration    33610/10000000 | consumed samples:      4302080 | consumed tokens:   8810659840 | elapsed time per iteration (ms): 4255.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.695302E-01 | loss scale: 512.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.080 | tokens per gpu per second (tgs): 1925.147 | TFLOPs: 15.49 |
g0184: [2024-08-11 01:58:31,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=33620, skipped=54, lr=[0.0001998204658278345, 0.0001998204658278345], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33620 loss: 0.7661 iter time (s): 4.265 samples/sec: 30.013
g0198:  iteration    33620/10000000 | consumed samples:      4303360 | consumed tokens:   8813281280 | elapsed time per iteration (ms): 4297.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.686124E-01 | loss scale: 512.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.785 | tokens per gpu per second (tgs): 1906.228 | TFLOPs: 15.34 |
g0184: [2024-08-11 01:59:13,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=33630, skipped=54, lr=[0.00019982030391688682, 0.00019982030391688682], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33630 loss: 0.7666 iter time (s): 4.167 samples/sec: 30.717
g0198:  iteration    33630/10000000 | consumed samples:      4304640 | consumed tokens:   8815902720 | elapsed time per iteration (ms): 4200.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.613098E-01 | loss scale: 512.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.475 | tokens per gpu per second (tgs): 1950.413 | TFLOPs: 15.70 |
g0184: [2024-08-11 01:59:55,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=33640, skipped=54, lr=[0.00019982014193303238, 0.00019982014193303238], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33640 loss: 0.7490 iter time (s): 4.243 samples/sec: 30.165
g0198:  iteration    33640/10000000 | consumed samples:      4305920 | consumed tokens:   8818524160 | elapsed time per iteration (ms): 4276.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.618304E-01 | loss scale: 512.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.932 | tokens per gpu per second (tgs): 1915.642 | TFLOPs: 15.42 |
g0184: [2024-08-11 02:00:36,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=33650, skipped=54, lr=[0.00019981997987627122, 0.00019981997987627122], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33650 loss: 0.7510 iter time (s): 4.074 samples/sec: 31.415
g0198:  iteration    33650/10000000 | consumed samples:      4307200 | consumed tokens:   8821145600 | elapsed time per iteration (ms): 4106.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.733984E-01 | loss scale: 512.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.167 | tokens per gpu per second (tgs): 1994.682 | TFLOPs: 16.05 |
g0184: [2024-08-11 02:01:20,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=33660, skipped=54, lr=[0.00019981981774660354, 0.00019981981774660354], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33660 loss: 0.7661 iter time (s): 4.362 samples/sec: 29.341
g0198:  iteration    33660/10000000 | consumed samples:      4308480 | consumed tokens:   8823767040 | elapsed time per iteration (ms): 4397.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.695405E-01 | loss scale: 512.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.111 | tokens per gpu per second (tgs): 1863.081 | TFLOPs: 14.99 |
g0184: [2024-08-11 02:02:04,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=33670, skipped=54, lr=[0.00019981965554402938, 0.00019981965554402938], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33670 loss: 0.7470 iter time (s): 4.369 samples/sec: 29.300
g0198:  iteration    33670/10000000 | consumed samples:      4309760 | consumed tokens:   8826388480 | elapsed time per iteration (ms): 4401.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.595309E-01 | loss scale: 512.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.083 | tokens per gpu per second (tgs): 1861.334 | TFLOPs: 14.98 |
g0184: [2024-08-11 02:02:47,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=33680, skipped=54, lr=[0.00019981949326854898, 0.00019981949326854898], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33680 loss: 0.7621 iter time (s): 4.228 samples/sec: 30.274
g0198:  iteration    33680/10000000 | consumed samples:      4311040 | consumed tokens:   8829009920 | elapsed time per iteration (ms): 4260.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.719221E-01 | loss scale: 512.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.043 | tokens per gpu per second (tgs): 1922.772 | TFLOPs: 15.47 |
g0184: [2024-08-11 02:03:29,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=33690, skipped=54, lr=[0.00019981933092016237, 0.00019981933092016237], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33690 loss: 0.7641 iter time (s): 4.220 samples/sec: 30.328
g0198:  iteration    33690/10000000 | consumed samples:      4312320 | consumed tokens:   8831631360 | elapsed time per iteration (ms): 4253.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.678866E-01 | loss scale: 512.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.093 | tokens per gpu per second (tgs): 1925.949 | TFLOPs: 15.50 |
g0184: [2024-08-11 02:04:12,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=33700, skipped=54, lr=[0.00019981916849886973, 0.00019981916849886973], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33700 loss: 0.7531 iter time (s): 4.201 samples/sec: 30.468
g0198:  iteration    33700/10000000 | consumed samples:      4313600 | consumed tokens:   8834252800 | elapsed time per iteration (ms): 4233.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.588688E-01 | loss scale: 512.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.234 | tokens per gpu per second (tgs): 1934.984 | TFLOPs: 15.57 |
g0184: [2024-08-11 02:04:54,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=33710, skipped=54, lr=[0.00019981900600467113, 0.00019981900600467113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33710 loss: 0.7491 iter time (s): 4.186 samples/sec: 30.581
g0198:  iteration    33710/10000000 | consumed samples:      4314880 | consumed tokens:   8836874240 | elapsed time per iteration (ms): 4218.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.781244E-01 | loss scale: 512.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.341 | tokens per gpu per second (tgs): 1941.846 | TFLOPs: 15.63 |
g0184: [2024-08-11 02:05:38,117] [INFO] [logging.py:96:log_dist] [Rank 0] step=33720, skipped=54, lr=[0.00019981884343756677, 0.00019981884343756677], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33720 loss: 0.7502 iter time (s): 4.329 samples/sec: 29.566
g0198:  iteration    33720/10000000 | consumed samples:      4316160 | consumed tokens:   8839495680 | elapsed time per iteration (ms): 4361.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.719880E-01 | loss scale: 512.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.346 | tokens per gpu per second (tgs): 1878.174 | TFLOPs: 15.11 |
g0184: [2024-08-11 02:06:21,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=33730, skipped=54, lr=[0.0001998186807975567, 0.0001998186807975567], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33730 loss: 0.7582 iter time (s): 4.302 samples/sec: 29.751
g0198:  iteration    33730/10000000 | consumed samples:      4317440 | consumed tokens:   8842117120 | elapsed time per iteration (ms): 4335.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.699203E-01 | loss scale: 512.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.522 | tokens per gpu per second (tgs): 1889.436 | TFLOPs: 15.20 |
g0184: [2024-08-11 02:07:04,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=33740, skipped=54, lr=[0.0001998185180846411, 0.0001998185180846411], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33740 loss: 0.7401 iter time (s): 4.306 samples/sec: 29.725
g0198:  iteration    33740/10000000 | consumed samples:      4318720 | consumed tokens:   8844738560 | elapsed time per iteration (ms): 4339.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.637593E-01 | loss scale: 512.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.499 | tokens per gpu per second (tgs): 1887.917 | TFLOPs: 15.19 |
g0184: [2024-08-11 02:07:51,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=33750, skipped=54, lr=[0.00019981835529882006, 0.00019981835529882006], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33750 loss: 0.7559 iter time (s): 4.606 samples/sec: 27.792
g0198:  iteration    33750/10000000 | consumed samples:      4320000 | consumed tokens:   8847360000 | elapsed time per iteration (ms): 4639.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.597430E-01 | loss scale: 512.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.591 | tokens per gpu per second (tgs): 1765.802 | TFLOPs: 14.21 |
g0184: [2024-08-11 02:08:32,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=33760, skipped=54, lr=[0.00019981819244009372, 0.00019981819244009372], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33760 loss: 0.7342 iter time (s): 4.072 samples/sec: 31.433
g0198:  iteration    33760/10000000 | consumed samples:      4321280 | consumed tokens:   8849981440 | elapsed time per iteration (ms): 4105.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.553194E-01 | loss scale: 512.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.181 | tokens per gpu per second (tgs): 1995.581 | TFLOPs: 16.06 |
g0184: [2024-08-11 02:09:16,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=33770, skipped=54, lr=[0.00019981802950846223, 0.00019981802950846223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33770 loss: 0.7273 iter time (s): 4.389 samples/sec: 29.164
g0198:  iteration    33770/10000000 | consumed samples:      4322560 | consumed tokens:   8852602880 | elapsed time per iteration (ms): 4422.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.748547E-01 | loss scale: 512.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.943 | tokens per gpu per second (tgs): 1852.329 | TFLOPs: 14.91 |
g0184: [2024-08-11 02:09:59,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=33780, skipped=54, lr=[0.00019981786650392566, 0.00019981786650392566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33780 loss: 0.7635 iter time (s): 4.306 samples/sec: 29.727
g0198:  iteration    33780/10000000 | consumed samples:      4323840 | consumed tokens:   8855224320 | elapsed time per iteration (ms): 4339.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.642160E-01 | loss scale: 512.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.499 | tokens per gpu per second (tgs): 1887.966 | TFLOPs: 15.19 |
g0184: [2024-08-11 02:10:42,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=33790, skipped=54, lr=[0.00019981770342648416, 0.00019981770342648416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33790 loss: 0.7914 iter time (s): 4.186 samples/sec: 30.581
g0198:  iteration    33790/10000000 | consumed samples:      4325120 | consumed tokens:   8857845760 | elapsed time per iteration (ms): 4218.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.651449E-01 | loss scale: 512.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.345 | tokens per gpu per second (tgs): 1942.049 | TFLOPs: 15.63 |
g0184: [2024-08-11 02:11:26,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=33800, skipped=54, lr=[0.00019981754027613788, 0.00019981754027613788], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33800 loss: 0.7737 iter time (s): 4.377 samples/sec: 29.245
g0198:  iteration    33800/10000000 | consumed samples:      4326400 | consumed tokens:   8860467200 | elapsed time per iteration (ms): 4412.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.718713E-01 | loss scale: 512.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.006 | tokens per gpu per second (tgs): 1856.405 | TFLOPs: 14.94 |
g0184: [2024-08-11 02:12:11,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=33810, skipped=54, lr=[0.00019981737705288693, 0.00019981737705288693], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33810 loss: 0.7116 iter time (s): 4.446 samples/sec: 28.787
g0198:  iteration    33810/10000000 | consumed samples:      4327680 | consumed tokens:   8863088640 | elapsed time per iteration (ms): 4479.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.550327E-01 | loss scale: 512.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.578 | tokens per gpu per second (tgs): 1828.966 | TFLOPs: 14.72 |
g0184: [2024-08-11 02:12:54,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=33820, skipped=54, lr=[0.0001998172137567314, 0.0001998172137567314], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33820 loss: 0.7773 iter time (s): 4.266 samples/sec: 30.005
g0198:  iteration    33820/10000000 | consumed samples:      4328960 | consumed tokens:   8865710080 | elapsed time per iteration (ms): 4299.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.638359E-01 | loss scale: 512.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.770 | tokens per gpu per second (tgs): 1905.283 | TFLOPs: 15.33 |
g0184: [2024-08-11 02:13:37,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=33830, skipped=54, lr=[0.00019981705038767147, 0.00019981705038767147], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33830 loss: 0.7777 iter time (s): 4.311 samples/sec: 29.695
g0198:  iteration    33830/10000000 | consumed samples:      4330240 | consumed tokens:   8868331520 | elapsed time per iteration (ms): 4343.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.694745E-01 | loss scale: 512.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.468 | tokens per gpu per second (tgs): 1885.927 | TFLOPs: 15.18 |
g0184: [2024-08-11 02:14:18,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=33840, skipped=54, lr=[0.00019981688694570724, 0.00019981688694570724], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33840 loss: 0.7813 iter time (s): 4.102 samples/sec: 31.202
g0198:  iteration    33840/10000000 | consumed samples:      4331520 | consumed tokens:   8870952960 | elapsed time per iteration (ms): 4135.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.795051E-01 | loss scale: 512.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.952 | tokens per gpu per second (tgs): 1980.952 | TFLOPs: 15.94 |
g0184: [2024-08-11 02:15:01,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=33850, skipped=54, lr=[0.00019981672343083882, 0.00019981672343083882], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33850 loss: 0.7675 iter time (s): 4.260 samples/sec: 30.045
g0198:  iteration    33850/10000000 | consumed samples:      4332800 | consumed tokens:   8873574400 | elapsed time per iteration (ms): 4293.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.634975E-01 | loss scale: 512.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.814 | tokens per gpu per second (tgs): 1908.128 | TFLOPs: 15.36 |
g0184: [2024-08-11 02:15:44,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=33860, skipped=54, lr=[0.00019981655984306637, 0.00019981655984306637], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33860 loss: 0.7817 iter time (s): 4.277 samples/sec: 29.929
g0198:  iteration    33860/10000000 | consumed samples:      4334080 | consumed tokens:   8876195840 | elapsed time per iteration (ms): 4310.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.725503E-01 | loss scale: 512.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.693 | tokens per gpu per second (tgs): 1900.376 | TFLOPs: 15.29 |
g0184: [2024-08-11 02:16:27,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=33870, skipped=54, lr=[0.00019981639618239002, 0.00019981639618239002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33870 loss: 0.7701 iter time (s): 4.269 samples/sec: 29.985
g0198:  iteration    33870/10000000 | consumed samples:      4335360 | consumed tokens:   8878817280 | elapsed time per iteration (ms): 4301.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.558925E-01 | loss scale: 512.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.756 | tokens per gpu per second (tgs): 1904.404 | TFLOPs: 15.33 |
g0184: [2024-08-11 02:17:11,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=33880, skipped=54, lr=[0.00019981623244880983, 0.00019981623244880983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33880 loss: 0.7367 iter time (s): 4.340 samples/sec: 29.491
g0198:  iteration    33880/10000000 | consumed samples:      4336640 | consumed tokens:   8881438720 | elapsed time per iteration (ms): 4372.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.605227E-01 | loss scale: 512.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.273 | tokens per gpu per second (tgs): 1873.461 | TFLOPs: 15.08 |
g0184: [2024-08-11 02:17:54,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=33890, skipped=54, lr=[0.000199816068642326, 0.000199816068642326], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33890 loss: 0.7563 iter time (s): 4.276 samples/sec: 29.933
g0198:  iteration    33890/10000000 | consumed samples:      4337920 | consumed tokens:   8884060160 | elapsed time per iteration (ms): 4309.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.748116E-01 | loss scale: 512.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.705 | tokens per gpu per second (tgs): 1901.098 | TFLOPs: 15.30 |
g0197: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0198: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0194: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0195: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0185: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0197: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0195: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0197: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0197: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0187: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0187: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0187: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0187: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0184: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0188: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0195: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0184: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0195: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0188: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0198: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0198: [2024-08-11 02:18:29,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0198: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0198: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0195: [2024-08-11 02:18:29,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0197: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0194: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0185: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0188: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0195: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0187: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0198: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0184: [2024-08-11 02:18:29,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 02:18:29,742] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
g0184: [2024-08-11 02:18:38,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=33900, skipped=54, lr=[0.0001998159047629386, 0.0001998159047629386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33900 loss: 0.7884 iter time (s): 4.335 samples/sec: 29.527
g0198:  iteration    33900/10000000 | consumed samples:      4339200 | consumed tokens:   8886681600 | elapsed time per iteration (ms): 4369.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.657358E-01 | loss scale: 1024.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.298 | tokens per gpu per second (tgs): 1875.042 | TFLOPs: 15.09 |
g0184: [2024-08-11 02:19:23,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=33910, skipped=54, lr=[0.00019981574081064782, 0.00019981574081064782], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33910 loss: 0.7528 iter time (s): 4.471 samples/sec: 28.626
g0198:  iteration    33910/10000000 | consumed samples:      4340480 | consumed tokens:   8889303040 | elapsed time per iteration (ms): 4505.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.649127E-01 | loss scale: 1024.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.413 | tokens per gpu per second (tgs): 1818.417 | TFLOPs: 14.63 |
g0184: [2024-08-11 02:20:07,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=33920, skipped=54, lr=[0.00019981557678545373, 0.00019981557678545373], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33920 loss: 0.7680 iter time (s): 4.387 samples/sec: 29.176
g0198:  iteration    33920/10000000 | consumed samples:      4341760 | consumed tokens:   8891924480 | elapsed time per iteration (ms): 4420.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.651248E-01 | loss scale: 1024.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.956 | tokens per gpu per second (tgs): 1853.163 | TFLOPs: 14.91 |
g0184: [2024-08-11 02:20:53,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=33930, skipped=54, lr=[0.00019981541268735648, 0.00019981541268735648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33930 loss: 0.7715 iter time (s): 4.558 samples/sec: 28.081
g0198:  iteration    33930/10000000 | consumed samples:      4343040 | consumed tokens:   8894545920 | elapsed time per iteration (ms): 4592.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.570357E-01 | loss scale: 1024.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.874 | tokens per gpu per second (tgs): 1783.967 | TFLOPs: 14.36 |
g0184: [2024-08-11 02:21:39,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=33940, skipped=54, lr=[0.0001998152485163562, 0.0001998152485163562], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33940 loss: 0.7842 iter time (s): 4.523 samples/sec: 28.302
g0198:  iteration    33940/10000000 | consumed samples:      4344320 | consumed tokens:   8897167360 | elapsed time per iteration (ms): 4565.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.705514E-01 | loss scale: 1024.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.040 | tokens per gpu per second (tgs): 1794.541 | TFLOPs: 14.44 |
g0184: [2024-08-11 02:22:24,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=33950, skipped=54, lr=[0.000199815084272453, 0.000199815084272453], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33950 loss: 0.7856 iter time (s): 4.466 samples/sec: 28.661
g0198:  iteration    33950/10000000 | consumed samples:      4345600 | consumed tokens:   8899788800 | elapsed time per iteration (ms): 4499.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.720797E-01 | loss scale: 1024.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.449 | tokens per gpu per second (tgs): 1820.712 | TFLOPs: 14.65 |
g0184: [2024-08-11 02:23:08,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=33960, skipped=54, lr=[0.000199814919955647, 0.000199814919955647], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33960 loss: 0.7707 iter time (s): 4.352 samples/sec: 29.415
g0198:  iteration    33960/10000000 | consumed samples:      4346880 | consumed tokens:   8902410240 | elapsed time per iteration (ms): 4384.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.564496E-01 | loss scale: 1024.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.193 | tokens per gpu per second (tgs): 1868.359 | TFLOPs: 15.03 |
g0184: [2024-08-11 02:23:53,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=33970, skipped=54, lr=[0.00019981475556593836, 0.00019981475556593836], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33970 loss: 0.7672 iter time (s): 4.512 samples/sec: 28.371
g0198:  iteration    33970/10000000 | consumed samples:      4348160 | consumed tokens:   8905031680 | elapsed time per iteration (ms): 4545.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.717796E-01 | loss scale: 1024.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.160 | tokens per gpu per second (tgs): 1802.232 | TFLOPs: 14.50 |
g0184: [2024-08-11 02:24:36,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=33980, skipped=54, lr=[0.00019981459110332716, 0.00019981459110332716], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33980 loss: 0.7526 iter time (s): 4.245 samples/sec: 30.155
g0198:  iteration    33980/10000000 | consumed samples:      4349440 | consumed tokens:   8907653120 | elapsed time per iteration (ms): 4278.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.777068E-01 | loss scale: 1024.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.916 | tokens per gpu per second (tgs): 1914.620 | TFLOPs: 15.41 |
g0184: [2024-08-11 02:25:21,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=33990, skipped=54, lr=[0.00019981442656781358, 0.00019981442656781358], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 33990 loss: 0.7550 iter time (s): 4.489 samples/sec: 28.515
g0198:  iteration    33990/10000000 | consumed samples:      4350720 | consumed tokens:   8910274560 | elapsed time per iteration (ms): 4521.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.755691E-01 | loss scale: 1024.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.309 | tokens per gpu per second (tgs): 1811.803 | TFLOPs: 14.58 |
g0184: [2024-08-11 02:26:04,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=54, lr=[0.00019981426195939773, 0.00019981426195939773], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34000 loss: 0.7637 iter time (s): 4.313 samples/sec: 29.678
g0198:  iteration    34000/10000000 | consumed samples:      4352000 | consumed tokens:   8912896000 | elapsed time per iteration (ms): 4345.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.712095E-01 | loss scale: 1024.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.453 | tokens per gpu per second (tgs): 1885.014 | TFLOPs: 15.17 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 34000 | lm loss value: 7.666810E-01 | lm loss PPL: 2.152610E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   34000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 02:33:03,569] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step34000 is about to be saved!
g0198: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0198: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0198: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0188: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0188: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0188: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0184: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0184: [2024-08-11 02:33:03,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0184: [2024-08-11 02:33:03,578] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0194: [2024-08-11 02:33:03,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0194: [2024-08-11 02:33:03,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0194: [2024-08-11 02:33:03,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0197: [2024-08-11 02:33:03,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0197: [2024-08-11 02:33:03,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0197: [2024-08-11 02:33:03,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0195: [2024-08-11 02:33:03,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0195: [2024-08-11 02:33:03,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0195: [2024-08-11 02:33:03,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0187: [2024-08-11 02:33:03,584] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0187: [2024-08-11 02:33:03,585] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0187: [2024-08-11 02:33:03,585] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0185: [2024-08-11 02:33:03,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0185: [2024-08-11 02:33:03,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0185: [2024-08-11 02:33:03,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0198: [2024-08-11 02:33:03,603] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_23-model_00-model_states.pt...
g0184: [2024-08-11 02:33:03,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_01-model_00-model_states.pt...
g0197: [2024-08-11 02:33:03,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_20-model_00-model_states.pt...
g0187: [2024-08-11 02:33:03,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_08-model_00-model_states.pt...
g0188: [2024-08-11 02:33:03,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_11-model_00-model_states.pt...
g0194: [2024-08-11 02:33:03,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_14-model_00-model_states.pt...
g0195: [2024-08-11 02:33:03,620] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_17-model_00-model_states.pt...
g0185: [2024-08-11 02:33:03,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_05-model_00-model_states.pt...
g0187: [2024-08-11 02:33:03,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_08-model_00-model_states.pt.
g0197: [2024-08-11 02:33:03,737] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_20-model_00-model_states.pt.
g0194: [2024-08-11 02:33:03,751] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_14-model_00-model_states.pt.
g0187: [2024-08-11 02:33:03,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_09-model_00-model_states.pt...
g0198: [2024-08-11 02:33:03,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 02:33:03,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_24-model_00-model_states.pt...
g0197: [2024-08-11 02:33:03,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_21-model_00-model_states.pt...
g0198: [2024-08-11 02:33:03,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_24-model_00-model_states.pt.
g0194: [2024-08-11 02:33:03,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_15-model_00-model_states.pt...
g0188: [2024-08-11 02:33:03,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_11-model_00-model_states.pt.
g0195: [2024-08-11 02:33:03,815] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_17-model_00-model_states.pt.
g0198: [2024-08-11 02:33:03,823] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_25-model_00-model_states.pt...
g0188: [2024-08-11 02:33:03,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_12-model_00-model_states.pt...
g0195: [2024-08-11 02:33:03,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_18-model_00-model_states.pt...
g0184: [2024-08-11 02:33:03,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_01-model_00-model_states.pt.
g0197: [2024-08-11 02:33:03,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_21-model_00-model_states.pt.
g0187: [2024-08-11 02:33:03,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_09-model_00-model_states.pt.
g0184: [2024-08-11 02:33:03,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_02-model_00-model_states.pt...
g0197: [2024-08-11 02:33:03,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_22-model_00-model_states.pt...
g0187: [2024-08-11 02:33:03,906] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_10-model_00-model_states.pt...
g0194: [2024-08-11 02:33:03,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 02:33:03,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_16-model_00-model_states.pt...
g0195: [2024-08-11 02:33:04,009] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_18-model_00-model_states.pt.
g0188: [2024-08-11 02:33:04,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_12-model_00-model_states.pt.
g0197: [2024-08-11 02:33:04,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 02:33:04,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_06_model_states.pt...
g0187: [2024-08-11 02:33:04,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_10-model_00-model_states.pt.
g0195: [2024-08-11 02:33:04,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_19-model_00-model_states.pt...
g0187: [2024-08-11 02:33:04,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_02_model_states.pt...
g0185: [2024-08-11 02:33:04,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_05-model_00-model_states.pt.
g0188: [2024-08-11 02:33:04,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_13-model_00-model_states.pt...
g0185: [2024-08-11 02:33:04,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_06-model_00-model_states.pt...
g0184: [2024-08-11 02:33:04,098] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 02:33:04,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_03-model_00-model_states.pt...
g0194: [2024-08-11 02:33:04,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 02:33:04,147] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_04_model_states.pt...
g0195: [2024-08-11 02:33:04,151] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_19-model_00-model_states.pt.
g0188: [2024-08-11 02:33:04,153] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_13-model_00-model_states.pt.
g0195: [2024-08-11 02:33:04,153] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_05_model_states.pt...
g0188: [2024-08-11 02:33:04,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_03_model_states.pt...
g0198: [2024-08-11 02:33:04,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 02:33:04,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_07_model_states.pt...
g0185: [2024-08-11 02:33:04,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_06-model_00-model_states.pt.
g0184: [2024-08-11 02:33:04,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 02:33:04,233] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_04-model_00-model_states.pt...
g0185: [2024-08-11 02:33:04,243] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_07-model_00-model_states.pt...
g0185: [2024-08-11 02:33:04,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 02:33:04,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_01_model_states.pt...
g0184: [2024-08-11 02:33:04,514] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 02:33:04,514] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_00_model_states.pt
g0184: [2024-08-11 02:33:04,515] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_00_model_states.pt...
g0187: [2024-08-11 02:33:06,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 02:33:06,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0197: [2024-08-11 02:33:06,411] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 02:33:06,411] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0194: [2024-08-11 02:33:06,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 02:33:06,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0188: [2024-08-11 02:33:06,627] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 02:33:06,627] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0185: [2024-08-11 02:33:06,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 02:33:06,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0195: [2024-08-11 02:33:06,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 02:33:06,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0184: [2024-08-11 02:33:08,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 02:33:08,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0198: [2024-08-11 02:33:08,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step34000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 02:33:08,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34000 is ready now!
g0184:   successfully saved checkpoint at iteration   34000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.51, Latency(second): 4.995
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4994.35, 4995.62)
g0184: [2024-08-11 02:33:51,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=34010, skipped=54, lr=[0.00019981409727807967, 0.00019981409727807967], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34010 loss: 0.7748 iter time (s): 4.253 samples/sec: 30.094
g0198:  iteration    34010/10000000 | consumed samples:      4353280 | consumed tokens:   8915517440 | elapsed time per iteration (ms): 46646.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.613101E-01 | loss scale: 1024.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.744 | tokens per gpu per second (tgs): 175.619 | TFLOPs: 1.41 |
g0184: [2024-08-11 02:34:34,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=34020, skipped=54, lr=[0.00019981393252385966, 0.00019981393252385966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34020 loss: 0.7769 iter time (s): 4.274 samples/sec: 29.950
g0198:  iteration    34020/10000000 | consumed samples:      4354560 | consumed tokens:   8918138880 | elapsed time per iteration (ms): 4320.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.671065E-01 | loss scale: 1024.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.624 | tokens per gpu per second (tgs): 1895.905 | TFLOPs: 15.26 |
g0184: [2024-08-11 02:35:17,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=34030, skipped=54, lr=[0.0001998137676967377, 0.0001998137676967377], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34030 loss: 0.7817 iter time (s): 4.239 samples/sec: 30.197
g0198:  iteration    34030/10000000 | consumed samples:      4355840 | consumed tokens:   8920760320 | elapsed time per iteration (ms): 4272.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.692937E-01 | loss scale: 1024.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.961 | tokens per gpu per second (tgs): 1917.526 | TFLOPs: 15.43 |
g0184: [2024-08-11 02:36:00,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=34040, skipped=54, lr=[0.00019981360279671396, 0.00019981360279671396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34040 loss: 0.7448 iter time (s): 4.289 samples/sec: 29.842
g0198:  iteration    34040/10000000 | consumed samples:      4357120 | consumed tokens:   8923381760 | elapsed time per iteration (ms): 4321.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.645241E-01 | loss scale: 1024.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.617 | tokens per gpu per second (tgs): 1895.485 | TFLOPs: 15.25 |
g0184: [2024-08-11 02:36:44,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=34050, skipped=54, lr=[0.00019981343782378858, 0.00019981343782378858], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34050 loss: 0.7590 iter time (s): 4.331 samples/sec: 29.554
g0198:  iteration    34050/10000000 | consumed samples:      4358400 | consumed tokens:   8926003200 | elapsed time per iteration (ms): 4364.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.620759E-01 | loss scale: 1024.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.330 | tokens per gpu per second (tgs): 1877.091 | TFLOPs: 15.11 |
g0184: [2024-08-11 02:37:28,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=34060, skipped=54, lr=[0.0001998132727779617, 0.0001998132727779617], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34060 loss: 0.7632 iter time (s): 4.443 samples/sec: 28.809
g0198:  iteration    34060/10000000 | consumed samples:      4359680 | consumed tokens:   8928624640 | elapsed time per iteration (ms): 4475.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.541044E-01 | loss scale: 1024.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.600 | tokens per gpu per second (tgs): 1830.383 | TFLOPs: 14.73 |
g0184: [2024-08-11 02:38:13,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=34070, skipped=54, lr=[0.0001998131076592334, 0.0001998131076592334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34070 loss: 0.8081 iter time (s): 4.415 samples/sec: 28.993
g0198:  iteration    34070/10000000 | consumed samples:      4360960 | consumed tokens:   8931246080 | elapsed time per iteration (ms): 4447.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.625001E-01 | loss scale: 1024.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.780 | tokens per gpu per second (tgs): 1841.918 | TFLOPs: 14.82 |
g0184: [2024-08-11 02:38:57,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=34080, skipped=54, lr=[0.00019981294246760385, 0.00019981294246760385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34080 loss: 0.7620 iter time (s): 4.335 samples/sec: 29.530
g0198:  iteration    34080/10000000 | consumed samples:      4362240 | consumed tokens:   8933867520 | elapsed time per iteration (ms): 4367.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.597032E-01 | loss scale: 1024.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.310 | tokens per gpu per second (tgs): 1875.830 | TFLOPs: 15.10 |
g0184: [2024-08-11 02:39:41,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=34090, skipped=54, lr=[0.00019981277720307317, 0.00019981277720307317], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34090 loss: 0.7730 iter time (s): 4.446 samples/sec: 28.790
g0198:  iteration    34090/10000000 | consumed samples:      4363520 | consumed tokens:   8936488960 | elapsed time per iteration (ms): 4478.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.659138E-01 | loss scale: 1024.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.580 | tokens per gpu per second (tgs): 1829.108 | TFLOPs: 14.72 |
g0184: [2024-08-11 02:40:24,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=34100, skipped=54, lr=[0.00019981261186564146, 0.00019981261186564146], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34100 loss: 0.7770 iter time (s): 4.206 samples/sec: 30.430
g0198:  iteration    34100/10000000 | consumed samples:      4364800 | consumed tokens:   8939110400 | elapsed time per iteration (ms): 4239.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.729208E-01 | loss scale: 1024.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.196 | tokens per gpu per second (tgs): 1932.513 | TFLOPs: 15.55 |
g0184: [2024-08-11 02:41:06,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=34110, skipped=54, lr=[0.00019981244645530887, 0.00019981244645530887], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34110 loss: 0.7592 iter time (s): 4.153 samples/sec: 30.823
g0198:  iteration    34110/10000000 | consumed samples:      4366080 | consumed tokens:   8941731840 | elapsed time per iteration (ms): 4185.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.671564E-01 | loss scale: 1024.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.583 | tokens per gpu per second (tgs): 1957.344 | TFLOPs: 15.75 |
g0184: [2024-08-11 02:41:49,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=34120, skipped=54, lr=[0.00019981228097207555, 0.00019981228097207555], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34120 loss: 0.7825 iter time (s): 4.331 samples/sec: 29.555
g0198:  iteration    34120/10000000 | consumed samples:      4367360 | consumed tokens:   8944353280 | elapsed time per iteration (ms): 4365.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.576777E-01 | loss scale: 1024.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.324 | tokens per gpu per second (tgs): 1876.732 | TFLOPs: 15.10 |
g0184: [2024-08-11 02:42:32,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=34130, skipped=54, lr=[0.00019981211541594154, 0.00019981211541594154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34130 loss: 0.7661 iter time (s): 4.216 samples/sec: 30.362
g0198:  iteration    34130/10000000 | consumed samples:      4368640 | consumed tokens:   8946974720 | elapsed time per iteration (ms): 4255.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.733294E-01 | loss scale: 1024.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.079 | tokens per gpu per second (tgs): 1925.065 | TFLOPs: 15.49 |
g0184: [2024-08-11 02:43:15,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=34140, skipped=54, lr=[0.00019981194978690707, 0.00019981194978690707], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34140 loss: 0.7758 iter time (s): 4.311 samples/sec: 29.688
g0198:  iteration    34140/10000000 | consumed samples:      4369920 | consumed tokens:   8949596160 | elapsed time per iteration (ms): 4344.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.614707E-01 | loss scale: 1024.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.466 | tokens per gpu per second (tgs): 1885.819 | TFLOPs: 15.18 |
g0184: [2024-08-11 02:43:59,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=34150, skipped=54, lr=[0.00019981178408497223, 0.00019981178408497223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34150 loss: 0.7751 iter time (s): 4.353 samples/sec: 29.408
g0198:  iteration    34150/10000000 | consumed samples:      4371200 | consumed tokens:   8952217600 | elapsed time per iteration (ms): 4387.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.678630E-01 | loss scale: 1024.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.173 | tokens per gpu per second (tgs): 1867.058 | TFLOPs: 15.02 |
g0184: [2024-08-11 02:44:41,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=34160, skipped=54, lr=[0.0001998116183101371, 0.0001998116183101371], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34160 loss: 0.8045 iter time (s): 4.118 samples/sec: 31.084
g0198:  iteration    34160/10000000 | consumed samples:      4372480 | consumed tokens:   8954839040 | elapsed time per iteration (ms): 4156.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.727817E-01 | loss scale: 1024.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.792 | tokens per gpu per second (tgs): 1970.690 | TFLOPs: 15.86 |
g0184: [2024-08-11 02:45:22,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=34170, skipped=54, lr=[0.0001998114524624019, 0.0001998114524624019], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34170 loss: 0.7689 iter time (s): 4.068 samples/sec: 31.461
g0198:  iteration    34170/10000000 | consumed samples:      4373760 | consumed tokens:   8957460480 | elapsed time per iteration (ms): 4101.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.691762E-01 | loss scale: 1024.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.209 | tokens per gpu per second (tgs): 1997.356 | TFLOPs: 16.07 |
g0184: [2024-08-11 02:46:04,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=34180, skipped=54, lr=[0.00019981128654176667, 0.00019981128654176667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34180 loss: 0.7744 iter time (s): 4.239 samples/sec: 30.197
g0198:  iteration    34180/10000000 | consumed samples:      4375040 | consumed tokens:   8960081920 | elapsed time per iteration (ms): 4272.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.691871E-01 | loss scale: 1024.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.959 | tokens per gpu per second (tgs): 1917.346 | TFLOPs: 15.43 |
g0184: [2024-08-11 02:46:46,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=34190, skipped=54, lr=[0.0001998111205482316, 0.0001998111205482316], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34190 loss: 0.7802 iter time (s): 4.070 samples/sec: 31.452
g0198:  iteration    34190/10000000 | consumed samples:      4376320 | consumed tokens:   8962703360 | elapsed time per iteration (ms): 4102.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.658638E-01 | loss scale: 1024.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.198 | tokens per gpu per second (tgs): 1996.693 | TFLOPs: 16.07 |
g0184: [2024-08-11 02:47:28,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=34200, skipped=54, lr=[0.00019981095448179677, 0.00019981095448179677], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34200 loss: 0.7739 iter time (s): 4.262 samples/sec: 30.036
g0198:  iteration    34200/10000000 | consumed samples:      4377600 | consumed tokens:   8965324800 | elapsed time per iteration (ms): 4294.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.619277E-01 | loss scale: 1024.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.805 | tokens per gpu per second (tgs): 1907.491 | TFLOPs: 15.35 |
g0184: [2024-08-11 02:48:12,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=34210, skipped=54, lr=[0.00019981078834246233, 0.00019981078834246233], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34210 loss: 0.7603 iter time (s): 4.323 samples/sec: 29.606
g0198:  iteration    34210/10000000 | consumed samples:      4378880 | consumed tokens:   8967946240 | elapsed time per iteration (ms): 4357.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.683594E-01 | loss scale: 1024.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.377 | tokens per gpu per second (tgs): 1880.100 | TFLOPs: 15.13 |
g0184: [2024-08-11 02:48:57,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=34220, skipped=54, lr=[0.0001998106221302284, 0.0001998106221302284], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34220 loss: 0.7809 iter time (s): 4.428 samples/sec: 28.905
g0198:  iteration    34220/10000000 | consumed samples:      4380160 | consumed tokens:   8970567680 | elapsed time per iteration (ms): 4461.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.712889E-01 | loss scale: 1024.0 | grad norm: 0.240 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.689 | tokens per gpu per second (tgs): 1836.127 | TFLOPs: 14.78 |
g0184: [2024-08-11 02:49:41,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=34230, skipped=54, lr=[0.00019981045584509515, 0.00019981045584509515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34230 loss: 0.7828 iter time (s): 4.367 samples/sec: 29.311
g0198:  iteration    34230/10000000 | consumed samples:      4381440 | consumed tokens:   8973189120 | elapsed time per iteration (ms): 4399.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.659388E-01 | loss scale: 1024.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.091 | tokens per gpu per second (tgs): 1861.850 | TFLOPs: 14.98 |
g0184: [2024-08-11 02:50:23,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=34240, skipped=54, lr=[0.00019981028948706266, 0.00019981028948706266], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34240 loss: 0.7365 iter time (s): 4.187 samples/sec: 30.569
g0198:  iteration    34240/10000000 | consumed samples:      4382720 | consumed tokens:   8975810560 | elapsed time per iteration (ms): 4220.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.529119E-01 | loss scale: 1024.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.329 | tokens per gpu per second (tgs): 1941.039 | TFLOPs: 15.62 |
g0184: [2024-08-11 02:51:06,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=34250, skipped=54, lr=[0.00019981012305613104, 0.00019981012305613104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34250 loss: 0.7743 iter time (s): 4.320 samples/sec: 29.630
g0198:  iteration    34250/10000000 | consumed samples:      4384000 | consumed tokens:   8978432000 | elapsed time per iteration (ms): 4352.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.735373E-01 | loss scale: 1024.0 | grad norm: 0.250 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.409 | tokens per gpu per second (tgs): 1882.159 | TFLOPs: 15.15 |
g0184: [2024-08-11 02:51:50,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=34260, skipped=54, lr=[0.00019980995655230046, 0.00019980995655230046], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34260 loss: 0.7651 iter time (s): 4.310 samples/sec: 29.698
g0198:  iteration    34260/10000000 | consumed samples:      4385280 | consumed tokens:   8981053440 | elapsed time per iteration (ms): 4362.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.664332E-01 | loss scale: 1024.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.340 | tokens per gpu per second (tgs): 1877.757 | TFLOPs: 15.11 |
g0184: [2024-08-11 02:52:33,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=34270, skipped=54, lr=[0.00019980978997557105, 0.00019980978997557105], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34270 loss: 0.8146 iter time (s): 4.245 samples/sec: 30.156
g0198:  iteration    34270/10000000 | consumed samples:      4386560 | consumed tokens:   8983674880 | elapsed time per iteration (ms): 4277.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.749938E-01 | loss scale: 1024.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.926 | tokens per gpu per second (tgs): 1915.280 | TFLOPs: 15.41 |
g0184: [2024-08-11 02:53:16,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=34280, skipped=54, lr=[0.0001998096233259429, 0.0001998096233259429], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34280 loss: 0.7577 iter time (s): 4.254 samples/sec: 30.087
g0198:  iteration    34280/10000000 | consumed samples:      4387840 | consumed tokens:   8986296320 | elapsed time per iteration (ms): 4323.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.741148E-01 | loss scale: 1024.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.605 | tokens per gpu per second (tgs): 1894.725 | TFLOPs: 15.25 |
g0184: [2024-08-11 02:53:59,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=34290, skipped=54, lr=[0.00019980945660341622, 0.00019980945660341622], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34290 loss: 0.7636 iter time (s): 4.293 samples/sec: 29.814
g0198:  iteration    34290/10000000 | consumed samples:      4389120 | consumed tokens:   8988917760 | elapsed time per iteration (ms): 4326.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.637971E-01 | loss scale: 1024.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.586 | tokens per gpu per second (tgs): 1893.519 | TFLOPs: 15.24 |
g0184: [2024-08-11 02:54:43,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=34300, skipped=54, lr=[0.00019980928980799104, 0.00019980928980799104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34300 loss: 0.7604 iter time (s): 4.360 samples/sec: 29.358
g0198:  iteration    34300/10000000 | consumed samples:      4390400 | consumed tokens:   8991539200 | elapsed time per iteration (ms): 4393.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.700406E-01 | loss scale: 1024.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.137 | tokens per gpu per second (tgs): 1864.789 | TFLOPs: 15.01 |
g0184: [2024-08-11 02:55:26,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=34310, skipped=54, lr=[0.00019980912293966752, 0.00019980912293966752], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34310 loss: 0.7408 iter time (s): 4.279 samples/sec: 29.915
g0198:  iteration    34310/10000000 | consumed samples:      4391680 | consumed tokens:   8994160640 | elapsed time per iteration (ms): 4311.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.642416E-01 | loss scale: 1024.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.689 | tokens per gpu per second (tgs): 1900.069 | TFLOPs: 15.29 |
g0184: [2024-08-11 02:56:09,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=34320, skipped=54, lr=[0.00019980895599844582, 0.00019980895599844582], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34320 loss: 0.7844 iter time (s): 4.243 samples/sec: 30.170
g0198:  iteration    34320/10000000 | consumed samples:      4392960 | consumed tokens:   8996782080 | elapsed time per iteration (ms): 4275.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.676593E-01 | loss scale: 1024.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.939 | tokens per gpu per second (tgs): 1916.081 | TFLOPs: 15.42 |
g0184: [2024-08-11 02:56:54,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=34330, skipped=54, lr=[0.00019980878898432603, 0.00019980878898432603], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34330 loss: 0.7937 iter time (s): 4.500 samples/sec: 28.442
g0198:  iteration    34330/10000000 | consumed samples:      4394240 | consumed tokens:   8999403520 | elapsed time per iteration (ms): 4534.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.672594E-01 | loss scale: 1024.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.230 | tokens per gpu per second (tgs): 1806.747 | TFLOPs: 14.54 |
g0184: [2024-08-11 02:57:38,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=34340, skipped=54, lr=[0.0001998086218973083, 0.0001998086218973083], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34340 loss: 0.7540 iter time (s): 4.276 samples/sec: 29.936
g0198:  iteration    34340/10000000 | consumed samples:      4395520 | consumed tokens:   9002024960 | elapsed time per iteration (ms): 4308.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.622014E-01 | loss scale: 1024.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.706 | tokens per gpu per second (tgs): 1901.177 | TFLOPs: 15.30 |
g0184: [2024-08-11 02:58:21,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=34350, skipped=54, lr=[0.00019980845473739276, 0.00019980845473739276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34350 loss: 0.7924 iter time (s): 4.326 samples/sec: 29.590
g0198:  iteration    34350/10000000 | consumed samples:      4396800 | consumed tokens:   9004646400 | elapsed time per iteration (ms): 4358.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.770844E-01 | loss scale: 1024.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.365 | tokens per gpu per second (tgs): 1879.360 | TFLOPs: 15.12 |
g0184: [2024-08-11 02:59:05,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=34360, skipped=54, lr=[0.00019980828750457952, 0.00019980828750457952], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34360 loss: 0.7716 iter time (s): 4.322 samples/sec: 29.613
g0198:  iteration    34360/10000000 | consumed samples:      4398080 | consumed tokens:   9007267840 | elapsed time per iteration (ms): 4355.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.646964E-01 | loss scale: 1024.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.392 | tokens per gpu per second (tgs): 1881.064 | TFLOPs: 15.14 |
g0184: [2024-08-11 02:59:47,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=34370, skipped=54, lr=[0.00019980812019886873, 0.00019980812019886873], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34370 loss: 0.7832 iter time (s): 4.240 samples/sec: 30.187
g0198:  iteration    34370/10000000 | consumed samples:      4399360 | consumed tokens:   9009889280 | elapsed time per iteration (ms): 4273.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.625926E-01 | loss scale: 1024.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.952 | tokens per gpu per second (tgs): 1916.943 | TFLOPs: 15.43 |
g0184: [2024-08-11 03:00:33,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=34380, skipped=54, lr=[0.0001998079528202605, 0.0001998079528202605], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34380 loss: 0.7745 iter time (s): 4.486 samples/sec: 28.536
g0198:  iteration    34380/10000000 | consumed samples:      4400640 | consumed tokens:   9012510720 | elapsed time per iteration (ms): 4518.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.580201E-01 | loss scale: 1024.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.330 | tokens per gpu per second (tgs): 1813.095 | TFLOPs: 14.59 |
g0184: [2024-08-11 03:01:16,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=34390, skipped=54, lr=[0.00019980778536875495, 0.00019980778536875495], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34390 loss: 0.7610 iter time (s): 4.274 samples/sec: 29.949
g0198:  iteration    34390/10000000 | consumed samples:      4401920 | consumed tokens:   9015132160 | elapsed time per iteration (ms): 4306.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.671449E-01 | loss scale: 1024.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.722 | tokens per gpu per second (tgs): 1902.212 | TFLOPs: 15.31 |
g0184: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0194: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0194: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0188: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0185: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0194: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0188: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0188: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0188: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0188: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0185: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:01:49,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0185: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0195: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:01:49,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:01:57,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=34400, skipped=54, lr=[0.00019980761784435227, 0.00019980761784435227], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34400 loss: 0.7747 iter time (s): 4.094 samples/sec: 31.262
g0198:  iteration    34400/10000000 | consumed samples:      4403200 | consumed tokens:   9017753600 | elapsed time per iteration (ms): 4128.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.644457E-01 | loss scale: 2048.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.002 | tokens per gpu per second (tgs): 1984.102 | TFLOPs: 15.97 |
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 34401
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 34401
g0195: Grad overflow on iteration 34401
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 34401
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 34401
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 34401
g0194: Grad overflow on iteration 34401
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: Grad overflow on iteration 34401
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 34401
g0185: Grad overflow on iteration 34401
g0184: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 34401
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: Grad overflow on iteration 34401
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 34401
g0198: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 34401
g0198: Grad overflow on iteration 34401
g0195: Grad overflow on iteration 34401
g0184: Grad overflow on iteration 34401
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 34401
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0198: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: Grad overflow on iteration 34401
g0184: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 34401
g0185: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0195: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 03:02:05,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
g0188: Grad overflow on iteration 34401
g0187: Grad overflow on iteration 34401
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 34401
g0187: Grad overflow on iteration 34401
g0198: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: Grad overflow on iteration 34401
g0198: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: Grad overflow on iteration 34401
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 34401
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: Grad overflow on iteration 34401
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0187: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: Grad overflow on iteration 34401
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 34401
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0194: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0198: Grad overflow on iteration 34401
g0188: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0198: [2024-08-11 03:02:05,822] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 03:02:05,821] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 34401
g0184: [2024-08-11 03:02:05,822] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0184: [2024-08-11 03:02:40,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=34410, skipped=55, lr=[0.0001998074502470525, 0.0001998074502470525], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34410 loss: 0.7314 iter time (s): 4.228 samples/sec: 30.275
g0198:  iteration    34410/10000000 | consumed samples:      4404480 | consumed tokens:   9020375040 | elapsed time per iteration (ms): 4261.8 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 1024.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.034 | tokens per gpu per second (tgs): 1922.202 | TFLOPs: 15.47 |
g0184: [2024-08-11 03:03:23,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=34420, skipped=55, lr=[0.00019980728257685586, 0.00019980728257685586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34420 loss: 0.7784 iter time (s): 4.345 samples/sec: 29.462
g0198:  iteration    34420/10000000 | consumed samples:      4405760 | consumed tokens:   9022996480 | elapsed time per iteration (ms): 4377.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.673082E-01 | loss scale: 1024.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.238 | tokens per gpu per second (tgs): 1871.225 | TFLOPs: 15.06 |
g0184: [2024-08-11 03:04:06,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=34430, skipped=55, lr=[0.00019980711483376238, 0.00019980711483376238], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34430 loss: 0.7990 iter time (s): 4.275 samples/sec: 29.942
g0198:  iteration    34430/10000000 | consumed samples:      4407040 | consumed tokens:   9025617920 | elapsed time per iteration (ms): 4307.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.691996E-01 | loss scale: 1024.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.715 | tokens per gpu per second (tgs): 1901.752 | TFLOPs: 15.30 |
g0184: [2024-08-11 03:04:50,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=34440, skipped=55, lr=[0.00019980694701777227, 0.00019980694701777227], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34440 loss: 0.7729 iter time (s): 4.292 samples/sec: 29.821
g0198:  iteration    34440/10000000 | consumed samples:      4408320 | consumed tokens:   9028239360 | elapsed time per iteration (ms): 4325.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.647859E-01 | loss scale: 1024.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.594 | tokens per gpu per second (tgs): 1894.006 | TFLOPs: 15.24 |
g0184: [2024-08-11 03:05:33,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=34450, skipped=55, lr=[0.00019980677912888564, 0.00019980677912888564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34450 loss: 0.7614 iter time (s): 4.267 samples/sec: 30.001
g0198:  iteration    34450/10000000 | consumed samples:      4409600 | consumed tokens:   9030860800 | elapsed time per iteration (ms): 4299.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.713496E-01 | loss scale: 1024.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.771 | tokens per gpu per second (tgs): 1905.334 | TFLOPs: 15.33 |
g0184: [2024-08-11 03:06:14,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=34460, skipped=55, lr=[0.0001998066111671026, 0.0001998066111671026], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34460 loss: 0.7558 iter time (s): 4.139 samples/sec: 30.928
g0198:  iteration    34460/10000000 | consumed samples:      4410880 | consumed tokens:   9033482240 | elapsed time per iteration (ms): 4171.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.691709E-01 | loss scale: 1024.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.687 | tokens per gpu per second (tgs): 1963.959 | TFLOPs: 15.80 |
g0184: [2024-08-11 03:06:57,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=34470, skipped=55, lr=[0.0001998064431324233, 0.0001998064431324233], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34470 loss: 0.7605 iter time (s): 4.191 samples/sec: 30.540
g0198:  iteration    34470/10000000 | consumed samples:      4412160 | consumed tokens:   9036103680 | elapsed time per iteration (ms): 4223.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.557559E-01 | loss scale: 1024.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.305 | tokens per gpu per second (tgs): 1939.548 | TFLOPs: 15.61 |
g0184: [2024-08-11 03:07:39,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=34480, skipped=55, lr=[0.00019980627502484783, 0.00019980627502484783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34480 loss: 0.7879 iter time (s): 4.163 samples/sec: 30.747
g0198:  iteration    34480/10000000 | consumed samples:      4413440 | consumed tokens:   9038725120 | elapsed time per iteration (ms): 4196.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.773573E-01 | loss scale: 1024.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.502 | tokens per gpu per second (tgs): 1952.123 | TFLOPs: 15.71 |
g0184: [2024-08-11 03:08:21,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=34490, skipped=55, lr=[0.00019980610684437638, 0.00019980610684437638], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34490 loss: 0.7527 iter time (s): 4.172 samples/sec: 30.677
g0198:  iteration    34490/10000000 | consumed samples:      4414720 | consumed tokens:   9041346560 | elapsed time per iteration (ms): 4205.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.599228E-01 | loss scale: 1024.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.436 | tokens per gpu per second (tgs): 1947.902 | TFLOPs: 15.68 |
g0184: [2024-08-11 03:09:02,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=34500, skipped=55, lr=[0.000199805938591009, 0.000199805938591009], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34500 loss: 0.7462 iter time (s): 4.128 samples/sec: 31.009
g0198:  iteration    34500/10000000 | consumed samples:      4416000 | consumed tokens:   9043968000 | elapsed time per iteration (ms): 4160.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.606570E-01 | loss scale: 1024.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.767 | tokens per gpu per second (tgs): 1969.057 | TFLOPs: 15.85 |
g0184: [2024-08-11 03:09:45,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=34510, skipped=55, lr=[0.0001998057702647459, 0.0001998057702647459], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34510 loss: 0.7336 iter time (s): 4.210 samples/sec: 30.401
g0198:  iteration    34510/10000000 | consumed samples:      4417280 | consumed tokens:   9046589440 | elapsed time per iteration (ms): 4243.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.548585E-01 | loss scale: 1024.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.164 | tokens per gpu per second (tgs): 1930.478 | TFLOPs: 15.53 |
g0184: [2024-08-11 03:10:27,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=34520, skipped=55, lr=[0.00019980560186558718, 0.00019980560186558718], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34520 loss: 0.7790 iter time (s): 4.230 samples/sec: 30.260
g0198:  iteration    34520/10000000 | consumed samples:      4418560 | consumed tokens:   9049210880 | elapsed time per iteration (ms): 4263.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.600669E-01 | loss scale: 1024.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.023 | tokens per gpu per second (tgs): 1921.442 | TFLOPs: 15.46 |
g0184: [2024-08-11 03:11:08,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=34530, skipped=55, lr=[0.00019980543339353295, 0.00019980543339353295], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34530 loss: 0.7814 iter time (s): 4.026 samples/sec: 31.791
g0198:  iteration    34530/10000000 | consumed samples:      4419840 | consumed tokens:   9051832320 | elapsed time per iteration (ms): 4059.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.741340E-01 | loss scale: 1024.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.533 | tokens per gpu per second (tgs): 2018.111 | TFLOPs: 16.24 |
g0184: [2024-08-11 03:11:49,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=34540, skipped=55, lr=[0.00019980526484858336, 0.00019980526484858336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34540 loss: 0.7534 iter time (s): 4.106 samples/sec: 31.175
g0198:  iteration    34540/10000000 | consumed samples:      4421120 | consumed tokens:   9054453760 | elapsed time per iteration (ms): 4138.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.633570E-01 | loss scale: 1024.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.928 | tokens per gpu per second (tgs): 1979.417 | TFLOPs: 15.93 |
g0184: [2024-08-11 03:12:31,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=34550, skipped=55, lr=[0.00019980509623073855, 0.00019980509623073855], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34550 loss: 0.7462 iter time (s): 4.164 samples/sec: 30.742
g0198:  iteration    34550/10000000 | consumed samples:      4422400 | consumed tokens:   9057075200 | elapsed time per iteration (ms): 4196.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.597840E-01 | loss scale: 1024.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.504 | tokens per gpu per second (tgs): 1952.273 | TFLOPs: 15.71 |
g0184: [2024-08-11 03:13:16,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=34560, skipped=55, lr=[0.00019980492753999862, 0.00019980492753999862], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34560 loss: 0.7939 iter time (s): 4.467 samples/sec: 28.653
g0198:  iteration    34560/10000000 | consumed samples:      4423680 | consumed tokens:   9059696640 | elapsed time per iteration (ms): 4500.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.707744E-01 | loss scale: 1024.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.444 | tokens per gpu per second (tgs): 1820.415 | TFLOPs: 14.65 |
g0184: [2024-08-11 03:13:58,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=34570, skipped=55, lr=[0.0001998047587763637, 0.0001998047587763637], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34570 loss: 0.7772 iter time (s): 4.190 samples/sec: 30.548
g0198:  iteration    34570/10000000 | consumed samples:      4424960 | consumed tokens:   9062318080 | elapsed time per iteration (ms): 4222.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.559415E-01 | loss scale: 1024.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.314 | tokens per gpu per second (tgs): 1940.073 | TFLOPs: 15.61 |
g0184: [2024-08-11 03:14:41,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=34580, skipped=55, lr=[0.00019980458993983395, 0.00019980458993983395], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34580 loss: 0.7546 iter time (s): 4.244 samples/sec: 30.163
g0198:  iteration    34580/10000000 | consumed samples:      4426240 | consumed tokens:   9064939520 | elapsed time per iteration (ms): 4277.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.597582E-01 | loss scale: 1024.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.926 | tokens per gpu per second (tgs): 1915.240 | TFLOPs: 15.41 |
g0184: [2024-08-11 03:15:25,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=34590, skipped=55, lr=[0.00019980442103040947, 0.00019980442103040947], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34590 loss: 0.7481 iter time (s): 4.346 samples/sec: 29.453
g0198:  iteration    34590/10000000 | consumed samples:      4427520 | consumed tokens:   9067560960 | elapsed time per iteration (ms): 4379.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.745744E-01 | loss scale: 1024.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.227 | tokens per gpu per second (tgs): 1870.528 | TFLOPs: 15.05 |
g0184: [2024-08-11 03:16:08,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=34600, skipped=55, lr=[0.00019980425204809042, 0.00019980425204809042], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34600 loss: 0.7554 iter time (s): 4.214 samples/sec: 30.374
g0198:  iteration    34600/10000000 | consumed samples:      4428800 | consumed tokens:   9070182400 | elapsed time per iteration (ms): 4249.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.657124E-01 | loss scale: 1024.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.121 | tokens per gpu per second (tgs): 1927.768 | TFLOPs: 15.51 |
g0184: [2024-08-11 03:16:51,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=34610, skipped=55, lr=[0.0001998040829928769, 0.0001998040829928769], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34610 loss: 0.7870 iter time (s): 4.327 samples/sec: 29.582
g0198:  iteration    34610/10000000 | consumed samples:      4430080 | consumed tokens:   9072803840 | elapsed time per iteration (ms): 4360.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.667702E-01 | loss scale: 1024.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.354 | tokens per gpu per second (tgs): 1878.656 | TFLOPs: 15.12 |
g0184: [2024-08-11 03:17:33,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=34620, skipped=55, lr=[0.00019980391386476904, 0.00019980391386476904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34620 loss: 0.7770 iter time (s): 4.174 samples/sec: 30.668
g0198:  iteration    34620/10000000 | consumed samples:      4431360 | consumed tokens:   9075425280 | elapsed time per iteration (ms): 4206.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.659173E-01 | loss scale: 1024.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.431 | tokens per gpu per second (tgs): 1947.557 | TFLOPs: 15.67 |
g0184: [2024-08-11 03:18:16,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=34630, skipped=55, lr=[0.000199803744663767, 0.000199803744663767], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34630 loss: 0.7788 iter time (s): 4.215 samples/sec: 30.364
g0198:  iteration    34630/10000000 | consumed samples:      4432640 | consumed tokens:   9078046720 | elapsed time per iteration (ms): 4248.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.699974E-01 | loss scale: 1024.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.129 | tokens per gpu per second (tgs): 1928.283 | TFLOPs: 15.52 |
g0184: [2024-08-11 03:18:57,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=34640, skipped=55, lr=[0.0001998035753898709, 0.0001998035753898709], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34640 loss: 0.7587 iter time (s): 4.130 samples/sec: 30.993
g0198:  iteration    34640/10000000 | consumed samples:      4433920 | consumed tokens:   9080668160 | elapsed time per iteration (ms): 4162.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.574322E-01 | loss scale: 1024.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.748 | tokens per gpu per second (tgs): 1967.840 | TFLOPs: 15.84 |
g0184: [2024-08-11 03:19:39,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=34650, skipped=55, lr=[0.00019980340604308086, 0.00019980340604308086], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34650 loss: 0.7874 iter time (s): 4.188 samples/sec: 30.560
g0198:  iteration    34650/10000000 | consumed samples:      4435200 | consumed tokens:   9083289600 | elapsed time per iteration (ms): 4221.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.518015E-01 | loss scale: 1024.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.324 | tokens per gpu per second (tgs): 1940.742 | TFLOPs: 15.62 |
g0184: [2024-08-11 03:20:23,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=34660, skipped=55, lr=[0.00019980323662339698, 0.00019980323662339698], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34660 loss: 0.7612 iter time (s): 4.285 samples/sec: 29.874
g0198:  iteration    34660/10000000 | consumed samples:      4436480 | consumed tokens:   9085911040 | elapsed time per iteration (ms): 4317.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.668899E-01 | loss scale: 1024.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.649 | tokens per gpu per second (tgs): 1897.548 | TFLOPs: 15.27 |
g0184: [2024-08-11 03:21:05,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=34670, skipped=55, lr=[0.00019980306713081948, 0.00019980306713081948], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34670 loss: 0.7843 iter time (s): 4.197 samples/sec: 30.497
g0198:  iteration    34670/10000000 | consumed samples:      4437760 | consumed tokens:   9088532480 | elapsed time per iteration (ms): 4229.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.732655E-01 | loss scale: 1024.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.261 | tokens per gpu per second (tgs): 1936.692 | TFLOPs: 15.58 |
g0184: [2024-08-11 03:21:46,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=34680, skipped=55, lr=[0.0001998028975653484, 0.0001998028975653484], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34680 loss: 0.7748 iter time (s): 4.119 samples/sec: 31.073
g0198:  iteration    34680/10000000 | consumed samples:      4439040 | consumed tokens:   9091153920 | elapsed time per iteration (ms): 4152.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.581086E-01 | loss scale: 1024.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.826 | tokens per gpu per second (tgs): 1972.843 | TFLOPs: 15.88 |
g0184: [2024-08-11 03:22:29,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=34690, skipped=55, lr=[0.00019980272792698392, 0.00019980272792698392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34690 loss: 0.7646 iter time (s): 4.177 samples/sec: 30.646
g0198:  iteration    34690/10000000 | consumed samples:      4440320 | consumed tokens:   9093775360 | elapsed time per iteration (ms): 4210.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.617224E-01 | loss scale: 1024.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.404 | tokens per gpu per second (tgs): 1945.833 | TFLOPs: 15.66 |
g0184: [2024-08-11 03:23:10,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=34700, skipped=55, lr=[0.00019980255821572612, 0.00019980255821572612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34700 loss: 0.7795 iter time (s): 4.147 samples/sec: 30.862
g0198:  iteration    34700/10000000 | consumed samples:      4441600 | consumed tokens:   9096396800 | elapsed time per iteration (ms): 4180.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.660064E-01 | loss scale: 1024.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.616 | tokens per gpu per second (tgs): 1959.410 | TFLOPs: 15.77 |
g0184: [2024-08-11 03:23:51,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=34710, skipped=55, lr=[0.0001998023884315752, 0.0001998023884315752], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34710 loss: 0.7527 iter time (s): 4.060 samples/sec: 31.525
g0198:  iteration    34710/10000000 | consumed samples:      4442880 | consumed tokens:   9099018240 | elapsed time per iteration (ms): 4093.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.601861E-01 | loss scale: 1024.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.271 | tokens per gpu per second (tgs): 2001.326 | TFLOPs: 16.11 |
g0184: [2024-08-11 03:24:33,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=34720, skipped=55, lr=[0.00019980221857453123, 0.00019980221857453123], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34720 loss: 0.7459 iter time (s): 4.160 samples/sec: 30.770
g0198:  iteration    34720/10000000 | consumed samples:      4444160 | consumed tokens:   9101639680 | elapsed time per iteration (ms): 4193.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.710204E-01 | loss scale: 1024.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.525 | tokens per gpu per second (tgs): 1953.630 | TFLOPs: 15.72 |
g0184: [2024-08-11 03:25:17,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=34730, skipped=55, lr=[0.00019980204864459438, 0.00019980204864459438], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34730 loss: 0.7632 iter time (s): 4.307 samples/sec: 29.722
g0198:  iteration    34730/10000000 | consumed samples:      4445440 | consumed tokens:   9104261120 | elapsed time per iteration (ms): 4339.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.619631E-01 | loss scale: 1024.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.497 | tokens per gpu per second (tgs): 1887.824 | TFLOPs: 15.19 |
g0184: [2024-08-11 03:26:00,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=34740, skipped=55, lr=[0.00019980187864176478, 0.00019980187864176478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34740 loss: 0.7577 iter time (s): 4.284 samples/sec: 29.879
g0198:  iteration    34740/10000000 | consumed samples:      4446720 | consumed tokens:   9106882560 | elapsed time per iteration (ms): 4322.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.602007E-01 | loss scale: 1024.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.611 | tokens per gpu per second (tgs): 1895.113 | TFLOPs: 15.25 |
g0184: [2024-08-11 03:26:42,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=34750, skipped=55, lr=[0.00019980170856604253, 0.00019980170856604253], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34750 loss: 0.7603 iter time (s): 4.201 samples/sec: 30.465
g0198:  iteration    34750/10000000 | consumed samples:      4448000 | consumed tokens:   9109504000 | elapsed time per iteration (ms): 4235.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.597828E-01 | loss scale: 1024.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.223 | tokens per gpu per second (tgs): 1934.272 | TFLOPs: 15.57 |
g0184: [2024-08-11 03:27:25,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=34760, skipped=55, lr=[0.00019980153841742777, 0.00019980153841742777], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34760 loss: 0.7840 iter time (s): 4.236 samples/sec: 30.216
g0198:  iteration    34760/10000000 | consumed samples:      4449280 | consumed tokens:   9112125440 | elapsed time per iteration (ms): 4268.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.630792E-01 | loss scale: 1024.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.986 | tokens per gpu per second (tgs): 1919.094 | TFLOPs: 15.44 |
g0184: [2024-08-11 03:28:08,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=34770, skipped=55, lr=[0.00019980136819592066, 0.00019980136819592066], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34770 loss: 0.7816 iter time (s): 4.294 samples/sec: 29.808
g0198:  iteration    34770/10000000 | consumed samples:      4450560 | consumed tokens:   9114746880 | elapsed time per iteration (ms): 4326.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.721979E-01 | loss scale: 1024.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.583 | tokens per gpu per second (tgs): 1893.315 | TFLOPs: 15.24 |
g0184: [2024-08-11 03:28:50,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=34780, skipped=55, lr=[0.0001998011979015213, 0.0001998011979015213], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34780 loss: 0.7710 iter time (s): 4.187 samples/sec: 30.571
g0198:  iteration    34780/10000000 | consumed samples:      4451840 | consumed tokens:   9117368320 | elapsed time per iteration (ms): 4219.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.665442E-01 | loss scale: 1024.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.335 | tokens per gpu per second (tgs): 1941.413 | TFLOPs: 15.62 |
g0184: [2024-08-11 03:29:33,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=34790, skipped=55, lr=[0.00019980102753422985, 0.00019980102753422985], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34790 loss: 0.7476 iter time (s): 4.211 samples/sec: 30.398
g0198:  iteration    34790/10000000 | consumed samples:      4453120 | consumed tokens:   9119989760 | elapsed time per iteration (ms): 4243.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.725679E-01 | loss scale: 1024.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.163 | tokens per gpu per second (tgs): 1930.411 | TFLOPs: 15.53 |
g0184: [2024-08-11 03:30:16,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=34800, skipped=55, lr=[0.0001998008570940464, 0.0001998008570940464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34800 loss: 0.7709 iter time (s): 4.321 samples/sec: 29.622
g0198:  iteration    34800/10000000 | consumed samples:      4454400 | consumed tokens:   9122611200 | elapsed time per iteration (ms): 4354.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.695381E-01 | loss scale: 1024.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.397 | tokens per gpu per second (tgs): 1881.406 | TFLOPs: 15.14 |
g0184: [2024-08-11 03:30:59,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=34810, skipped=55, lr=[0.0001998006865809711, 0.0001998006865809711], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34810 loss: 0.7606 iter time (s): 4.266 samples/sec: 30.002
g0198:  iteration    34810/10000000 | consumed samples:      4455680 | consumed tokens:   9125232640 | elapsed time per iteration (ms): 4298.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.534752E-01 | loss scale: 1024.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.775 | tokens per gpu per second (tgs): 1905.607 | TFLOPs: 15.33 |
g0184: [2024-08-11 03:31:42,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=34820, skipped=55, lr=[0.0001998005159950041, 0.0001998005159950041], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34820 loss: 0.7278 iter time (s): 4.206 samples/sec: 30.436
g0198:  iteration    34820/10000000 | consumed samples:      4456960 | consumed tokens:   9127854080 | elapsed time per iteration (ms): 4245.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.599231E-01 | loss scale: 1024.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.151 | tokens per gpu per second (tgs): 1929.676 | TFLOPs: 15.53 |
g0184: [2024-08-11 03:32:25,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=34830, skipped=55, lr=[0.00019980034533614548, 0.00019980034533614548], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34830 loss: 0.7731 iter time (s): 4.275 samples/sec: 29.940
g0198:  iteration    34830/10000000 | consumed samples:      4458240 | consumed tokens:   9130475520 | elapsed time per iteration (ms): 4307.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.718874E-01 | loss scale: 1024.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.715 | tokens per gpu per second (tgs): 1901.753 | TFLOPs: 15.30 |
g0184: [2024-08-11 03:33:07,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=34840, skipped=55, lr=[0.00019980017460439545, 0.00019980017460439545], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34840 loss: 0.7918 iter time (s): 4.154 samples/sec: 30.816
g0198:  iteration    34840/10000000 | consumed samples:      4459520 | consumed tokens:   9133096960 | elapsed time per iteration (ms): 4198.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.664581E-01 | loss scale: 1024.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.486 | tokens per gpu per second (tgs): 1951.091 | TFLOPs: 15.70 |
g0184: [2024-08-11 03:33:49,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=34850, skipped=55, lr=[0.0001998000037997541, 0.0001998000037997541], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34850 loss: 0.7896 iter time (s): 4.160 samples/sec: 30.768
g0198:  iteration    34850/10000000 | consumed samples:      4460800 | consumed tokens:   9135718400 | elapsed time per iteration (ms): 4192.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.647626E-01 | loss scale: 1024.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.813 | TFLOPs: 15.72 |
g0184: [2024-08-11 03:34:31,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=34860, skipped=55, lr=[0.00019979983292222153, 0.00019979983292222153], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34860 loss: 0.7560 iter time (s): 4.213 samples/sec: 30.380
g0198:  iteration    34860/10000000 | consumed samples:      4462080 | consumed tokens:   9138339840 | elapsed time per iteration (ms): 4246.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.549678E-01 | loss scale: 1024.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.145 | tokens per gpu per second (tgs): 1929.258 | TFLOPs: 15.53 |
g0184: [2024-08-11 03:35:13,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=34870, skipped=55, lr=[0.00019979966197179792, 0.00019979966197179792], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34870 loss: 0.7798 iter time (s): 4.116 samples/sec: 31.096
g0198:  iteration    34870/10000000 | consumed samples:      4463360 | consumed tokens:   9140961280 | elapsed time per iteration (ms): 4150.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.724346E-01 | loss scale: 1024.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.840 | tokens per gpu per second (tgs): 1973.743 | TFLOPs: 15.88 |
g0184: [2024-08-11 03:35:55,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=34880, skipped=55, lr=[0.00019979949094848335, 0.00019979949094848335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34880 loss: 0.7534 iter time (s): 4.195 samples/sec: 30.515
g0198:  iteration    34880/10000000 | consumed samples:      4464640 | consumed tokens:   9143582720 | elapsed time per iteration (ms): 4227.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.648239E-01 | loss scale: 1024.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.277 | tokens per gpu per second (tgs): 1937.729 | TFLOPs: 15.59 |
g0184: [2024-08-11 03:36:36,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=34890, skipped=55, lr=[0.000199799319852278, 0.000199799319852278], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34890 loss: 0.7819 iter time (s): 4.102 samples/sec: 31.201
g0198:  iteration    34890/10000000 | consumed samples:      4465920 | consumed tokens:   9146204160 | elapsed time per iteration (ms): 4137.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.662868E-01 | loss scale: 1024.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.939 | tokens per gpu per second (tgs): 1980.126 | TFLOPs: 15.93 |
g0184: [2024-08-11 03:37:19,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=34900, skipped=55, lr=[0.00019979914868318202, 0.00019979914868318202], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34900 loss: 0.7837 iter time (s): 4.197 samples/sec: 30.496
g0198:  iteration    34900/10000000 | consumed samples:      4467200 | consumed tokens:   9148825600 | elapsed time per iteration (ms): 4229.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.708924E-01 | loss scale: 1024.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.264 | tokens per gpu per second (tgs): 1936.894 | TFLOPs: 15.59 |
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0185: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0185: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0195: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 03:37:31,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0194: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0194: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0194: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0187: [2024-08-11 03:37:31,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0188: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0194: [2024-08-11 03:37:31,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0197: [2024-08-11 03:37:31,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0185: [2024-08-11 03:37:31,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0188: [2024-08-11 03:37:31,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 03:37:31,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0198: [2024-08-11 03:37:31,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
g0184: [2024-08-11 03:38:02,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=34910, skipped=55, lr=[0.00019979897744119547, 0.00019979897744119547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34910 loss: 0.7541 iter time (s): 4.283 samples/sec: 29.884
g0198:  iteration    34910/10000000 | consumed samples:      4468480 | consumed tokens:   9151447040 | elapsed time per iteration (ms): 4315.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.698252E-01 | loss scale: 2048.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.658 | tokens per gpu per second (tgs): 1898.108 | TFLOPs: 15.27 |
g0184: [2024-08-11 03:38:42,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=34920, skipped=55, lr=[0.00019979880612631853, 0.00019979880612631853], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34920 loss: 0.7622 iter time (s): 4.023 samples/sec: 31.815
g0198:  iteration    34920/10000000 | consumed samples:      4469760 | consumed tokens:   9154068480 | elapsed time per iteration (ms): 4055.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.693618E-01 | loss scale: 2048.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.560 | tokens per gpu per second (tgs): 2019.832 | TFLOPs: 16.25 |
g0184: [2024-08-11 03:39:25,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=34930, skipped=55, lr=[0.00019979863473855132, 0.00019979863473855132], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34930 loss: 0.7765 iter time (s): 4.196 samples/sec: 30.509
g0198:  iteration    34930/10000000 | consumed samples:      4471040 | consumed tokens:   9156689920 | elapsed time per iteration (ms): 4228.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.691941E-01 | loss scale: 2048.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.273 | tokens per gpu per second (tgs): 1937.442 | TFLOPs: 15.59 |
g0184: [2024-08-11 03:40:06,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=34940, skipped=55, lr=[0.00019979846327789397, 0.00019979846327789397], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34940 loss: 0.7471 iter time (s): 4.135 samples/sec: 30.957
g0198:  iteration    34940/10000000 | consumed samples:      4472320 | consumed tokens:   9159311360 | elapsed time per iteration (ms): 4167.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.616192E-01 | loss scale: 2048.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.715 | tokens per gpu per second (tgs): 1965.731 | TFLOPs: 15.82 |
g0184: [2024-08-11 03:40:47,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=34950, skipped=55, lr=[0.00019979829174434662, 0.00019979829174434662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34950 loss: 0.7832 iter time (s): 4.025 samples/sec: 31.800
g0198:  iteration    34950/10000000 | consumed samples:      4473600 | consumed tokens:   9161932800 | elapsed time per iteration (ms): 4057.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.607330E-01 | loss scale: 2048.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.544 | tokens per gpu per second (tgs): 2018.837 | TFLOPs: 16.25 |
g0184: [2024-08-11 03:41:28,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=34960, skipped=55, lr=[0.00019979812013790941, 0.00019979812013790941], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34960 loss: 0.7533 iter time (s): 4.074 samples/sec: 31.420
g0198:  iteration    34960/10000000 | consumed samples:      4474880 | consumed tokens:   9164554240 | elapsed time per iteration (ms): 4106.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.640975E-01 | loss scale: 2048.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.167 | tokens per gpu per second (tgs): 1994.709 | TFLOPs: 16.05 |
g0184: [2024-08-11 03:42:10,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=34970, skipped=55, lr=[0.0001997979484585824, 0.0001997979484585824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34970 loss: 0.7389 iter time (s): 4.163 samples/sec: 30.751
g0198:  iteration    34970/10000000 | consumed samples:      4476160 | consumed tokens:   9167175680 | elapsed time per iteration (ms): 4195.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.617517E-01 | loss scale: 2048.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.509 | tokens per gpu per second (tgs): 1952.570 | TFLOPs: 15.71 |
g0184: [2024-08-11 03:42:52,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=34980, skipped=55, lr=[0.00019979777670636582, 0.00019979777670636582], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34980 loss: 0.7808 iter time (s): 4.203 samples/sec: 30.453
g0198:  iteration    34980/10000000 | consumed samples:      4477440 | consumed tokens:   9169797120 | elapsed time per iteration (ms): 4237.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.610508E-01 | loss scale: 2048.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.207 | tokens per gpu per second (tgs): 1933.245 | TFLOPs: 15.56 |
g0184: [2024-08-11 03:43:34,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=34990, skipped=55, lr=[0.00019979760488125976, 0.00019979760488125976], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 34990 loss: 0.7518 iter time (s): 4.169 samples/sec: 30.705
g0198:  iteration    34990/10000000 | consumed samples:      4478720 | consumed tokens:   9172418560 | elapsed time per iteration (ms): 4201.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.546774E-01 | loss scale: 2048.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.468 | tokens per gpu per second (tgs): 1949.957 | TFLOPs: 15.69 |
g0184: [2024-08-11 03:44:16,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=55, lr=[0.00019979743298326435, 0.00019979743298326435], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35000 loss: 0.7587 iter time (s): 4.102 samples/sec: 31.206
g0198:  iteration    35000/10000000 | consumed samples:      4480000 | consumed tokens:   9175040000 | elapsed time per iteration (ms): 4135.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.635221E-01 | loss scale: 2048.0 | grad norm: 0.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.950 | tokens per gpu per second (tgs): 1980.825 | TFLOPs: 15.94 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 35000 | lm loss value: 7.632046E-01 | lm loss PPL: 2.145140E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   35000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 03:51:00,059] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step35000 is about to be saved!
g0184: [2024-08-11 03:51:00,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0184: [2024-08-11 03:51:00,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0198: [2024-08-11 03:51:00,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0198: [2024-08-11 03:51:00,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0198: [2024-08-11 03:51:00,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0184: [2024-08-11 03:51:00,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0185: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0185: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0185: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0195: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0195: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0195: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0187: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0187: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0187: [2024-08-11 03:51:00,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0188: [2024-08-11 03:51:00,072] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0188: [2024-08-11 03:51:00,072] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0188: [2024-08-11 03:51:00,072] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0194: [2024-08-11 03:51:00,072] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0194: [2024-08-11 03:51:00,072] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0194: [2024-08-11 03:51:00,073] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0197: [2024-08-11 03:51:00,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0197: [2024-08-11 03:51:00,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0197: [2024-08-11 03:51:00,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0184: [2024-08-11 03:51:00,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_01-model_00-model_states.pt...
g0198: [2024-08-11 03:51:00,098] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_23-model_00-model_states.pt...
g0187: [2024-08-11 03:51:00,102] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_08-model_00-model_states.pt...
g0185: [2024-08-11 03:51:00,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_05-model_00-model_states.pt...
g0195: [2024-08-11 03:51:00,107] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_17-model_00-model_states.pt...
g0188: [2024-08-11 03:51:00,108] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_11-model_00-model_states.pt...
g0197: [2024-08-11 03:51:00,108] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_20-model_00-model_states.pt...
g0194: [2024-08-11 03:51:00,111] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_14-model_00-model_states.pt...
g0185: [2024-08-11 03:51:00,237] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_05-model_00-model_states.pt.
g0184: [2024-08-11 03:51:00,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_01-model_00-model_states.pt.
g0195: [2024-08-11 03:51:00,247] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_17-model_00-model_states.pt.
g0197: [2024-08-11 03:51:00,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_20-model_00-model_states.pt.
g0198: [2024-08-11 03:51:00,251] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 03:51:00,251] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 03:51:00,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_24-model_00-model_states.pt.
g0187: [2024-08-11 03:51:00,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_08-model_00-model_states.pt.
g0194: [2024-08-11 03:51:00,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_14-model_00-model_states.pt.
g0184: [2024-08-11 03:51:00,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_02-model_00-model_states.pt...
g0185: [2024-08-11 03:51:00,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_06-model_00-model_states.pt...
g0197: [2024-08-11 03:51:00,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_21-model_00-model_states.pt...
g0195: [2024-08-11 03:51:00,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_18-model_00-model_states.pt...
g0187: [2024-08-11 03:51:00,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_09-model_00-model_states.pt...
g0194: [2024-08-11 03:51:00,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_15-model_00-model_states.pt...
g0198: [2024-08-11 03:51:00,302] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_25-model_00-model_states.pt...
g0188: [2024-08-11 03:51:00,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_11-model_00-model_states.pt.
g0197: [2024-08-11 03:51:00,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_21-model_00-model_states.pt.
g0187: [2024-08-11 03:51:00,404] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_09-model_00-model_states.pt.
g0188: [2024-08-11 03:51:00,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_12-model_00-model_states.pt...
g0194: [2024-08-11 03:51:00,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_15-model_00-model_states.pt.
g0195: [2024-08-11 03:51:00,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_18-model_00-model_states.pt.
g0187: [2024-08-11 03:51:00,430] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_10-model_00-model_states.pt...
g0185: [2024-08-11 03:51:00,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_06-model_00-model_states.pt.
g0197: [2024-08-11 03:51:00,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_22-model_00-model_states.pt...
g0194: [2024-08-11 03:51:00,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_16-model_00-model_states.pt...
g0195: [2024-08-11 03:51:00,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_19-model_00-model_states.pt...
g0185: [2024-08-11 03:51:00,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_07-model_00-model_states.pt...
g0198: [2024-08-11 03:51:00,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 03:51:00,488] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_07_model_states.pt...
g0188: [2024-08-11 03:51:00,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_12-model_00-model_states.pt.
g0185: [2024-08-11 03:51:00,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 03:51:00,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_01_model_states.pt...
g0188: [2024-08-11 03:51:00,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_13-model_00-model_states.pt...
g0197: [2024-08-11 03:51:00,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 03:51:00,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_06_model_states.pt...
g0187: [2024-08-11 03:51:00,567] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 03:51:00,569] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_02_model_states.pt...
g0195: [2024-08-11 03:51:00,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 03:51:00,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_05_model_states.pt...
g0194: [2024-08-11 03:51:00,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 03:51:00,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_04_model_states.pt...
g0188: [2024-08-11 03:51:00,965] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 03:51:00,967] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_03_model_states.pt...
g0184: [2024-08-11 03:51:01,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 03:51:01,045] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 03:51:01,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 03:51:01,247] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 03:51:01,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 03:51:01,361] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_00_model_states.pt
g0184: [2024-08-11 03:51:01,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 03:51:02,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 03:51:02,430] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0187: [2024-08-11 03:51:02,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 03:51:02,894] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0197: [2024-08-11 03:51:02,945] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 03:51:02,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0185: [2024-08-11 03:51:03,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 03:51:03,197] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0194: [2024-08-11 03:51:03,310] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 03:51:03,311] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0188: [2024-08-11 03:51:03,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 03:51:03,336] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0195: [2024-08-11 03:51:04,369] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 03:51:04,369] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0184: [2024-08-11 03:51:04,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step35000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 03:51:04,873] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!
g0184:   successfully saved checkpoint at iteration   35000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.65, Latency(second): 4.84
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4838.63, 4839.90)
g0184: [2024-08-11 03:51:48,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=35010, skipped=55, lr=[0.00019979726101237974, 0.00019979726101237974], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35010 loss: 0.7459 iter time (s): 4.312 samples/sec: 29.686
g0198:  iteration    35010/10000000 | consumed samples:      4481280 | consumed tokens:   9177661440 | elapsed time per iteration (ms): 45206.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.615762E-01 | loss scale: 2048.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.831 | tokens per gpu per second (tgs): 181.212 | TFLOPs: 1.46 |
g0184: [2024-08-11 03:52:30,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=35020, skipped=55, lr=[0.000199797088968606, 0.000199797088968606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35020 loss: 0.7950 iter time (s): 4.186 samples/sec: 30.581
g0198:  iteration    35020/10000000 | consumed samples:      4482560 | consumed tokens:   9180282880 | elapsed time per iteration (ms): 4218.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.659296E-01 | loss scale: 2048.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.344 | tokens per gpu per second (tgs): 1942.013 | TFLOPs: 15.63 |
g0184: [2024-08-11 03:53:12,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=35030, skipped=55, lr=[0.0001997969168519434, 0.0001997969168519434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35030 loss: 0.7856 iter time (s): 4.179 samples/sec: 30.628
g0198:  iteration    35030/10000000 | consumed samples:      4483840 | consumed tokens:   9182904320 | elapsed time per iteration (ms): 4211.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.671412E-01 | loss scale: 2048.0 | grad norm: 0.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.392 | tokens per gpu per second (tgs): 1945.084 | TFLOPs: 15.65 |
g0184: [2024-08-11 03:53:53,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=35040, skipped=55, lr=[0.0001997967446623919, 0.0001997967446623919], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35040 loss: 0.7722 iter time (s): 4.063 samples/sec: 31.507
g0198:  iteration    35040/10000000 | consumed samples:      4485120 | consumed tokens:   9185525760 | elapsed time per iteration (ms): 4095.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.682466E-01 | loss scale: 2048.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.254 | tokens per gpu per second (tgs): 2000.265 | TFLOPs: 16.10 |
g0184: [2024-08-11 03:54:35,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=35050, skipped=55, lr=[0.00019979657239995176, 0.00019979657239995176], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35050 loss: 0.7222 iter time (s): 4.174 samples/sec: 30.668
g0198:  iteration    35050/10000000 | consumed samples:      4486400 | consumed tokens:   9188147200 | elapsed time per iteration (ms): 4206.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.559179E-01 | loss scale: 2048.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.429 | tokens per gpu per second (tgs): 1947.469 | TFLOPs: 15.67 |
g0184: [2024-08-11 03:55:15,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=35060, skipped=55, lr=[0.00019979640006462303, 0.00019979640006462303], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35060 loss: 0.7868 iter time (s): 3.976 samples/sec: 32.191
g0198:  iteration    35060/10000000 | consumed samples:      4487680 | consumed tokens:   9190768640 | elapsed time per iteration (ms): 4010.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.641343E-01 | loss scale: 2048.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.916 | tokens per gpu per second (tgs): 2042.593 | TFLOPs: 16.44 |
g0184: [2024-08-11 03:55:58,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=35070, skipped=55, lr=[0.0001997962276564059, 0.0001997962276564059], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35070 loss: 0.7816 iter time (s): 4.202 samples/sec: 30.464
g0198:  iteration    35070/10000000 | consumed samples:      4488960 | consumed tokens:   9193390080 | elapsed time per iteration (ms): 4234.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.602210E-01 | loss scale: 2048.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.230 | tokens per gpu per second (tgs): 1934.733 | TFLOPs: 15.57 |
g0184: [2024-08-11 03:56:42,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=35080, skipped=55, lr=[0.0001997960551753005, 0.0001997960551753005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35080 loss: 0.7672 iter time (s): 4.366 samples/sec: 29.319
g0198:  iteration    35080/10000000 | consumed samples:      4490240 | consumed tokens:   9196011520 | elapsed time per iteration (ms): 4412.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.599741E-01 | loss scale: 2048.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.011 | tokens per gpu per second (tgs): 1856.707 | TFLOPs: 14.94 |
g0184: [2024-08-11 03:57:24,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=35090, skipped=55, lr=[0.0001997958826213069, 0.0001997958826213069], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35090 loss: 0.7606 iter time (s): 4.166 samples/sec: 30.726
g0198:  iteration    35090/10000000 | consumed samples:      4491520 | consumed tokens:   9198632960 | elapsed time per iteration (ms): 4198.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.554117E-01 | loss scale: 2048.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1950.969 | TFLOPs: 15.70 |
g0184: [2024-08-11 03:58:07,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=35100, skipped=55, lr=[0.00019979570999442535, 0.00019979570999442535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35100 loss: 0.7629 iter time (s): 4.291 samples/sec: 29.833
g0198:  iteration    35100/10000000 | consumed samples:      4492800 | consumed tokens:   9201254400 | elapsed time per iteration (ms): 4325.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.551873E-01 | loss scale: 2048.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.594 | tokens per gpu per second (tgs): 1894.023 | TFLOPs: 15.24 |
g0184: [2024-08-11 03:58:51,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=35110, skipped=55, lr=[0.00019979553729465585, 0.00019979553729465585], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35110 loss: 0.7666 iter time (s): 4.380 samples/sec: 29.222
g0198:  iteration    35110/10000000 | consumed samples:      4494080 | consumed tokens:   9203875840 | elapsed time per iteration (ms): 4413.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.595565E-01 | loss scale: 2048.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.001 | tokens per gpu per second (tgs): 1856.065 | TFLOPs: 14.94 |
g0184: [2024-08-11 03:59:34,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=35120, skipped=55, lr=[0.0001997953645219986, 0.0001997953645219986], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35120 loss: 0.7615 iter time (s): 4.276 samples/sec: 29.932
g0198:  iteration    35120/10000000 | consumed samples:      4495360 | consumed tokens:   9206497280 | elapsed time per iteration (ms): 4309.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.658936E-01 | loss scale: 2048.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.703 | tokens per gpu per second (tgs): 1901.014 | TFLOPs: 15.30 |
g0184: [2024-08-11 04:00:17,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=35130, skipped=55, lr=[0.00019979519167645374, 0.00019979519167645374], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35130 loss: 0.7442 iter time (s): 4.273 samples/sec: 29.956
g0198:  iteration    35130/10000000 | consumed samples:      4496640 | consumed tokens:   9209118720 | elapsed time per iteration (ms): 4306.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.542827E-01 | loss scale: 2048.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.722 | tokens per gpu per second (tgs): 1902.213 | TFLOPs: 15.31 |
g0184: [2024-08-11 04:01:00,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=35140, skipped=55, lr=[0.00019979501875802138, 0.00019979501875802138], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35140 loss: 0.7577 iter time (s): 4.290 samples/sec: 29.839
g0198:  iteration    35140/10000000 | consumed samples:      4497920 | consumed tokens:   9211740160 | elapsed time per iteration (ms): 4322.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.628621E-01 | loss scale: 2048.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.612 | tokens per gpu per second (tgs): 1895.147 | TFLOPs: 15.25 |
g0184: [2024-08-11 04:01:42,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=35150, skipped=55, lr=[0.00019979484576670167, 0.00019979484576670167], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35150 loss: 0.7575 iter time (s): 4.115 samples/sec: 31.108
g0198:  iteration    35150/10000000 | consumed samples:      4499200 | consumed tokens:   9214361600 | elapsed time per iteration (ms): 4147.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.630698E-01 | loss scale: 2048.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.864 | tokens per gpu per second (tgs): 1975.268 | TFLOPs: 15.90 |
g0184: [2024-08-11 04:02:24,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=35160, skipped=55, lr=[0.00019979467270249475, 0.00019979467270249475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35160 loss: 0.7622 iter time (s): 4.171 samples/sec: 30.691
g0198:  iteration    35160/10000000 | consumed samples:      4500480 | consumed tokens:   9216983040 | elapsed time per iteration (ms): 4203.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.621264E-01 | loss scale: 2048.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.453 | tokens per gpu per second (tgs): 1948.976 | TFLOPs: 15.68 |
g0184: [2024-08-11 04:03:06,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=35170, skipped=55, lr=[0.00019979449956540073, 0.00019979449956540073], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35170 loss: 0.7545 iter time (s): 4.203 samples/sec: 30.454
g0198:  iteration    35170/10000000 | consumed samples:      4501760 | consumed tokens:   9219604480 | elapsed time per iteration (ms): 4235.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.488751E-01 | loss scale: 2048.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.220 | tokens per gpu per second (tgs): 1934.100 | TFLOPs: 15.56 |
g0184: [2024-08-11 04:03:48,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=35180, skipped=55, lr=[0.0001997943263554197, 0.0001997943263554197], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35180 loss: 0.7579 iter time (s): 4.132 samples/sec: 30.979
g0198:  iteration    35180/10000000 | consumed samples:      4503040 | consumed tokens:   9222225920 | elapsed time per iteration (ms): 4164.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.710907E-01 | loss scale: 2048.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.737 | tokens per gpu per second (tgs): 1967.169 | TFLOPs: 15.83 |
g0184: [2024-08-11 04:04:30,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=35190, skipped=55, lr=[0.0001997941530725519, 0.0001997941530725519], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35190 loss: 0.7550 iter time (s): 4.151 samples/sec: 30.837
g0198:  iteration    35190/10000000 | consumed samples:      4504320 | consumed tokens:   9224847360 | elapsed time per iteration (ms): 4183.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.635237E-01 | loss scale: 2048.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.596 | tokens per gpu per second (tgs): 1958.160 | TFLOPs: 15.76 |
g0184: [2024-08-11 04:05:11,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=35200, skipped=55, lr=[0.0001997939797167974, 0.0001997939797167974], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35200 loss: 0.7793 iter time (s): 4.132 samples/sec: 30.978
g0198:  iteration    35200/10000000 | consumed samples:      4505600 | consumed tokens:   9227468800 | elapsed time per iteration (ms): 4164.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.716050E-01 | loss scale: 2048.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.735 | tokens per gpu per second (tgs): 1967.014 | TFLOPs: 15.83 |
g0184: [2024-08-11 04:05:53,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=35210, skipped=55, lr=[0.00019979380628815633, 0.00019979380628815633], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35210 loss: 0.7547 iter time (s): 4.129 samples/sec: 30.997
g0198:  iteration    35210/10000000 | consumed samples:      4506880 | consumed tokens:   9230090240 | elapsed time per iteration (ms): 4170.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.684876E-01 | loss scale: 2048.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.692 | tokens per gpu per second (tgs): 1964.301 | TFLOPs: 15.81 |
g0184: [2024-08-11 04:06:35,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=35220, skipped=55, lr=[0.00019979363278662887, 0.00019979363278662887], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35220 loss: 0.7337 iter time (s): 4.147 samples/sec: 30.865
g0198:  iteration    35220/10000000 | consumed samples:      4508160 | consumed tokens:   9232711680 | elapsed time per iteration (ms): 4180.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.566319E-01 | loss scale: 2048.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.616 | tokens per gpu per second (tgs): 1959.411 | TFLOPs: 15.77 |
g0184: [2024-08-11 04:07:18,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=35230, skipped=55, lr=[0.00019979345921221508, 0.00019979345921221508], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35230 loss: 0.7821 iter time (s): 4.246 samples/sec: 30.144
g0198:  iteration    35230/10000000 | consumed samples:      4509440 | consumed tokens:   9235333120 | elapsed time per iteration (ms): 4279.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.620986E-01 | loss scale: 2048.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.911 | tokens per gpu per second (tgs): 1914.328 | TFLOPs: 15.40 |
g0184: [2024-08-11 04:07:59,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=35240, skipped=55, lr=[0.00019979328556491514, 0.00019979328556491514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35240 loss: 0.7454 iter time (s): 4.093 samples/sec: 31.271
g0198:  iteration    35240/10000000 | consumed samples:      4510720 | consumed tokens:   9237954560 | elapsed time per iteration (ms): 4130.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.619614E-01 | loss scale: 2048.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.990 | tokens per gpu per second (tgs): 1983.342 | TFLOPs: 15.96 |
g0184: [2024-08-11 04:08:42,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=35250, skipped=55, lr=[0.00019979311184472917, 0.00019979311184472917], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35250 loss: 0.7465 iter time (s): 4.228 samples/sec: 30.273
g0198:  iteration    35250/10000000 | consumed samples:      4512000 | consumed tokens:   9240576000 | elapsed time per iteration (ms): 4261.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.681743E-01 | loss scale: 2048.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.039 | tokens per gpu per second (tgs): 1922.519 | TFLOPs: 15.47 |
g0184: [2024-08-11 04:09:22,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=35260, skipped=55, lr=[0.00019979293805165733, 0.00019979293805165733], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35260 loss: 0.7600 iter time (s): 3.988 samples/sec: 32.093
g0198:  iteration    35260/10000000 | consumed samples:      4513280 | consumed tokens:   9243197440 | elapsed time per iteration (ms): 4021.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.670859E-01 | loss scale: 2048.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.829 | tokens per gpu per second (tgs): 2037.070 | TFLOPs: 16.39 |
g0184: [2024-08-11 04:10:04,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=35270, skipped=55, lr=[0.0001997927641856997, 0.0001997927641856997], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35270 loss: 0.7682 iter time (s): 4.173 samples/sec: 30.675
g0198:  iteration    35270/10000000 | consumed samples:      4514560 | consumed tokens:   9245818880 | elapsed time per iteration (ms): 4206.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.663610E-01 | loss scale: 2048.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.431 | tokens per gpu per second (tgs): 1947.559 | TFLOPs: 15.67 |
g0184: [2024-08-11 04:10:45,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=35280, skipped=55, lr=[0.00019979259024685648, 0.00019979259024685648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35280 loss: 0.7624 iter time (s): 4.064 samples/sec: 31.497
g0198:  iteration    35280/10000000 | consumed samples:      4515840 | consumed tokens:   9248440320 | elapsed time per iteration (ms): 4097.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.504929E-01 | loss scale: 2048.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.238 | tokens per gpu per second (tgs): 1999.222 | TFLOPs: 16.09 |
g0184: [2024-08-11 04:11:26,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=35290, skipped=55, lr=[0.0001997924162351277, 0.0001997924162351277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35290 loss: 0.7720 iter time (s): 4.116 samples/sec: 31.096
g0198:  iteration    35290/10000000 | consumed samples:      4517120 | consumed tokens:   9251061760 | elapsed time per iteration (ms): 4149.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.662701E-01 | loss scale: 2048.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.850 | tokens per gpu per second (tgs): 1974.420 | TFLOPs: 15.89 |
g0184: [2024-08-11 04:12:08,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=35300, skipped=55, lr=[0.00019979224215051363, 0.00019979224215051363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35300 loss: 0.7514 iter time (s): 4.132 samples/sec: 30.977
g0198:  iteration    35300/10000000 | consumed samples:      4518400 | consumed tokens:   9253683200 | elapsed time per iteration (ms): 4166.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.591729E-01 | loss scale: 2048.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.725 | tokens per gpu per second (tgs): 1966.382 | TFLOPs: 15.82 |
g0184: [2024-08-11 04:12:50,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=35310, skipped=55, lr=[0.00019979206799301434, 0.00019979206799301434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35310 loss: 0.7331 iter time (s): 4.121 samples/sec: 31.057
g0198:  iteration    35310/10000000 | consumed samples:      4519680 | consumed tokens:   9256304640 | elapsed time per iteration (ms): 4154.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.556597E-01 | loss scale: 2048.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.810 | tokens per gpu per second (tgs): 1971.827 | TFLOPs: 15.87 |
g0184: [2024-08-11 04:13:32,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=35320, skipped=55, lr=[0.00019979189376262993, 0.00019979189376262993], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35320 loss: 0.7861 iter time (s): 4.169 samples/sec: 30.700
g0198:  iteration    35320/10000000 | consumed samples:      4520960 | consumed tokens:   9258926080 | elapsed time per iteration (ms): 4202.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.736783E-01 | loss scale: 2048.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.455 | tokens per gpu per second (tgs): 1949.108 | TFLOPs: 15.68 |
g0184: [2024-08-11 04:14:13,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=35330, skipped=55, lr=[0.00019979171945936058, 0.00019979171945936058], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35330 loss: 0.7742 iter time (s): 4.127 samples/sec: 31.019
g0198:  iteration    35330/10000000 | consumed samples:      4522240 | consumed tokens:   9261547520 | elapsed time per iteration (ms): 4159.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.612354E-01 | loss scale: 2048.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.772 | tokens per gpu per second (tgs): 1969.424 | TFLOPs: 15.85 |
g0184: [2024-08-11 04:14:54,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=35340, skipped=55, lr=[0.0001997915450832064, 0.0001997915450832064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35340 loss: 0.7566 iter time (s): 4.063 samples/sec: 31.505
g0198:  iteration    35340/10000000 | consumed samples:      4523520 | consumed tokens:   9264168960 | elapsed time per iteration (ms): 4095.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.572977E-01 | loss scale: 2048.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.252 | tokens per gpu per second (tgs): 2000.129 | TFLOPs: 16.10 |
g0184: [2024-08-11 04:15:38,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=35350, skipped=55, lr=[0.00019979137063416756, 0.00019979137063416756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35350 loss: 0.7532 iter time (s): 4.358 samples/sec: 29.368
g0198:  iteration    35350/10000000 | consumed samples:      4524800 | consumed tokens:   9266790400 | elapsed time per iteration (ms): 4391.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.565133E-01 | loss scale: 2048.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.145 | tokens per gpu per second (tgs): 1865.308 | TFLOPs: 15.01 |
g0184: [2024-08-11 04:16:20,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=35360, skipped=55, lr=[0.00019979119611224412, 0.00019979119611224412], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35360 loss: 0.7757 iter time (s): 4.186 samples/sec: 30.576
g0198:  iteration    35360/10000000 | consumed samples:      4526080 | consumed tokens:   9269411840 | elapsed time per iteration (ms): 4219.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.550755E-01 | loss scale: 2048.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.336 | tokens per gpu per second (tgs): 1941.502 | TFLOPs: 15.62 |
g0184: [2024-08-11 04:17:02,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=35370, skipped=55, lr=[0.0001997910215174363, 0.0001997910215174363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35370 loss: 0.7677 iter time (s): 4.144 samples/sec: 30.888
g0198:  iteration    35370/10000000 | consumed samples:      4527360 | consumed tokens:   9272033280 | elapsed time per iteration (ms): 4176.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.504429E-01 | loss scale: 2048.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.644 | tokens per gpu per second (tgs): 1961.243 | TFLOPs: 15.78 |
g0184: [2024-08-11 04:17:45,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=35380, skipped=55, lr=[0.00019979084684974418, 0.00019979084684974418], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35380 loss: 0.7464 iter time (s): 4.289 samples/sec: 29.841
g0198:  iteration    35380/10000000 | consumed samples:      4528640 | consumed tokens:   9274654720 | elapsed time per iteration (ms): 4323.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.674164E-01 | loss scale: 2048.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.608 | tokens per gpu per second (tgs): 1894.888 | TFLOPs: 15.25 |
g0184: [2024-08-11 04:18:28,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=35390, skipped=55, lr=[0.00019979067210916792, 0.00019979067210916792], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35390 loss: 0.7877 iter time (s): 4.219 samples/sec: 30.341
g0198:  iteration    35390/10000000 | consumed samples:      4529920 | consumed tokens:   9277276160 | elapsed time per iteration (ms): 4252.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.568930E-01 | loss scale: 2048.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.103 | tokens per gpu per second (tgs): 1926.617 | TFLOPs: 15.50 |
g0184: [2024-08-11 04:19:10,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=35400, skipped=55, lr=[0.00019979049729570763, 0.00019979049729570763], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35400 loss: 0.7661 iter time (s): 4.197 samples/sec: 30.496
g0198:  iteration    35400/10000000 | consumed samples:      4531200 | consumed tokens:   9279897600 | elapsed time per iteration (ms): 4232.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.625509E-01 | loss scale: 2048.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.242 | tokens per gpu per second (tgs): 1935.463 | TFLOPs: 15.57 |
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0198: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0198: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0198: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0198: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0198: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0195: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0187: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0188: [2024-08-11 04:19:23,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0198: [2024-08-11 04:19:23,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
g0184: [2024-08-11 04:19:51,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=35410, skipped=55, lr=[0.00019979032240936346, 0.00019979032240936346], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35410 loss: 0.7868 iter time (s): 4.024 samples/sec: 31.811
g0198:  iteration    35410/10000000 | consumed samples:      4532480 | consumed tokens:   9282519040 | elapsed time per iteration (ms): 4056.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.622210E-01 | loss scale: 4096.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.554 | tokens per gpu per second (tgs): 2019.449 | TFLOPs: 16.25 |
g0184: [2024-08-11 04:20:33,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=35420, skipped=55, lr=[0.00019979014745013555, 0.00019979014745013555], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35420 loss: 0.7335 iter time (s): 4.234 samples/sec: 30.229
g0198:  iteration    35420/10000000 | consumed samples:      4533760 | consumed tokens:   9285140480 | elapsed time per iteration (ms): 4267.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.526775E-01 | loss scale: 4096.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.997 | tokens per gpu per second (tgs): 1919.837 | TFLOPs: 15.45 |
g0184: [2024-08-11 04:21:15,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=35430, skipped=55, lr=[0.00019978997241802403, 0.00019978997241802403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35430 loss: 0.7592 iter time (s): 4.097 samples/sec: 31.240
g0198:  iteration    35430/10000000 | consumed samples:      4535040 | consumed tokens:   9287761920 | elapsed time per iteration (ms): 4132.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.615024E-01 | loss scale: 4096.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.973 | tokens per gpu per second (tgs): 1982.292 | TFLOPs: 15.95 |
g0184: [2024-08-11 04:21:58,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=35440, skipped=55, lr=[0.00019978979731302902, 0.00019978979731302902], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35440 loss: 0.7559 iter time (s): 4.265 samples/sec: 30.015
g0198:  iteration    35440/10000000 | consumed samples:      4536320 | consumed tokens:   9290383360 | elapsed time per iteration (ms): 4297.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.579757E-01 | loss scale: 4096.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.785 | tokens per gpu per second (tgs): 1906.212 | TFLOPs: 15.34 |
g0184: [2024-08-11 04:22:39,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=35450, skipped=55, lr=[0.00019978962213515068, 0.00019978962213515068], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35450 loss: 0.7938 iter time (s): 4.065 samples/sec: 31.488
g0198:  iteration    35450/10000000 | consumed samples:      4537600 | consumed tokens:   9293004800 | elapsed time per iteration (ms): 4097.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.632866E-01 | loss scale: 4096.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.236 | tokens per gpu per second (tgs): 1999.123 | TFLOPs: 16.09 |
g0184: [2024-08-11 04:23:21,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=35460, skipped=55, lr=[0.00019978944688438913, 0.00019978944688438913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35460 loss: 0.7840 iter time (s): 4.171 samples/sec: 30.686
g0198:  iteration    35460/10000000 | consumed samples:      4538880 | consumed tokens:   9295626240 | elapsed time per iteration (ms): 4206.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.717018E-01 | loss scale: 4096.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.432 | tokens per gpu per second (tgs): 1947.660 | TFLOPs: 15.67 |
g0184: [2024-08-11 04:24:03,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=35470, skipped=55, lr=[0.0001997892715607445, 0.0001997892715607445], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35470 loss: 0.7602 iter time (s): 4.210 samples/sec: 30.406
g0198:  iteration    35470/10000000 | consumed samples:      4540160 | consumed tokens:   9298247680 | elapsed time per iteration (ms): 4242.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.608582E-01 | loss scale: 4096.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.170 | tokens per gpu per second (tgs): 1930.906 | TFLOPs: 15.54 |
g0184: [2024-08-11 04:24:47,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=35480, skipped=55, lr=[0.00019978909616421694, 0.00019978909616421694], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35480 loss: 0.7384 iter time (s): 4.348 samples/sec: 29.437
g0198:  iteration    35480/10000000 | consumed samples:      4541440 | consumed tokens:   9300869120 | elapsed time per iteration (ms): 4380.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.491773E-01 | loss scale: 4096.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.218 | tokens per gpu per second (tgs): 1869.953 | TFLOPs: 15.05 |
g0184: [2024-08-11 04:25:32,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=35490, skipped=55, lr=[0.00019978892069480656, 0.00019978892069480656], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35490 loss: 0.7670 iter time (s): 4.421 samples/sec: 28.950
g0198:  iteration    35490/10000000 | consumed samples:      4542720 | consumed tokens:   9303490560 | elapsed time per iteration (ms): 4453.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.687279E-01 | loss scale: 4096.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.739 | tokens per gpu per second (tgs): 1839.290 | TFLOPs: 14.80 |
g0184: [2024-08-11 04:26:15,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=35500, skipped=55, lr=[0.00019978874515251354, 0.00019978874515251354], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35500 loss: 0.7581 iter time (s): 4.301 samples/sec: 29.763
g0198:  iteration    35500/10000000 | consumed samples:      4544000 | consumed tokens:   9306112000 | elapsed time per iteration (ms): 4333.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.608220E-01 | loss scale: 4096.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.534 | tokens per gpu per second (tgs): 1890.197 | TFLOPs: 15.21 |
g0184: [2024-08-11 04:26:57,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=35510, skipped=55, lr=[0.00019978856953733795, 0.00019978856953733795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35510 loss: 0.7524 iter time (s): 4.170 samples/sec: 30.695
g0198:  iteration    35510/10000000 | consumed samples:      4545280 | consumed tokens:   9308733440 | elapsed time per iteration (ms): 4202.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.654119E-01 | loss scale: 4096.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.458 | tokens per gpu per second (tgs): 1949.294 | TFLOPs: 15.69 |
g0184: [2024-08-11 04:27:40,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=35520, skipped=55, lr=[0.00019978839384928, 0.00019978839384928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35520 loss: 0.7734 iter time (s): 4.259 samples/sec: 30.051
g0198:  iteration    35520/10000000 | consumed samples:      4546560 | consumed tokens:   9311354880 | elapsed time per iteration (ms): 4292.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.598207E-01 | loss scale: 4096.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.822 | tokens per gpu per second (tgs): 1908.599 | TFLOPs: 15.36 |
g0184: [2024-08-11 04:28:21,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=35530, skipped=55, lr=[0.00019978821808833974, 0.00019978821808833974], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35530 loss: 0.7408 iter time (s): 4.047 samples/sec: 31.632
g0198:  iteration    35530/10000000 | consumed samples:      4547840 | consumed tokens:   9313976320 | elapsed time per iteration (ms): 4080.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.583038E-01 | loss scale: 4096.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.366 | tokens per gpu per second (tgs): 2007.436 | TFLOPs: 16.15 |
g0184: [2024-08-11 04:29:02,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=35540, skipped=55, lr=[0.00019978804225451738, 0.00019978804225451738], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35540 loss: 0.7778 iter time (s): 4.103 samples/sec: 31.200
g0198:  iteration    35540/10000000 | consumed samples:      4549120 | consumed tokens:   9316597760 | elapsed time per iteration (ms): 4135.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.648324E-01 | loss scale: 4096.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.950 | tokens per gpu per second (tgs): 1980.826 | TFLOPs: 15.94 |
g0184: [2024-08-11 04:29:45,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=35550, skipped=55, lr=[0.00019978786634781303, 0.00019978786634781303], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35550 loss: 0.7175 iter time (s): 4.314 samples/sec: 29.673
g0198:  iteration    35550/10000000 | consumed samples:      4550400 | consumed tokens:   9319219200 | elapsed time per iteration (ms): 4346.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.502236E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.450 | tokens per gpu per second (tgs): 1884.781 | TFLOPs: 15.17 |
g0184: [2024-08-11 04:30:28,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=35560, skipped=55, lr=[0.00019978769036822682, 0.00019978769036822682], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35560 loss: 0.7670 iter time (s): 4.179 samples/sec: 30.628
g0198:  iteration    35560/10000000 | consumed samples:      4551680 | consumed tokens:   9321840640 | elapsed time per iteration (ms): 4213.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.562474E-01 | loss scale: 4096.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.378 | tokens per gpu per second (tgs): 1944.200 | TFLOPs: 15.65 |
g0184: [2024-08-11 04:31:10,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=35570, skipped=55, lr=[0.00019978751431575886, 0.00019978751431575886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35570 loss: 0.7377 iter time (s): 4.183 samples/sec: 30.603
g0198:  iteration    35570/10000000 | consumed samples:      4552960 | consumed tokens:   9324462080 | elapsed time per iteration (ms): 4215.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.540060E-01 | loss scale: 4096.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.366 | tokens per gpu per second (tgs): 1943.422 | TFLOPs: 15.64 |
g0184: [2024-08-11 04:31:53,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=35580, skipped=55, lr=[0.00019978733819040934, 0.00019978733819040934], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35580 loss: 0.7806 iter time (s): 4.249 samples/sec: 30.128
g0198:  iteration    35580/10000000 | consumed samples:      4554240 | consumed tokens:   9327083520 | elapsed time per iteration (ms): 4283.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.635499E-01 | loss scale: 4096.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.883 | tokens per gpu per second (tgs): 1912.499 | TFLOPs: 15.39 |
g0184: [2024-08-11 04:32:35,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=35590, skipped=55, lr=[0.00019978716199217837, 0.00019978716199217837], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35590 loss: 0.7657 iter time (s): 4.174 samples/sec: 30.664
g0198:  iteration    35590/10000000 | consumed samples:      4555520 | consumed tokens:   9329704960 | elapsed time per iteration (ms): 4206.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.602974E-01 | loss scale: 4096.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.427 | tokens per gpu per second (tgs): 1947.327 | TFLOPs: 15.67 |
g0184: [2024-08-11 04:33:16,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=35600, skipped=55, lr=[0.00019978698572106608, 0.00019978698572106608], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35600 loss: 0.7551 iter time (s): 4.153 samples/sec: 30.824
g0198:  iteration    35600/10000000 | consumed samples:      4556800 | consumed tokens:   9332326400 | elapsed time per iteration (ms): 4185.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.693070E-01 | loss scale: 4096.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.582 | tokens per gpu per second (tgs): 1957.276 | TFLOPs: 15.75 |
g0184: [2024-08-11 04:33:58,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=35610, skipped=55, lr=[0.00019978680937707258, 0.00019978680937707258], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35610 loss: 0.7745 iter time (s): 4.099 samples/sec: 31.230
g0198:  iteration    35610/10000000 | consumed samples:      4558080 | consumed tokens:   9334947840 | elapsed time per iteration (ms): 4131.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.604700E-01 | loss scale: 4096.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.984 | tokens per gpu per second (tgs): 1982.985 | TFLOPs: 15.96 |
g0184: [2024-08-11 04:34:41,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=35620, skipped=55, lr=[0.00019978663296019806, 0.00019978663296019806], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35620 loss: 0.7501 iter time (s): 4.288 samples/sec: 29.848
g0198:  iteration    35620/10000000 | consumed samples:      4559360 | consumed tokens:   9337569280 | elapsed time per iteration (ms): 4322.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.618991E-01 | loss scale: 4096.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.615 | tokens per gpu per second (tgs): 1895.351 | TFLOPs: 15.25 |
g0184: [2024-08-11 04:35:24,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=35630, skipped=55, lr=[0.0001997864564704426, 0.0001997864564704426], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35630 loss: 0.7623 iter time (s): 4.296 samples/sec: 29.796
g0198:  iteration    35630/10000000 | consumed samples:      4560640 | consumed tokens:   9340190720 | elapsed time per iteration (ms): 4329.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.692925E-01 | loss scale: 4096.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.563 | tokens per gpu per second (tgs): 1892.018 | TFLOPs: 15.23 |
g0184: [2024-08-11 04:36:07,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=35640, skipped=55, lr=[0.0001997862799078064, 0.0001997862799078064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35640 loss: 0.7598 iter time (s): 4.251 samples/sec: 30.114
g0198:  iteration    35640/10000000 | consumed samples:      4561920 | consumed tokens:   9342812160 | elapsed time per iteration (ms): 4283.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.607732E-01 | loss scale: 4096.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.885 | tokens per gpu per second (tgs): 1912.610 | TFLOPs: 15.39 |
g0184: [2024-08-11 04:36:49,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=35650, skipped=55, lr=[0.00019978610327228957, 0.00019978610327228957], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35650 loss: 0.7605 iter time (s): 4.166 samples/sec: 30.727
g0198:  iteration    35650/10000000 | consumed samples:      4563200 | consumed tokens:   9345433600 | elapsed time per iteration (ms): 4198.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.594645E-01 | loss scale: 4096.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.485 | tokens per gpu per second (tgs): 1951.022 | TFLOPs: 15.70 |
g0184: [2024-08-11 04:37:31,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=35660, skipped=55, lr=[0.00019978592656389222, 0.00019978592656389222], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35660 loss: 0.7343 iter time (s): 4.180 samples/sec: 30.619
g0198:  iteration    35660/10000000 | consumed samples:      4564480 | consumed tokens:   9348055040 | elapsed time per iteration (ms): 4213.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.545474E-01 | loss scale: 4096.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.382 | tokens per gpu per second (tgs): 1944.433 | TFLOPs: 15.65 |
g0184: [2024-08-11 04:38:12,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=35670, skipped=55, lr=[0.0001997857497826145, 0.0001997857497826145], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35670 loss: 0.7387 iter time (s): 4.041 samples/sec: 31.672
g0198:  iteration    35670/10000000 | consumed samples:      4565760 | consumed tokens:   9350676480 | elapsed time per iteration (ms): 4077.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.565333E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.392 | tokens per gpu per second (tgs): 2009.111 | TFLOPs: 16.17 |
g0184: [2024-08-11 04:38:54,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=35680, skipped=55, lr=[0.00019978557292845656, 0.00019978557292845656], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35680 loss: 0.7486 iter time (s): 4.166 samples/sec: 30.723
g0198:  iteration    35680/10000000 | consumed samples:      4567040 | consumed tokens:   9353297920 | elapsed time per iteration (ms): 4200.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.510185E-01 | loss scale: 4096.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.476 | tokens per gpu per second (tgs): 1950.488 | TFLOPs: 15.70 |
g0184: [2024-08-11 04:39:36,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=35690, skipped=55, lr=[0.00019978539600141852, 0.00019978539600141852], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35690 loss: 0.7559 iter time (s): 4.214 samples/sec: 30.375
g0198:  iteration    35690/10000000 | consumed samples:      4568320 | consumed tokens:   9355919360 | elapsed time per iteration (ms): 4250.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.599821E-01 | loss scale: 4096.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.115 | tokens per gpu per second (tgs): 1927.347 | TFLOPs: 15.51 |
g0184: [2024-08-11 04:40:18,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=35700, skipped=55, lr=[0.0001997852190015005, 0.0001997852190015005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35700 loss: 0.7728 iter time (s): 4.161 samples/sec: 30.761
g0198:  iteration    35700/10000000 | consumed samples:      4569600 | consumed tokens:   9358540800 | elapsed time per iteration (ms): 4195.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.532959E-01 | loss scale: 4096.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.508 | tokens per gpu per second (tgs): 1952.539 | TFLOPs: 15.71 |
g0184: [2024-08-11 04:41:02,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=35710, skipped=55, lr=[0.00019978504192870268, 0.00019978504192870268], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35710 loss: 0.7605 iter time (s): 4.295 samples/sec: 29.803
g0198:  iteration    35710/10000000 | consumed samples:      4570880 | consumed tokens:   9361162240 | elapsed time per iteration (ms): 4329.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.603814E-01 | loss scale: 4096.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.563 | tokens per gpu per second (tgs): 1892.014 | TFLOPs: 15.23 |
g0184: [2024-08-11 04:41:43,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=35720, skipped=55, lr=[0.0001997848647830252, 0.0001997848647830252], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35720 loss: 0.7768 iter time (s): 4.116 samples/sec: 31.101
g0198:  iteration    35720/10000000 | consumed samples:      4572160 | consumed tokens:   9363783680 | elapsed time per iteration (ms): 4147.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.649273E-01 | loss scale: 4096.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.860 | tokens per gpu per second (tgs): 1975.019 | TFLOPs: 15.89 |
g0184: [2024-08-11 04:42:25,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=35730, skipped=55, lr=[0.00019978468756446814, 0.00019978468756446814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35730 loss: 0.7205 iter time (s): 4.157 samples/sec: 30.795
g0198:  iteration    35730/10000000 | consumed samples:      4573440 | consumed tokens:   9366405120 | elapsed time per iteration (ms): 4189.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.627983E-01 | loss scale: 4096.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.550 | tokens per gpu per second (tgs): 1955.199 | TFLOPs: 15.73 |
g0184: [2024-08-11 04:43:07,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=35740, skipped=55, lr=[0.00019978451027303164, 0.00019978451027303164], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35740 loss: 0.7445 iter time (s): 4.172 samples/sec: 30.683
g0198:  iteration    35740/10000000 | consumed samples:      4574720 | consumed tokens:   9369026560 | elapsed time per iteration (ms): 4203.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.625002E-01 | loss scale: 4096.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.450 | tokens per gpu per second (tgs): 1948.768 | TFLOPs: 15.68 |
g0184: [2024-08-11 04:43:48,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=35750, skipped=55, lr=[0.0001997843329087159, 0.0001997843329087159], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35750 loss: 0.7494 iter time (s): 4.047 samples/sec: 31.631
g0198:  iteration    35750/10000000 | consumed samples:      4576000 | consumed tokens:   9371648000 | elapsed time per iteration (ms): 4078.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.615908E-01 | loss scale: 4096.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.381 | tokens per gpu per second (tgs): 2008.381 | TFLOPs: 16.16 |
g0184: [2024-08-11 04:44:31,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=35760, skipped=55, lr=[0.000199784155471521, 0.000199784155471521], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35760 loss: 0.7692 iter time (s): 4.316 samples/sec: 29.656
g0198:  iteration    35760/10000000 | consumed samples:      4577280 | consumed tokens:   9374269440 | elapsed time per iteration (ms): 4349.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.664348E-01 | loss scale: 4096.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.427 | tokens per gpu per second (tgs): 1883.316 | TFLOPs: 15.16 |
g0184: [2024-08-11 04:45:13,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=35770, skipped=55, lr=[0.00019978397796144708, 0.00019978397796144708], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35770 loss: 0.7598 iter time (s): 4.158 samples/sec: 30.787
g0198:  iteration    35770/10000000 | consumed samples:      4578560 | consumed tokens:   9376890880 | elapsed time per iteration (ms): 4189.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.519461E-01 | loss scale: 4096.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.550 | tokens per gpu per second (tgs): 1955.177 | TFLOPs: 15.73 |
g0184: [2024-08-11 04:45:55,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=35780, skipped=55, lr=[0.00019978380037849434, 0.00019978380037849434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35780 loss: 0.7403 iter time (s): 4.177 samples/sec: 30.647
g0198:  iteration    35780/10000000 | consumed samples:      4579840 | consumed tokens:   9379512320 | elapsed time per iteration (ms): 4209.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.662008E-01 | loss scale: 4096.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.411 | tokens per gpu per second (tgs): 1946.275 | TFLOPs: 15.66 |
g0184: [2024-08-11 04:46:39,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=35790, skipped=55, lr=[0.0001997836227226628, 0.0001997836227226628], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35790 loss: 0.7620 iter time (s): 4.326 samples/sec: 29.590
g0198:  iteration    35790/10000000 | consumed samples:      4581120 | consumed tokens:   9382133760 | elapsed time per iteration (ms): 4360.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.641381E-01 | loss scale: 4096.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.358 | tokens per gpu per second (tgs): 1878.903 | TFLOPs: 15.12 |
g0184: [2024-08-11 04:47:21,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=35800, skipped=55, lr=[0.00019978344499395272, 0.00019978344499395272], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35800 loss: 0.7324 iter time (s): 4.178 samples/sec: 30.640
g0198:  iteration    35800/10000000 | consumed samples:      4582400 | consumed tokens:   9384755200 | elapsed time per iteration (ms): 4210.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.678848E-01 | loss scale: 4096.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.400 | tokens per gpu per second (tgs): 1945.607 | TFLOPs: 15.66 |
g0184: [2024-08-11 04:48:03,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=35810, skipped=55, lr=[0.00019978326719236416, 0.00019978326719236416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35810 loss: 0.7576 iter time (s): 4.191 samples/sec: 30.540
g0198:  iteration    35810/10000000 | consumed samples:      4583680 | consumed tokens:   9387376640 | elapsed time per iteration (ms): 4223.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.644471E-01 | loss scale: 4096.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.564 | TFLOPs: 15.61 |
g0184: [2024-08-11 04:48:45,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=35820, skipped=55, lr=[0.0001997830893178973, 0.0001997830893178973], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35820 loss: 0.7664 iter time (s): 4.114 samples/sec: 31.111
g0198:  iteration    35820/10000000 | consumed samples:      4584960 | consumed tokens:   9389998080 | elapsed time per iteration (ms): 4146.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.647947E-01 | loss scale: 4096.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.867 | tokens per gpu per second (tgs): 1975.515 | TFLOPs: 15.90 |
g0184: [2024-08-11 04:49:27,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=35830, skipped=55, lr=[0.00019978291137055223, 0.00019978291137055223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35830 loss: 0.7396 iter time (s): 4.141 samples/sec: 30.908
g0198:  iteration    35830/10000000 | consumed samples:      4586240 | consumed tokens:   9392619520 | elapsed time per iteration (ms): 4173.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.566175E-01 | loss scale: 4096.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.667 | tokens per gpu per second (tgs): 1962.718 | TFLOPs: 15.79 |
g0184: [2024-08-11 04:50:08,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=35840, skipped=55, lr=[0.0001997827333503291, 0.0001997827333503291], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35840 loss: 0.7864 iter time (s): 4.067 samples/sec: 31.476
g0198:  iteration    35840/10000000 | consumed samples:      4587520 | consumed tokens:   9395240960 | elapsed time per iteration (ms): 4098.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.507437E-01 | loss scale: 4096.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.228 | tokens per gpu per second (tgs): 1998.574 | TFLOPs: 16.08 |
g0184: [2024-08-11 04:50:51,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=35850, skipped=55, lr=[0.00019978255525722807, 0.00019978255525722807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35850 loss: 0.7573 iter time (s): 4.325 samples/sec: 29.596
g0198:  iteration    35850/10000000 | consumed samples:      4588800 | consumed tokens:   9397862400 | elapsed time per iteration (ms): 4357.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.625042E-01 | loss scale: 4096.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.372 | tokens per gpu per second (tgs): 1879.816 | TFLOPs: 15.13 |
g0184: [2024-08-11 04:51:34,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=35860, skipped=55, lr=[0.00019978237709124928, 0.00019978237709124928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35860 loss: 0.7673 iter time (s): 4.228 samples/sec: 30.275
g0198:  iteration    35860/10000000 | consumed samples:      4590080 | consumed tokens:   9400483840 | elapsed time per iteration (ms): 4262.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.538964E-01 | loss scale: 4096.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.028 | tokens per gpu per second (tgs): 1921.772 | TFLOPs: 15.46 |
g0184: [2024-08-11 04:52:17,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=35870, skipped=55, lr=[0.00019978219885239285, 0.00019978219885239285], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35870 loss: 0.7703 iter time (s): 4.329 samples/sec: 29.568
g0198:  iteration    35870/10000000 | consumed samples:      4591360 | consumed tokens:   9403105280 | elapsed time per iteration (ms): 4361.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.588848E-01 | loss scale: 4096.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.346 | tokens per gpu per second (tgs): 1878.161 | TFLOPs: 15.11 |
g0184: [2024-08-11 04:53:00,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=35880, skipped=55, lr=[0.00019978202054065893, 0.00019978202054065893], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35880 loss: 0.7899 iter time (s): 4.248 samples/sec: 30.129
g0198:  iteration    35880/10000000 | consumed samples:      4592640 | consumed tokens:   9405726720 | elapsed time per iteration (ms): 4281.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.737991E-01 | loss scale: 4096.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.899 | tokens per gpu per second (tgs): 1913.539 | TFLOPs: 15.40 |
g0184: [2024-08-11 04:53:42,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=35890, skipped=55, lr=[0.00019978184215604762, 0.00019978184215604762], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35890 loss: 0.7378 iter time (s): 4.137 samples/sec: 30.939
g0198:  iteration    35890/10000000 | consumed samples:      4593920 | consumed tokens:   9408348160 | elapsed time per iteration (ms): 4169.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.616184E-01 | loss scale: 4096.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.696 | tokens per gpu per second (tgs): 1964.558 | TFLOPs: 15.81 |
g0184: [2024-08-11 04:54:25,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=35900, skipped=55, lr=[0.0001997816636985591, 0.0001997816636985591], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35900 loss: 0.7543 iter time (s): 4.281 samples/sec: 29.900
g0198:  iteration    35900/10000000 | consumed samples:      4595200 | consumed tokens:   9410969600 | elapsed time per iteration (ms): 4314.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.599543E-01 | loss scale: 4096.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.668 | tokens per gpu per second (tgs): 1898.775 | TFLOPs: 15.28 |
g0184: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 04:54:38,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 04:54:38,847] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 04:54:38,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 04:55:08,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=35910, skipped=55, lr=[0.0001997814851681935, 0.0001997814851681935], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35910 loss: 0.7704 iter time (s): 4.240 samples/sec: 30.191
g0198:  iteration    35910/10000000 | consumed samples:      4596480 | consumed tokens:   9413591040 | elapsed time per iteration (ms): 4274.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.574569E-01 | loss scale: 8192.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.942 | tokens per gpu per second (tgs): 1916.300 | TFLOPs: 15.42 |
g0184: [2024-08-11 04:55:49,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=35920, skipped=55, lr=[0.0001997813065649509, 0.0001997813065649509], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35920 loss: 0.7739 iter time (s): 4.123 samples/sec: 31.044
g0198:  iteration    35920/10000000 | consumed samples:      4597760 | consumed tokens:   9416212480 | elapsed time per iteration (ms): 4155.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.680311E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.802 | tokens per gpu per second (tgs): 1971.328 | TFLOPs: 15.86 |
g0184: [2024-08-11 04:56:31,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=35930, skipped=55, lr=[0.00019978112788883153, 0.00019978112788883153], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35930 loss: 0.7481 iter time (s): 4.155 samples/sec: 30.805
g0198:  iteration    35930/10000000 | consumed samples:      4599040 | consumed tokens:   9418833920 | elapsed time per iteration (ms): 4187.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.596476E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.568 | tokens per gpu per second (tgs): 1956.322 | TFLOPs: 15.74 |
g0184: [2024-08-11 04:57:14,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=35940, skipped=55, lr=[0.00019978094913983545, 0.00019978094913983545], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35940 loss: 0.7667 iter time (s): 4.201 samples/sec: 30.467
g0198:  iteration    35940/10000000 | consumed samples:      4600320 | consumed tokens:   9421455360 | elapsed time per iteration (ms): 4233.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.571208E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.235 | tokens per gpu per second (tgs): 1935.045 | TFLOPs: 15.57 |
g0184: [2024-08-11 04:57:55,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=35950, skipped=55, lr=[0.00019978077031796286, 0.00019978077031796286], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35950 loss: 0.7735 iter time (s): 4.081 samples/sec: 31.363
g0198:  iteration    35950/10000000 | consumed samples:      4601600 | consumed tokens:   9424076800 | elapsed time per iteration (ms): 4113.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.626657E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.116 | tokens per gpu per second (tgs): 1991.435 | TFLOPs: 16.03 |
g0184: [2024-08-11 04:58:37,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=35960, skipped=55, lr=[0.00019978059142321386, 0.00019978059142321386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35960 loss: 0.7456 iter time (s): 4.241 samples/sec: 30.185
g0198:  iteration    35960/10000000 | consumed samples:      4602880 | consumed tokens:   9426698240 | elapsed time per iteration (ms): 4272.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.655322E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.957 | tokens per gpu per second (tgs): 1917.238 | TFLOPs: 15.43 |
g0184: [2024-08-11 04:59:19,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=35970, skipped=55, lr=[0.0001997804124555886, 0.0001997804124555886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35970 loss: 0.7283 iter time (s): 4.158 samples/sec: 30.780
g0198:  iteration    35970/10000000 | consumed samples:      4604160 | consumed tokens:   9429319680 | elapsed time per iteration (ms): 4191.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.613650E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.542 | tokens per gpu per second (tgs): 1954.668 | TFLOPs: 15.73 |
g0184: [2024-08-11 05:00:02,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=35980, skipped=55, lr=[0.00019978023341508721, 0.00019978023341508721], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35980 loss: 0.7478 iter time (s): 4.250 samples/sec: 30.120
g0198:  iteration    35980/10000000 | consumed samples:      4605440 | consumed tokens:   9431941120 | elapsed time per iteration (ms): 4281.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.567057E-01 | loss scale: 8192.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.893 | tokens per gpu per second (tgs): 1913.150 | TFLOPs: 15.40 |
g0184: [2024-08-11 05:00:44,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=35990, skipped=55, lr=[0.00019978005430170982, 0.00019978005430170982], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 35990 loss: 0.7812 iter time (s): 4.175 samples/sec: 30.662
g0198:  iteration    35990/10000000 | consumed samples:      4606720 | consumed tokens:   9434562560 | elapsed time per iteration (ms): 4206.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.564615E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.427 | tokens per gpu per second (tgs): 1947.331 | TFLOPs: 15.67 |
g0184: [2024-08-11 05:01:26,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=55, lr=[0.00019977987511545658, 0.00019977987511545658], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36000 loss: 0.7452 iter time (s): 4.137 samples/sec: 30.939
g0198:  iteration    36000/10000000 | consumed samples:      4608000 | consumed tokens:   9437184000 | elapsed time per iteration (ms): 4184.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.685455E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.587 | tokens per gpu per second (tgs): 1957.547 | TFLOPs: 15.75 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 36000 | lm loss value: 7.572619E-01 | lm loss PPL: 2.132429E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   36000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 05:08:10,059] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step36000 is about to be saved!
g0198: [2024-08-11 05:08:10,064] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0198: [2024-08-11 05:08:10,064] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0198: [2024-08-11 05:08:10,064] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0184: [2024-08-11 05:08:10,065] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0184: [2024-08-11 05:08:10,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0184: [2024-08-11 05:08:10,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0188: [2024-08-11 05:08:10,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0188: [2024-08-11 05:08:10,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0188: [2024-08-11 05:08:10,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0187: [2024-08-11 05:08:10,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0187: [2024-08-11 05:08:10,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0187: [2024-08-11 05:08:10,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0194: [2024-08-11 05:08:10,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0185: [2024-08-11 05:08:10,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0194: [2024-08-11 05:08:10,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0185: [2024-08-11 05:08:10,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0185: [2024-08-11 05:08:10,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0194: [2024-08-11 05:08:10,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0197: [2024-08-11 05:08:10,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0197: [2024-08-11 05:08:10,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0197: [2024-08-11 05:08:10,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0195: [2024-08-11 05:08:10,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0195: [2024-08-11 05:08:10,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0195: [2024-08-11 05:08:10,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0198: [2024-08-11 05:08:10,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_23-model_00-model_states.pt...
g0197: [2024-08-11 05:08:10,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_20-model_00-model_states.pt...
g0187: [2024-08-11 05:08:10,101] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_08-model_00-model_states.pt...
g0188: [2024-08-11 05:08:10,104] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_11-model_00-model_states.pt...
g0185: [2024-08-11 05:08:10,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_05-model_00-model_states.pt...
g0194: [2024-08-11 05:08:10,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_14-model_00-model_states.pt...
g0195: [2024-08-11 05:08:10,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_17-model_00-model_states.pt...
g0184: [2024-08-11 05:08:10,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_01-model_00-model_states.pt...
g0198: [2024-08-11 05:08:10,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 05:08:10,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 05:08:10,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 05:08:10,243] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_25-model_00-model_states.pt...
g0197: [2024-08-11 05:08:10,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_20-model_00-model_states.pt.
g0194: [2024-08-11 05:08:10,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_14-model_00-model_states.pt.
g0185: [2024-08-11 05:08:10,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_05-model_00-model_states.pt.
g0195: [2024-08-11 05:08:10,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_17-model_00-model_states.pt.
g0197: [2024-08-11 05:08:10,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_21-model_00-model_states.pt...
g0194: [2024-08-11 05:08:10,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_15-model_00-model_states.pt...
g0185: [2024-08-11 05:08:10,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_06-model_00-model_states.pt...
g0187: [2024-08-11 05:08:10,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_08-model_00-model_states.pt.
g0195: [2024-08-11 05:08:10,314] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_18-model_00-model_states.pt...
g0187: [2024-08-11 05:08:10,328] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_09-model_00-model_states.pt...
g0188: [2024-08-11 05:08:10,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_11-model_00-model_states.pt.
g0184: [2024-08-11 05:08:10,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 05:08:10,368] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_02-model_00-model_states.pt...
g0188: [2024-08-11 05:08:10,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_12-model_00-model_states.pt...
g0197: [2024-08-11 05:08:10,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_21-model_00-model_states.pt.
g0185: [2024-08-11 05:08:10,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_06-model_00-model_states.pt.
g0195: [2024-08-11 05:08:10,451] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_18-model_00-model_states.pt.
g0197: [2024-08-11 05:08:10,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_22-model_00-model_states.pt...
g0184: [2024-08-11 05:08:10,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_02-model_00-model_states.pt.
g0185: [2024-08-11 05:08:10,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_07-model_00-model_states.pt...
g0195: [2024-08-11 05:08:10,486] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_19-model_00-model_states.pt...
g0184: [2024-08-11 05:08:10,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_03-model_00-model_states.pt...
g0188: [2024-08-11 05:08:10,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_12-model_00-model_states.pt.
g0198: [2024-08-11 05:08:10,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 05:08:10,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_07_model_states.pt...
g0187: [2024-08-11 05:08:10,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_09-model_00-model_states.pt.
g0188: [2024-08-11 05:08:10,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_13-model_00-model_states.pt...
g0187: [2024-08-11 05:08:10,540] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_10-model_00-model_states.pt...
g0185: [2024-08-11 05:08:10,576] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 05:08:10,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_01_model_states.pt...
g0184: [2024-08-11 05:08:10,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_03-model_00-model_states.pt.
g0197: [2024-08-11 05:08:10,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 05:08:10,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_06_model_states.pt...
g0195: [2024-08-11 05:08:10,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 05:08:10,610] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 05:08:10,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_04-model_00-model_states.pt...
g0187: [2024-08-11 05:08:10,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_10-model_00-model_states.pt.
g0188: [2024-08-11 05:08:10,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 05:08:10,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_03_model_states.pt...
g0187: [2024-08-11 05:08:10,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_02_model_states.pt...
g0184: [2024-08-11 05:08:10,705] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 05:08:10,706] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_00_model_states.pt
g0184: [2024-08-11 05:08:10,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_00_model_states.pt...
g0194: [2024-08-11 05:08:10,897] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 05:08:10,927] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 05:08:11,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 05:08:11,099] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_04_model_states.pt...
g0198: [2024-08-11 05:08:12,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 05:08:12,485] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0197: [2024-08-11 05:08:12,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 05:08:12,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0185: [2024-08-11 05:08:12,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 05:08:12,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0188: [2024-08-11 05:08:13,091] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 05:08:13,091] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0187: [2024-08-11 05:08:13,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 05:08:13,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0195: [2024-08-11 05:08:13,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 05:08:13,543] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0194: [2024-08-11 05:08:13,588] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 05:08:13,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0184: [2024-08-11 05:08:14,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step36000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 05:08:14,163] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36000 is ready now!
g0184:   successfully saved checkpoint at iteration   36000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.46, Latency(second): 4.125
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4124.34, 4125.78)
g0184: [2024-08-11 05:08:55,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=36010, skipped=55, lr=[0.00019977969585632764, 0.00019977969585632764], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36010 loss: 0.7643 iter time (s): 4.063 samples/sec: 31.500
g0198:  iteration    36010/10000000 | consumed samples:      4609280 | consumed tokens:   9439805440 | elapsed time per iteration (ms): 44848.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.687492E-01 | loss scale: 8192.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.854 | tokens per gpu per second (tgs): 182.661 | TFLOPs: 1.47 |
g0184: [2024-08-11 05:09:38,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=36020, skipped=55, lr=[0.00019977951652432312, 0.00019977951652432312], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36020 loss: 0.7415 iter time (s): 4.265 samples/sec: 30.009
g0198:  iteration    36020/10000000 | consumed samples:      4610560 | consumed tokens:   9442426880 | elapsed time per iteration (ms): 4297.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.514919E-01 | loss scale: 8192.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.784 | tokens per gpu per second (tgs): 1906.176 | TFLOPs: 15.34 |
g0184: [2024-08-11 05:10:21,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=36030, skipped=55, lr=[0.00019977933711944315, 0.00019977933711944315], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36030 loss: 0.7229 iter time (s): 4.305 samples/sec: 29.736
g0198:  iteration    36030/10000000 | consumed samples:      4611840 | consumed tokens:   9445048320 | elapsed time per iteration (ms): 4337.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.497704E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.511 | tokens per gpu per second (tgs): 1888.719 | TFLOPs: 15.20 |
g0184: [2024-08-11 05:11:04,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=36040, skipped=55, lr=[0.0001997791576416879, 0.0001997791576416879], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36040 loss: 0.7217 iter time (s): 4.306 samples/sec: 29.723
g0198:  iteration    36040/10000000 | consumed samples:      4613120 | consumed tokens:   9447669760 | elapsed time per iteration (ms): 4338.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.716658E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.500 | tokens per gpu per second (tgs): 1888.023 | TFLOPs: 15.19 |
g0184: [2024-08-11 05:11:47,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=36050, skipped=55, lr=[0.00019977897809105746, 0.00019977897809105746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36050 loss: 0.8040 iter time (s): 4.268 samples/sec: 29.988
g0198:  iteration    36050/10000000 | consumed samples:      4614400 | consumed tokens:   9450291200 | elapsed time per iteration (ms): 4300.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.657237E-01 | loss scale: 8192.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.763 | tokens per gpu per second (tgs): 1904.853 | TFLOPs: 15.33 |
g0184: [2024-08-11 05:12:29,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=36060, skipped=55, lr=[0.00019977879846755202, 0.00019977879846755202], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36060 loss: 0.7693 iter time (s): 4.134 samples/sec: 30.963
g0198:  iteration    36060/10000000 | consumed samples:      4615680 | consumed tokens:   9452912640 | elapsed time per iteration (ms): 4168.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.657825E-01 | loss scale: 8192.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.710 | tokens per gpu per second (tgs): 1965.447 | TFLOPs: 15.82 |
g0184: [2024-08-11 05:13:11,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=36070, skipped=55, lr=[0.00019977861877117165, 0.00019977861877117165], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36070 loss: 0.7772 iter time (s): 4.207 samples/sec: 30.423
g0198:  iteration    36070/10000000 | consumed samples:      4616960 | consumed tokens:   9455534080 | elapsed time per iteration (ms): 4239.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.678423E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.190 | tokens per gpu per second (tgs): 1932.188 | TFLOPs: 15.55 |
g0184: [2024-08-11 05:13:55,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=36080, skipped=55, lr=[0.00019977843900191656, 0.00019977843900191656], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36080 loss: 0.7684 iter time (s): 4.342 samples/sec: 29.479
g0198:  iteration    36080/10000000 | consumed samples:      4618240 | consumed tokens:   9458155520 | elapsed time per iteration (ms): 4374.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.576399E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.263 | tokens per gpu per second (tgs): 1872.810 | TFLOPs: 15.07 |
g0184: [2024-08-11 05:14:37,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=36090, skipped=55, lr=[0.0001997782591597869, 0.0001997782591597869], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36090 loss: 0.7006 iter time (s): 4.125 samples/sec: 31.033
g0198:  iteration    36090/10000000 | consumed samples:      4619520 | consumed tokens:   9460776960 | elapsed time per iteration (ms): 4156.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.606662E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.792 | tokens per gpu per second (tgs): 1970.707 | TFLOPs: 15.86 |
g0184: [2024-08-11 05:15:18,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=36100, skipped=55, lr=[0.00019977807924478273, 0.00019977807924478273], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36100 loss: 0.7641 iter time (s): 4.127 samples/sec: 31.014
g0198:  iteration    36100/10000000 | consumed samples:      4620800 | consumed tokens:   9463398400 | elapsed time per iteration (ms): 4159.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.636826E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.774 | tokens per gpu per second (tgs): 1969.538 | TFLOPs: 15.85 |
g0184: [2024-08-11 05:16:00,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=36110, skipped=55, lr=[0.00019977789925690423, 0.00019977789925690423], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36110 loss: 0.7889 iter time (s): 4.140 samples/sec: 30.915
g0198:  iteration    36110/10000000 | consumed samples:      4622080 | consumed tokens:   9466019840 | elapsed time per iteration (ms): 4173.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.620643E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.674 | tokens per gpu per second (tgs): 1963.111 | TFLOPs: 15.80 |
g0184: [2024-08-11 05:16:43,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=36120, skipped=55, lr=[0.00019977771919615155, 0.00019977771919615155], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36120 loss: 0.7529 iter time (s): 4.247 samples/sec: 30.139
g0198:  iteration    36120/10000000 | consumed samples:      4623360 | consumed tokens:   9468641280 | elapsed time per iteration (ms): 4279.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.589061E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.913 | tokens per gpu per second (tgs): 1914.418 | TFLOPs: 15.41 |
g0184: [2024-08-11 05:17:24,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=36130, skipped=55, lr=[0.00019977753906252478, 0.00019977753906252478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36130 loss: 0.7839 iter time (s): 4.109 samples/sec: 31.153
g0198:  iteration    36130/10000000 | consumed samples:      4624640 | consumed tokens:   9471262720 | elapsed time per iteration (ms): 4141.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.595606E-01 | loss scale: 8192.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.911 | tokens per gpu per second (tgs): 1978.277 | TFLOPs: 15.92 |
g0184: [2024-08-11 05:18:06,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=36140, skipped=55, lr=[0.0001997773588560241, 0.0001997773588560241], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36140 loss: 0.7696 iter time (s): 4.135 samples/sec: 30.955
g0198:  iteration    36140/10000000 | consumed samples:      4625920 | consumed tokens:   9473884160 | elapsed time per iteration (ms): 4167.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.616605E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.717 | tokens per gpu per second (tgs): 1965.866 | TFLOPs: 15.82 |
g0184: [2024-08-11 05:18:47,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=36150, skipped=55, lr=[0.00019977717857664966, 0.00019977717857664966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36150 loss: 0.7862 iter time (s): 4.112 samples/sec: 31.131
g0198:  iteration    36150/10000000 | consumed samples:      4627200 | consumed tokens:   9476505600 | elapsed time per iteration (ms): 4143.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.665326E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.889 | tokens per gpu per second (tgs): 1976.894 | TFLOPs: 15.91 |
g0184: [2024-08-11 05:19:29,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=36160, skipped=55, lr=[0.00019977699822440158, 0.00019977699822440158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36160 loss: 0.7533 iter time (s): 4.179 samples/sec: 30.629
g0198:  iteration    36160/10000000 | consumed samples:      4628480 | consumed tokens:   9479127040 | elapsed time per iteration (ms): 4211.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.578965E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.394 | tokens per gpu per second (tgs): 1945.190 | TFLOPs: 15.65 |
g0184: [2024-08-11 05:20:10,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=36170, skipped=55, lr=[0.00019977681779928, 0.00019977681779928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36170 loss: 0.7537 iter time (s): 4.002 samples/sec: 31.985
g0198:  iteration    36170/10000000 | consumed samples:      4629760 | consumed tokens:   9481748480 | elapsed time per iteration (ms): 4034.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.564039E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.727 | tokens per gpu per second (tgs): 2030.547 | TFLOPs: 16.34 |
g0184: [2024-08-11 05:20:51,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=36180, skipped=55, lr=[0.00019977663730128502, 0.00019977663730128502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36180 loss: 0.7413 iter time (s): 4.099 samples/sec: 31.230
g0198:  iteration    36180/10000000 | consumed samples:      4631040 | consumed tokens:   9484369920 | elapsed time per iteration (ms): 4131.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.529748E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.984 | tokens per gpu per second (tgs): 1982.959 | TFLOPs: 15.96 |
g0184: [2024-08-11 05:21:32,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=36190, skipped=55, lr=[0.00019977645673041687, 0.00019977645673041687], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36190 loss: 0.7752 iter time (s): 4.076 samples/sec: 31.402
g0198:  iteration    36190/10000000 | consumed samples:      4632320 | consumed tokens:   9486991360 | elapsed time per iteration (ms): 4108.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.624104E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.156 | tokens per gpu per second (tgs): 1993.955 | TFLOPs: 16.05 |
g0184: [2024-08-11 05:22:13,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=36200, skipped=55, lr=[0.00019977627608667558, 0.00019977627608667558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36200 loss: 0.7484 iter time (s): 4.061 samples/sec: 31.523
g0198:  iteration    36200/10000000 | consumed samples:      4633600 | consumed tokens:   9489612800 | elapsed time per iteration (ms): 4093.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.519184E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.273 | tokens per gpu per second (tgs): 2001.487 | TFLOPs: 16.11 |
g0184: [2024-08-11 05:22:55,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=36210, skipped=55, lr=[0.0001997760953700614, 0.0001997760953700614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36210 loss: 0.7323 iter time (s): 4.185 samples/sec: 30.584
g0198:  iteration    36210/10000000 | consumed samples:      4634880 | consumed tokens:   9492234240 | elapsed time per iteration (ms): 4217.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.694108E-01 | loss scale: 8192.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.350 | tokens per gpu per second (tgs): 1942.384 | TFLOPs: 15.63 |
g0184: [2024-08-11 05:23:37,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=36220, skipped=55, lr=[0.00019977591458057438, 0.00019977591458057438], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36220 loss: 0.7394 iter time (s): 4.173 samples/sec: 30.676
g0198:  iteration    36220/10000000 | consumed samples:      4636160 | consumed tokens:   9494855680 | elapsed time per iteration (ms): 4205.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.533321E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.437 | tokens per gpu per second (tgs): 1947.957 | TFLOPs: 15.68 |
g0184: [2024-08-11 05:24:20,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=36230, skipped=55, lr=[0.0001997757337182147, 0.0001997757337182147], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36230 loss: 0.7962 iter time (s): 4.214 samples/sec: 30.377
g0198:  iteration    36230/10000000 | consumed samples:      4637440 | consumed tokens:   9497477120 | elapsed time per iteration (ms): 4246.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.569151E-01 | loss scale: 8192.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.146 | tokens per gpu per second (tgs): 1929.345 | TFLOPs: 15.53 |
g0184: [2024-08-11 05:25:03,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=36240, skipped=55, lr=[0.00019977555278298246, 0.00019977555278298246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36240 loss: 0.7528 iter time (s): 4.295 samples/sec: 29.800
g0198:  iteration    36240/10000000 | consumed samples:      4638720 | consumed tokens:   9500098560 | elapsed time per iteration (ms): 4327.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.492086E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.579 | tokens per gpu per second (tgs): 1893.026 | TFLOPs: 15.23 |
g0184: [2024-08-11 05:25:44,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=36250, skipped=55, lr=[0.00019977537177487786, 0.00019977537177487786], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36250 loss: 0.7543 iter time (s): 4.083 samples/sec: 31.351
g0198:  iteration    36250/10000000 | consumed samples:      4640000 | consumed tokens:   9502720000 | elapsed time per iteration (ms): 4115.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.622422E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.106 | tokens per gpu per second (tgs): 1990.779 | TFLOPs: 16.02 |
g0184: [2024-08-11 05:26:25,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=36260, skipped=55, lr=[0.000199775190693901, 0.000199775190693901], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36260 loss: 0.7651 iter time (s): 4.010 samples/sec: 31.923
g0198:  iteration    36260/10000000 | consumed samples:      4641280 | consumed tokens:   9505341440 | elapsed time per iteration (ms): 4044.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.588061E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.651 | tokens per gpu per second (tgs): 2025.651 | TFLOPs: 16.30 |
g0184: [2024-08-11 05:27:08,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=36270, skipped=55, lr=[0.00019977500954005203, 0.00019977500954005203], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36270 loss: 0.7396 iter time (s): 4.248 samples/sec: 30.135
g0198:  iteration    36270/10000000 | consumed samples:      4642560 | consumed tokens:   9507962880 | elapsed time per iteration (ms): 4279.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.589188E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.909 | tokens per gpu per second (tgs): 1914.146 | TFLOPs: 15.40 |
g0184: [2024-08-11 05:27:50,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=36280, skipped=55, lr=[0.0001997748283133311, 0.0001997748283133311], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36280 loss: 0.7671 iter time (s): 4.205 samples/sec: 30.443
g0198:  iteration    36280/10000000 | consumed samples:      4643840 | consumed tokens:   9510584320 | elapsed time per iteration (ms): 4236.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.485442E-01 | loss scale: 8192.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.211 | tokens per gpu per second (tgs): 1933.514 | TFLOPs: 15.56 |
g0184: [2024-08-11 05:28:31,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=36290, skipped=55, lr=[0.0001997746470137383, 0.0001997746470137383], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36290 loss: 0.7806 iter time (s): 4.079 samples/sec: 31.382
g0198:  iteration    36290/10000000 | consumed samples:      4645120 | consumed tokens:   9513205760 | elapsed time per iteration (ms): 4111.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.597341E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.135 | tokens per gpu per second (tgs): 1992.637 | TFLOPs: 16.04 |
g0184: [2024-08-11 05:29:13,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=36300, skipped=55, lr=[0.00019977446564127383, 0.00019977446564127383], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36300 loss: 0.8233 iter time (s): 4.166 samples/sec: 30.725
g0198:  iteration    36300/10000000 | consumed samples:      4646400 | consumed tokens:   9515827200 | elapsed time per iteration (ms): 4199.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.688877E-01 | loss scale: 8192.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.483 | tokens per gpu per second (tgs): 1950.926 | TFLOPs: 15.70 |
g0184: [2024-08-11 05:29:56,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=36310, skipped=55, lr=[0.0001997742841959378, 0.0001997742841959378], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36310 loss: 0.7079 iter time (s): 4.239 samples/sec: 30.199
g0198:  iteration    36310/10000000 | consumed samples:      4647680 | consumed tokens:   9518448640 | elapsed time per iteration (ms): 4271.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.552478E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.969 | tokens per gpu per second (tgs): 1918.044 | TFLOPs: 15.43 |
g0184: [2024-08-11 05:30:37,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=36320, skipped=55, lr=[0.00019977410267773036, 0.00019977410267773036], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36320 loss: 0.7733 iter time (s): 4.054 samples/sec: 31.573
g0198:  iteration    36320/10000000 | consumed samples:      4648960 | consumed tokens:   9521070080 | elapsed time per iteration (ms): 4086.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.620948E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.323 | tokens per gpu per second (tgs): 2004.666 | TFLOPs: 16.13 |
g0184: [2024-08-11 05:31:17,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=36330, skipped=55, lr=[0.00019977392108665165, 0.00019977392108665165], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36330 loss: 0.7312 iter time (s): 4.051 samples/sec: 31.595
g0198:  iteration    36330/10000000 | consumed samples:      4650240 | consumed tokens:   9523691520 | elapsed time per iteration (ms): 4083.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.590585E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.343 | tokens per gpu per second (tgs): 2005.923 | TFLOPs: 16.14 |
g0184: [2024-08-11 05:31:58,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=36340, skipped=55, lr=[0.00019977373942270182, 0.00019977373942270182], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36340 loss: 0.7592 iter time (s): 4.051 samples/sec: 31.597
g0198:  iteration    36340/10000000 | consumed samples:      4651520 | consumed tokens:   9526312960 | elapsed time per iteration (ms): 4083.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.579544E-01 | loss scale: 8192.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.344 | tokens per gpu per second (tgs): 2006.037 | TFLOPs: 16.14 |
g0184: [2024-08-11 05:32:42,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=36350, skipped=55, lr=[0.00019977355768588097, 0.00019977355768588097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36350 loss: 0.7639 iter time (s): 4.304 samples/sec: 29.738
g0198:  iteration    36350/10000000 | consumed samples:      4652800 | consumed tokens:   9528934400 | elapsed time per iteration (ms): 4336.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.600074E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.516 | tokens per gpu per second (tgs): 1889.053 | TFLOPs: 15.20 |
g0184: [2024-08-11 05:33:23,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=36360, skipped=55, lr=[0.00019977337587618924, 0.00019977337587618924], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36360 loss: 0.7633 iter time (s): 4.121 samples/sec: 31.063
g0198:  iteration    36360/10000000 | consumed samples:      4654080 | consumed tokens:   9531555840 | elapsed time per iteration (ms): 4152.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.617687E-01 | loss scale: 8192.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.823 | tokens per gpu per second (tgs): 1972.665 | TFLOPs: 15.87 |
g0184: [2024-08-11 05:34:05,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=36370, skipped=55, lr=[0.00019977319399362684, 0.00019977319399362684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36370 loss: 0.7654 iter time (s): 4.164 samples/sec: 30.737
g0198:  iteration    36370/10000000 | consumed samples:      4655360 | consumed tokens:   9534177280 | elapsed time per iteration (ms): 4196.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.681390E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.501 | tokens per gpu per second (tgs): 1952.035 | TFLOPs: 15.71 |
g0184: [2024-08-11 05:34:46,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=36380, skipped=55, lr=[0.00019977301203819383, 0.00019977301203819383], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36380 loss: 0.7397 iter time (s): 4.109 samples/sec: 31.153
g0198:  iteration    36380/10000000 | consumed samples:      4656640 | consumed tokens:   9536798720 | elapsed time per iteration (ms): 4141.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.537194E-01 | loss scale: 8192.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.910 | tokens per gpu per second (tgs): 1978.226 | TFLOPs: 15.92 |
g0184: [2024-08-11 05:35:31,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=36390, skipped=55, lr=[0.00019977283000989038, 0.00019977283000989038], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36390 loss: 0.7816 iter time (s): 4.433 samples/sec: 28.871
g0198:  iteration    36390/10000000 | consumed samples:      4657920 | consumed tokens:   9539420160 | elapsed time per iteration (ms): 4465.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.629025E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.662 | tokens per gpu per second (tgs): 1834.342 | TFLOPs: 14.76 |
g0184: [2024-08-11 05:36:14,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=36400, skipped=55, lr=[0.00019977264790871667, 0.00019977264790871667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36400 loss: 0.7679 iter time (s): 4.223 samples/sec: 30.311
g0198:  iteration    36400/10000000 | consumed samples:      4659200 | consumed tokens:   9542041600 | elapsed time per iteration (ms): 4255.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.547213E-01 | loss scale: 8192.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.078 | tokens per gpu per second (tgs): 1925.013 | TFLOPs: 15.49 |
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 05:36:27,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 05:36:56,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=36410, skipped=55, lr=[0.00019977246573467278, 0.00019977246573467278], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36410 loss: 0.7702 iter time (s): 4.196 samples/sec: 30.502
g0198:  iteration    36410/10000000 | consumed samples:      4660480 | consumed tokens:   9544663040 | elapsed time per iteration (ms): 4228.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.565542E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.268 | tokens per gpu per second (tgs): 1937.161 | TFLOPs: 15.59 |
g0184: [2024-08-11 05:37:41,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=36420, skipped=55, lr=[0.00019977228348775887, 0.00019977228348775887], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36420 loss: 0.7343 iter time (s): 4.492 samples/sec: 28.495
g0198:  iteration    36420/10000000 | consumed samples:      4661760 | consumed tokens:   9547284480 | elapsed time per iteration (ms): 4525.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.684618E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.287 | tokens per gpu per second (tgs): 1810.355 | TFLOPs: 14.57 |
g0184: [2024-08-11 05:38:23,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=36430, skipped=55, lr=[0.0001997721011679751, 0.0001997721011679751], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36430 loss: 0.7399 iter time (s): 4.132 samples/sec: 30.978
g0198:  iteration    36430/10000000 | consumed samples:      4663040 | consumed tokens:   9549905920 | elapsed time per iteration (ms): 4164.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.511740E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.739 | tokens per gpu per second (tgs): 1967.290 | TFLOPs: 15.83 |
g0184: [2024-08-11 05:39:05,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=36440, skipped=55, lr=[0.00019977191877532157, 0.00019977191877532157], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36440 loss: 0.7186 iter time (s): 4.184 samples/sec: 30.589
g0198:  iteration    36440/10000000 | consumed samples:      4664320 | consumed tokens:   9552527360 | elapsed time per iteration (ms): 4216.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.532653E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.354 | tokens per gpu per second (tgs): 1942.675 | TFLOPs: 15.63 |
g0184: [2024-08-11 05:39:48,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=36450, skipped=55, lr=[0.00019977173630979843, 0.00019977173630979843], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36450 loss: 0.7709 iter time (s): 4.235 samples/sec: 30.223
g0198:  iteration    36450/10000000 | consumed samples:      4665600 | consumed tokens:   9555148800 | elapsed time per iteration (ms): 4267.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.707547E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.991 | tokens per gpu per second (tgs): 1919.433 | TFLOPs: 15.45 |
g0184: [2024-08-11 05:40:31,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=36460, skipped=55, lr=[0.00019977155377140586, 0.00019977155377140586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36460 loss: 0.7696 iter time (s): 4.256 samples/sec: 30.073
g0198:  iteration    36460/10000000 | consumed samples:      4666880 | consumed tokens:   9557770240 | elapsed time per iteration (ms): 4289.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.699723E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.841 | tokens per gpu per second (tgs): 1909.849 | TFLOPs: 15.37 |
g0184: [2024-08-11 05:41:12,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=36470, skipped=55, lr=[0.000199771371160144, 0.000199771371160144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36470 loss: 0.7811 iter time (s): 4.142 samples/sec: 30.902
g0198:  iteration    36470/10000000 | consumed samples:      4668160 | consumed tokens:   9560391680 | elapsed time per iteration (ms): 4174.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.507657E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.663 | tokens per gpu per second (tgs): 1962.407 | TFLOPs: 15.79 |
g0184: [2024-08-11 05:41:55,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=36480, skipped=55, lr=[0.00019977118847601294, 0.00019977118847601294], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36480 loss: 0.7715 iter time (s): 4.225 samples/sec: 30.295
g0198:  iteration    36480/10000000 | consumed samples:      4669440 | consumed tokens:   9563013120 | elapsed time per iteration (ms): 4257.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.621957E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.066 | tokens per gpu per second (tgs): 1924.233 | TFLOPs: 15.48 |
g0184: [2024-08-11 05:42:39,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=36490, skipped=55, lr=[0.0001997710057190128, 0.0001997710057190128], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36490 loss: 0.7675 iter time (s): 4.412 samples/sec: 29.011
g0198:  iteration    36490/10000000 | consumed samples:      4670720 | consumed tokens:   9565634560 | elapsed time per iteration (ms): 4444.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.511426E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.800 | tokens per gpu per second (tgs): 1843.212 | TFLOPs: 14.83 |
g0184: [2024-08-11 05:43:20,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=36500, skipped=55, lr=[0.0001997708228891438, 0.0001997708228891438], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36500 loss: 0.7862 iter time (s): 4.067 samples/sec: 31.470
g0198:  iteration    36500/10000000 | consumed samples:      4672000 | consumed tokens:   9568256000 | elapsed time per iteration (ms): 4104.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.599854E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.189 | tokens per gpu per second (tgs): 1996.068 | TFLOPs: 16.06 |
g0184: [2024-08-11 05:44:02,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=36510, skipped=55, lr=[0.0001997706399864061, 0.0001997706399864061], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36510 loss: 0.7248 iter time (s): 4.109 samples/sec: 31.152
g0198:  iteration    36510/10000000 | consumed samples:      4673280 | consumed tokens:   9570877440 | elapsed time per iteration (ms): 4141.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.521057E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.910 | tokens per gpu per second (tgs): 1978.228 | TFLOPs: 15.92 |
g0184: [2024-08-11 05:44:44,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=36520, skipped=55, lr=[0.00019977045701079972, 0.00019977045701079972], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36520 loss: 0.7614 iter time (s): 4.177 samples/sec: 30.644
g0198:  iteration    36520/10000000 | consumed samples:      4674560 | consumed tokens:   9573498880 | elapsed time per iteration (ms): 4210.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.578957E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.399 | tokens per gpu per second (tgs): 1945.524 | TFLOPs: 15.66 |
g0184: [2024-08-11 05:45:26,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=36530, skipped=55, lr=[0.0001997702739623249, 0.0001997702739623249], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36530 loss: 0.7311 iter time (s): 4.157 samples/sec: 30.790
g0198:  iteration    36530/10000000 | consumed samples:      4675840 | consumed tokens:   9576120320 | elapsed time per iteration (ms): 4189.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.503092E-01 | loss scale: 16384.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.552 | tokens per gpu per second (tgs): 1955.359 | TFLOPs: 15.74 |
g0184: [2024-08-11 05:46:07,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=36540, skipped=55, lr=[0.00019977009084098173, 0.00019977009084098173], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36540 loss: 0.7500 iter time (s): 4.092 samples/sec: 31.277
g0198:  iteration    36540/10000000 | consumed samples:      4677120 | consumed tokens:   9578741760 | elapsed time per iteration (ms): 4133.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.564706E-01 | loss scale: 16384.0 | grad norm: 0.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.965 | tokens per gpu per second (tgs): 1981.758 | TFLOPs: 15.95 |
g0184: [2024-08-11 05:46:50,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=36550, skipped=55, lr=[0.00019976990764677036, 0.00019976990764677036], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36550 loss: 0.7707 iter time (s): 4.216 samples/sec: 30.358
g0198:  iteration    36550/10000000 | consumed samples:      4678400 | consumed tokens:   9581363200 | elapsed time per iteration (ms): 4248.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.515919E-01 | loss scale: 16384.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.129 | tokens per gpu per second (tgs): 1928.244 | TFLOPs: 15.52 |
g0184: [2024-08-11 05:47:33,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=36560, skipped=55, lr=[0.00019976972437969097, 0.00019976972437969097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36560 loss: 0.7830 iter time (s): 4.256 samples/sec: 30.074
g0198:  iteration    36560/10000000 | consumed samples:      4679680 | consumed tokens:   9583984640 | elapsed time per iteration (ms): 4288.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.580964E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.848 | tokens per gpu per second (tgs): 1910.246 | TFLOPs: 15.37 |
g0184: [2024-08-11 05:48:15,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=36570, skipped=55, lr=[0.00019976954103974364, 0.00019976954103974364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36570 loss: 0.7696 iter time (s): 4.187 samples/sec: 30.571
g0198:  iteration    36570/10000000 | consumed samples:      4680960 | consumed tokens:   9586606080 | elapsed time per iteration (ms): 4219.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.611482E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.337 | tokens per gpu per second (tgs): 1941.572 | TFLOPs: 15.62 |
g0184: [2024-08-11 05:48:57,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=36580, skipped=55, lr=[0.00019976935762692856, 0.00019976935762692856], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36580 loss: 0.7570 iter time (s): 4.228 samples/sec: 30.272
g0198:  iteration    36580/10000000 | consumed samples:      4682240 | consumed tokens:   9589227520 | elapsed time per iteration (ms): 4260.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.509177E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.042 | tokens per gpu per second (tgs): 1922.698 | TFLOPs: 15.47 |
g0184: [2024-08-11 05:49:39,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=36590, skipped=55, lr=[0.00019976917414124586, 0.00019976917414124586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36590 loss: 0.7672 iter time (s): 4.179 samples/sec: 30.628
g0198:  iteration    36590/10000000 | consumed samples:      4683520 | consumed tokens:   9591848960 | elapsed time per iteration (ms): 4211.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.623852E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.395 | tokens per gpu per second (tgs): 1945.308 | TFLOPs: 15.65 |
g0184: [2024-08-11 05:50:21,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=36600, skipped=55, lr=[0.00019976899058269567, 0.00019976899058269567], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36600 loss: 0.7537 iter time (s): 4.077 samples/sec: 31.395
g0198:  iteration    36600/10000000 | consumed samples:      4684800 | consumed tokens:   9594470400 | elapsed time per iteration (ms): 4109.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.604022E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.149 | tokens per gpu per second (tgs): 1993.516 | TFLOPs: 16.04 |
g0184: [2024-08-11 05:51:02,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=36610, skipped=55, lr=[0.0001997688069512781, 0.0001997688069512781], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36610 loss: 0.7612 iter time (s): 4.121 samples/sec: 31.059
g0198:  iteration    36610/10000000 | consumed samples:      4686080 | consumed tokens:   9597091840 | elapsed time per iteration (ms): 4153.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.471987E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.815 | tokens per gpu per second (tgs): 1972.160 | TFLOPs: 15.87 |
g0184: [2024-08-11 05:51:42,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=36620, skipped=55, lr=[0.00019976862324699336, 0.00019976862324699336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36620 loss: 0.7268 iter time (s): 3.959 samples/sec: 32.332
g0198:  iteration    36620/10000000 | consumed samples:      4687360 | consumed tokens:   9599713280 | elapsed time per iteration (ms): 3991.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.545253E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.071 | tokens per gpu per second (tgs): 2052.550 | TFLOPs: 16.52 |
g0184: [2024-08-11 05:52:24,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=36630, skipped=55, lr=[0.00019976843946984158, 0.00019976843946984158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36630 loss: 0.7126 iter time (s): 4.184 samples/sec: 30.594
g0198:  iteration    36630/10000000 | consumed samples:      4688640 | consumed tokens:   9602334720 | elapsed time per iteration (ms): 4216.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.500452E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.359 | tokens per gpu per second (tgs): 1943.007 | TFLOPs: 15.64 |
g0184: [2024-08-11 05:53:05,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=36640, skipped=55, lr=[0.00019976825561982285, 0.00019976825561982285], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36640 loss: 0.7612 iter time (s): 4.098 samples/sec: 31.232
g0198:  iteration    36640/10000000 | consumed samples:      4689920 | consumed tokens:   9604956160 | elapsed time per iteration (ms): 4130.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.518614E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.990 | tokens per gpu per second (tgs): 1983.347 | TFLOPs: 15.96 |
g0184: [2024-08-11 05:53:47,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=36650, skipped=55, lr=[0.00019976807169693733, 0.00019976807169693733], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36650 loss: 0.7475 iter time (s): 4.095 samples/sec: 31.258
g0198:  iteration    36650/10000000 | consumed samples:      4691200 | consumed tokens:   9607577600 | elapsed time per iteration (ms): 4127.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.506313E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.010 | tokens per gpu per second (tgs): 1984.659 | TFLOPs: 15.97 |
g0184: [2024-08-11 05:54:30,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=36660, skipped=55, lr=[0.00019976788770118518, 0.00019976788770118518], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36660 loss: 0.7521 iter time (s): 4.262 samples/sec: 30.035
g0198:  iteration    36660/10000000 | consumed samples:      4692480 | consumed tokens:   9610199040 | elapsed time per iteration (ms): 4294.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.534205E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.808 | tokens per gpu per second (tgs): 1907.683 | TFLOPs: 15.35 |
g0184: [2024-08-11 05:55:11,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=36670, skipped=55, lr=[0.00019976770363256655, 0.00019976770363256655], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36670 loss: 0.7476 iter time (s): 4.099 samples/sec: 31.231
g0198:  iteration    36670/10000000 | consumed samples:      4693760 | consumed tokens:   9612820480 | elapsed time per iteration (ms): 4130.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.573325E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.988 | tokens per gpu per second (tgs): 1983.261 | TFLOPs: 15.96 |
g0184: [2024-08-11 05:55:53,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=36680, skipped=55, lr=[0.00019976751949108155, 0.00019976751949108155], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36680 loss: 0.7298 iter time (s): 4.139 samples/sec: 30.927
g0198:  iteration    36680/10000000 | consumed samples:      4695040 | consumed tokens:   9615441920 | elapsed time per iteration (ms): 4171.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.557489E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.688 | tokens per gpu per second (tgs): 1964.021 | TFLOPs: 15.80 |
g0184: [2024-08-11 05:56:35,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=36690, skipped=55, lr=[0.00019976733527673034, 0.00019976733527673034], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36690 loss: 0.7587 iter time (s): 4.151 samples/sec: 30.839
g0198:  iteration    36690/10000000 | consumed samples:      4696320 | consumed tokens:   9618063360 | elapsed time per iteration (ms): 4182.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.634084E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.602 | tokens per gpu per second (tgs): 1958.528 | TFLOPs: 15.76 |
g0184: [2024-08-11 05:57:16,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=36700, skipped=55, lr=[0.00019976715098951305, 0.00019976715098951305], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36700 loss: 0.7428 iter time (s): 4.091 samples/sec: 31.284
g0198:  iteration    36700/10000000 | consumed samples:      4697600 | consumed tokens:   9620684800 | elapsed time per iteration (ms): 4123.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.612853E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.041 | tokens per gpu per second (tgs): 1986.627 | TFLOPs: 15.99 |
g0184: [2024-08-11 05:57:58,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=36710, skipped=55, lr=[0.00019976696662942983, 0.00019976696662942983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36710 loss: 0.8100 iter time (s): 4.227 samples/sec: 30.285
g0198:  iteration    36710/10000000 | consumed samples:      4698880 | consumed tokens:   9623306240 | elapsed time per iteration (ms): 4258.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.559673E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.056 | tokens per gpu per second (tgs): 1923.600 | TFLOPs: 15.48 |
g0184: [2024-08-11 05:58:41,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=36720, skipped=55, lr=[0.00019976678219648083, 0.00019976678219648083], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36720 loss: 0.7873 iter time (s): 4.253 samples/sec: 30.099
g0198:  iteration    36720/10000000 | consumed samples:      4700160 | consumed tokens:   9625927680 | elapsed time per iteration (ms): 4284.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.636736E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.872 | tokens per gpu per second (tgs): 1911.837 | TFLOPs: 15.38 |
g0184: [2024-08-11 05:59:24,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=36730, skipped=55, lr=[0.00019976659769066618, 0.00019976659769066618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36730 loss: 0.7574 iter time (s): 4.268 samples/sec: 29.992
g0198:  iteration    36730/10000000 | consumed samples:      4701440 | consumed tokens:   9628549120 | elapsed time per iteration (ms): 4302.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.553020E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.752 | tokens per gpu per second (tgs): 1904.108 | TFLOPs: 15.32 |
g0184: [2024-08-11 06:00:07,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=36740, skipped=55, lr=[0.00019976641311198604, 0.00019976641311198604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36740 loss: 0.7727 iter time (s): 4.199 samples/sec: 30.481
g0198:  iteration    36740/10000000 | consumed samples:      4702720 | consumed tokens:   9631170560 | elapsed time per iteration (ms): 4233.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.640777E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.233 | tokens per gpu per second (tgs): 1934.912 | TFLOPs: 15.57 |
g0184: [2024-08-11 06:00:50,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=36750, skipped=55, lr=[0.00019976622846044052, 0.00019976622846044052], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36750 loss: 0.7719 iter time (s): 4.266 samples/sec: 30.007
g0198:  iteration    36750/10000000 | consumed samples:      4704000 | consumed tokens:   9633792000 | elapsed time per iteration (ms): 4298.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.524271E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.780 | tokens per gpu per second (tgs): 1905.937 | TFLOPs: 15.34 |
g0184: [2024-08-11 06:01:32,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=36760, skipped=55, lr=[0.00019976604373602976, 0.00019976604373602976], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36760 loss: 0.7808 iter time (s): 4.201 samples/sec: 30.470
g0198:  iteration    36760/10000000 | consumed samples:      4705280 | consumed tokens:   9636413440 | elapsed time per iteration (ms): 4235.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.552557E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.219 | tokens per gpu per second (tgs): 1933.996 | TFLOPs: 15.56 |
g0184: [2024-08-11 06:02:14,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=36770, skipped=55, lr=[0.00019976585893875394, 0.00019976585893875394], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36770 loss: 0.7609 iter time (s): 4.207 samples/sec: 30.423
g0198:  iteration    36770/10000000 | consumed samples:      4706560 | consumed tokens:   9639034880 | elapsed time per iteration (ms): 4239.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.676301E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.192 | tokens per gpu per second (tgs): 1932.260 | TFLOPs: 15.55 |
g0184: [2024-08-11 06:02:58,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=36780, skipped=55, lr=[0.0001997656740686132, 0.0001997656740686132], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36780 loss: 0.7384 iter time (s): 4.371 samples/sec: 29.281
g0198:  iteration    36780/10000000 | consumed samples:      4707840 | consumed tokens:   9641656320 | elapsed time per iteration (ms): 4403.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.521587E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.067 | tokens per gpu per second (tgs): 1860.305 | TFLOPs: 14.97 |
g0184: [2024-08-11 06:03:42,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=36790, skipped=55, lr=[0.00019976548912560763, 0.00019976548912560763], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36790 loss: 0.7743 iter time (s): 4.364 samples/sec: 29.329
g0198:  iteration    36790/10000000 | consumed samples:      4709120 | consumed tokens:   9644277760 | elapsed time per iteration (ms): 4396.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.582153E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.115 | tokens per gpu per second (tgs): 1863.357 | TFLOPs: 14.99 |
g0184: [2024-08-11 06:04:24,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=36800, skipped=55, lr=[0.00019976530410973744, 0.00019976530410973744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36800 loss: 0.7788 iter time (s): 4.185 samples/sec: 30.582
g0198:  iteration    36800/10000000 | consumed samples:      4710400 | consumed tokens:   9646899200 | elapsed time per iteration (ms): 4218.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.577114E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.129 | TFLOPs: 15.63 |
g0184: [2024-08-11 06:05:07,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=36810, skipped=55, lr=[0.0001997651190210027, 0.0001997651190210027], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36810 loss: 0.7934 iter time (s): 4.169 samples/sec: 30.703
g0198:  iteration    36810/10000000 | consumed samples:      4711680 | consumed tokens:   9649520640 | elapsed time per iteration (ms): 4200.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.578139E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.469 | tokens per gpu per second (tgs): 1950.042 | TFLOPs: 15.69 |
g0184: [2024-08-11 06:05:49,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=36820, skipped=55, lr=[0.00019976493385940363, 0.00019976493385940363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36820 loss: 0.7571 iter time (s): 4.183 samples/sec: 30.597
g0198:  iteration    36820/10000000 | consumed samples:      4712960 | consumed tokens:   9652142080 | elapsed time per iteration (ms): 4215.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.595942E-01 | loss scale: 16384.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.362 | tokens per gpu per second (tgs): 1943.188 | TFLOPs: 15.64 |
g0184: [2024-08-11 06:06:32,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=36830, skipped=55, lr=[0.0001997647486249403, 0.0001997647486249403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36830 loss: 0.7438 iter time (s): 4.259 samples/sec: 30.056
g0198:  iteration    36830/10000000 | consumed samples:      4714240 | consumed tokens:   9654763520 | elapsed time per iteration (ms): 4291.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.463037E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.829 | tokens per gpu per second (tgs): 1909.072 | TFLOPs: 15.36 |
g0184: [2024-08-11 06:07:12,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=36840, skipped=55, lr=[0.00019976456331761294, 0.00019976456331761294], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36840 loss: 0.7434 iter time (s): 4.033 samples/sec: 31.739
g0198:  iteration    36840/10000000 | consumed samples:      4715520 | consumed tokens:   9657384960 | elapsed time per iteration (ms): 4079.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.614693E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.377 | tokens per gpu per second (tgs): 2008.152 | TFLOPs: 16.16 |
g0184: [2024-08-11 06:07:53,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=36850, skipped=55, lr=[0.0001997643779374216, 0.0001997643779374216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36850 loss: 0.7763 iter time (s): 4.073 samples/sec: 31.430
g0198:  iteration    36850/10000000 | consumed samples:      4716800 | consumed tokens:   9660006400 | elapsed time per iteration (ms): 4106.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.651780E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.169 | tokens per gpu per second (tgs): 1994.789 | TFLOPs: 16.05 |
g0184: [2024-08-11 06:08:35,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=36860, skipped=55, lr=[0.00019976419248436645, 0.00019976419248436645], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36860 loss: 0.7764 iter time (s): 4.158 samples/sec: 30.783
g0198:  iteration    36860/10000000 | consumed samples:      4718080 | consumed tokens:   9662627840 | elapsed time per iteration (ms): 4190.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.552195E-01 | loss scale: 16384.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.546 | tokens per gpu per second (tgs): 1954.950 | TFLOPs: 15.73 |
g0184: [2024-08-11 06:09:17,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=36870, skipped=55, lr=[0.0001997640069584477, 0.0001997640069584477], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36870 loss: 0.7496 iter time (s): 4.162 samples/sec: 30.753
g0198:  iteration    36870/10000000 | consumed samples:      4719360 | consumed tokens:   9665249280 | elapsed time per iteration (ms): 4194.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.570426E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.516 | tokens per gpu per second (tgs): 1953.044 | TFLOPs: 15.72 |
g0184: [2024-08-11 06:10:00,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=36880, skipped=55, lr=[0.00019976382135966539, 0.00019976382135966539], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36880 loss: 0.7355 iter time (s): 4.267 samples/sec: 29.995
g0198:  iteration    36880/10000000 | consumed samples:      4720640 | consumed tokens:   9667870720 | elapsed time per iteration (ms): 4299.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.615370E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.768 | tokens per gpu per second (tgs): 1905.178 | TFLOPs: 15.33 |
g0184: [2024-08-11 06:10:42,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=36890, skipped=55, lr=[0.0001997636356880197, 0.0001997636356880197], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36890 loss: 0.7526 iter time (s): 4.157 samples/sec: 30.790
g0198:  iteration    36890/10000000 | consumed samples:      4721920 | consumed tokens:   9670492160 | elapsed time per iteration (ms): 4189.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.522215E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.551 | tokens per gpu per second (tgs): 1955.284 | TFLOPs: 15.73 |
g0184: [2024-08-11 06:11:24,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=36900, skipped=55, lr=[0.00019976344994351083, 0.00019976344994351083], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36900 loss: 0.7806 iter time (s): 4.187 samples/sec: 30.568
g0198:  iteration    36900/10000000 | consumed samples:      4723200 | consumed tokens:   9673113600 | elapsed time per iteration (ms): 4219.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.541903E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.337 | tokens per gpu per second (tgs): 1941.545 | TFLOPs: 15.62 |
g0184: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 06:11:37,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 06:11:37,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 06:12:07,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=36910, skipped=55, lr=[0.0001997632641261388, 0.0001997632641261388], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36910 loss: 0.7598 iter time (s): 4.184 samples/sec: 30.593
g0198:  iteration    36910/10000000 | consumed samples:      4724480 | consumed tokens:   9675735040 | elapsed time per iteration (ms): 4216.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.505003E-01 | loss scale: 32768.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.358 | tokens per gpu per second (tgs): 1942.943 | TFLOPs: 15.64 |
g0184: [2024-08-11 06:12:48,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=36920, skipped=55, lr=[0.0001997630782359039, 0.0001997630782359039], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36920 loss: 0.7596 iter time (s): 4.126 samples/sec: 31.026
g0198:  iteration    36920/10000000 | consumed samples:      4725760 | consumed tokens:   9678356480 | elapsed time per iteration (ms): 4160.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.513588E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.764 | tokens per gpu per second (tgs): 1968.888 | TFLOPs: 15.84 |
g0184: [2024-08-11 06:13:30,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=36930, skipped=55, lr=[0.00019976289227280618, 0.00019976289227280618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36930 loss: 0.7781 iter time (s): 4.153 samples/sec: 30.823
g0198:  iteration    36930/10000000 | consumed samples:      4727040 | consumed tokens:   9680977920 | elapsed time per iteration (ms): 4185.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.539436E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.585 | tokens per gpu per second (tgs): 1957.410 | TFLOPs: 15.75 |
g0184: [2024-08-11 06:14:11,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=36940, skipped=55, lr=[0.0001997627062368458, 0.0001997627062368458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36940 loss: 0.7425 iter time (s): 4.069 samples/sec: 31.459
g0198:  iteration    36940/10000000 | consumed samples:      4728320 | consumed tokens:   9683599360 | elapsed time per iteration (ms): 4101.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.543881E-01 | loss scale: 32768.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.206 | tokens per gpu per second (tgs): 1997.194 | TFLOPs: 16.07 |
g0184: [2024-08-11 06:14:54,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=36950, skipped=55, lr=[0.00019976252012802293, 0.00019976252012802293], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36950 loss: 0.7588 iter time (s): 4.263 samples/sec: 30.023
g0198:  iteration    36950/10000000 | consumed samples:      4729600 | consumed tokens:   9686220800 | elapsed time per iteration (ms): 4296.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.550219E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.792 | tokens per gpu per second (tgs): 1906.715 | TFLOPs: 15.34 |
g0184: [2024-08-11 06:15:37,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=36960, skipped=55, lr=[0.00019976233394633765, 0.00019976233394633765], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36960 loss: 0.7426 iter time (s): 4.301 samples/sec: 29.759
g0198:  iteration    36960/10000000 | consumed samples:      4730880 | consumed tokens:   9688842240 | elapsed time per iteration (ms): 4333.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.445781E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.537 | tokens per gpu per second (tgs): 1890.398 | TFLOPs: 15.21 |
g0184: [2024-08-11 06:16:21,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=36970, skipped=55, lr=[0.00019976214769179014, 0.00019976214769179014], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36970 loss: 0.7908 iter time (s): 4.326 samples/sec: 29.589
g0198:  iteration    36970/10000000 | consumed samples:      4732160 | consumed tokens:   9691463680 | elapsed time per iteration (ms): 4358.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.700297E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.367 | tokens per gpu per second (tgs): 1879.501 | TFLOPs: 15.12 |
g0184: [2024-08-11 06:17:03,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=36980, skipped=55, lr=[0.0001997619613643806, 0.0001997619613643806], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36980 loss: 0.7401 iter time (s): 4.136 samples/sec: 30.947
g0198:  iteration    36980/10000000 | consumed samples:      4733440 | consumed tokens:   9694085120 | elapsed time per iteration (ms): 4168.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.561887E-01 | loss scale: 32768.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.709 | tokens per gpu per second (tgs): 1965.359 | TFLOPs: 15.82 |
g0184: [2024-08-11 06:17:44,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=36990, skipped=55, lr=[0.0001997617749641091, 0.0001997617749641091], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 36990 loss: 0.7619 iter time (s): 4.113 samples/sec: 31.118
g0198:  iteration    36990/10000000 | consumed samples:      4734720 | consumed tokens:   9696706560 | elapsed time per iteration (ms): 4148.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.646299E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.808 | TFLOPs: 15.89 |
g0184: [2024-08-11 06:18:26,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=55, lr=[0.00019976158849097577, 0.00019976158849097577], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37000 loss: 0.7940 iter time (s): 4.206 samples/sec: 30.430
g0198:  iteration    37000/10000000 | consumed samples:      4736000 | consumed tokens:   9699328000 | elapsed time per iteration (ms): 4238.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.627112E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.197 | tokens per gpu per second (tgs): 1932.634 | TFLOPs: 15.55 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 37000 | lm loss value: 7.611743E-01 | lm loss PPL: 2.140789E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   37000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 06:25:03,647] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step37000 is about to be saved!
g0198: [2024-08-11 06:25:03,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0198: [2024-08-11 06:25:03,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0198: [2024-08-11 06:25:03,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0184: [2024-08-11 06:25:03,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0184: [2024-08-11 06:25:03,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0184: [2024-08-11 06:25:03,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0188: [2024-08-11 06:25:03,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0188: [2024-08-11 06:25:03,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0188: [2024-08-11 06:25:03,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0197: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0197: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0197: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0195: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0195: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0195: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0194: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0194: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0185: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0185: [2024-08-11 06:25:03,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0194: [2024-08-11 06:25:03,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0185: [2024-08-11 06:25:03,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0187: [2024-08-11 06:25:03,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0187: [2024-08-11 06:25:03,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0187: [2024-08-11 06:25:03,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0198: [2024-08-11 06:25:03,678] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_23-model_00-model_states.pt...
g0197: [2024-08-11 06:25:03,688] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_20-model_00-model_states.pt...
g0187: [2024-08-11 06:25:03,689] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_08-model_00-model_states.pt...
g0188: [2024-08-11 06:25:03,691] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_11-model_00-model_states.pt...
g0194: [2024-08-11 06:25:03,694] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_14-model_00-model_states.pt...
g0195: [2024-08-11 06:25:03,694] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_17-model_00-model_states.pt...
g0185: [2024-08-11 06:25:03,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_05-model_00-model_states.pt...
g0184: [2024-08-11 06:25:03,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_01-model_00-model_states.pt...
g0195: [2024-08-11 06:25:03,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_17-model_00-model_states.pt.
g0185: [2024-08-11 06:25:03,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_05-model_00-model_states.pt.
g0187: [2024-08-11 06:25:03,818] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_08-model_00-model_states.pt.
g0194: [2024-08-11 06:25:03,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_14-model_00-model_states.pt.
g0198: [2024-08-11 06:25:03,840] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 06:25:03,841] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 06:25:03,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_24-model_00-model_states.pt.
g0195: [2024-08-11 06:25:03,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_18-model_00-model_states.pt...
g0185: [2024-08-11 06:25:03,847] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_06-model_00-model_states.pt...
g0187: [2024-08-11 06:25:03,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_09-model_00-model_states.pt...
g0188: [2024-08-11 06:25:03,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_11-model_00-model_states.pt.
g0194: [2024-08-11 06:25:03,866] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_15-model_00-model_states.pt...
g0184: [2024-08-11 06:25:03,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_01-model_00-model_states.pt.
g0198: [2024-08-11 06:25:03,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_25-model_00-model_states.pt...
g0184: [2024-08-11 06:25:03,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_02-model_00-model_states.pt...
g0188: [2024-08-11 06:25:03,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_12-model_00-model_states.pt...
g0185: [2024-08-11 06:25:03,947] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_06-model_00-model_states.pt.
g0187: [2024-08-11 06:25:03,954] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_09-model_00-model_states.pt.
g0185: [2024-08-11 06:25:03,978] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_07-model_00-model_states.pt...
g0187: [2024-08-11 06:25:03,985] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_10-model_00-model_states.pt...
g0194: [2024-08-11 06:25:03,987] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_15-model_00-model_states.pt.
g0195: [2024-08-11 06:25:03,991] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_18-model_00-model_states.pt.
g0197: [2024-08-11 06:25:04,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_20-model_00-model_states.pt.
g0194: [2024-08-11 06:25:04,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_16-model_00-model_states.pt...
g0195: [2024-08-11 06:25:04,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_19-model_00-model_states.pt...
g0184: [2024-08-11 06:25:04,031] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_02-model_00-model_states.pt.
g0188: [2024-08-11 06:25:04,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_12-model_00-model_states.pt.
g0197: [2024-08-11 06:25:04,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_21-model_00-model_states.pt...
g0184: [2024-08-11 06:25:04,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_03-model_00-model_states.pt...
g0188: [2024-08-11 06:25:04,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_13-model_00-model_states.pt...
g0185: [2024-08-11 06:25:04,104] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 06:25:04,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_01_model_states.pt...
g0187: [2024-08-11 06:25:04,117] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 06:25:04,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_02_model_states.pt...
g0194: [2024-08-11 06:25:04,142] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 06:25:04,144] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_04_model_states.pt...
g0184: [2024-08-11 06:25:04,151] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_03-model_00-model_states.pt.
g0188: [2024-08-11 06:25:04,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 06:25:04,161] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_03_model_states.pt...
g0195: [2024-08-11 06:25:04,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 06:25:04,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 06:25:04,173] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_04-model_00-model_states.pt...
g0198: [2024-08-11 06:25:04,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 06:25:04,243] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_07_model_states.pt...
g0197: [2024-08-11 06:25:04,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 06:25:04,332] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_22-model_00-model_states.pt...
g0184: [2024-08-11 06:25:04,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 06:25:04,349] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_00_model_states.pt
g0184: [2024-08-11 06:25:04,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_00_model_states.pt...
g0197: [2024-08-11 06:25:04,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 06:25:04,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_06_model_states.pt...
g0198: [2024-08-11 06:25:06,143] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 06:25:06,143] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0187: [2024-08-11 06:25:06,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 06:25:06,472] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0185: [2024-08-11 06:25:06,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 06:25:06,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0195: [2024-08-11 06:25:06,552] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 06:25:06,552] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0188: [2024-08-11 06:25:06,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 06:25:06,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0194: [2024-08-11 06:25:06,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 06:25:06,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0197: [2024-08-11 06:25:06,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 06:25:06,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0184: [2024-08-11 06:25:07,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step37000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 06:25:07,953] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37000 is ready now!
g0184:   successfully saved checkpoint at iteration   37000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.16, Latency(second): 4.364
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4364.00, 4364.43)
g0184: [2024-08-11 06:25:50,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=37010, skipped=55, lr=[0.0001997614019449808, 0.0001997614019449808], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37010 loss: 0.7719 iter time (s): 4.187 samples/sec: 30.569
g0198:  iteration    37010/10000000 | consumed samples:      4737280 | consumed tokens:   9701949440 | elapsed time per iteration (ms): 44320.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.529817E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.888 | tokens per gpu per second (tgs): 184.836 | TFLOPs: 1.49 |
g0184: [2024-08-11 06:26:31,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=37020, skipped=55, lr=[0.00019976121532612432, 0.00019976121532612432], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37020 loss: 0.7647 iter time (s): 4.149 samples/sec: 30.847
g0198:  iteration    37020/10000000 | consumed samples:      4738560 | consumed tokens:   9704570880 | elapsed time per iteration (ms): 4182.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.577853E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.601 | tokens per gpu per second (tgs): 1958.471 | TFLOPs: 15.76 |
g0184: [2024-08-11 06:27:13,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=37030, skipped=55, lr=[0.0001997610286344065, 0.0001997610286344065], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37030 loss: 0.7706 iter time (s): 4.115 samples/sec: 31.109
g0198:  iteration    37030/10000000 | consumed samples:      4739840 | consumed tokens:   9707192320 | elapsed time per iteration (ms): 4147.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.524033E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.865 | tokens per gpu per second (tgs): 1975.385 | TFLOPs: 15.90 |
g0184: [2024-08-11 06:27:54,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=37040, skipped=55, lr=[0.00019976084186982743, 0.00019976084186982743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37040 loss: 0.7710 iter time (s): 4.102 samples/sec: 31.204
g0198:  iteration    37040/10000000 | consumed samples:      4741120 | consumed tokens:   9709813760 | elapsed time per iteration (ms): 4135.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.511842E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.949 | tokens per gpu per second (tgs): 1980.748 | TFLOPs: 15.94 |
g0184: [2024-08-11 06:28:36,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=37050, skipped=55, lr=[0.00019976065503238727, 0.00019976065503238727], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37050 loss: 0.7376 iter time (s): 4.148 samples/sec: 30.856
g0198:  iteration    37050/10000000 | consumed samples:      4742400 | consumed tokens:   9712435200 | elapsed time per iteration (ms): 4180.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.608933E-01 | loss scale: 32768.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.616 | tokens per gpu per second (tgs): 1959.439 | TFLOPs: 15.77 |
g0184: [2024-08-11 06:29:18,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=37060, skipped=55, lr=[0.0001997604681220862, 0.0001997604681220862], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37060 loss: 0.7192 iter time (s): 4.181 samples/sec: 30.612
g0198:  iteration    37060/10000000 | consumed samples:      4743680 | consumed tokens:   9715056640 | elapsed time per iteration (ms): 4213.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.480629E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.376 | tokens per gpu per second (tgs): 1944.078 | TFLOPs: 15.64 |
g0184: [2024-08-11 06:30:00,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=37070, skipped=55, lr=[0.00019976028113892434, 0.00019976028113892434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37070 loss: 0.7630 iter time (s): 4.141 samples/sec: 30.908
g0198:  iteration    37070/10000000 | consumed samples:      4744960 | consumed tokens:   9717678080 | elapsed time per iteration (ms): 4173.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.519819E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.670 | tokens per gpu per second (tgs): 1962.901 | TFLOPs: 15.80 |
g0185: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37076
g0185: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37076
g0185: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37076
g0185: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37076
g0185: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37076
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37076
g0194: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: Grad overflow on iteration 37076
g0194: Grad overflow on iteration 37076
g0197: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 37076
g0187: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 06:30:31,089] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 37076
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 37076
g0194: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: Grad overflow on iteration 37076
g0187: Grad overflow on iteration 37076
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: Grad overflow on iteration 37076
g0197: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 37076
g0187: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: Grad overflow on iteration 37076
g0195: Grad overflow on iteration 37076
g0184: Grad overflow on iteration 37076
g0187: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 37076
g0198: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 37076
g0187: Grad overflow on iteration 37076
g0197: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 37076
g0187: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0187: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 37076
g0195: Grad overflow on iteration 37076
g0188: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 37076
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: Grad overflow on iteration 37076
g0188: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: Grad overflow on iteration 37076
g0197: Grad overflow on iteration 37076
g0197: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: Grad overflow on iteration 37076
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-11 06:30:31,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0188: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: Grad overflow on iteration 37076
g0195: Grad overflow on iteration 37076
g0197: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 37076
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 06:30:31,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-11 06:30:42,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=37080, skipped=56, lr=[0.00019976009408290181, 0.00019976009408290181], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37080 loss: 0.7637 iter time (s): 4.209 samples/sec: 30.409
g0198:  iteration    37080/10000000 | consumed samples:      4746240 | consumed tokens:   9720299520 | elapsed time per iteration (ms): 4241.6 | learning rate: 1.998E-04 | global batch size:   128 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.177 | tokens per gpu per second (tgs): 1931.339 | TFLOPs: 15.54 |
g0184: [2024-08-11 06:31:25,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=37090, skipped=56, lr=[0.00019975990695401881, 0.00019975990695401881], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37090 loss: 0.7353 iter time (s): 4.196 samples/sec: 30.506
g0198:  iteration    37090/10000000 | consumed samples:      4747520 | consumed tokens:   9722920960 | elapsed time per iteration (ms): 4230.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.526801E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.254 | tokens per gpu per second (tgs): 1936.251 | TFLOPs: 15.58 |
g0184: [2024-08-11 06:32:06,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=37100, skipped=56, lr=[0.00019975971975227542, 0.00019975971975227542], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37100 loss: 0.7432 iter time (s): 4.094 samples/sec: 31.267
g0198:  iteration    37100/10000000 | consumed samples:      4748800 | consumed tokens:   9725542400 | elapsed time per iteration (ms): 4125.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.617983E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.023 | tokens per gpu per second (tgs): 1985.498 | TFLOPs: 15.98 |
g0184: [2024-08-11 06:32:49,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=37110, skipped=56, lr=[0.00019975953247767183, 0.00019975953247767183], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37110 loss: 0.7500 iter time (s): 4.272 samples/sec: 29.966
g0198:  iteration    37110/10000000 | consumed samples:      4750080 | consumed tokens:   9728163840 | elapsed time per iteration (ms): 4304.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.440856E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.740 | tokens per gpu per second (tgs): 1903.367 | TFLOPs: 15.32 |
g0184: [2024-08-11 06:33:31,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=37120, skipped=56, lr=[0.00019975934513020814, 0.00019975934513020814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37120 loss: 0.7875 iter time (s): 4.172 samples/sec: 30.684
g0198:  iteration    37120/10000000 | consumed samples:      4751360 | consumed tokens:   9730785280 | elapsed time per iteration (ms): 4203.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.497499E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.448 | tokens per gpu per second (tgs): 1948.674 | TFLOPs: 15.68 |
g0184: [2024-08-11 06:34:13,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=37130, skipped=56, lr=[0.00019975915770988455, 0.00019975915770988455], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37130 loss: 0.7527 iter time (s): 4.139 samples/sec: 30.925
g0198:  iteration    37130/10000000 | consumed samples:      4752640 | consumed tokens:   9733406720 | elapsed time per iteration (ms): 4171.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.520828E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.687 | tokens per gpu per second (tgs): 1963.995 | TFLOPs: 15.80 |
g0184: [2024-08-11 06:34:54,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=37140, skipped=56, lr=[0.00019975897021670116, 0.00019975897021670116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37140 loss: 0.7375 iter time (s): 4.072 samples/sec: 31.434
g0198:  iteration    37140/10000000 | consumed samples:      4753920 | consumed tokens:   9736028160 | elapsed time per iteration (ms): 4104.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.499077E-01 | loss scale: 16384.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.184 | tokens per gpu per second (tgs): 1995.784 | TFLOPs: 16.06 |
g0184: [2024-08-11 06:35:37,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=37150, skipped=56, lr=[0.00019975878265065814, 0.00019975878265065814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37150 loss: 0.7726 iter time (s): 4.267 samples/sec: 29.996
g0198:  iteration    37150/10000000 | consumed samples:      4755200 | consumed tokens:   9738649600 | elapsed time per iteration (ms): 4300.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.602614E-01 | loss scale: 16384.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.764 | tokens per gpu per second (tgs): 1904.888 | TFLOPs: 15.33 |
g0184: [2024-08-11 06:36:19,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=37160, skipped=56, lr=[0.00019975859501175563, 0.00019975859501175563], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37160 loss: 0.7631 iter time (s): 4.215 samples/sec: 30.367
g0198:  iteration    37160/10000000 | consumed samples:      4756480 | consumed tokens:   9741271040 | elapsed time per iteration (ms): 4247.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.596544E-01 | loss scale: 16384.0 | grad norm: 0.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.137 | tokens per gpu per second (tgs): 1928.741 | TFLOPs: 15.52 |
g0184: [2024-08-11 06:37:01,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=37170, skipped=56, lr=[0.00019975840729999375, 0.00019975840729999375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37170 loss: 0.7249 iter time (s): 4.160 samples/sec: 30.766
g0198:  iteration    37170/10000000 | consumed samples:      4757760 | consumed tokens:   9743892480 | elapsed time per iteration (ms): 4193.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.520285E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.526 | tokens per gpu per second (tgs): 1953.659 | TFLOPs: 15.72 |
g0184: [2024-08-11 06:37:43,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=37180, skipped=56, lr=[0.00019975821951537267, 0.00019975821951537267], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37180 loss: 0.7661 iter time (s): 4.168 samples/sec: 30.713
g0198:  iteration    37180/10000000 | consumed samples:      4759040 | consumed tokens:   9746513920 | elapsed time per iteration (ms): 4199.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.454703E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.478 | tokens per gpu per second (tgs): 1950.576 | TFLOPs: 15.70 |
g0184: [2024-08-11 06:38:28,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=37190, skipped=56, lr=[0.0001997580316578925, 0.0001997580316578925], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37190 loss: 0.7497 iter time (s): 4.471 samples/sec: 28.628
g0198:  iteration    37190/10000000 | consumed samples:      4760320 | consumed tokens:   9749135360 | elapsed time per iteration (ms): 4503.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.565095E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.420 | tokens per gpu per second (tgs): 1818.883 | TFLOPs: 14.64 |
g0184: [2024-08-11 06:39:10,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=37200, skipped=56, lr=[0.00019975784372755346, 0.00019975784372755346], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37200 loss: 0.7257 iter time (s): 4.129 samples/sec: 31.003
g0198:  iteration    37200/10000000 | consumed samples:      4761600 | consumed tokens:   9751756800 | elapsed time per iteration (ms): 4162.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.553679E-01 | loss scale: 16384.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.750 | tokens per gpu per second (tgs): 1967.972 | TFLOPs: 15.84 |
g0184: [2024-08-11 06:39:53,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=37210, skipped=56, lr=[0.00019975765572435562, 0.00019975765572435562], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37210 loss: 0.7717 iter time (s): 4.282 samples/sec: 29.896
g0198:  iteration    37210/10000000 | consumed samples:      4762880 | consumed tokens:   9754378240 | elapsed time per iteration (ms): 4313.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.624678E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.673 | tokens per gpu per second (tgs): 1899.057 | TFLOPs: 15.28 |
g0184: [2024-08-11 06:40:36,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=37220, skipped=56, lr=[0.00019975746764829916, 0.00019975746764829916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37220 loss: 0.7330 iter time (s): 4.276 samples/sec: 29.937
g0198:  iteration    37220/10000000 | consumed samples:      4764160 | consumed tokens:   9756999680 | elapsed time per iteration (ms): 4308.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.626277E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.711 | tokens per gpu per second (tgs): 1901.511 | TFLOPs: 15.30 |
g0184: [2024-08-11 06:41:19,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=37230, skipped=56, lr=[0.0001997572794993842, 0.0001997572794993842], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37230 loss: 0.7491 iter time (s): 4.215 samples/sec: 30.365
g0198:  iteration    37230/10000000 | consumed samples:      4765440 | consumed tokens:   9759621120 | elapsed time per iteration (ms): 4247.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.518714E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.134 | tokens per gpu per second (tgs): 1928.544 | TFLOPs: 15.52 |
g0184: [2024-08-11 06:42:03,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=37240, skipped=56, lr=[0.0001997570912776109, 0.0001997570912776109], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37240 loss: 0.7515 iter time (s): 4.376 samples/sec: 29.248
g0198:  iteration    37240/10000000 | consumed samples:      4766720 | consumed tokens:   9762242560 | elapsed time per iteration (ms): 4409.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.476388E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.031 | tokens per gpu per second (tgs): 1857.956 | TFLOPs: 14.95 |
g0184: [2024-08-11 06:42:44,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=37250, skipped=56, lr=[0.00019975690298297942, 0.00019975690298297942], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37250 loss: 0.7650 iter time (s): 4.102 samples/sec: 31.205
g0198:  iteration    37250/10000000 | consumed samples:      4768000 | consumed tokens:   9764864000 | elapsed time per iteration (ms): 4134.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.492036E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.961 | tokens per gpu per second (tgs): 1981.497 | TFLOPs: 15.95 |
g0184: [2024-08-11 06:43:26,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=37260, skipped=56, lr=[0.00019975671461548987, 0.00019975671461548987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37260 loss: 0.7624 iter time (s): 4.188 samples/sec: 30.565
g0198:  iteration    37260/10000000 | consumed samples:      4769280 | consumed tokens:   9767485440 | elapsed time per iteration (ms): 4220.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.455614E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.329 | tokens per gpu per second (tgs): 1941.026 | TFLOPs: 15.62 |
g0184: [2024-08-11 06:44:09,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=37270, skipped=56, lr=[0.00019975652617514243, 0.00019975652617514243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37270 loss: 0.7747 iter time (s): 4.230 samples/sec: 30.260
g0198:  iteration    37270/10000000 | consumed samples:      4770560 | consumed tokens:   9770106880 | elapsed time per iteration (ms): 4262.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.543627E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.029 | tokens per gpu per second (tgs): 1921.850 | TFLOPs: 15.47 |
g0184: [2024-08-11 06:44:51,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=37280, skipped=56, lr=[0.00019975633766193725, 0.00019975633766193725], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37280 loss: 0.7519 iter time (s): 4.201 samples/sec: 30.466
g0198:  iteration    37280/10000000 | consumed samples:      4771840 | consumed tokens:   9772728320 | elapsed time per iteration (ms): 4234.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.517451E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.227 | tokens per gpu per second (tgs): 1934.540 | TFLOPs: 15.57 |
g0184: [2024-08-11 06:45:34,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=37290, skipped=56, lr=[0.0001997561490758744, 0.0001997561490758744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37290 loss: 0.7508 iter time (s): 4.256 samples/sec: 30.074
g0198:  iteration    37290/10000000 | consumed samples:      4773120 | consumed tokens:   9775349760 | elapsed time per iteration (ms): 4288.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.552788E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.845 | tokens per gpu per second (tgs): 1910.099 | TFLOPs: 15.37 |
g0184: [2024-08-11 06:46:16,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=37300, skipped=56, lr=[0.00019975596041695414, 0.00019975596041695414], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37300 loss: 0.7442 iter time (s): 4.162 samples/sec: 30.753
g0198:  iteration    37300/10000000 | consumed samples:      4774400 | consumed tokens:   9777971200 | elapsed time per iteration (ms): 4194.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.526757E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.515 | tokens per gpu per second (tgs): 1952.960 | TFLOPs: 15.72 |
g0184: [2024-08-11 06:46:58,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=37310, skipped=56, lr=[0.00019975577168517648, 0.00019975577168517648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37310 loss: 0.7583 iter time (s): 4.180 samples/sec: 30.622
g0198:  iteration    37310/10000000 | consumed samples:      4775680 | consumed tokens:   9780592640 | elapsed time per iteration (ms): 4214.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.573414E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.374 | tokens per gpu per second (tgs): 1943.907 | TFLOPs: 15.64 |
g0184: [2024-08-11 06:47:39,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=37320, skipped=56, lr=[0.00019975558288054169, 0.00019975558288054169], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37320 loss: 0.7546 iter time (s): 4.091 samples/sec: 31.292
g0198:  iteration    37320/10000000 | consumed samples:      4776960 | consumed tokens:   9783214080 | elapsed time per iteration (ms): 4123.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.547976E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.040 | tokens per gpu per second (tgs): 1986.582 | TFLOPs: 15.99 |
g0184: [2024-08-11 06:48:21,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=37330, skipped=56, lr=[0.00019975539400304986, 0.00019975539400304986], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37330 loss: 0.7610 iter time (s): 4.133 samples/sec: 30.969
g0198:  iteration    37330/10000000 | consumed samples:      4778240 | consumed tokens:   9785835520 | elapsed time per iteration (ms): 4168.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.647100E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.704 | tokens per gpu per second (tgs): 1965.025 | TFLOPs: 15.81 |
g0184: [2024-08-11 06:49:04,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=37340, skipped=56, lr=[0.00019975520505270114, 0.00019975520505270114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37340 loss: 0.7240 iter time (s): 4.296 samples/sec: 29.793
g0198:  iteration    37340/10000000 | consumed samples:      4779520 | consumed tokens:   9788456960 | elapsed time per iteration (ms): 4328.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.548919E-01 | loss scale: 16384.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.569 | tokens per gpu per second (tgs): 1892.436 | TFLOPs: 15.23 |
g0184: [2024-08-11 06:49:48,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=37350, skipped=56, lr=[0.00019975501602949566, 0.00019975501602949566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37350 loss: 0.7660 iter time (s): 4.278 samples/sec: 29.923
g0198:  iteration    37350/10000000 | consumed samples:      4780800 | consumed tokens:   9791078400 | elapsed time per iteration (ms): 4314.0 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.490434E-01 | loss scale: 16384.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.671 | tokens per gpu per second (tgs): 1898.953 | TFLOPs: 15.28 |
g0184: [2024-08-11 06:50:29,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=37360, skipped=56, lr=[0.00019975482693343358, 0.00019975482693343358], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37360 loss: 0.7481 iter time (s): 4.096 samples/sec: 31.248
g0198:  iteration    37360/10000000 | consumed samples:      4782080 | consumed tokens:   9793699840 | elapsed time per iteration (ms): 4129.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.530213E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.997 | tokens per gpu per second (tgs): 1983.798 | TFLOPs: 15.96 |
g0184: [2024-08-11 06:51:11,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=37370, skipped=56, lr=[0.00019975463776451507, 0.00019975463776451507], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37370 loss: 0.7479 iter time (s): 4.211 samples/sec: 30.394
g0198:  iteration    37370/10000000 | consumed samples:      4783360 | consumed tokens:   9796321280 | elapsed time per iteration (ms): 4246.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.532612E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.143 | tokens per gpu per second (tgs): 1929.137 | TFLOPs: 15.52 |
g0184: [2024-08-11 06:51:53,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=37380, skipped=56, lr=[0.00019975444852274023, 0.00019975444852274023], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37380 loss: 0.7389 iter time (s): 4.151 samples/sec: 30.836
g0198:  iteration    37380/10000000 | consumed samples:      4784640 | consumed tokens:   9798942720 | elapsed time per iteration (ms): 4183.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.496320E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.598 | tokens per gpu per second (tgs): 1958.266 | TFLOPs: 15.76 |
g0184: [2024-08-11 06:52:36,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=37390, skipped=56, lr=[0.00019975425920810923, 0.00019975425920810923], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37390 loss: 0.7691 iter time (s): 4.226 samples/sec: 30.288
g0198:  iteration    37390/10000000 | consumed samples:      4785920 | consumed tokens:   9801564160 | elapsed time per iteration (ms): 4258.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.539032E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.058 | tokens per gpu per second (tgs): 1923.733 | TFLOPs: 15.48 |
g0184: [2024-08-11 06:53:19,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=37400, skipped=56, lr=[0.0001997540698206222, 0.0001997540698206222], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37400 loss: 0.7731 iter time (s): 4.340 samples/sec: 29.495
g0198:  iteration    37400/10000000 | consumed samples:      4787200 | consumed tokens:   9804185600 | elapsed time per iteration (ms): 4372.1 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.505193E-01 | loss scale: 16384.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.276 | tokens per gpu per second (tgs): 1873.695 | TFLOPs: 15.08 |
g0184: [2024-08-11 06:54:02,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=37410, skipped=56, lr=[0.00019975388036027933, 0.00019975388036027933], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37410 loss: 0.7522 iter time (s): 4.273 samples/sec: 29.953
g0198:  iteration    37410/10000000 | consumed samples:      4788480 | consumed tokens:   9806807040 | elapsed time per iteration (ms): 4306.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.486520E-01 | loss scale: 16384.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.721 | tokens per gpu per second (tgs): 1902.175 | TFLOPs: 15.31 |
g0184: [2024-08-11 06:54:46,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=37420, skipped=56, lr=[0.0001997536908270807, 0.0001997536908270807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37420 loss: 0.7527 iter time (s): 4.317 samples/sec: 29.652
g0198:  iteration    37420/10000000 | consumed samples:      4789760 | consumed tokens:   9809428480 | elapsed time per iteration (ms): 4351.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.578262E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.416 | tokens per gpu per second (tgs): 1882.613 | TFLOPs: 15.15 |
g0184: [2024-08-11 06:55:30,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=37430, skipped=56, lr=[0.0001997535012210265, 0.0001997535012210265], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37430 loss: 0.7719 iter time (s): 4.373 samples/sec: 29.272
g0198:  iteration    37430/10000000 | consumed samples:      4791040 | consumed tokens:   9812049920 | elapsed time per iteration (ms): 4405.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.661644E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.056 | tokens per gpu per second (tgs): 1859.580 | TFLOPs: 14.96 |
g0184: [2024-08-11 06:56:13,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=37440, skipped=56, lr=[0.00019975331154211686, 0.00019975331154211686], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37440 loss: 0.7728 iter time (s): 4.258 samples/sec: 30.060
g0198:  iteration    37440/10000000 | consumed samples:      4792320 | consumed tokens:   9814671360 | elapsed time per iteration (ms): 4290.4 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.581522E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.834 | tokens per gpu per second (tgs): 1909.375 | TFLOPs: 15.37 |
g0184: [2024-08-11 06:56:55,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=37450, skipped=56, lr=[0.00019975312179035194, 0.00019975312179035194], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37450 loss: 0.7420 iter time (s): 4.186 samples/sec: 30.575
g0198:  iteration    37450/10000000 | consumed samples:      4793600 | consumed tokens:   9817292800 | elapsed time per iteration (ms): 4220.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.508767E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.331 | tokens per gpu per second (tgs): 1941.155 | TFLOPs: 15.62 |
g0184: [2024-08-11 06:57:38,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=37460, skipped=56, lr=[0.0001997529319657319, 0.0001997529319657319], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37460 loss: 0.7427 iter time (s): 4.265 samples/sec: 30.009
g0198:  iteration    37460/10000000 | consumed samples:      4794880 | consumed tokens:   9819914240 | elapsed time per iteration (ms): 4298.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.551697E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.777 | tokens per gpu per second (tgs): 1905.729 | TFLOPs: 15.34 |
g0184: [2024-08-11 06:58:19,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=37470, skipped=56, lr=[0.0001997527420682568, 0.0001997527420682568], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37470 loss: 0.7433 iter time (s): 4.095 samples/sec: 31.256
g0198:  iteration    37470/10000000 | consumed samples:      4796160 | consumed tokens:   9822535680 | elapsed time per iteration (ms): 4129.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.532255E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.993 | tokens per gpu per second (tgs): 1983.581 | TFLOPs: 15.96 |
g0184: [2024-08-11 06:59:04,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=37480, skipped=56, lr=[0.0001997525520979269, 0.0001997525520979269], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37480 loss: 0.7578 iter time (s): 4.448 samples/sec: 28.780
g0198:  iteration    37480/10000000 | consumed samples:      4797440 | consumed tokens:   9825157120 | elapsed time per iteration (ms): 4480.3 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.609384E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.570 | tokens per gpu per second (tgs): 1828.468 | TFLOPs: 14.71 |
g0184: [2024-08-11 06:59:49,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=37490, skipped=56, lr=[0.0001997523620547423, 0.0001997523620547423], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37490 loss: 0.7299 iter time (s): 4.400 samples/sec: 29.091
g0198:  iteration    37490/10000000 | consumed samples:      4798720 | consumed tokens:   9827778560 | elapsed time per iteration (ms): 4433.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.520466E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.870 | tokens per gpu per second (tgs): 1847.696 | TFLOPs: 14.87 |
g0184: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37498
g0184: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37498
g0184: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37498
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 37498
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 37498
g0187: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 37498
g0187: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 37498
g0187: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 37498
g0187: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 37498
g0185: Grad overflow on iteration 37498
g0195: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 37498
g0187: Grad overflow on iteration 37498
g0195: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: Grad overflow on iteration 37498
g0187: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 37498
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 37498
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 37498
g0198: Grad overflow on iteration 37498
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 37498
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: Grad overflow on iteration 37498
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 37498
g0194: Grad overflow on iteration 37498
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: Grad overflow on iteration 37498
g0185: Grad overflow on iteration 37498
g0184: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 37498
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: Grad overflow on iteration 37498
g0197: Grad overflow on iteration 37498
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37498
g0184: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 37498
g0195: [2024-08-11 07:00:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: [2024-08-11 07:00:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 37498
g0184: [2024-08-11 07:00:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 37498
g0185: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 37498
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 37498
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 07:00:27,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 07:00:27,082] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0184: [2024-08-11 07:00:31,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=37500, skipped=57, lr=[0.00019975217193870308, 0.00019975217193870308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37500 loss: 0.7537 iter time (s): 4.187 samples/sec: 30.570
g0198:  iteration    37500/10000000 | consumed samples:      4800000 | consumed tokens:   9830400000 | elapsed time per iteration (ms): 4219.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.599802E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.334 | tokens per gpu per second (tgs): 1941.395 | TFLOPs: 15.62 |
g0184: [2024-08-11 07:01:15,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=37510, skipped=57, lr=[0.0001997519817498095, 0.0001997519817498095], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37510 loss: 0.7271 iter time (s): 4.404 samples/sec: 29.063
g0198:  iteration    37510/10000000 | consumed samples:      4801280 | consumed tokens:   9833021440 | elapsed time per iteration (ms): 4437.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.573762E-01 | loss scale: 8192.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.843 | tokens per gpu per second (tgs): 1845.949 | TFLOPs: 14.85 |
g0184: [2024-08-11 07:01:58,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=37520, skipped=57, lr=[0.00019975179148806166, 0.00019975179148806166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37520 loss: 0.7812 iter time (s): 4.225 samples/sec: 30.296
g0198:  iteration    37520/10000000 | consumed samples:      4802560 | consumed tokens:   9835642880 | elapsed time per iteration (ms): 4257.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.491165E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.067 | tokens per gpu per second (tgs): 1924.270 | TFLOPs: 15.48 |
g0184: [2024-08-11 07:02:39,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=37530, skipped=57, lr=[0.00019975160115345966, 0.00019975160115345966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37530 loss: 0.7522 iter time (s): 4.096 samples/sec: 31.247
g0198:  iteration    37530/10000000 | consumed samples:      4803840 | consumed tokens:   9838264320 | elapsed time per iteration (ms): 4128.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.651409E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.004 | tokens per gpu per second (tgs): 1984.228 | TFLOPs: 15.97 |
g0184: [2024-08-11 07:03:21,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=37540, skipped=57, lr=[0.00019975141074600372, 0.00019975141074600372], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37540 loss: 0.7664 iter time (s): 4.124 samples/sec: 31.041
g0198:  iteration    37540/10000000 | consumed samples:      4805120 | consumed tokens:   9840885760 | elapsed time per iteration (ms): 4155.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.467325E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.801 | tokens per gpu per second (tgs): 1971.281 | TFLOPs: 15.86 |
g0184: [2024-08-11 07:04:04,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=37550, skipped=57, lr=[0.00019975122026569391, 0.00019975122026569391], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37550 loss: 0.7652 iter time (s): 4.274 samples/sec: 29.948
g0198:  iteration    37550/10000000 | consumed samples:      4806400 | consumed tokens:   9843507200 | elapsed time per iteration (ms): 4308.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.534079E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.711 | tokens per gpu per second (tgs): 1901.486 | TFLOPs: 15.30 |
g0184: [2024-08-11 07:04:46,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=37560, skipped=57, lr=[0.00019975102971253047, 0.00019975102971253047], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37560 loss: 0.7599 iter time (s): 4.237 samples/sec: 30.209
g0198:  iteration    37560/10000000 | consumed samples:      4807680 | consumed tokens:   9846128640 | elapsed time per iteration (ms): 4274.8 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.610181E-01 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.943 | tokens per gpu per second (tgs): 1916.331 | TFLOPs: 15.42 |
g0184: [2024-08-11 07:05:29,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=37570, skipped=57, lr=[0.00019975083908651346, 0.00019975083908651346], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37570 loss: 0.7841 iter time (s): 4.260 samples/sec: 30.044
g0198:  iteration    37570/10000000 | consumed samples:      4808960 | consumed tokens:   9848750080 | elapsed time per iteration (ms): 4292.7 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.522812E-01 | loss scale: 8192.0 | grad norm: 0.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.818 | tokens per gpu per second (tgs): 1908.342 | TFLOPs: 15.36 |
g0184: [2024-08-11 07:06:12,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=37580, skipped=57, lr=[0.00019975064838764309, 0.00019975064838764309], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37580 loss: 0.7607 iter time (s): 4.264 samples/sec: 30.021
g0198:  iteration    37580/10000000 | consumed samples:      4810240 | consumed tokens:   9851371520 | elapsed time per iteration (ms): 4295.9 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.511571E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.796 | tokens per gpu per second (tgs): 1906.927 | TFLOPs: 15.35 |
g0184: [2024-08-11 07:06:59,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=37590, skipped=57, lr=[0.00019975045761591945, 0.00019975045761591945], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37590 loss: 0.7241 iter time (s): 4.592 samples/sec: 27.874
g0198:  iteration    37590/10000000 | consumed samples:      4811520 | consumed tokens:   9853992960 | elapsed time per iteration (ms): 4624.6 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.441000E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.678 | tokens per gpu per second (tgs): 1771.409 | TFLOPs: 14.25 |
g0184: [2024-08-11 07:07:41,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=37600, skipped=57, lr=[0.00019975026677134273, 0.00019975026677134273], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37600 loss: 0.7857 iter time (s): 4.178 samples/sec: 30.636
g0198:  iteration    37600/10000000 | consumed samples:      4812800 | consumed tokens:   9856614400 | elapsed time per iteration (ms): 4210.5 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.629992E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.400 | tokens per gpu per second (tgs): 1945.621 | TFLOPs: 15.66 |
g0184: [2024-08-11 07:08:21,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=37610, skipped=57, lr=[0.00019975007585391305, 0.00019975007585391305], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37610 loss: 0.7648 iter time (s): 4.024 samples/sec: 31.812
g0198:  iteration    37610/10000000 | consumed samples:      4814080 | consumed tokens:   9859235840 | elapsed time per iteration (ms): 4056.2 | learning rate: 1.998E-04 | global batch size:   128 | lm loss: 7.438906E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.557 | tokens per gpu per second (tgs): 2019.645 | TFLOPs: 16.25 |
g0184: [2024-08-11 07:09:04,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=37620, skipped=57, lr=[0.00019974988486363057, 0.00019974988486363057], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37620 loss: 0.7818 iter time (s): 4.275 samples/sec: 29.944
g0198:  iteration    37620/10000000 | consumed samples:      4815360 | consumed tokens:   9861857280 | elapsed time per iteration (ms): 4307.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.534039E-01 | loss scale: 8192.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.719 | tokens per gpu per second (tgs): 1901.998 | TFLOPs: 15.31 |
g0184: [2024-08-11 07:09:47,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=37630, skipped=57, lr=[0.00019974969380049545, 0.00019974969380049545], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37630 loss: 0.7395 iter time (s): 4.268 samples/sec: 29.990
g0198:  iteration    37630/10000000 | consumed samples:      4816640 | consumed tokens:   9864478720 | elapsed time per iteration (ms): 4300.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.489087E-01 | loss scale: 8192.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.762 | tokens per gpu per second (tgs): 1904.746 | TFLOPs: 15.33 |
g0184: [2024-08-11 07:10:29,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=37640, skipped=57, lr=[0.00019974950266450782, 0.00019974950266450782], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37640 loss: 0.7269 iter time (s): 4.135 samples/sec: 30.956
g0198:  iteration    37640/10000000 | consumed samples:      4817920 | consumed tokens:   9867100160 | elapsed time per iteration (ms): 4167.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.533487E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.718 | tokens per gpu per second (tgs): 1965.946 | TFLOPs: 15.82 |
g0184: [2024-08-11 07:11:12,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=37650, skipped=57, lr=[0.00019974931145566783, 0.00019974931145566783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37650 loss: 0.7232 iter time (s): 4.302 samples/sec: 29.753
g0198:  iteration    37650/10000000 | consumed samples:      4819200 | consumed tokens:   9869721600 | elapsed time per iteration (ms): 4334.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.464662E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.530 | tokens per gpu per second (tgs): 1889.944 | TFLOPs: 15.21 |
g0184: [2024-08-11 07:11:55,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=37660, skipped=57, lr=[0.0001997491201739756, 0.0001997491201739756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37660 loss: 0.7138 iter time (s): 4.233 samples/sec: 30.238
g0198:  iteration    37660/10000000 | consumed samples:      4820480 | consumed tokens:   9872343040 | elapsed time per iteration (ms): 4280.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.479632E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.907 | tokens per gpu per second (tgs): 1914.026 | TFLOPs: 15.40 |
g0184: [2024-08-11 07:12:37,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=37670, skipped=57, lr=[0.0001997489288194313, 0.0001997489288194313], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37670 loss: 0.7644 iter time (s): 4.125 samples/sec: 31.031
g0198:  iteration    37670/10000000 | consumed samples:      4821760 | consumed tokens:   9874964480 | elapsed time per iteration (ms): 4157.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.566998E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.790 | tokens per gpu per second (tgs): 1970.574 | TFLOPs: 15.86 |
g0184: [2024-08-11 07:13:20,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=37680, skipped=57, lr=[0.00019974873739203513, 0.00019974873739203513], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37680 loss: 0.7910 iter time (s): 4.275 samples/sec: 29.943
g0198:  iteration    37680/10000000 | consumed samples:      4823040 | consumed tokens:   9877585920 | elapsed time per iteration (ms): 4307.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.676507E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.718 | tokens per gpu per second (tgs): 1901.922 | TFLOPs: 15.31 |
g0184: [2024-08-11 07:14:01,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=37690, skipped=57, lr=[0.00019974854589178718, 0.00019974854589178718], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37690 loss: 0.7287 iter time (s): 4.129 samples/sec: 31.001
g0198:  iteration    37690/10000000 | consumed samples:      4824320 | consumed tokens:   9880207360 | elapsed time per iteration (ms): 4161.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.413753E-01 | loss scale: 8192.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.760 | tokens per gpu per second (tgs): 1968.608 | TFLOPs: 15.84 |
g0184: [2024-08-11 07:14:45,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=37700, skipped=57, lr=[0.00019974835431868756, 0.00019974835431868756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37700 loss: 0.7714 iter time (s): 4.286 samples/sec: 29.863
g0198:  iteration    37700/10000000 | consumed samples:      4825600 | consumed tokens:   9882828800 | elapsed time per iteration (ms): 4318.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.518386E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.638 | tokens per gpu per second (tgs): 1896.846 | TFLOPs: 15.26 |
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37702
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37702
g0197: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 37702
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37702
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37702
g0184: Grad overflow on iteration 37702
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 37702
g0197: Grad overflow on iteration 37702
g0197: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0197: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0197: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 37702
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 37702
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0195: Grad overflow on iteration 37702
g0198: Grad overflow on iteration 37702
g0184: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0187: Grad overflow on iteration 37702
g0185: Grad overflow on iteration 37702
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 37702
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0187: Grad overflow on iteration 37702
g0195: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 07:14:59,740] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0197: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 37702
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 37702
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 37702
g0197: Grad overflow on iteration 37702
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 37702
g0195: Grad overflow on iteration 37702
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0195: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0194: Grad overflow on iteration 37702
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: Grad overflow on iteration 37702
g0188: Grad overflow on iteration 37702
g0187: Grad overflow on iteration 37702
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 37702
g0188: Grad overflow on iteration 37702
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 37702
g0195: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0188: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0185: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0195: Grad overflow on iteration 37702
g0187: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 37702
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 37702
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0194: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0198: [2024-08-11 07:14:59,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0197: [2024-08-11 07:14:59,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0195: [2024-08-11 07:14:59,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0184: [2024-08-11 07:15:29,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=37710, skipped=58, lr=[0.0001997481626727365, 0.0001997481626727365], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37710 loss: 0.7472 iter time (s): 4.446 samples/sec: 28.793
g0198:  iteration    37710/10000000 | consumed samples:      4826880 | consumed tokens:   9885450240 | elapsed time per iteration (ms): 4478.6 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 4096.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.580 | tokens per gpu per second (tgs): 1829.138 | TFLOPs: 14.72 |
g0184: [2024-08-11 07:16:12,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=37720, skipped=58, lr=[0.0001997479709539341, 0.0001997479709539341], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37720 loss: 0.7425 iter time (s): 4.218 samples/sec: 30.345
g0198:  iteration    37720/10000000 | consumed samples:      4828160 | consumed tokens:   9888071680 | elapsed time per iteration (ms): 4254.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.579277E-01 | loss scale: 4096.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.084 | tokens per gpu per second (tgs): 1925.348 | TFLOPs: 15.49 |
g0184: [2024-08-11 07:16:55,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=37730, skipped=58, lr=[0.0001997477791622805, 0.0001997477791622805], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37730 loss: 0.7415 iter time (s): 4.280 samples/sec: 29.909
g0198:  iteration    37730/10000000 | consumed samples:      4829440 | consumed tokens:   9890693120 | elapsed time per iteration (ms): 4312.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.550028E-01 | loss scale: 4096.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.685 | tokens per gpu per second (tgs): 1899.820 | TFLOPs: 15.29 |
g0184: [2024-08-11 07:17:38,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=37740, skipped=58, lr=[0.00019974758729777588, 0.00019974758729777588], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37740 loss: 0.7648 iter time (s): 4.243 samples/sec: 30.169
g0198:  iteration    37740/10000000 | consumed samples:      4830720 | consumed tokens:   9893314560 | elapsed time per iteration (ms): 4275.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.521071E-01 | loss scale: 4096.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.939 | tokens per gpu per second (tgs): 1916.080 | TFLOPs: 15.42 |
g0184: [2024-08-11 07:18:20,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=37750, skipped=58, lr=[0.00019974739536042038, 0.00019974739536042038], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37750 loss: 0.7172 iter time (s): 4.161 samples/sec: 30.764
g0198:  iteration    37750/10000000 | consumed samples:      4832000 | consumed tokens:   9895936000 | elapsed time per iteration (ms): 4193.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.409375E-01 | loss scale: 4096.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.527 | tokens per gpu per second (tgs): 1953.705 | TFLOPs: 15.72 |
g0184: [2024-08-11 07:19:02,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=37760, skipped=58, lr=[0.00019974720335021412, 0.00019974720335021412], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37760 loss: 0.7217 iter time (s): 4.236 samples/sec: 30.217
g0198:  iteration    37760/10000000 | consumed samples:      4833280 | consumed tokens:   9898557440 | elapsed time per iteration (ms): 4270.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.547639E-01 | loss scale: 4096.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.975 | tokens per gpu per second (tgs): 1918.424 | TFLOPs: 15.44 |
g0184: [2024-08-11 07:19:44,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=37770, skipped=58, lr=[0.00019974701126715728, 0.00019974701126715728], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37770 loss: 0.7309 iter time (s): 4.154 samples/sec: 30.810
g0198:  iteration    37770/10000000 | consumed samples:      4834560 | consumed tokens:   9901178880 | elapsed time per iteration (ms): 4187.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.596028E-01 | loss scale: 4096.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.571 | tokens per gpu per second (tgs): 1956.548 | TFLOPs: 15.74 |
g0184: [2024-08-11 07:20:29,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=37780, skipped=58, lr=[0.00019974681911125, 0.00019974681911125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37780 loss: 0.7432 iter time (s): 4.487 samples/sec: 28.527
g0198:  iteration    37780/10000000 | consumed samples:      4835840 | consumed tokens:   9903800320 | elapsed time per iteration (ms): 4519.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.516059E-01 | loss scale: 4096.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.322 | tokens per gpu per second (tgs): 1812.625 | TFLOPs: 14.59 |
g0184: [2024-08-11 07:21:13,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=37790, skipped=58, lr=[0.00019974662688249242, 0.00019974662688249242], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37790 loss: 0.7535 iter time (s): 4.277 samples/sec: 29.930
g0198:  iteration    37790/10000000 | consumed samples:      4837120 | consumed tokens:   9906421760 | elapsed time per iteration (ms): 4309.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.418275E-01 | loss scale: 4096.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.705 | tokens per gpu per second (tgs): 1901.111 | TFLOPs: 15.30 |
g0184: [2024-08-11 07:21:55,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=37800, skipped=58, lr=[0.00019974643458088467, 0.00019974643458088467], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37800 loss: 0.7352 iter time (s): 4.243 samples/sec: 30.169
g0198:  iteration    37800/10000000 | consumed samples:      4838400 | consumed tokens:   9909043200 | elapsed time per iteration (ms): 4275.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.505733E-01 | loss scale: 4096.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.940 | tokens per gpu per second (tgs): 1916.159 | TFLOPs: 15.42 |
g0184: [2024-08-11 07:22:39,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=37810, skipped=58, lr=[0.00019974624220642692, 0.00019974624220642692], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37810 loss: 0.7560 iter time (s): 4.318 samples/sec: 29.644
g0198:  iteration    37810/10000000 | consumed samples:      4839680 | consumed tokens:   9911664640 | elapsed time per iteration (ms): 4352.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.577969E-01 | loss scale: 4096.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.407 | tokens per gpu per second (tgs): 1882.055 | TFLOPs: 15.15 |
g0184: [2024-08-11 07:23:22,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=37820, skipped=58, lr=[0.00019974604975911932, 0.00019974604975911932], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37820 loss: 0.7558 iter time (s): 4.263 samples/sec: 30.024
g0198:  iteration    37820/10000000 | consumed samples:      4840960 | consumed tokens:   9914286080 | elapsed time per iteration (ms): 4296.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.521368E-01 | loss scale: 4096.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.791 | tokens per gpu per second (tgs): 1906.646 | TFLOPs: 15.34 |
g0184: [2024-08-11 07:24:04,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=37830, skipped=58, lr=[0.00019974585723896203, 0.00019974585723896203], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37830 loss: 0.7539 iter time (s): 4.151 samples/sec: 30.836
g0198:  iteration    37830/10000000 | consumed samples:      4842240 | consumed tokens:   9916907520 | elapsed time per iteration (ms): 4183.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.507200E-01 | loss scale: 4096.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.595 | tokens per gpu per second (tgs): 1958.050 | TFLOPs: 15.76 |
g0184: [2024-08-11 07:24:46,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=37840, skipped=58, lr=[0.00019974566464595517, 0.00019974566464595517], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37840 loss: 0.7527 iter time (s): 4.240 samples/sec: 30.189
g0198:  iteration    37840/10000000 | consumed samples:      4843520 | consumed tokens:   9919528960 | elapsed time per iteration (ms): 4272.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.433539E-01 | loss scale: 4096.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.961 | tokens per gpu per second (tgs): 1917.481 | TFLOPs: 15.43 |
g0184: [2024-08-11 07:25:29,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=37850, skipped=58, lr=[0.00019974547198009892, 0.00019974547198009892], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37850 loss: 0.7615 iter time (s): 4.258 samples/sec: 30.063
g0198:  iteration    37850/10000000 | consumed samples:      4844800 | consumed tokens:   9922150400 | elapsed time per iteration (ms): 4290.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.532359E-01 | loss scale: 4096.0 | grad norm: 0.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.837 | tokens per gpu per second (tgs): 1909.544 | TFLOPs: 15.37 |
g0184: [2024-08-11 07:26:11,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=37860, skipped=58, lr=[0.00019974527924139341, 0.00019974527924139341], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37860 loss: 0.7553 iter time (s): 4.142 samples/sec: 30.899
g0198:  iteration    37860/10000000 | consumed samples:      4846080 | consumed tokens:   9924771840 | elapsed time per iteration (ms): 4175.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.658005E-01 | loss scale: 4096.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.659 | tokens per gpu per second (tgs): 1962.172 | TFLOPs: 15.79 |
g0184: [2024-08-11 07:26:54,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=37870, skipped=58, lr=[0.00019974508642983874, 0.00019974508642983874], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37870 loss: 0.7234 iter time (s): 4.274 samples/sec: 29.951
g0198:  iteration    37870/10000000 | consumed samples:      4847360 | consumed tokens:   9927393280 | elapsed time per iteration (ms): 4306.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.532882E-01 | loss scale: 4096.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.723 | tokens per gpu per second (tgs): 1902.265 | TFLOPs: 15.31 |
g0184: [2024-08-11 07:27:38,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=37880, skipped=58, lr=[0.00019974489354543514, 0.00019974489354543514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37880 loss: 0.7851 iter time (s): 4.367 samples/sec: 29.312
g0198:  iteration    37880/10000000 | consumed samples:      4848640 | consumed tokens:   9930014720 | elapsed time per iteration (ms): 4398.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.576331E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.098 | tokens per gpu per second (tgs): 1862.273 | TFLOPs: 14.99 |
g0184: [2024-08-11 07:28:21,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=37890, skipped=58, lr=[0.00019974470058818273, 0.00019974470058818273], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37890 loss: 0.7670 iter time (s): 4.259 samples/sec: 30.054
g0198:  iteration    37890/10000000 | consumed samples:      4849920 | consumed tokens:   9932636160 | elapsed time per iteration (ms): 4291.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.553679E-01 | loss scale: 4096.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.825 | tokens per gpu per second (tgs): 1908.781 | TFLOPs: 15.36 |
g0184: [2024-08-11 07:29:04,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=37900, skipped=58, lr=[0.00019974450755808165, 0.00019974450755808165], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37900 loss: 0.7643 iter time (s): 4.281 samples/sec: 29.899
g0198:  iteration    37900/10000000 | consumed samples:      4851200 | consumed tokens:   9935257600 | elapsed time per iteration (ms): 4313.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.626943E-01 | loss scale: 4096.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.677 | tokens per gpu per second (tgs): 1899.297 | TFLOPs: 15.28 |
g0184: [2024-08-11 07:29:46,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=37910, skipped=58, lr=[0.00019974431445513204, 0.00019974431445513204], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37910 loss: 0.7360 iter time (s): 4.119 samples/sec: 31.076
g0198:  iteration    37910/10000000 | consumed samples:      4852480 | consumed tokens:   9937879040 | elapsed time per iteration (ms): 4151.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.457058E-01 | loss scale: 4096.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.836 | tokens per gpu per second (tgs): 1973.476 | TFLOPs: 15.88 |
g0184: [2024-08-11 07:30:27,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=37920, skipped=58, lr=[0.00019974412127933406, 0.00019974412127933406], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37920 loss: 0.7701 iter time (s): 4.113 samples/sec: 31.122
g0198:  iteration    37920/10000000 | consumed samples:      4853760 | consumed tokens:   9940500480 | elapsed time per iteration (ms): 4145.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.527540E-01 | loss scale: 4096.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.877 | tokens per gpu per second (tgs): 1976.133 | TFLOPs: 15.90 |
g0184: [2024-08-11 07:31:11,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=37930, skipped=58, lr=[0.00019974392803068783, 0.00019974392803068783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37930 loss: 0.7524 iter time (s): 4.386 samples/sec: 29.185
g0198:  iteration    37930/10000000 | consumed samples:      4855040 | consumed tokens:   9943121920 | elapsed time per iteration (ms): 4418.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.440171E-01 | loss scale: 4096.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.971 | tokens per gpu per second (tgs): 1854.162 | TFLOPs: 14.92 |
g0184: [2024-08-11 07:31:55,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=37940, skipped=58, lr=[0.00019974373470919358, 0.00019974373470919358], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37940 loss: 0.7476 iter time (s): 4.326 samples/sec: 29.591
g0198:  iteration    37940/10000000 | consumed samples:      4856320 | consumed tokens:   9945743360 | elapsed time per iteration (ms): 4357.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.541434E-01 | loss scale: 4096.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.373 | tokens per gpu per second (tgs): 1879.864 | TFLOPs: 15.13 |
g0184: [2024-08-11 07:32:38,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=37950, skipped=58, lr=[0.00019974354131485137, 0.00019974354131485137], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37950 loss: 0.7328 iter time (s): 4.266 samples/sec: 30.002
g0198:  iteration    37950/10000000 | consumed samples:      4857600 | consumed tokens:   9948364800 | elapsed time per iteration (ms): 4299.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.475111E-01 | loss scale: 4096.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.773 | tokens per gpu per second (tgs): 1905.474 | TFLOPs: 15.33 |
g0184: [2024-08-11 07:33:22,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=37960, skipped=58, lr=[0.00019974334784766138, 0.00019974334784766138], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37960 loss: 0.7374 iter time (s): 4.341 samples/sec: 29.487
g0198:  iteration    37960/10000000 | consumed samples:      4858880 | consumed tokens:   9950986240 | elapsed time per iteration (ms): 4373.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.465440E-01 | loss scale: 4096.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.269 | tokens per gpu per second (tgs): 1873.202 | TFLOPs: 15.07 |
g0184: [2024-08-11 07:34:05,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=37970, skipped=58, lr=[0.00019974315430762378, 0.00019974315430762378], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37970 loss: 0.7301 iter time (s): 4.347 samples/sec: 29.448
g0198:  iteration    37970/10000000 | consumed samples:      4860160 | consumed tokens:   9953607680 | elapsed time per iteration (ms): 4378.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.593482E-01 | loss scale: 4096.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.232 | tokens per gpu per second (tgs): 1870.856 | TFLOPs: 15.06 |
g0184: [2024-08-11 07:34:49,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=37980, skipped=58, lr=[0.0001997429606947387, 0.0001997429606947387], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37980 loss: 0.7466 iter time (s): 4.325 samples/sec: 29.598
g0198:  iteration    37980/10000000 | consumed samples:      4861440 | consumed tokens:   9956229120 | elapsed time per iteration (ms): 4356.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.487772E-01 | loss scale: 4096.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.380 | tokens per gpu per second (tgs): 1880.348 | TFLOPs: 15.13 |
g0184: [2024-08-11 07:35:33,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=37990, skipped=58, lr=[0.00019974276700900626, 0.00019974276700900626], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 37990 loss: 0.7380 iter time (s): 4.385 samples/sec: 29.192
g0198:  iteration    37990/10000000 | consumed samples:      4862720 | consumed tokens:   9958850560 | elapsed time per iteration (ms): 4417.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.540761E-01 | loss scale: 4096.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.977 | tokens per gpu per second (tgs): 1854.512 | TFLOPs: 14.92 |
g0184: [2024-08-11 07:36:15,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=58, lr=[0.00019974257325042663, 0.00019974257325042663], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38000 loss: 0.7452 iter time (s): 4.183 samples/sec: 30.600
g0198:  iteration    38000/10000000 | consumed samples:      4864000 | consumed tokens:   9961472000 | elapsed time per iteration (ms): 4220.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.599218E-01 | loss scale: 4096.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.329 | tokens per gpu per second (tgs): 1941.039 | TFLOPs: 15.62 |
g0198: ----------------------------------------------------------------------------------------
g0198:  validation loss at iteration 38000 | lm loss value: NAN | lm loss PPL: 4.851652E+08 | 
g0198: ----------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   38000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 07:43:06,826] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step38000 is about to be saved!
g0184: [2024-08-11 07:43:06,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0184: [2024-08-11 07:43:06,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0198: [2024-08-11 07:43:06,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0198: [2024-08-11 07:43:06,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0198: [2024-08-11 07:43:06,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0184: [2024-08-11 07:43:06,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0194: [2024-08-11 07:43:06,834] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0194: [2024-08-11 07:43:06,834] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0194: [2024-08-11 07:43:06,834] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0185: [2024-08-11 07:43:06,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0185: [2024-08-11 07:43:06,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0185: [2024-08-11 07:43:06,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0195: [2024-08-11 07:43:06,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0195: [2024-08-11 07:43:06,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0195: [2024-08-11 07:43:06,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0187: [2024-08-11 07:43:06,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0187: [2024-08-11 07:43:06,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0187: [2024-08-11 07:43:06,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0197: [2024-08-11 07:43:06,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0197: [2024-08-11 07:43:06,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0197: [2024-08-11 07:43:06,837] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0188: [2024-08-11 07:43:06,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0188: [2024-08-11 07:43:06,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0188: [2024-08-11 07:43:06,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0198: [2024-08-11 07:43:06,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0187: [2024-08-11 07:43:06,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0197: [2024-08-11 07:43:06,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0194: [2024-08-11 07:43:06,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0185: [2024-08-11 07:43:06,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0195: [2024-08-11 07:43:06,874] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0188: [2024-08-11 07:43:06,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0184: [2024-08-11 07:43:06,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0187: [2024-08-11 07:43:06,989] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0198: [2024-08-11 07:43:06,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 07:43:06,991] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 07:43:06,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0185: [2024-08-11 07:43:07,007] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0197: [2024-08-11 07:43:07,017] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0194: [2024-08-11 07:43:07,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0187: [2024-08-11 07:43:07,024] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0195: [2024-08-11 07:43:07,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0188: [2024-08-11 07:43:07,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0198: [2024-08-11 07:43:07,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0185: [2024-08-11 07:43:07,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0197: [2024-08-11 07:43:07,051] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0194: [2024-08-11 07:43:07,061] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0195: [2024-08-11 07:43:07,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0188: [2024-08-11 07:43:07,073] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0184: [2024-08-11 07:43:07,089] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 07:43:07,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0185: [2024-08-11 07:43:07,150] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0187: [2024-08-11 07:43:07,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0197: [2024-08-11 07:43:07,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0188: [2024-08-11 07:43:07,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0185: [2024-08-11 07:43:07,180] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0187: [2024-08-11 07:43:07,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0194: [2024-08-11 07:43:07,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0197: [2024-08-11 07:43:07,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0188: [2024-08-11 07:43:07,205] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0194: [2024-08-11 07:43:07,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0198: [2024-08-11 07:43:07,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 07:43:07,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0188: [2024-08-11 07:43:07,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 07:43:07,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0185: [2024-08-11 07:43:07,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 07:43:07,300] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0187: [2024-08-11 07:43:07,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 07:43:07,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0184: [2024-08-11 07:43:07,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0195: [2024-08-11 07:43:07,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0194: [2024-08-11 07:43:07,327] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 07:43:07,329] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0184: [2024-08-11 07:43:07,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0197: [2024-08-11 07:43:07,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 07:43:07,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0195: [2024-08-11 07:43:07,352] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0195: [2024-08-11 07:43:07,473] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 07:43:07,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 07:43:07,562] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 07:43:07,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 07:43:07,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 07:43:07,691] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt
g0184: [2024-08-11 07:43:07,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 07:43:09,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 07:43:09,163] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0197: [2024-08-11 07:43:09,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 07:43:09,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0187: [2024-08-11 07:43:09,637] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 07:43:09,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0185: [2024-08-11 07:43:09,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 07:43:09,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0195: [2024-08-11 07:43:09,845] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 07:43:09,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0194: [2024-08-11 07:43:09,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 07:43:09,881] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0188: [2024-08-11 07:43:10,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 07:43:10,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0184: [2024-08-11 07:43:11,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 07:43:11,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38000 is ready now!
g0184:   successfully saved checkpoint at iteration   38000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.96, Latency(second): 4.537
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4536.55, 4536.75)
g0184: [2024-08-11 07:43:53,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=38010, skipped=58, lr=[0.00019974237941900003, 0.00019974237941900003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38010 loss: 0.7453 iter time (s): 4.230 samples/sec: 30.258
g0198:  iteration    38010/10000000 | consumed samples:      4865280 | consumed tokens:   9964093440 | elapsed time per iteration (ms): 45817.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.592067E-01 | loss scale: 4096.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.794 | tokens per gpu per second (tgs): 178.798 | TFLOPs: 1.44 |
g0184: [2024-08-11 07:44:36,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=38020, skipped=58, lr=[0.0001997421855147265, 0.0001997421855147265], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38020 loss: 0.7869 iter time (s): 4.237 samples/sec: 30.209
g0198:  iteration    38020/10000000 | consumed samples:      4866560 | consumed tokens:   9966714880 | elapsed time per iteration (ms): 4269.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.603627E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.978 | tokens per gpu per second (tgs): 1918.574 | TFLOPs: 15.44 |
g0184: [2024-08-11 07:45:19,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=38030, skipped=58, lr=[0.00019974199153760625, 0.00019974199153760625], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38030 loss: 0.7586 iter time (s): 4.262 samples/sec: 30.031
g0198:  iteration    38030/10000000 | consumed samples:      4867840 | consumed tokens:   9969336320 | elapsed time per iteration (ms): 4294.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.550112E-01 | loss scale: 4096.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.806 | tokens per gpu per second (tgs): 1907.608 | TFLOPs: 15.35 |
g0184: [2024-08-11 07:46:01,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=38040, skipped=58, lr=[0.00019974179748763943, 0.00019974179748763943], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38040 loss: 0.7300 iter time (s): 4.133 samples/sec: 30.971
g0198:  iteration    38040/10000000 | consumed samples:      4869120 | consumed tokens:   9971957760 | elapsed time per iteration (ms): 4165.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.448355E-01 | loss scale: 4096.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.730 | tokens per gpu per second (tgs): 1966.729 | TFLOPs: 15.83 |
g0184: [2024-08-11 07:46:44,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=38050, skipped=58, lr=[0.00019974160336482612, 0.00019974160336482612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38050 loss: 0.7809 iter time (s): 4.331 samples/sec: 29.558
g0198:  iteration    38050/10000000 | consumed samples:      4870400 | consumed tokens:   9974579200 | elapsed time per iteration (ms): 4363.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.511564E-01 | loss scale: 4096.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.337 | tokens per gpu per second (tgs): 1877.566 | TFLOPs: 15.11 |
g0184: [2024-08-11 07:47:28,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=38060, skipped=58, lr=[0.00019974140916916656, 0.00019974140916916656], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38060 loss: 0.7486 iter time (s): 4.328 samples/sec: 29.573
g0198:  iteration    38060/10000000 | consumed samples:      4871680 | consumed tokens:   9977200640 | elapsed time per iteration (ms): 4360.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.604636E-01 | loss scale: 4096.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.355 | tokens per gpu per second (tgs): 1878.716 | TFLOPs: 15.12 |
g0184: [2024-08-11 07:48:10,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=38070, skipped=58, lr=[0.00019974121490066088, 0.00019974121490066088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38070 loss: 0.7279 iter time (s): 4.123 samples/sec: 31.045
g0198:  iteration    38070/10000000 | consumed samples:      4872960 | consumed tokens:   9979822080 | elapsed time per iteration (ms): 4155.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.593251E-01 | loss scale: 4096.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.803 | tokens per gpu per second (tgs): 1971.388 | TFLOPs: 15.86 |
g0184: [2024-08-11 07:48:52,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=38080, skipped=58, lr=[0.00019974102055930916, 0.00019974102055930916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38080 loss: 0.7425 iter time (s): 4.259 samples/sec: 30.057
g0198:  iteration    38080/10000000 | consumed samples:      4874240 | consumed tokens:   9982443520 | elapsed time per iteration (ms): 4290.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.616147E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.832 | tokens per gpu per second (tgs): 1909.243 | TFLOPs: 15.36 |
g0184: [2024-08-11 07:49:35,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=38090, skipped=58, lr=[0.00019974082614511167, 0.00019974082614511167], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38090 loss: 0.7692 iter time (s): 4.261 samples/sec: 30.041
g0198:  iteration    38090/10000000 | consumed samples:      4875520 | consumed tokens:   9985064960 | elapsed time per iteration (ms): 4291.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.408710E-01 | loss scale: 4096.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.825 | tokens per gpu per second (tgs): 1908.795 | TFLOPs: 15.36 |
g0184: [2024-08-11 07:50:18,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=38100, skipped=58, lr=[0.00019974063165806844, 0.00019974063165806844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38100 loss: 0.7374 iter time (s): 4.247 samples/sec: 30.141
g0198:  iteration    38100/10000000 | consumed samples:      4876800 | consumed tokens:   9987686400 | elapsed time per iteration (ms): 4280.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.567073E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.906 | tokens per gpu per second (tgs): 1913.971 | TFLOPs: 15.40 |
g0184: [2024-08-11 07:51:02,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=38110, skipped=58, lr=[0.00019974043709817965, 0.00019974043709817965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38110 loss: 0.7349 iter time (s): 4.375 samples/sec: 29.256
g0198:  iteration    38110/10000000 | consumed samples:      4878080 | consumed tokens:   9990307840 | elapsed time per iteration (ms): 4407.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.484991E-01 | loss scale: 4096.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.043 | tokens per gpu per second (tgs): 1858.727 | TFLOPs: 14.96 |
g0184: [2024-08-11 07:51:47,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=38120, skipped=58, lr=[0.00019974024246544552, 0.00019974024246544552], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38120 loss: 0.7546 iter time (s): 4.486 samples/sec: 28.531
g0198:  iteration    38120/10000000 | consumed samples:      4879360 | consumed tokens:   9992929280 | elapsed time per iteration (ms): 4519.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.597222E-01 | loss scale: 4096.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.324 | tokens per gpu per second (tgs): 1812.728 | TFLOPs: 14.59 |
g0184: [2024-08-11 07:52:30,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=38130, skipped=58, lr=[0.00019974004775986614, 0.00019974004775986614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38130 loss: 0.7901 iter time (s): 4.247 samples/sec: 30.137
g0198:  iteration    38130/10000000 | consumed samples:      4880640 | consumed tokens:   9995550720 | elapsed time per iteration (ms): 4280.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.512328E-01 | loss scale: 4096.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.905 | tokens per gpu per second (tgs): 1913.901 | TFLOPs: 15.40 |
g0184: [2024-08-11 07:53:13,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=38140, skipped=58, lr=[0.00019973985298144166, 0.00019973985298144166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38140 loss: 0.7445 iter time (s): 4.241 samples/sec: 30.183
g0198:  iteration    38140/10000000 | consumed samples:      4881920 | consumed tokens:   9998172160 | elapsed time per iteration (ms): 4275.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.468012E-01 | loss scale: 4096.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.936 | tokens per gpu per second (tgs): 1915.875 | TFLOPs: 15.42 |
g0184: [2024-08-11 07:53:55,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=38150, skipped=58, lr=[0.00019973965813017223, 0.00019973965813017223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38150 loss: 0.7513 iter time (s): 4.167 samples/sec: 30.719
g0198:  iteration    38150/10000000 | consumed samples:      4883200 | consumed tokens:  10000793600 | elapsed time per iteration (ms): 4201.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.517238E-01 | loss scale: 4096.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.466 | tokens per gpu per second (tgs): 1949.833 | TFLOPs: 15.69 |
g0184: [2024-08-11 07:54:36,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=38160, skipped=58, lr=[0.00019973946320605803, 0.00019973946320605803], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38160 loss: 0.7175 iter time (s): 4.111 samples/sec: 31.137
g0198:  iteration    38160/10000000 | consumed samples:      4884480 | consumed tokens:  10003415040 | elapsed time per iteration (ms): 4142.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.449821E-01 | loss scale: 4096.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.896 | tokens per gpu per second (tgs): 1977.361 | TFLOPs: 15.91 |
g0184: [2024-08-11 07:55:19,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=38170, skipped=58, lr=[0.00019973926820909916, 0.00019973926820909916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38170 loss: 0.7366 iter time (s): 4.199 samples/sec: 30.482
g0198:  iteration    38170/10000000 | consumed samples:      4885760 | consumed tokens:  10006036480 | elapsed time per iteration (ms): 4233.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.384816E-01 | loss scale: 4096.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.234 | tokens per gpu per second (tgs): 1934.996 | TFLOPs: 15.57 |
g0184: [2024-08-11 07:56:01,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=38180, skipped=58, lr=[0.00019973907313929586, 0.00019973907313929586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38180 loss: 0.7476 iter time (s): 4.155 samples/sec: 30.810
g0198:  iteration    38180/10000000 | consumed samples:      4887040 | consumed tokens:  10008657920 | elapsed time per iteration (ms): 4187.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.553305E-01 | loss scale: 4096.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.568 | tokens per gpu per second (tgs): 1956.339 | TFLOPs: 15.74 |
g0184: [2024-08-11 07:56:43,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=38190, skipped=58, lr=[0.00019973887799664813, 0.00019973887799664813], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38190 loss: 0.7441 iter time (s): 4.250 samples/sec: 30.120
g0198:  iteration    38190/10000000 | consumed samples:      4888320 | consumed tokens:  10011279360 | elapsed time per iteration (ms): 4284.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.548617E-01 | loss scale: 4096.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.877 | tokens per gpu per second (tgs): 1912.119 | TFLOPs: 15.39 |
g0184: [2024-08-11 07:57:25,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=38200, skipped=58, lr=[0.00019973868278115627, 0.00019973868278115627], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38200 loss: 0.7448 iter time (s): 4.152 samples/sec: 30.831
g0198:  iteration    38200/10000000 | consumed samples:      4889600 | consumed tokens:  10013900800 | elapsed time per iteration (ms): 4184.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.551303E-01 | loss scale: 4096.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.591 | tokens per gpu per second (tgs): 1957.841 | TFLOPs: 15.76 |
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 07:57:43,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 07:57:43,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 07:57:43,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 07:57:43,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 07:57:43,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 07:57:43,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 07:58:09,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=38210, skipped=58, lr=[0.00019973848749282034, 0.00019973848749282034], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38210 loss: 0.7531 iter time (s): 4.304 samples/sec: 29.739
g0198:  iteration    38210/10000000 | consumed samples:      4890880 | consumed tokens:  10016522240 | elapsed time per iteration (ms): 4336.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.500674E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.518 | tokens per gpu per second (tgs): 1889.171 | TFLOPs: 15.20 |
g0184: [2024-08-11 07:58:50,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=38220, skipped=58, lr=[0.00019973829213164055, 0.00019973829213164055], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38220 loss: 0.7585 iter time (s): 4.115 samples/sec: 31.107
g0198:  iteration    38220/10000000 | consumed samples:      4892160 | consumed tokens:  10019143680 | elapsed time per iteration (ms): 4147.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.553021E-01 | loss scale: 8192.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.866 | tokens per gpu per second (tgs): 1975.393 | TFLOPs: 15.90 |
g0184: [2024-08-11 07:59:31,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=38230, skipped=58, lr=[0.00019973809669761699, 0.00019973809669761699], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38230 loss: 0.7529 iter time (s): 4.100 samples/sec: 31.217
g0198:  iteration    38230/10000000 | consumed samples:      4893440 | consumed tokens:  10021765120 | elapsed time per iteration (ms): 4132.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.547963E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.974 | tokens per gpu per second (tgs): 1982.341 | TFLOPs: 15.95 |
g0184: [2024-08-11 08:00:14,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=38240, skipped=58, lr=[0.00019973790119074983, 0.00019973790119074983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38240 loss: 0.7486 iter time (s): 4.214 samples/sec: 30.377
g0198:  iteration    38240/10000000 | consumed samples:      4894720 | consumed tokens:  10024386560 | elapsed time per iteration (ms): 4245.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.501328E-01 | loss scale: 8192.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.147 | tokens per gpu per second (tgs): 1929.400 | TFLOPs: 15.53 |
g0184: [2024-08-11 08:00:56,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=38250, skipped=58, lr=[0.00019973770561103926, 0.00019973770561103926], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38250 loss: 0.7660 iter time (s): 4.211 samples/sec: 30.397
g0198:  iteration    38250/10000000 | consumed samples:      4896000 | consumed tokens:  10027008000 | elapsed time per iteration (ms): 4243.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.559855E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.166 | tokens per gpu per second (tgs): 1930.605 | TFLOPs: 15.54 |
g0184: [2024-08-11 08:01:38,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=38260, skipped=58, lr=[0.00019973750995848537, 0.00019973750995848537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38260 loss: 0.7613 iter time (s): 4.155 samples/sec: 30.806
g0198:  iteration    38260/10000000 | consumed samples:      4897280 | consumed tokens:  10029629440 | elapsed time per iteration (ms): 4187.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.559011E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.566 | tokens per gpu per second (tgs): 1956.233 | TFLOPs: 15.74 |
