
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0184
    HostName g0184
    Port 2222
    StrictHostKeyChecking no

Host g0185
    HostName g0185
    Port 2222
    StrictHostKeyChecking no

Host g0187
    HostName g0187
    Port 2222
    StrictHostKeyChecking no

Host g0188
    HostName g0188
    Port 2222
    StrictHostKeyChecking no

Host g0194
    HostName g0194
    Port 2222
    StrictHostKeyChecking no

Host g0195
    HostName g0195
    Port 2222
    StrictHostKeyChecking no

Host g0197
    HostName g0197
    Port 2222
    StrictHostKeyChecking no

Host g0198
    HostName g0198
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42826720
g0184 slots=4
g0185 slots=4
g0187 slots=4
g0188 slots=4
g0194 slots=4
g0195 slots=4
g0197 slots=4
g0198 slots=4

[2024-08-11 08:17:10,947] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-11 08:17:17,007] [INFO] [runner.py:463:main] Using IP address of 10.1.6.14 for node g0184
[2024-08-11 08:17:17,009] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0184,g0185,g0187,g0188,g0194,g0195,g0197,g0198
[2024-08-11 08:17:17,009] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0184,g0185,g0187,g0188,g0194,g0195,g0197,g0198 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDE4NCI6IFswLCAxLCAyLCAzXSwgImcwMTg1IjogWzAsIDEsIDIsIDNdLCAiZzAxODciOiBbMCwgMSwgMiwgM10sICJnMDE4OCI6IFswLCAxLCAyLCAzXSwgImcwMTk0IjogWzAsIDEsIDIsIDNdLCAiZzAxOTUiOiBbMCwgMSwgMiwgM10sICJnMDE5NyI6IFswLCAxLCAyLCAzXSwgImcwMTk4IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.6.14 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True' --wandb_tag 'other_gpu'
g0184: [2024-08-11 08:17:20,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-11 08:17:22,688] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0184: [2024-08-11 08:17:22,688] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0184: [2024-08-11 08:17:22,688] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0184: [2024-08-11 08:17:22,689] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0184: [2024-08-11 08:17:22,689] [INFO] [launch.py:163:main] dist_world_size=32
g0184: [2024-08-11 08:17:22,689] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0184: [2024-08-11 08:17:25,836] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-11 08:17:25,837] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-11 08:17:25,872] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: [2024-08-11 08:17:25,872] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-11 08:17:28,096] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-11 08:17:28,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-11 08:17:28,318] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-11 08:17:28,509] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-11 08:17:28,534] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-11 08:17:28,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-11 08:17:28,712] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0184: --------------------------------------------------
g0184: DeepSpeed C++/CUDA extension op report
g0184: --------------------------------------------------
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: --------------------------------------------------
g0184: JIT compiled ops requires ninja
g0184: --------------------------------------------------
g0184: DeepSpeed C++/CUDA extension op report
g0184: --------------------------------------------------
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: --------------------------------------------------
g0184: JIT compiled ops requires ninja
g0184: --------------------------------------------------
g0184: DeepSpeed C++/CUDA extension op report
g0184: --------------------------------------------------
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: --------------------------------------------------
g0184: JIT compiled ops requires ninja
g0184: --------------------------------------------------
g0184: DeepSpeed C++/CUDA extension op report
g0184: --------------------------------------------------
g0184: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0184:       runtime if needed. Op compatibility means that your system
g0184:       meet the required dependencies to JIT install the op.
g0184: --------------------------------------------------
g0184: JIT compiled ops requires ninja
g0184: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0184: 
g0184: ----------------------------------------------------------------------------------------------------
g0184: 
g0184: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0184: 
g0184: ----------------------------------------------------------------------------------------------------
g0184: 
g0184: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0184: 
g0184: ----------------------------------------------------------------------------------------------------
g0184: 
g0184: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0184: 
g0184: ----------------------------------------------------------------------------------------------------
g0184: 
g0184: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0184: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0184: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0184: cpu_adagrad ............async_io  [92m[YES][0m .....................  [92m[YES][0m[92m[OKAY][0m 
g0184: ...... [92m[OKAY][0mcpu_lion
g0184:  ............... [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0184:  ............. [92m[YES][0m ...... [92m[OKAY][0masync_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0184:  
g0184: ...............cpu_adamevoformer_attn   [92m[YES][0m........................   ......[92m[YES][0m [93m[NO][0m[92m[OKAY][0m  .......
g0184: ......  [93m[NO][0m[92m[OKAY][0m
g0184: 
g0184: fused_adamfused_lambcpu_adagrad   ......................................   [92m[YES][0m[92m[YES][0m[92m[YES][0m   ..................   [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0184: 
g0184: 
g0184: cpu_adamcpu_lion  ..............................fused_lion   .............[92m[YES][0m[92m[YES][0m   [92m[YES][0m............   ......[92m[OKAY][0m[92m[OKAY][0m 
g0184: 
g0184: [92m[OKAY][0m
g0184: cpu_adagrad ............ [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0184:  [92m[OKAY][0m
g0184: evoformer_attn ......... cpu_lion[93m[NO][0m  ......................  [92m[YES][0m[93m[NO][0m 
g0184: ...... [92m[OKAY][0m
g0184: fused_lamb ............. [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0184: 
g0184: evoformer_attn ......... [93m[NO][0m ....... fused_lion[93m[NO][0m 
g0184: ............. [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0184: [92m[YES][0m ...... [92m[OKAY][0m
g0184: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0184: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0184: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0184: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: ragged_device_opsragged_device_ops  ............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0184: 
g0184: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0184: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... ragged_ops[93m[NO][0m 
g0184: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0184: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0184: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0184: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0184: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0184: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0184: --------------------------------------------------
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: DeepSpeed general environment info:
g0184: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0184: torch version .................... 2.0.1+cu118
g0184: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0184: deepspeed info ................... 0.12.4, unknown, unknown
g0184: torch cuda version ............... 11.8
g0184: torch hip version ................ None
g0184: nvcc version ..................... 11.8
g0184: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0184: shared memory (/dev/shm) size .... 188.13 GB
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0184: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0184: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0184: using torch.float32 for parameters ...
g0184: ------------------------ arguments ------------------------
g0184:   accumulate_allreduce_grads_in_fp32 .............. False
g0184:   adam_beta1 ...................................... 0.9
g0184:   adam_beta2 ...................................... 0.95
g0184:   adam_eps ........................................ 1e-08
g0184:   add_bias_linear ................................. False
g0184:   add_position_embedding .......................... False
g0184:   adlr_autoresume ................................. False
g0184:   adlr_autoresume_interval ........................ 1000
g0184:   aml_data_download_path .......................... None
g0184:   apply_layernorm_1p .............................. False
g0184:   apply_query_key_layer_scaling ................... False
g0184:   apply_residual_connection_post_layernorm ........ False
g0184:   async_tensor_model_parallel_allreduce ........... False
g0184:   attention_dropout ............................... 0.0
g0184:   attention_softmax_in_fp32 ....................... False
g0184:   barrier_with_L1_time ............................ True
g0184:   bert_binary_head ................................ True
g0184:   bert_embedder_type .............................. megatron
g0184:   bert_load ....................................... None
g0184:   bf16 ............................................ False
g0184:   bias_dropout_fusion ............................. True
g0184:   bias_gelu_fusion ................................ False
g0184:   biencoder_projection_dim ........................ 0
g0184:   biencoder_shared_query_context_model ............ False
g0184:   block_data_path ................................. None
g0184:   checkpoint_activations .......................... False
g0184:   checkpoint_in_cpu ............................... False
g0184:   checkpoint_num_layers ........................... 1
g0184:   classes_fraction ................................ 1.0
g0184:   clip_grad ....................................... 1.0
g0184:   compression_training ............................ False
g0184:   consumed_train_samples .......................... 0
g0184:   consumed_train_tokens ........................... 0
g0184:   consumed_valid_samples .......................... 0
g0184:   contigious_checkpointing ........................ False
g0184:   cpu_optimizer ................................... False
g0184:   cpu_torch_adam .................................. False
g0184:   create_moe_param_group .......................... False
g0184:   curriculum_learning_legacy ...................... False
g0184:   data_cache_path ................................. None
g0184:   data_efficiency_curriculum_learning ............. False
g0184:   data_impl ....................................... mmap
g0184:   data_parallel_random_init ....................... False
g0184:   data_parallel_size .............................. 4
g0184:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document']
g0184:   data_per_class_fraction ......................... 1.0
g0184:   data_sharding ................................... True
g0184:   dataloader_type ................................. single
g0184:   DDP_impl ........................................ local
g0184:   decoder_num_layers .............................. None
g0184:   decoder_seq_length .............................. None
g0184:   deepscale ....................................... False
g0184:   deepscale_config ................................ None
g0184:   deepspeed ....................................... True
g0184:   deepspeed_activation_checkpointing .............. False
g0184:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0184:   deepspeed_mpi ................................... False
g0184:   dino_bottleneck_size ............................ 256
g0184:   dino_freeze_last_layer .......................... 1
g0184:   dino_head_hidden_size ........................... 2048
g0184:   dino_local_crops_number ......................... 10
g0184:   dino_local_img_size ............................. 96
g0184:   dino_norm_last_layer ............................ False
g0184:   dino_teacher_temp ............................... 0.07
g0184:   dino_warmup_teacher_temp ........................ 0.04
g0184:   dino_warmup_teacher_temp_epochs ................. 30
g0184:   distribute_checkpointed_activations ............. False
g0184:   distribute_saved_activations .................... False
g0184:   distributed_backend ............................. nccl
g0184:   distributed_timeout_minutes ..................... 10
g0184:   ds_fused_adam ................................... False
g0184:   ds_inference .................................... False
g0184:   ds_pipeline_enabled ............................. True
g0184:   ds_sequence_parallel_size ....................... 1
g0184:   embedding_path .................................. None
g0184:   embedding_weights_in_fp32 ....................... False
g0184:   empty_unused_memory_level ....................... 0
g0184:   enable_expert_tensor_parallelism ................ False
g0184:   encoder_num_layers .............................. 22
g0184:   encoder_seq_length .............................. 2048
g0184:   end_weight_decay ................................ 0.1
g0184:   eod_mask_loss ................................... False
g0184:   eval_interval ................................... 1000
g0184:   eval_iters ...................................... 100
g0184:   evidence_data_path .............................. None
g0184:   exit_duration_in_mins ........................... 30000000
g0184:   exit_interval ................................... None
g0184:   exit_on_missing_checkpoint ...................... False
g0184:   exit_signal_handler ............................. False
g0184:   expert_interval ................................. 2
g0184:   ffn_hidden_size ................................. 5632
g0184:   finetune ........................................ False
g0184:   force_ds_sequence_parallel ...................... False
g0184:   fp16 ............................................ False
g0184:   fp16_lm_cross_entropy ........................... False
g0184:   fp32_residual_connection ........................ False
g0184:   fp8_amax_compute_algo ........................... most_recent
g0184:   fp8_amax_history_len ............................ 1
g0184:   fp8_e4m3 ........................................ False
g0184:   fp8_hybrid ...................................... False
g0184:   fp8_interval .................................... 1
g0184:   fp8_margin ...................................... 0
g0184:   fp8_wgrad ....................................... True
g0184:   global_batch_size ............................... 128
g0184:   gradient_accumulation_fusion .................... True
g0184:   head_lr_mult .................................... 1.0
g0184:   hidden_dropout .................................. 0.0
g0184:   hidden_size ..................................... 2048
g0184:   hidden_size_teacher ............................. None
g0184:   hysteresis ...................................... 2
g0184:   ict_head_size ................................... None
g0184:   ict_load ........................................ None
g0184:   img_h ........................................... 224
g0184:   img_w ........................................... 224
g0184:   indexer_batch_size .............................. 128
g0184:   indexer_log_interval ............................ 1000
g0184:   inference ....................................... False
g0184:   inference_batch_times_seqlen_threshold .......... 512
g0184:   init_method_std ................................. 0.013
g0184:   init_method_xavier_uniform ...................... False
g0184:   initial_loss_scale .............................. 4294967296
g0184:   iter_per_epoch .................................. 1250
g0184:   kd .............................................. False
g0184:   kd_alpha_ce ..................................... 1
g0184:   kd_beta_ce ...................................... 1
g0184:   kd_temp ......................................... 1.0
g0184:   kv_channels ..................................... 128
g0184:   layernorm_epsilon ............................... 1e-05
g0184:   lazy_mpu_init ................................... None
g0184:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184:   load_teacher .................................... None
g0184:   local_rank ...................................... 0
g0184:   log_batch_size_to_tensorboard ................... True
g0184:   log_interval .................................... 10
g0184:   log_learning_rate_to_tensorboard ................ True
g0184:   log_loss_scale_to_tensorboard ................... True
g0184:   log_memory_to_tensorboard ....................... False
g0184:   log_num_zeros_in_grad ........................... False
g0184:   log_optimizer_states_to_tensorboard ............. True
g0184:   log_params_norm ................................. False
g0184:   log_timers_to_tensorboard ....................... True
g0184:   log_validation_ppl_to_tensorboard ............... True
g0184:   log_world_size_to_tensorboard ................... False
g0184:   loss_scale ...................................... None
g0184:   loss_scale_window ............................... 1000
g0184:   lr .............................................. 0.0002
g0184:   lr_decay_iters .................................. None
g0184:   lr_decay_samples ................................ None
g0184:   lr_decay_style .................................. cosine
g0184:   lr_decay_tokens ................................. 300000000000
g0184:   lr_warmup_fraction .............................. None
g0184:   lr_warmup_iters ................................. 0
g0184:   lr_warmup_samples ............................... 0
g0184:   lr_warmup_tokens ................................ 3000000000
g0184:   make_vocab_size_divisible_by .................... 128
g0184:   mask_factor ..................................... 1.0
g0184:   mask_prob ....................................... 0.15
g0184:   mask_type ....................................... random
g0184:   masked_softmax_fusion ........................... True
g0184:   max_position_embeddings ......................... 2048
g0184:   max_tokens_to_oom ............................... 12000
g0184:   mem_efficient_ln ................................ True
g0184:   memory_centric_tiled_linear ..................... False
g0184:   merge_file ...................................... None
g0184:   micro_batch_size ................................ 1
g0184:   min_loss_scale .................................. 1.0
g0184:   min_lr .......................................... 1e-05
g0184:   mlp_type ........................................ standard
g0184:   mmap_warmup ..................................... False
g0184:   moe_eval_capacity_factor ........................ 1.0
g0184:   moe_expert_parallel_size ........................ 1
g0184:   moe_loss_coeff .................................. 0.1
g0184:   moe_min_capacity ................................ 4
g0184:   moe_token_dropping .............................. True
g0184:   moe_train_capacity_factor ....................... 1.0
g0184:   mos ............................................. False
g0184:   no_load_lr_state ................................ False
g0184:   no_load_optim ................................... None
g0184:   no_load_rng ..................................... None
g0184:   no_persist_layer_norm ........................... False
g0184:   no_pipeline_parallel ............................ False
g0184:   no_save_optim ................................... None
g0184:   no_save_rng ..................................... None
g0184:   normalization ................................... rmsnorm
g0184:   num_attention_heads ............................. 16
g0184:   num_attention_heads_teacher ..................... None
g0184:   num_channels .................................... 3
g0184:   num_classes ..................................... 1000
g0184:   num_experts ..................................... [1]
g0184:   num_experts_switch .............................. None
g0184:   num_experts_teacher ............................. [1]
g0184:   num_key_value_heads ............................. 4
g0184:   num_layers ...................................... 22
g0184:   num_layers_per_virtual_pipeline_stage ........... None
g0184:   num_layers_teacher .............................. None
g0184:   num_workers ..................................... 0
g0184:   onnx_safe ....................................... None
g0184:   openai_gelu ..................................... False
g0184:   optimizer ....................................... adam
g0184:   output_bert_embeddings .......................... False
g0184:   overlap_p2p_comm ................................ False
g0184:   override_opt_param_scheduler .................... True
g0184:   params_dtype .................................... torch.float32
g0184:   partition_activations ........................... False
g0184:   patch_dim ....................................... 16
g0184:   perform_initialization .......................... True
g0184:   pipeline_model_parallel_size .................... 8
g0184:   pipeline_model_parallel_split_rank .............. None
g0184:   profile_backward ................................ False
g0184:   query_in_block_prob ............................. 0.1
g0184:   rampup_batch_size ............................... None
g0184:   random_ltd ...................................... False
g0184:   rank ............................................ 0
g0184:   recompute_granularity ........................... None
g0184:   recompute_method ................................ None
g0184:   recompute_num_layers ............................ 1
g0184:   remote_device ................................... none
g0184:   repeated_dataloader ............................. False
g0184:   reset_attention_mask ............................ False
g0184:   reset_iteration ................................. False
g0184:   reset_position_ids .............................. False
g0184:   retriever_report_topk_accuracies ................ []
g0184:   retriever_score_scaling ......................... False
g0184:   retriever_seq_length ............................ 256
g0184:   retro_add_retriever ............................. False
g0184:   retro_cyclic_train_iters ........................ None
g0184:   retro_encoder_attention_dropout ................. 0.1
g0184:   retro_encoder_hidden_dropout .................... 0.1
g0184:   retro_encoder_layers ............................ 2
g0184:   retro_num_neighbors ............................. 2
g0184:   retro_num_retrieved_chunks ...................... 2
g0184:   retro_return_doc_ids ............................ False
g0184:   retro_workdir ................................... None
g0184:   return_data_index ............................... False
g0184:   rotary_percent .................................. 1.0
g0184:   sample_rate ..................................... 1.0
g0184:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184:   save_interval ................................... 1000
g0184:   scatter_gather_tensors_in_pipeline .............. True
g0184:   scattered_embeddings ............................ False
g0184:   seed ............................................ 1234
g0184:   seq_length ...................................... 2048
g0184:   sequence_parallel ............................... False
g0184:   sgd_momentum .................................... 0.9
g0184:   short_seq_prob .................................. 0.1
g0184:   skip_train ...................................... False
g0184:   split ........................................... 949,50,1
g0184:   split_transformers .............................. False
g0184:   squared_relu .................................... False
g0184:   standalone_embedding_stage ...................... False
g0184:   start_weight_decay .............................. 0.1
g0184:   swiglu .......................................... True
g0184:   swin_backbone_type .............................. tiny
g0184:   synchronize_each_layer .......................... False
g0184:   tensor_model_parallel_size ...................... 1
g0184:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True
g0184:   tensorboard_log_interval ........................ 1
g0184:   tensorboard_queue_size .......................... 1
g0184:   test_data_path .................................. None
g0184:   tf32 ............................................ False
g0184:   tile_factor ..................................... 1
g0184:   timing_log_level ................................ 0
g0184:   timing_log_option ............................... minmax
g0184:   titles_data_path ................................ None
g0184:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
g0184:   tokenizer_type .................................. SentencePieceTokenizer
g0184:   topk ............................................ 1
g0184:   train_data_exact_num_epochs ..................... 1
g0184:   train_data_path ................................. None
g0184:   train_desc_path ................................. None
g0184:   train_doc_idx_path .............................. None
g0184:   train_idx_path .................................. None
g0184:   train_iters ..................................... None
g0184:   train_sample_idx_path ........................... None
g0184:   train_samples ................................... 1280000000
g0184:   train_shuffle_idx_path .......................... None
g0184:   train_tokens .................................... 2621440000000
g0184:   transformer_impl ................................ local
g0184:   transformer_pipeline_model_parallel_size ........ 8
g0184:   universal_checkpoint ............................ False
g0184:   untie_embeddings_and_output_weights ............. True
g0184:   use_checkpoint_args ............................. False
g0184:   use_checkpoint_opt_param_scheduler .............. False
g0184:   use_contiguous_buffers_in_local_ddp ............. True
g0184:   use_cpu_initialization .......................... None
g0184:   use_dataset_only ................................ False
g0184:   use_distributed_optimizer ....................... False
g0184:   use_flash_attn .................................. False
g0184:   use_flash_attn_triton ........................... False
g0184:   use_flash_attn_v1 ............................... False
g0184:   use_flash_attn_v2 ............................... False
g0184:   use_one_sent_docs ............................... False
g0184:   use_pin_memory .................................. False
g0184:   use_ring_exchange_p2p ........................... False
g0184:   use_rotary_position_embeddings .................. True
g0184:   use_tutel ....................................... False
g0184:   use_wandb ....................................... True
g0184:   valid_data_path ................................. None
g0184:   variable_seq_lengths ............................ False
g0184:   virtual_pipeline_model_parallel_size ............ None
g0184:   vision_backbone_type ............................ vit
g0184:   vision_pretraining .............................. False
g0184:   vision_pretraining_type ......................... classify
g0184:   vocab_extra_ids ................................. 0
g0184:   vocab_file ...................................... None
g0184:   vocab_size ...................................... None
g0184:   wandb_entity .................................... yohei-kobashi
g0184:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True
g0184:   wandb_project ................................... encrypted_data_LLM
g0184:   wandb_tag ....................................... other_gpu
g0184:   weight_decay .................................... 0.1
g0184:   weight_decay_incr_style ......................... constant
g0184:   world_size ...................................... 32
g0184:   zero_allgather_bucket_size ...................... 0.0
g0184:   zero_contigious_gradients ....................... False
g0184:   zero_reduce_bucket_size ......................... 0.0
g0184:   zero_reduce_scatter ............................. False
g0184:   zero_stage ...................................... 0
g0184: -------------------- end of arguments ---------------------
g0184: setting number of micro-batches to constant 32
g0184: > building SentencePieceTokenizer tokenizer ...
g0184: [2024-08-11 08:17:30,726] [INFO] [comm.py:637:init_distributed] cdb=None
g0184:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0184: > initializing torch distributed ...
g0184: [2024-08-11 08:17:30,727] [INFO] [comm.py:637:init_distributed] cdb=None
g0184: [2024-08-11 08:17:30,727] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0184: [2024-08-11 08:17:30,729] [INFO] [comm.py:637:init_distributed] cdb=None
g0184: [2024-08-11 08:17:30,730] [INFO] [comm.py:637:init_distributed] cdb=None
g0184: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [2024-08-11 08:17:32,155] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0188: [2024-08-11 08:17:32,156] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0188: [2024-08-11 08:17:32,156] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0188: [2024-08-11 08:17:32,156] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0188: [2024-08-11 08:17:32,156] [INFO] [launch.py:163:main] dist_world_size=32
g0188: [2024-08-11 08:17:32,156] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0187: [2024-08-11 08:17:32,386] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0187: [2024-08-11 08:17:32,386] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0187: [2024-08-11 08:17:32,386] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0187: [2024-08-11 08:17:32,386] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0187: [2024-08-11 08:17:32,386] [INFO] [launch.py:163:main] dist_world_size=32
g0187: [2024-08-11 08:17:32,386] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0185: [2024-08-11 08:17:32,396] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0185: [2024-08-11 08:17:32,396] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0185: [2024-08-11 08:17:32,396] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0185: [2024-08-11 08:17:32,396] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0185: [2024-08-11 08:17:32,396] [INFO] [launch.py:163:main] dist_world_size=32
g0185: [2024-08-11 08:17:32,396] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0195: [2024-08-11 08:17:32,481] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0195: [2024-08-11 08:17:32,481] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0195: [2024-08-11 08:17:32,482] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0195: [2024-08-11 08:17:32,482] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0195: [2024-08-11 08:17:32,482] [INFO] [launch.py:163:main] dist_world_size=32
g0195: [2024-08-11 08:17:32,482] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0194: [2024-08-11 08:17:32,656] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0194: [2024-08-11 08:17:32,656] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0194: [2024-08-11 08:17:32,656] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0194: [2024-08-11 08:17:32,656] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0194: [2024-08-11 08:17:32,656] [INFO] [launch.py:163:main] dist_world_size=32
g0194: [2024-08-11 08:17:32,656] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0197: [2024-08-11 08:17:32,793] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0197: [2024-08-11 08:17:32,793] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0197: [2024-08-11 08:17:32,793] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0197: [2024-08-11 08:17:32,793] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0197: [2024-08-11 08:17:32,794] [INFO] [launch.py:163:main] dist_world_size=32
g0197: [2024-08-11 08:17:32,794] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0198: [2024-08-11 08:17:32,802] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0198: [2024-08-11 08:17:32,802] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0184': [0, 1, 2, 3], 'g0185': [0, 1, 2, 3], 'g0187': [0, 1, 2, 3], 'g0188': [0, 1, 2, 3], 'g0194': [0, 1, 2, 3], 'g0195': [0, 1, 2, 3], 'g0197': [0, 1, 2, 3], 'g0198': [0, 1, 2, 3]}
g0198: [2024-08-11 08:17:32,802] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0198: [2024-08-11 08:17:32,802] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0184': [0, 1, 2, 3], 'g0185': [4, 5, 6, 7], 'g0187': [8, 9, 10, 11], 'g0188': [12, 13, 14, 15], 'g0194': [16, 17, 18, 19], 'g0195': [20, 21, 22, 23], 'g0197': [24, 25, 26, 27], 'g0198': [28, 29, 30, 31]})
g0198: [2024-08-11 08:17:32,802] [INFO] [launch.py:163:main] dist_world_size=32
g0198: [2024-08-11 08:17:32,802] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0188: [2024-08-11 08:17:35,265] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-11 08:17:35,265] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-11 08:17:35,289] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: [2024-08-11 08:17:35,289] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-11 08:17:35,492] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-11 08:17:35,493] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-11 08:17:35,493] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-11 08:17:35,501] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-11 08:17:35,502] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0185: [2024-08-11 08:17:35,549] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-11 08:17:35,561] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0187: [2024-08-11 08:17:35,562] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-11 08:17:35,625] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-11 08:17:35,691] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-11 08:17:35,692] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0195: [2024-08-11 08:17:35,692] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-11 08:17:35,747] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-11 08:17:35,748] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-11 08:17:35,749] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-11 08:17:35,885] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-11 08:17:35,885] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-11 08:17:35,904] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-11 08:17:35,904] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-11 08:17:35,904] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-11 08:17:35,906] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0194: [2024-08-11 08:17:35,958] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0198: [2024-08-11 08:17:35,998] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0197: [2024-08-11 08:17:36,036] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: --------------------------------------------------
g0188: DeepSpeed C++/CUDA extension op report
g0188: --------------------------------------------------
g0188: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0188:       runtime if needed. Op compatibility means that your system
g0188:       meet the required dependencies to JIT install the op.
g0188: --------------------------------------------------
g0188: JIT compiled ops requires ninja
g0188: ninjaninjaninja   ninja......................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m..................
g0188: 
g0188: 
g0188:  [92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0188: 
g0188: --------------------------------------------------
g0188: 
g0188: --------------------------------------------------op nameop name
g0188:  op name ................ op name ................................ installed  ................ installedinstalled ..  installed....   .. compatiblecompatible compatible
g0188: compatible
g0188: 
g0188: 
g0188: ----------------------------------------------------------------------------------------------------
g0188: ----------------------------------------------------------------------------------------------------
g0188: 
g0188: 
g0188: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: async_io ............... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0188:  ...... evoformer_attn[92m[OKAY][0m 
g0188: ......... [93m[NO][0m ....... [93m[NO][0mfused_adam
g0188:  ............. fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0188: ...... [92m[OKAY][0m
g0188: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: fused_lion cpu_adagrad.............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0188: [92m[OKAY][0m
g0188: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0188: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0188: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: async_io ............... [92m[YES][0m ......fused_lion  [92m[OKAY][0m.............
g0188:  async_io[92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0188: fused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0188:  ...... [92m[OKAY][0m
g0188: cpu_adam ............... fused_adam[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0188: ...... [92m[OKAY][0m
g0188: cpu_adagrad ............ cpu_adam[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0188: ...... [92m[OKAY][0m
g0188: cpu_lion ............... cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0188: ...... [92m[OKAY][0m
g0188: cpu_lion ............... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0188:  ...... evoformer_attn[92m[OKAY][0m 
g0188: ......... [93m[NO][0m ....... [93m[NO][0m
g0188: fused_lamb[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0188: ............. [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0188:  ....... [93m[NO][0m
g0188: fused_lamb .............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0188:  [92m[OKAY][0m
g0188: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: inference_core_ops .....inference_core_ops [92m[YES][0m  ...........  [92m[YES][0m[92m[OKAY][0m 
g0188: ...... [92m[OKAY][0m
g0188: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0188: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0185: 
g0185: 
g0185: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0185: 
g0185: 
g0185: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0185: 
g0185: 
g0185: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.
g0185: 
g0185: 
g0185: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0185: 
g0185: 
g0185: JIT compiled ops requires ninjaJIT compiled ops requires ninjaJIT compiled ops requires ninja
g0185: 
g0185: 
g0185: --------------------------------------------------
g0185: DeepSpeed C++/CUDA extension op report
g0185: --------------------------------------------------
g0185: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0185:       runtime if needed. Op compatibility means that your system
g0185:       meet the required dependencies to JIT install the op.
g0185: --------------------------------------------------
g0185: JIT compiled ops requires ninja
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0188: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0188: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0188: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0188: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: ninjaninjaninjaninja    ........................................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0185: 
g0185: 
g0185: 
g0185: ----------------------------------------------------------------------------------------------------
g0185: 
g0185: ----------------------------------------------------------------------------------------------------
g0185: 
g0185: op nameop name op name op name................ ................  ................ ................installed installed  installed installed.. ..  .. ..compatible compatible 
g0185: compatible
g0185: compatible--------------------------------------------------
g0185: 
g0185: --------------------------------------------------
g0185: ----------------------------------------------------------------------------------------------------
g0185: 
g0185: 
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0188: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0188: --------------------------------------------------
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: DeepSpeed general environment info:
g0188: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0188: torch version .................... 2.0.1+cu118
g0188: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0188: deepspeed info ................... 0.12.4, unknown, unknown
g0188: torch cuda version ............... 11.8
g0188: torch hip version ................ None
g0188: nvcc version ..................... 11.8
g0188: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0188: shared memory (/dev/shm) size .... 188.13 GB
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0188: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0185: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0185: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0185: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0185: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0185: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: DeepSpeed C++/CUDA extension op report
g0187: --------------------------------------------------
g0187: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.
g0187: --------------------------------------------------
g0187: JIT compiled ops requires ninja
g0185: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0185: inference_core_ops ..... [92m[YES][0m ......inference_core_ops [92m[OKAY][0m 
g0185: ..... [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: DeepSpeed C++/CUDA extension op report
g0187: --------------------------------------------------
g0187: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.
g0187: --------------------------------------------------
g0187: JIT compiled ops requires ninja
g0185: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: DeepSpeed C++/CUDA extension op report
g0187: --------------------------------------------------
g0187: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.
g0187: --------------------------------------------------
g0187: JIT compiled ops requires ninja
g0187: --------------------------------------------------
g0187: DeepSpeed C++/CUDA extension op report
g0187: --------------------------------------------------
g0187: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0187:       runtime if needed. Op compatibility means that your system
g0187:       meet the required dependencies to JIT install the op.
g0187: --------------------------------------------------
g0187: JIT compiled ops requires ninja
g0187: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0187: 
g0187: ----------------------------------------------------------------------------------------------------
g0187: 
g0187: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0187: 
g0187: ----------------------------------------------------------------------------------------------------
g0187: 
g0187: ninja .................. [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: op name ................ installed .. compatible
g0187: --------------------------------------------------
g0187: ninja .................. [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: op name ................ installed .. compatible
g0187: --------------------------------------------------
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ ragged_ops[93m[NO][0m  ....................  [92m[YES][0m[93m[NO][0m 
g0185: ...... [92m[OKAY][0m
g0185: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0185: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0185: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0185: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0185: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0185: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0185: --------------------------------------------------
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0185: DeepSpeed general environment info:
g0185: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0185: torch version .................... 2.0.1+cu118
g0185: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0185: deepspeed info ................... 0.12.4, unknown, unknown
g0185: torch cuda version ............... 11.8
g0185: torch hip version ................ None
g0185: nvcc version ..................... 11.8
g0185: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0185: shared memory (/dev/shm) size .... 188.13 GB
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0185: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0188: [2024-08-11 08:17:39,898] [INFO] [comm.py:637:init_distributed] cdb=None
g0188: [2024-08-11 08:17:39,900] [INFO] [comm.py:637:init_distributed] cdb=None
g0188: [2024-08-11 08:17:39,901] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0194: --------------------------------------------------
g0194: DeepSpeed C++/CUDA extension op report
g0194: --------------------------------------------------
g0194: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0194:       runtime if needed. Op compatibility means that your system
g0194:       meet the required dependencies to JIT install the op.
g0194: --------------------------------------------------
g0194: JIT compiled ops requires ninja
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [2024-08-11 08:17:39,908] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: ninjaninjaninjaninja    ........................................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0194: 
g0194: 
g0194: 
g0194: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0194: 
g0194: 
g0194: 
g0194: op nameop nameop nameop name    ................................................................    installedinstalledinstalledinstalled    ........    compatiblecompatiblecompatiblecompatible
g0194: 
g0194: 
g0194: 
g0194: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0194: 
g0194: 
g0194: 
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0188: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0187: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0187: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0187: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0187: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: async_io ...............async_io [92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0187: ...... [92m[OKAY][0m
g0187: fused_adam async_iofused_adam.............   .............[92m[YES][0m...............   [92m[YES][0m......[92m[YES][0m   ......[92m[OKAY][0m...... 
g0187:  [92m[OKAY][0m[92m[OKAY][0m
g0187: 
g0187: cpu_adam cpu_adam...............  ...............[92m[YES][0m  fused_adam[92m[YES][0m......   ......[92m[OKAY][0m............. 
g0187:  [92m[OKAY][0m[92m[YES][0m
g0187:  cpu_adagrad......  cpu_adagrad............[92m[OKAY][0m  
g0187: ............[92m[YES][0m  [92m[YES][0m......cpu_adam   ......[92m[OKAY][0m............... 
g0187:  [92m[OKAY][0m[92m[YES][0m
g0187:  cpu_lion......  cpu_lion...............[92m[OKAY][0m 
g0187:  ...............[92m[YES][0m  [92m[YES][0mcpu_adagrad......   ..................[92m[OKAY][0m  
g0187: [92m[OKAY][0m[92m[YES][0m
g0187:  ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_lion
g0187:  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0187:  evoformer_attn[92m[YES][0m evoformer_attn ......... ...... ......... [93m[NO][0m [92m[OKAY][0m [93m[NO][0m
g0187: .......  .......[93m[NO][0m 
g0187: [93m[NO][0m
g0187: fused_lamb fused_lamb[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH............. 
g0187:  .............[92m[YES][0m evoformer_attn[92m[YES][0m   .....................   [92m[OKAY][0m[93m[NO][0m[92m[OKAY][0m
g0187:  
g0187: ....... [93m[NO][0m
g0187: fused_lamb ............. fused_lionfused_lion[92m[YES][0m   ................................   [92m[YES][0m[92m[YES][0m[92m[OKAY][0m  
g0187: ............  [92m[OKAY][0m[92m[OKAY][0m
g0187: 
g0187: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0187: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0minference_core_ops
g0187:  ..... [92m[YES][0m ...... [92m[OKAY][0m
g0187: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0187: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m cutlass_ops......  [92m[OKAY][0m............
g0187:  [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0195: --------------------------------------------------
g0195: DeepSpeed C++/CUDA extension op report
g0195: --------------------------------------------------
g0195: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0195:       runtime if needed. Op compatibility means that your system
g0195:       meet the required dependencies to JIT install the op.
g0195: --------------------------------------------------
g0195: JIT compiled ops requires ninja
g0187: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0187: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0187: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0187: ragged_opssparse_attn  .........................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0187: 
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0187: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0187: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0187: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0187: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0195: ninjaninjaninja ninja .................. .................. ..................  [92m[OKAY][0m ..................[92m[OKAY][0m
g0195: [92m[OKAY][0m 
g0195: 
g0195: [92m[OKAY][0m--------------------------------------------------
g0195: 
g0195: ----------------------------------------------------------------------------------------------------
g0195: 
g0195: op name-------------------------------------------------- 
g0195: op nameop name................   op name................................installed    ................installedinstalled..    installed....compatible   
g0195: ..compatiblecompatible 
g0195: --------------------------------------------------
g0195: compatible
g0195: 
g0195: ----------------------------------------------------------------------------------------------------
g0195: 
g0195: --------------------------------------------------
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0187: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0187: --------------------------------------------------
g0187: DeepSpeed general environment info:
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0187: DeepSpeed general environment info:
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0187: DeepSpeed general environment info:
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0187: DeepSpeed general environment info:
g0187: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0187: torch version .................... 2.0.1+cu118
g0187: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0187: deepspeed info ................... 0.12.4, unknown, unknown
g0187: torch cuda version ............... 11.8
g0187: torch hip version ................ None
g0187: nvcc version ..................... 11.8
g0187: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0187: shared memory (/dev/shm) size .... 188.13 GB
g0185: [2024-08-11 08:17:39,979] [INFO] [comm.py:637:init_distributed] cdb=None
g0185: [2024-08-11 08:17:39,980] [INFO] [comm.py:637:init_distributed] cdb=None
g0185: [2024-08-11 08:17:39,980] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: async_iocpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0194: 
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0194: 
g0194: evoformer_attncpu_adam  ........................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0194: 
g0194: fused_lambcpu_adagrad  .........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0194: 
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0mfused_lion
g0194:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0194: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0194: async_iofused_lamb  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0194: 
g0194: fused_adam ............. fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0194: ...... [92m[OKAY][0mcpu_adam
g0194:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0194: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0194: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0194: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0194: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0185: [2024-08-11 08:17:39,983] [INFO] [comm.py:637:init_distributed] cdb=None
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0185: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0187: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0194: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0194: 
g0194: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0194: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0194: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0194: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ [93m[NO][0m .......[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0 [93m[NO][0m
g0194: 
g0194: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0194: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0194: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0194: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0194: --------------------------------------------------
g0195: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0195: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0195: fused_lamb .............async_io [92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0195:  ...... [92m[OKAY][0m
g0195: async_io fused_lion...............  .............fused_adam[92m[YES][0m   [92m[YES][0m...................   ......[92m[YES][0m[92m[OKAY][0m  [92m[OKAY][0m
g0195: ......
g0195:  [92m[OKAY][0m
g0195: cpu_adamfused_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0195: 
g0195: cpu_adagrad cpu_adam............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0195: [92m[OKAY][0m
g0195: cpu_lion cpu_adagrad...............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0195: [92m[OKAY][0m
g0195: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0195: 
g0195: evoformer_attnevoformer_attn  ..................  [93m[NO][0m[93m[NO][0m  ..............  [93m[NO][0m[93m[NO][0m
g0195: 
g0195: fused_lambfused_lamb  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0195: 
g0195: fused_lionfused_lion  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0195: 
g0195: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0195: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0195: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0194: DeepSpeed general environment info:
g0194: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0194: torch version .................... 2.0.1+cu118
g0194: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0194: deepspeed info ................... 0.12.4, unknown, unknown
g0194: torch cuda version ............... 11.8
g0194: torch hip version ................ None
g0194: nvcc version ..................... 11.8
g0194: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0194: shared memory (/dev/shm) size .... 188.13 GB
g0195: inference_core_ops ..... [92m[YES][0minference_core_ops ......  .....[92m[OKAY][0m 
g0195: [92m[YES][0m ...... [92m[OKAY][0m
g0195: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0195: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0195: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0195: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0195: ragged_opssparse_attn  .........................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0195: 
g0195: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0195: random_ltd sparse_attn.............  ............[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0195: [93m[NO][0m
g0195: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0195: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0195: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0194: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0195: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0195: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0195: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0195: --------------------------------------------------
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0195: DeepSpeed general environment info:
g0195: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0195: torch version .................... 2.0.1+cu118
g0195: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0195: deepspeed info ................... 0.12.4, unknown, unknown
g0195: torch cuda version ............... 11.8
g0195: torch hip version ................ None
g0195: nvcc version ..................... 11.8
g0195: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0195: shared memory (/dev/shm) size .... 188.13 GB
g0187: [2024-08-11 08:17:40,090] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [2024-08-11 08:17:40,090] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [2024-08-11 08:17:40,090] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [2024-08-11 08:17:40,090] [INFO] [comm.py:637:init_distributed] cdb=None
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0187: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: --------------------------------------------------
g0197: DeepSpeed C++/CUDA extension op report
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.
g0197: --------------------------------------------------
g0197: JIT compiled ops requires ninja
g0197: --------------------------------------------------
g0197: DeepSpeed C++/CUDA extension op report
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0197: --------------------------------------------------
g0197: 
g0197: DeepSpeed C++/CUDA extension op reportJIT compiled ops requires ninja
g0197: 
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.
g0197: --------------------------------------------------
g0197: JIT compiled ops requires ninja
g0197: --------------------------------------------------
g0197: DeepSpeed C++/CUDA extension op report
g0197: --------------------------------------------------
g0197: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0197:       runtime if needed. Op compatibility means that your system
g0197:       meet the required dependencies to JIT install the op.
g0197: --------------------------------------------------
g0197: JIT compiled ops requires ninja
g0197: ninjaninjaninjaninja    ........................................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0197: 
g0197: 
g0197: 
g0197: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0197: 
g0197: 
g0197: 
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: op nameop nameop nameop name    ................................................................    installedinstalledinstalledinstalled    ........    compatiblecompatiblecompatiblecompatible
g0197: 
g0197: 
g0197: 
g0197: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0197: 
g0197: 
g0197: 
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0195: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: --------------------------------------------------
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: --------------------------------------------------
g0198: JIT compiled ops requires ninja
g0198: --------------------------------------------------
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: ----------------------------------------------------------------------------------------------------
g0198: 
g0198: JIT compiled ops requires ninja
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: --------------------------------------------------
g0198: JIT compiled ops requires ninja
g0198: --------------------------------------------------
g0198: DeepSpeed C++/CUDA extension op report
g0198: --------------------------------------------------
g0198: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0198:       runtime if needed. Op compatibility means that your system
g0198:       meet the required dependencies to JIT install the op.
g0198: --------------------------------------------------
g0198: JIT compiled ops requires ninja
g0194: [2024-08-11 08:17:40,165] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: [2024-08-11 08:17:40,166] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: [2024-08-11 08:17:40,166] [INFO] [comm.py:637:init_distributed] cdb=None
g0194: [2024-08-11 08:17:40,167] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: ninjaninjaninja ninja  ......................................................    [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0198:  
g0198: 
g0198: [92m[OKAY][0m--------------------------------------------------
g0198: --------------------------------------------------
g0198: --------------------------------------------------
g0198: 
g0198: --------------------------------------------------op nameop name
g0198:  op name ................ ................op name ................  installed installed................ installed  .. ..installed ..  compatible compatible..
g0198: compatible
g0198:  
g0198: --------------------------------------------------compatible--------------------------------------------------
g0198: 
g0198: --------------------------------------------------
g0198: --------------------------------------------------
g0198: 
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0194: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0197: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0197: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: fused_lion ............. async_io[92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0197:  ...... [92m[OKAY][0m
g0197: fused_adamasync_io .............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0197: [92m[OKAY][0m
g0197: async_iocpu_adam  .............................. fused_adam [92m[YES][0m [92m[YES][0m ............. ...... ...... [92m[YES][0m [92m[OKAY][0m [92m[OKAY][0m
g0197: ......
g0197:  [92m[OKAY][0mcpu_adagrad
g0197:  ............ [92m[YES][0mcpu_adamfused_adam   ..................................  [92m[OKAY][0m [92m[YES][0m
g0197: [92m[YES][0m  ............  cpu_lion[92m[OKAY][0m[92m[OKAY][0m 
g0197: 
g0197: ............... [92m[YES][0m cpu_adamcpu_adagrad......   ...........................[92m[OKAY][0m  
g0197: [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0197: 
g0197: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0197: cpu_lioncpu_adagrad evoformer_attn ............... ............ ......... [92m[YES][0m [92m[YES][0m [93m[NO][0m ...... ...... ....... [92m[OKAY][0m [92m[OKAY][0m
g0197: [93m[NO][0m
g0197: 
g0197: cpu_lionfused_lamb  ............................  [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m 
g0197:  ............ evoformer_attn [92m[OKAY][0m [92m[OKAY][0m
g0197: .........
g0197:  [93m[NO][0m ....... [93m[NO][0m
g0197: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0197: fused_lionfused_lamb  evoformer_attn..........................   .........[92m[YES][0m[92m[YES][0m   ............[93m[NO][0m   [92m[OKAY][0m[92m[OKAY][0m.......
g0197: 
g0197:  [93m[NO][0m
g0197: fused_lamb ............. fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0197: ...... [92m[OKAY][0m
g0197: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0195: [2024-08-11 08:17:40,200] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [2024-08-11 08:17:40,201] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [2024-08-11 08:17:40,202] [INFO] [comm.py:637:init_distributed] cdb=None
g0195: [2024-08-11 08:17:40,202] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0197: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  [92m[YES][0m......  ......[92m[OKAY][0m 
g0197: [92m[OKAY][0m
g0197: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0195: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0197: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0197: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0197: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0197: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: ragged_ops[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0197: ............. [92m[YES][0msparse_attn  ..................  [92m[OKAY][0m[93m[NO][0m
g0197:  ....... [93m[NO][0mrandom_ltd
g0197:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0197: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0197: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0197: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0197: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0197: --------------------------------------------------
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0197: DeepSpeed general environment info:
g0197: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0197: torch version .................... 2.0.1+cu118
g0197: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0197: deepspeed info ................... 0.12.4, unknown, unknown
g0197: torch cuda version ............... 11.8
g0197: torch hip version ................ None
g0197: nvcc version ..................... 11.8
g0197: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0197: shared memory (/dev/shm) size .... 188.13 GB
g0198: async_io ............... [92m[YES][0m ...... async_io[92m[OKAY][0m 
g0198: ............... [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0198:  ............. [92m[YES][0m ...... fused_adam[92m[OKAY][0m 
g0198: ............. [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0198: [92m[YES][0m ...... cpu_adam[92m[OKAY][0m 
g0198: ............... [92m[YES][0m cpu_adagrad......  ............[92m[OKAY][0m 
g0198: [92m[YES][0m ......cpu_adagrad  [92m[OKAY][0m............
g0198:  [92m[YES][0m cpu_lion......  ...............[92m[OKAY][0m 
g0198: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0198:  [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0198: evoformer_attn [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.........
g0198:  [93m[NO][0m evoformer_attn.......  .........[93m[NO][0m 
g0198: [93m[NO][0m .......fused_lamb  [93m[NO][0m.............
g0198:  [92m[YES][0m ......fused_lamb async_io [92m[OKAY][0m.............
g0198:   ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0198: [92m[OKAY][0m
g0198: fused_lion ............. [92m[YES][0m ......fused_adam fused_lion[92m[OKAY][0m  
g0198: ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0198: 
g0198: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0198: async_iocpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0198: 
g0198: cpu_lion ............... [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0198: [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_adam
g0198:  ...............evoformer_attn  [92m[YES][0m.........  ......[93m[NO][0m  [92m[OKAY][0m.......
g0198:  [93m[NO][0m
g0198: cpu_adagrad ............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0198:  [92m[OKAY][0m
g0198: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0198: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0198: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0198: 
g0198: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0198: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0198: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0198: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0198: sparse_attn ............ ragged_ops[93m[NO][0m  ....................  [92m[YES][0m[93m[NO][0m 
g0198: ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0198: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0198: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0198: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0198: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0198: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0198: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: spatial_inference ...... [92m[YES][0m spatial_inference......  ......[92m[OKAY][0m 
g0198: [92m[YES][0m ...... transformer[92m[OKAY][0m 
g0198: ............ [92m[YES][0mtransformer  ..................  [92m[OKAY][0m[92m[YES][0m
g0198:  ...... stochastic_transformer[92m[OKAY][0m 
g0198: . [92m[YES][0mstochastic_transformer  .......  [92m[OKAY][0m[92m[YES][0m
g0198:  ...... [92m[OKAY][0m
g0198: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0198: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0198: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0198: --------------------------------------------------
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0198: --------------------------------------------------
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0198: --------------------------------------------------
g0198: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0198: --------------------------------------------------
g0198: DeepSpeed general environment info:
g0198: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: shared memory (/dev/shm) size .... 188.13 GB
g0198: DeepSpeed general environment info:
g0198: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: DeepSpeed general environment info:shared memory (/dev/shm) size
g0198:  .... torch install path188.13 GB 
g0198: ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: shared memory (/dev/shm) size .... 188.13 GB
g0198: DeepSpeed general environment info:
g0198: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0198: torch version .................... 2.0.1+cu118
g0198: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0198: deepspeed info ................... 0.12.4, unknown, unknown
g0198: torch cuda version ............... 11.8
g0198: torch hip version ................ None
g0198: nvcc version ..................... 11.8
g0198: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0198: shared memory (/dev/shm) size .... 188.13 GB
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0198: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0197: [2024-08-11 08:17:40,366] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [2024-08-11 08:17:40,366] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [2024-08-11 08:17:40,367] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [2024-08-11 08:17:40,380] [INFO] [comm.py:637:init_distributed] cdb=None
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0197: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [2024-08-11 08:17:40,419] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: [2024-08-11 08:17:40,419] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: [2024-08-11 08:17:40,419] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: > setting tensorboard ...
g0198: [2024-08-11 08:17:40,849] [INFO] [comm.py:637:init_distributed] cdb=None
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0198: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0184-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0184: > initialized tensor model parallel with size 1
g0184: > initialized pipeline model parallel with size 8
g0184: > setting random seeds to 1234 ...
g0184: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0184: > compiling dataset index builder ...
g0184: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0184: make: Nothing to be done for 'default'.
g0184: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0184: >>> done with dataset index builder. Compilation time: 0.079 seconds
g0184: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0184: > compiling and loading fused kernels ...
g0184: Detected CUDA files, patching ldflags
g0184: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0184: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0184: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0184: ninja: no work to do.
g0184: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0184: Detected CUDA files, patching ldflags
g0184: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0184: Building extension module scaled_masked_softmax_cuda...
g0184: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0184: ninja: no work to do.
g0184: Loading extension module scaled_masked_softmax_cuda...
g0184: Detected CUDA files, patching ldflags
g0184: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0184: Building extension module scaled_softmax_cuda...
g0184: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0184: ninja: no work to do.
g0184: Loading extension module scaled_softmax_cuda...
g0184: >>> done with compiling and loading fused kernels. Compilation time: 6.902 seconds
g0184: time to initialize megatron (seconds): 21.748
g0184: [after megatron is initialized] datetime: 2024-08-11 08:17:50 
g0187: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0197: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0195: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0198: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0184: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0188: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0185: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0194: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0187: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0187: wandb:  $ pip install wandb --upgrade
g0187: wandb: Tracking run with wandb version 0.17.5
g0187: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-eo31wk0c
g0187: wandb: Run `wandb offline` to turn off syncing.
g0187: wandb: Syncing run g0187.abci.local
g0187: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0187: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/eo31wk0c
g0198: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0198: wandb:  $ pip install wandb --upgrade
g0198: wandb: Tracking run with wandb version 0.17.5
g0198: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-8f7uj3yz
g0198: wandb: Run `wandb offline` to turn off syncing.
g0195: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0195: wandb:  $ pip install wandb --upgrade
g0195: wandb: Tracking run with wandb version 0.17.5
g0195: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-gza9wl2f
g0195: wandb: Run `wandb offline` to turn off syncing.
g0198: wandb: Syncing run g0198.abci.local
g0198: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0198: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/8f7uj3yz
g0195: wandb: Syncing run g0195.abci.local
g0195: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0195: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/gza9wl2f
g0188: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0188: wandb:  $ pip install wandb --upgrade
g0188: wandb: Tracking run with wandb version 0.17.5
g0188: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-wsokb3nb
g0188: wandb: Run `wandb offline` to turn off syncing.
g0188: wandb: Syncing run g0188.abci.local
g0188: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0188: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/wsokb3nb
g0185: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0185: wandb:  $ pip install wandb --upgrade
g0185: wandb: Tracking run with wandb version 0.17.5
g0185: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-3dx4u19f
g0185: wandb: Run `wandb offline` to turn off syncing.
g0185: wandb: Syncing run g0185.abci.local
g0185: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0185: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/3dx4u19f
g0184: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0184: wandb:  $ pip install wandb --upgrade
g0184: wandb: Tracking run with wandb version 0.17.5
g0184: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-6a6vcxtr
g0184: wandb: Run `wandb offline` to turn off syncing.
g0184: wandb: Syncing run g0184.abci.local
g0184: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0184: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/6a6vcxtr
g0197: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0197: wandb:  $ pip install wandb --upgrade
g0197: wandb: Tracking run with wandb version 0.17.5
g0197: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-c449a8wt
g0197: wandb: Run `wandb offline` to turn off syncing.
g0194: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0194: wandb:  $ pip install wandb --upgrade
g0194: wandb: Tracking run with wandb version 0.17.5
g0194: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240811_081751-xvc58974
g0194: wandb: Run `wandb offline` to turn off syncing.
g0197: wandb: Syncing run g0197.abci.local
g0197: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0197: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/c449a8wt
g0194: wandb: Syncing run g0194.abci.local
g0194: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0194: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/xvc58974
g0184: building GPT model ...
g0184: [2024-08-11 08:17:52,882] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0184: [2024-08-11 08:17:52,883] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0184: [2024-08-11 08:17:52,884] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 56.25 GB, percent = 14.9%
g0184: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0184: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0184: [2024-08-11 08:17:53,405] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0184: stage=0 layers=5
g0184:      0: _to_float16
g0184:      1: EmbeddingPipe
g0184:      2: ParallelTransformerLayerPipe
g0184:      3: ParallelTransformerLayerPipe
g0184:      4: ParallelTransformerLayerPipe
g0184: stage=1 layers=3
g0184:      5: ParallelTransformerLayerPipe
g0184:      6: ParallelTransformerLayerPipe
g0184:      7: ParallelTransformerLayerPipe
g0184: stage=2 layers=3
g0184:      8: ParallelTransformerLayerPipe
g0184:      9: ParallelTransformerLayerPipe
g0184:     10: ParallelTransformerLayerPipe
g0184: stage=3 layers=3
g0184:     11: ParallelTransformerLayerPipe
g0184:     12: ParallelTransformerLayerPipe
g0184:     13: ParallelTransformerLayerPipe
g0184: stage=4 layers=3
g0184:     14: ParallelTransformerLayerPipe
g0184:     15: ParallelTransformerLayerPipe
g0184:     16: ParallelTransformerLayerPipe
g0184: stage=5 layers=3
g0184:     17: ParallelTransformerLayerPipe
g0184:     18: ParallelTransformerLayerPipe
g0184:     19: ParallelTransformerLayerPipe
g0184: stage=6 layers=3
g0184:     20: ParallelTransformerLayerPipe
g0184:     21: ParallelTransformerLayerPipe
g0184:     22: ParallelTransformerLayerPipe
g0184: stage=7 layers=3
g0184:     23: ParallelTransformerLayerPipe
g0184:     24: MixedFusedRMSNorm
g0184:     25: LMHeadPipe
g0184:   loss: CrossEntropy
g0187:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0185:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0198:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0188:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0195:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0197:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0194:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0184: [2024-08-11 08:17:53,827] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0184: [2024-08-11 08:17:53,828] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0184: [2024-08-11 08:17:53,828] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 56.31 GB, percent = 15.0%
g0184:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0184: setting training iterations to 10000000
g0184: > learning rate decay style: cosine
g0184: DeepSpeed is enabled.
g0184: [2024-08-11 08:17:53,830] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0187: [2024-08-11 08:17:53,962] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0187: [2024-08-11 08:17:53,962] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0187: [2024-08-11 08:17:53,962] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0187: [2024-08-11 08:17:53,962] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-11 08:17:53,963] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-11 08:17:53,964] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-11 08:17:53,964] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-11 08:17:53,972] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-11 08:17:53,972] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-11 08:17:53,972] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0188: [2024-08-11 08:17:53,973] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0185: [2024-08-11 08:17:53,966] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0194: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0198: [2024-08-11 08:17:54,011] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-11 08:17:54,013] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-11 08:17:54,013] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-11 08:17:54,013] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0195: [2024-08-11 08:17:54,014] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0197: [2024-08-11 08:17:54,035] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0197: [2024-08-11 08:17:54,035] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0197: [2024-08-11 08:17:54,035] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0197: [2024-08-11 08:17:54,036] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-11 08:17:54,039] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0184: [2024-08-11 08:17:54,040] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0184: [2024-08-11 08:17:54,041] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0184: [2024-08-11 08:17:54,041] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0184: [2024-08-11 08:17:54,041] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0184: [2024-08-11 08:17:54,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0184: [2024-08-11 08:17:54,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0184: [2024-08-11 08:17:54,057] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-11 08:17:54,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f88147ff390>
g0184: [2024-08-11 08:17:54,057] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-11 08:17:54,057] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-11 08:17:54,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: [2024-08-11 08:17:54,057] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0184: [2024-08-11 08:17:54,058] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0184:     "partition_activations": false, 
g0184:     "contiguous_memory_optimization": false, 
g0184:     "cpu_checkpointing": false, 
g0184:     "number_checkpoints": null, 
g0184:     "synchronize_checkpoint_boundary": false, 
g0184:     "profile": false
g0184: }
g0184: [2024-08-11 08:17:54,058] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0184: [2024-08-11 08:17:54,058] [INFO] [config.py:983:print]   amp_enabled .................. False
g0184: [2024-08-11 08:17:54,058] [INFO] [config.py:983:print]   amp_params ................... False
g0184: [2024-08-11 08:17:54,059] [INFO] [config.py:983:print]   autotuning_config ............ {
g0184:     "enabled": false, 
g0184:     "start_step": null, 
g0184:     "end_step": null, 
g0184:     "metric_path": null, 
g0184:     "arg_mappings": null, 
g0184:     "metric": "throughput", 
g0184:     "model_info": null, 
g0184:     "results_dir": "autotuning_results", 
g0184:     "exps_dir": "autotuning_exps", 
g0184:     "overwrite": true, 
g0184:     "fast": true, 
g0184:     "start_profile_step": 3, 
g0184:     "end_profile_step": 5, 
g0184:     "tuner_type": "gridsearch", 
g0184:     "tuner_early_stopping": 5, 
g0184:     "tuner_num_trials": 50, 
g0184:     "model_info_path": null, 
g0184:     "mp_size": 1, 
g0184:     "max_train_batch_size": null, 
g0184:     "min_train_batch_size": 1, 
g0184:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0184:     "min_train_micro_batch_size_per_gpu": 1, 
g0184:     "num_tuning_micro_batch_sizes": 3
g0184: }
g0184: [2024-08-11 08:17:54,059] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0184: [2024-08-11 08:17:54,059] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0184: [2024-08-11 08:17:54,059] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0184: [2024-08-11 08:17:54,059] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0184: [2024-08-11 08:17:54,059] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f878e444510>
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   communication_data_type ...... None
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   disable_allgather ............ False
g0184: [2024-08-11 08:17:54,060] [INFO] [config.py:983:print]   dump_state ................... False
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0184: [2024-08-11 08:17:54,061] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0184:     "enabled": false, 
g0184:     "recompute_fwd_factor": 0.0, 
g0184:     "profile_step": 1, 
g0184:     "module_depth": -1, 
g0184:     "top_modules": 1, 
g0184:     "detailed": true, 
g0184:     "output_file": null
g0184: }
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   global_rank .................. 0
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0184: [2024-08-11 08:17:54,062] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   loss_scale ................... 0
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0184: [2024-08-11 08:17:54,063] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   nebula_config ................ {
g0184:     "enabled": false, 
g0184:     "persistent_storage_path": null, 
g0184:     "persistent_time_interval": 100, 
g0184:     "num_of_version_in_retention": 2, 
g0184:     "enable_nebula_load": true, 
g0184:     "load_path": null
g0184: }
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   optimizer_name ............... None
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   optimizer_params ............. None
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   pld_enabled .................. False
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   pld_params ................... False
g0184: [2024-08-11 08:17:54,064] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   scheduler_name ............... None
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   scheduler_params ............. None
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   sparse_attention ............. None
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0184: [2024-08-11 08:17:54,065] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   world_size ................... 4
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   zero_enabled ................. False
g0184: [2024-08-11 08:17:54,066] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0184: [2024-08-11 08:17:54,067] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0184: [2024-08-11 08:17:54,067] [INFO] [config.py:969:print_user_config]   json = {
g0184:     "train_batch_size": 128, 
g0184:     "train_micro_batch_size_per_gpu": 1, 
g0184:     "steps_per_print": 10, 
g0184:     "zero_optimization": {
g0184:         "stage": 0
g0184:     }, 
g0184:     "gradient_clipping": 1.0, 
g0184:     "prescale_gradients": true, 
g0184:     "fp16": {
g0184:         "enabled": true, 
g0184:         "loss_scale": 0, 
g0184:         "loss_scale_window": 500, 
g0184:         "hysteresis": 2, 
g0184:         "min_loss_scale": 1, 
g0184:         "initial_scale_power": 11
g0184:     }, 
g0184:     "wall_clock_breakdown": false
g0184: }
g0184: [2024-08-11 08:17:54,067] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0184: [2024-08-11 08:17:54,067] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0184: [2024-08-11 08:17:54,774] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0194: [2024-08-11 08:17:54,783] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0187: [2024-08-11 08:17:54,778] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0197: [2024-08-11 08:17:54,775] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0195: [2024-08-11 08:17:54,776] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0198: [2024-08-11 08:17:54,776] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0185: [2024-08-11 08:17:54,776] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0188: [2024-08-11 08:17:54,784] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0198: [2024-08-11 08:17:55,495] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 08:17:55,495] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0188: [2024-08-11 08:17:55,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0188: [2024-08-11 08:17:55,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0188: [2024-08-11 08:17:55,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0195: [2024-08-11 08:17:55,495] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0195: [2024-08-11 08:17:55,495] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0195: [2024-08-11 08:17:55,495] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0194: [2024-08-11 08:17:55,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0194: [2024-08-11 08:17:55,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0194: [2024-08-11 08:17:55,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0187: [2024-08-11 08:17:55,499] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0187: [2024-08-11 08:17:55,499] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0187: [2024-08-11 08:17:55,499] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0197: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0197: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0197: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0188: [2024-08-11 08:17:55,504] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0184: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0184: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0184: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0185: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0187: [2024-08-11 08:17:55,499] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0185: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0185: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0184: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0197: [2024-08-11 08:17:55,496] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0194: [2024-08-11 08:17:55,504] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0195: [2024-08-11 08:17:55,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0185: [2024-08-11 08:17:55,499] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0185: [2024-08-11 08:17:58,171] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0185: [2024-08-11 08:17:58,171] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0185: [2024-08-11 08:17:58,171] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0185: [2024-08-11 08:17:58,179] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0185: [2024-08-11 08:17:58,179] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0185: [2024-08-11 08:17:58,179] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0185: [2024-08-11 08:17:58,179] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0185: [2024-08-11 08:17:58,185] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0195: [2024-08-11 08:17:58,191] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0195: [2024-08-11 08:17:58,191] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0195: [2024-08-11 08:17:58,192] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0195: [2024-08-11 08:17:58,194] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0195: [2024-08-11 08:17:58,200] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0195: [2024-08-11 08:17:58,200] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0195: [2024-08-11 08:17:58,202] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0195: [2024-08-11 08:17:58,202] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 08:17:58,204] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:58,204] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:58,204] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:58,204] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0194: [2024-08-11 08:17:58,212] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0194: [2024-08-11 08:17:58,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0194: [2024-08-11 08:17:58,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:58,211] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0184: [2024-08-11 08:17:58,212] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0194: [2024-08-11 08:17:58,220] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0184: [2024-08-11 08:17:58,212] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0184: [2024-08-11 08:17:58,212] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0194: [2024-08-11 08:17:58,220] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0194: [2024-08-11 08:17:58,222] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0194: [2024-08-11 08:17:58,222] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0194: [2024-08-11 08:17:58,226] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0188: [2024-08-11 08:17:58,234] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0188: [2024-08-11 08:17:58,234] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0188: [2024-08-11 08:17:58,234] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0188: [2024-08-11 08:17:58,236] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0188: [2024-08-11 08:17:58,242] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0188: [2024-08-11 08:17:58,242] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0188: [2024-08-11 08:17:58,242] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0188: [2024-08-11 08:17:58,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0197: [2024-08-11 08:17:58,236] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0197: [2024-08-11 08:17:58,236] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0197: [2024-08-11 08:17:58,236] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0197: [2024-08-11 08:17:58,241] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0197: [2024-08-11 08:17:58,245] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0197: [2024-08-11 08:17:58,245] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0197: [2024-08-11 08:17:58,245] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0197: [2024-08-11 08:17:58,247] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0198: [2024-08-11 08:17:58,250] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0198: [2024-08-11 08:17:58,250] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0198: [2024-08-11 08:17:58,251] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0198: [2024-08-11 08:17:58,251] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0198: [2024-08-11 08:17:58,258] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0198: [2024-08-11 08:17:58,258] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0198: [2024-08-11 08:17:58,258] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0198: [2024-08-11 08:17:58,259] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0187: [2024-08-11 08:17:58,325] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0187: [2024-08-11 08:17:58,325] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0187: [2024-08-11 08:17:58,325] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0187: [2024-08-11 08:17:58,328] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0187: [2024-08-11 08:17:58,332] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0187: [2024-08-11 08:17:58,333] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0187: [2024-08-11 08:17:58,333] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0187: [2024-08-11 08:17:58,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0184: [2024-08-11 08:17:59,073] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:59,073] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:59,074] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,074] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:59,092] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 08:17:59,092] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,093] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,296] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,297] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,297] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,297] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,297] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,297] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,297] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,297] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,344] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,344] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,347] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,347] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,373] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,373] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,376] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,376] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0198: [2024-08-11 08:17:59,730] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 08:17:59,730] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 08:17:59,731] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0198: [2024-08-11 08:17:59,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 08:17:59,731] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0198: [2024-08-11 08:17:59,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 08:17:59,732] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0198: [2024-08-11 08:17:59,732] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,753] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,753] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,753] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,754] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,754] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,754] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,754] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,754] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,786] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,786] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,790] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,790] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 08:17:59,807] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,808] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,811] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 08:17:59,812] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0185: [2024-08-11 08:17:59,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 08:17:59,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 08:17:59,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 08:17:59,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 08:17:59,968] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0185: [2024-08-11 08:17:59,969] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0185: [2024-08-11 08:17:59,969] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0185: [2024-08-11 08:17:59,969] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,001] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 08:18:00,001] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 08:18:00,001] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 08:18:00,002] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,002] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,002] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 08:18:00,002] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,003] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,096] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,127] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,128] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,130] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,130] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,149] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,149] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,151] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,151] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,159] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 08:18:00,159] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 08:18:00,159] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 08:18:00,159] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 08:18:00,159] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,159] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,159] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,160] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,190] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 08:18:00,190] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 08:18:00,190] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 08:18:00,191] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,191] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,191] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,193] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 08:18:00,194] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,250] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 08:18:00,251] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,251] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 08:18:00,252] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,252] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,254] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,253] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,254] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,254] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,282] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,282] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,283] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,283] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,283] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,283] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,284] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,290] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,290] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,304] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,304] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,312] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,312] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,315] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,316] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,318] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,318] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,333] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,337] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,337] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,392] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,393] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,393] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,393] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,394] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,394] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,394] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,394] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,426] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,426] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,428] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,428] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,433] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,434] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,434] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,434] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,440] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,444] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,447] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,450] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,450] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,450] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,450] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,450] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,451] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0184: [2024-08-11 08:18:00,464] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,465] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,467] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 08:18:00,470] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,484] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,484] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,487] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,487] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,497] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,497] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,497] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,497] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,497] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,497] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,501] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,509] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,510] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,528] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,528] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,530] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,531] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,547] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,548] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,548] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,548] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,548] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,548] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,548] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,549] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,545] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,545] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,550] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,550] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,580] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,580] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,583] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,584] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,598] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,605] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,658] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,658] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,658] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,659] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,660] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,667] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,668] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,668] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,668] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,668] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,668] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,669] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,669] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,689] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,689] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,692] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,692] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,695] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,695] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,699] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,700] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,698] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,699] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,703] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,703] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0195: [2024-08-11 08:18:00,703] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,706] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,710] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,715] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,716] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,713] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0195: [2024-08-11 08:18:00,714] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,717] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,718] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,720] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,725] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,725] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,723] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,723] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,723] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0198: [2024-08-11 08:18:00,723] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,723] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,723] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0198: [2024-08-11 08:18:00,724] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,780] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,781] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,781] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,781] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,781] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,781] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,781] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,782] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,812] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,812] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,815] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,815] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0197: [2024-08-11 08:18:00,826] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,829] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,834] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,835] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,836] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,836] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,836] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,836] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,836] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,836] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0185: [2024-08-11 08:18:00,837] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0197: [2024-08-11 08:18:00,837] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,845] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,845] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,845] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,845] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,845] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,846] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,846] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,846] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,875] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,875] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,868] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,868] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,870] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 08:18:00,870] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0194: [2024-08-11 08:18:00,893] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,896] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,897] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 08:18:00,900] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,987] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,988] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,988] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,988] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,988] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 08:18:00,988] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0187: [2024-08-11 08:18:00,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0187: [2024-08-11 08:18:01,018] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 08:18:01,018] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,023] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 08:18:01,024] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 08:18:01,024] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 08:18:01,024] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,024] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 08:18:01,024] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,025] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0187: [2024-08-11 08:18:01,023] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 08:18:01,023] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,022] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,022] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0195: [2024-08-11 08:18:01,025] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,025] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,025] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,025] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0195: [2024-08-11 08:18:01,025] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0195: [2024-08-11 08:18:01,025] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0197: [2024-08-11 08:18:01,031] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,031] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,031] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0197: [2024-08-11 08:18:01,031] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0197: [2024-08-11 08:18:01,032] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,032] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,032] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0197: [2024-08-11 08:18:01,032] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0195: [2024-08-11 08:18:01,054] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,054] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,057] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 08:18:01,058] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,062] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,062] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,065] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 08:18:01,065] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,070] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,070] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,071] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0198: [2024-08-11 08:18:01,071] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0198: [2024-08-11 08:18:01,072] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,072] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,072] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0198: [2024-08-11 08:18:01,072] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0198: [2024-08-11 08:18:01,118] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,118] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,122] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 08:18:01,123] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,209] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,210] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 08:18:01,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,210] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 08:18:01,211] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 08:18:01,211] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0194: [2024-08-11 08:18:01,241] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,241] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,243] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 08:18:01,243] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,294] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,294] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,294] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,295] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,295] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,295] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,295] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,296] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,328] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,328] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,330] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,330] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,345] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,349] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,349] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,353] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,555] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,556] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,587] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,587] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,589] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,589] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,603] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,608] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,608] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,610] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,799] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,800] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,800] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,800] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,800] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,800] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,800] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,801] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0188: [2024-08-11 08:18:01,831] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,831] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,833] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 08:18:01,835] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0184:  > overriding learning rate value to 0.0002
g0184:  > overriding minimum learning rate value to 1e-05
g0184:  > overriding warmup iterations value to 0
g0184:  > overriding warmup tokens value to 3000000000
g0184:  > overriding total number of iterations value to 1280000000
g0184:  > overriding decay tokens value to 300000000000
g0184:  > overriding learning rate decay style value to cosine
g0184:  > overriding start weight decay value to 0.1
g0184:  > overriding end weight decay value to 0.1
g0184:  > overriding total number of weight decay iterations value to 1280000000
g0184:  > overriding weight decay incr style value to constant
g0184:  checkpoint version 3.0
g0184:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 38000
g0198: (min, max) time across ranks (ms):
g0198:     load-checkpoint ................................: (7388.61, 7390.99)
g0184: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-11 08:18:02 
g0184: > building train, validation, and test datasets ...
g0184:  > datasets target sizes (minimum size):
g0184:     train:      1280000000
g0184:     validation: 128012800
g0184:     test:       12800
g0184: > building train, validation, and test datasets for GPT ...
g0184: Single data path provided for train, valid & test
g0184:  > building dataset index ...
g0184:     reading sizes...
g0184:     reading pointers...
g0184:     reading document index...
g0184:     creating numpy buffer of mmap...
g0184:     creating memory view of numpy buffer...
g0184:  > finished creating indexed dataset in 0.040576 seconds
g0184:     number of documents: 2237032
g0184:  > dataset split:
g0184:     train:
g0184:      document indices in [0, 2122943) total of 2122943 documents
g0184:     validation:
g0184:      document indices in [2122943, 2234795) total of 111852 documents
g0184:     test:
g0184:      document indices in [2234795, 2237032) total of 2237 documents
g0184:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_doc_idx.npy
g0184:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_sample_idx.npy
g0184:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_shuffle_idx.npy
g0184:     loaded indexed file in 0.105 seconds
g0184:     total number of samples: 10738039
g0184:     total number of epochs: 1
g0184:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_doc_idx.npy
g0184:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_sample_idx.npy
g0184:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_shuffle_idx.npy
g0184:     loaded indexed file in 0.182 seconds
g0184:     total number of samples: 128391300
g0184:     total number of epochs: 228
g0184:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_doc_idx.npy
g0184:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_sample_idx.npy
g0184:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_shuffle_idx.npy
g0184:     loaded indexed file in 0.049 seconds
g0184:     total number of samples: 14458
g0184:     total number of epochs: 2
g0184: > finished creating GPT datasets ...
g0184: [after dataloaders are built] datetime: 2024-08-11 08:18:04 
g0184: done with setup ...
g0184: training ...
g0198: (min, max) time across ranks (ms):
g0198:     model-and-optimizer-setup ......................: (10140.13, 10153.99)
g0198:     train/valid/test-data-iterators-setup ..........: (1839.31, 1842.49)
g0184: [before the start of training step] datetime: 2024-08-11 08:18:04 
g0184: [2024-08-11 08:19:00,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=38010, skipped=58, lr=[0.00019974237941900003, 0.00019974237941900003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38010 loss: 0.7453 iter time (s): 5.483 samples/sec: 23.344
g0198:  iteration    38010/10000000 | consumed samples:      4865280 | consumed tokens:   9964093440 | elapsed time per iteration (ms): 5519.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.592058E-01 | loss scale: 4096.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.191 | tokens per gpu per second (tgs): 1484.226 | TFLOPs: 11.94 |
g0184: [Rank 0] (after 38010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11010.0 | max reserved: 11010.0
g0194: [Rank 16] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6328.0 | max reserved: 6328.0
g0185: [Rank 4] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 9010.0 | max reserved: 9010.0
g0188: [Rank 12] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7222.0 | max reserved: 7222.0
g0195: [Rank 20] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5414.0 | max reserved: 5414.0
g0187: [Rank 8] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8116.0 | max reserved: 8116.0
g0197: [Rank 24] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0198: [Rank 28] (after 38010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0184: [2024-08-11 08:19:41,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=38020, skipped=58, lr=[0.0001997421855147265, 0.0001997421855147265], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38020 loss: 0.7869 iter time (s): 4.015 samples/sec: 31.883
g0198:  iteration    38020/10000000 | consumed samples:      4866560 | consumed tokens:   9966714880 | elapsed time per iteration (ms): 4188.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.603631E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.564 | tokens per gpu per second (tgs): 1956.068 | TFLOPs: 15.74 |
g0184: [2024-08-11 08:20:23,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=38030, skipped=58, lr=[0.00019974199153760625, 0.00019974199153760625], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38030 loss: 0.7587 iter time (s): 4.102 samples/sec: 31.206
g0198:  iteration    38030/10000000 | consumed samples:      4867840 | consumed tokens:   9969336320 | elapsed time per iteration (ms): 4135.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.550414E-01 | loss scale: 4096.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.954 | tokens per gpu per second (tgs): 1981.042 | TFLOPs: 15.94 |
g0184: [2024-08-11 08:21:03,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=38040, skipped=58, lr=[0.00019974179748763943, 0.00019974179748763943], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38040 loss: 0.7300 iter time (s): 4.038 samples/sec: 31.698
g0198:  iteration    38040/10000000 | consumed samples:      4869120 | consumed tokens:   9971957760 | elapsed time per iteration (ms): 4071.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.448461E-01 | loss scale: 4096.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.440 | tokens per gpu per second (tgs): 2012.137 | TFLOPs: 16.19 |
g0184: [2024-08-11 08:21:44,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=38050, skipped=58, lr=[0.00019974160336482612, 0.00019974160336482612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38050 loss: 0.7808 iter time (s): 3.993 samples/sec: 32.058
g0198:  iteration    38050/10000000 | consumed samples:      4870400 | consumed tokens:   9974579200 | elapsed time per iteration (ms): 4026.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.511877E-01 | loss scale: 4096.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.792 | tokens per gpu per second (tgs): 2034.707 | TFLOPs: 16.37 |
g0184: [2024-08-11 08:22:25,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=38060, skipped=58, lr=[0.00019974140916916656, 0.00019974140916916656], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38060 loss: 0.7483 iter time (s): 4.066 samples/sec: 31.481
g0198:  iteration    38060/10000000 | consumed samples:      4871680 | consumed tokens:   9977200640 | elapsed time per iteration (ms): 4098.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.611133E-01 | loss scale: 4096.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.228 | tokens per gpu per second (tgs): 1998.568 | TFLOPs: 16.08 |
g0184: [2024-08-11 08:23:05,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=38070, skipped=58, lr=[0.00019974121490066088, 0.00019974121490066088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38070 loss: 0.7281 iter time (s): 3.947 samples/sec: 32.428
g0198:  iteration    38070/10000000 | consumed samples:      4872960 | consumed tokens:   9979822080 | elapsed time per iteration (ms): 3980.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.592323E-01 | loss scale: 4096.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.159 | tokens per gpu per second (tgs): 2058.162 | TFLOPs: 16.56 |
g0184: [2024-08-11 08:23:44,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=38080, skipped=58, lr=[0.00019974102055930916, 0.00019974102055930916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38080 loss: 0.7426 iter time (s): 3.956 samples/sec: 32.358
g0198:  iteration    38080/10000000 | consumed samples:      4874240 | consumed tokens:   9982443520 | elapsed time per iteration (ms): 3988.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.617031E-01 | loss scale: 4096.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.091 | tokens per gpu per second (tgs): 2053.832 | TFLOPs: 16.53 |
g0184: [2024-08-11 08:24:26,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=38090, skipped=58, lr=[0.00019974082614511167, 0.00019974082614511167], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38090 loss: 0.7692 iter time (s): 4.111 samples/sec: 31.136
g0198:  iteration    38090/10000000 | consumed samples:      4875520 | consumed tokens:   9985064960 | elapsed time per iteration (ms): 4143.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.408903E-01 | loss scale: 4096.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.889 | tokens per gpu per second (tgs): 1976.910 | TFLOPs: 15.91 |
g0184: [2024-08-11 08:25:06,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=38100, skipped=58, lr=[0.00019974063165806844, 0.00019974063165806844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38100 loss: 0.7379 iter time (s): 4.011 samples/sec: 31.911
g0198:  iteration    38100/10000000 | consumed samples:      4876800 | consumed tokens:   9987686400 | elapsed time per iteration (ms): 4044.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.570917E-01 | loss scale: 4096.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.652 | tokens per gpu per second (tgs): 2025.731 | TFLOPs: 16.30 |
g0184: [2024-08-11 08:25:48,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=38110, skipped=58, lr=[0.00019974043709817965, 0.00019974043709817965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38110 loss: 0.7350 iter time (s): 4.152 samples/sec: 30.827
g0198:  iteration    38110/10000000 | consumed samples:      4878080 | consumed tokens:   9990307840 | elapsed time per iteration (ms): 4185.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.484801E-01 | loss scale: 4096.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.582 | tokens per gpu per second (tgs): 1957.270 | TFLOPs: 15.75 |
g0184: [2024-08-11 08:26:29,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=38120, skipped=58, lr=[0.00019974024246544552, 0.00019974024246544552], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38120 loss: 0.7547 iter time (s): 4.052 samples/sec: 31.588
g0198:  iteration    38120/10000000 | consumed samples:      4879360 | consumed tokens:   9992929280 | elapsed time per iteration (ms): 4085.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.592866E-01 | loss scale: 4096.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.331 | tokens per gpu per second (tgs): 2005.171 | TFLOPs: 16.14 |
g0184: [2024-08-11 08:27:10,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=38130, skipped=58, lr=[0.00019974004775986614, 0.00019974004775986614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38130 loss: 0.7898 iter time (s): 4.041 samples/sec: 31.674
g0198:  iteration    38130/10000000 | consumed samples:      4880640 | consumed tokens:   9995550720 | elapsed time per iteration (ms): 4073.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.508269E-01 | loss scale: 4096.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.419 | tokens per gpu per second (tgs): 2010.842 | TFLOPs: 16.18 |
g0184: [2024-08-11 08:27:50,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=38140, skipped=58, lr=[0.00019973985298144166, 0.00019973985298144166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38140 loss: 0.7443 iter time (s): 3.978 samples/sec: 32.178
g0198:  iteration    38140/10000000 | consumed samples:      4881920 | consumed tokens:   9998172160 | elapsed time per iteration (ms): 4013.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.467436E-01 | loss scale: 4096.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.894 | tokens per gpu per second (tgs): 2041.194 | TFLOPs: 16.43 |
g0184: [2024-08-11 08:28:32,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=38150, skipped=58, lr=[0.00019973965813017223, 0.00019973965813017223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38150 loss: 0.7508 iter time (s): 4.135 samples/sec: 30.953
g0198:  iteration    38150/10000000 | consumed samples:      4883200 | consumed tokens:  10000793600 | elapsed time per iteration (ms): 4168.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.514725E-01 | loss scale: 4096.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.710 | tokens per gpu per second (tgs): 1965.409 | TFLOPs: 15.82 |
g0184: [2024-08-11 08:29:12,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=38160, skipped=58, lr=[0.00019973946320605803, 0.00019973946320605803], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38160 loss: 0.7174 iter time (s): 4.039 samples/sec: 31.689
g0198:  iteration    38160/10000000 | consumed samples:      4884480 | consumed tokens:  10003415040 | elapsed time per iteration (ms): 4072.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.450418E-01 | loss scale: 4096.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.433 | tokens per gpu per second (tgs): 2011.725 | TFLOPs: 16.19 |
g0184: [2024-08-11 08:29:54,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=38170, skipped=58, lr=[0.00019973926820909916, 0.00019973926820909916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38170 loss: 0.7366 iter time (s): 4.098 samples/sec: 31.233
g0198:  iteration    38170/10000000 | consumed samples:      4885760 | consumed tokens:  10006036480 | elapsed time per iteration (ms): 4130.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.386120E-01 | loss scale: 4096.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.986 | tokens per gpu per second (tgs): 1983.126 | TFLOPs: 15.96 |
g0184: [2024-08-11 08:30:35,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=38180, skipped=58, lr=[0.00019973907313929586, 0.00019973907313929586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38180 loss: 0.7468 iter time (s): 4.064 samples/sec: 31.493
g0198:  iteration    38180/10000000 | consumed samples:      4887040 | consumed tokens:  10008657920 | elapsed time per iteration (ms): 4097.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.549607E-01 | loss scale: 4096.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.241 | tokens per gpu per second (tgs): 1999.402 | TFLOPs: 16.09 |
g0184: [2024-08-11 08:31:15,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=38190, skipped=58, lr=[0.00019973887799664813, 0.00019973887799664813], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38190 loss: 0.7439 iter time (s): 3.989 samples/sec: 32.088
g0198:  iteration    38190/10000000 | consumed samples:      4888320 | consumed tokens:  10011279360 | elapsed time per iteration (ms): 4022.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.547708E-01 | loss scale: 4096.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.825 | tokens per gpu per second (tgs): 2036.769 | TFLOPs: 16.39 |
g0184: [2024-08-11 08:31:56,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=38200, skipped=58, lr=[0.00019973868278115627, 0.00019973868278115627], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38200 loss: 0.7448 iter time (s): 4.065 samples/sec: 31.491
g0198:  iteration    38200/10000000 | consumed samples:      4889600 | consumed tokens:  10013900800 | elapsed time per iteration (ms): 4097.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.553466E-01 | loss scale: 4096.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.236 | tokens per gpu per second (tgs): 1999.133 | TFLOPs: 16.09 |
g0194: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0195: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 08:32:12,876] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0188: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0198: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0197: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0187: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0194: [2024-08-11 08:32:12,878] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0185: [2024-08-11 08:32:12,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0184: [2024-08-11 08:32:38,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=38210, skipped=58, lr=[0.00019973848749282034, 0.00019973848749282034], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38210 loss: 0.7533 iter time (s): 4.164 samples/sec: 30.737
g0198:  iteration    38210/10000000 | consumed samples:      4890880 | consumed tokens:  10016522240 | elapsed time per iteration (ms): 4197.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.504338E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.495 | tokens per gpu per second (tgs): 1951.680 | TFLOPs: 15.71 |
g0184: [2024-08-11 08:33:19,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=38220, skipped=58, lr=[0.00019973829213164055, 0.00019973829213164055], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38220 loss: 0.7578 iter time (s): 4.102 samples/sec: 31.207
g0198:  iteration    38220/10000000 | consumed samples:      4892160 | consumed tokens:  10019143680 | elapsed time per iteration (ms): 4134.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.553690E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.958 | tokens per gpu per second (tgs): 1981.288 | TFLOPs: 15.94 |
g0184: [2024-08-11 08:34:01,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=38230, skipped=58, lr=[0.00019973809669761699, 0.00019973809669761699], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38230 loss: 0.7534 iter time (s): 4.145 samples/sec: 30.883
g0198:  iteration    38230/10000000 | consumed samples:      4893440 | consumed tokens:  10021765120 | elapsed time per iteration (ms): 4177.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.548211E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.638 | tokens per gpu per second (tgs): 1960.836 | TFLOPs: 15.78 |
g0184: [2024-08-11 08:34:42,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=38240, skipped=58, lr=[0.00019973790119074983, 0.00019973790119074983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38240 loss: 0.7486 iter time (s): 4.063 samples/sec: 31.504
g0198:  iteration    38240/10000000 | consumed samples:      4894720 | consumed tokens:  10024386560 | elapsed time per iteration (ms): 4095.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.499112E-01 | loss scale: 8192.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.253 | tokens per gpu per second (tgs): 2000.198 | TFLOPs: 16.10 |
g0184: [2024-08-11 08:35:23,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=38250, skipped=58, lr=[0.00019973770561103926, 0.00019973770561103926], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38250 loss: 0.7652 iter time (s): 4.059 samples/sec: 31.532
g0198:  iteration    38250/10000000 | consumed samples:      4896000 | consumed tokens:  10027008000 | elapsed time per iteration (ms): 4093.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.560279E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.268 | tokens per gpu per second (tgs): 2001.132 | TFLOPs: 16.10 |
g0184: [2024-08-11 08:36:04,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=38260, skipped=58, lr=[0.00019973750995848537, 0.00019973750995848537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38260 loss: 0.7609 iter time (s): 4.063 samples/sec: 31.503
g0198:  iteration    38260/10000000 | consumed samples:      4897280 | consumed tokens:  10029629440 | elapsed time per iteration (ms): 4096.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.558442E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.250 | tokens per gpu per second (tgs): 2000.017 | TFLOPs: 16.09 |
g0184: [2024-08-11 08:36:44,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=38270, skipped=58, lr=[0.00019973731423308838, 0.00019973731423308838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38270 loss: 0.7813 iter time (s): 4.042 samples/sec: 31.665
g0198:  iteration    38270/10000000 | consumed samples:      4898560 | consumed tokens:  10032250880 | elapsed time per iteration (ms): 4077.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.490617E-01 | loss scale: 8192.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.395 | tokens per gpu per second (tgs): 2009.292 | TFLOPs: 16.17 |
g0184: [2024-08-11 08:37:26,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=38280, skipped=58, lr=[0.00019973711843484837, 0.00019973711843484837], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38280 loss: 0.7994 iter time (s): 4.094 samples/sec: 31.265
g0198:  iteration    38280/10000000 | consumed samples:      4899840 | consumed tokens:  10034872320 | elapsed time per iteration (ms): 4127.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.635970E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.012 | tokens per gpu per second (tgs): 1984.742 | TFLOPs: 15.97 |
g0184: [2024-08-11 08:38:05,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=38290, skipped=58, lr=[0.00019973692256376553, 0.00019973692256376553], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38290 loss: 0.7349 iter time (s): 3.908 samples/sec: 32.756
g0198:  iteration    38290/10000000 | consumed samples:      4901120 | consumed tokens:  10037493760 | elapsed time per iteration (ms): 3940.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.548490E-01 | loss scale: 8192.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.483 | tokens per gpu per second (tgs): 2078.921 | TFLOPs: 16.73 |
g0184: [2024-08-11 08:38:46,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=38300, skipped=58, lr=[0.00019973672661984, 0.00019973672661984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38300 loss: 0.7512 iter time (s): 4.035 samples/sec: 31.720
g0198:  iteration    38300/10000000 | consumed samples:      4902400 | consumed tokens:  10040115200 | elapsed time per iteration (ms): 4068.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.518885E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.464 | tokens per gpu per second (tgs): 2013.670 | TFLOPs: 16.20 |
g0184: [2024-08-11 08:39:27,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=38310, skipped=58, lr=[0.00019973653060307192, 0.00019973653060307192], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38310 loss: 0.7558 iter time (s): 4.067 samples/sec: 31.472
g0198:  iteration    38310/10000000 | consumed samples:      4903680 | consumed tokens:  10042736640 | elapsed time per iteration (ms): 4100.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.523413E-01 | loss scale: 8192.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.219 | tokens per gpu per second (tgs): 1998.024 | TFLOPs: 16.08 |
g0184: [2024-08-11 08:40:08,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=38320, skipped=58, lr=[0.00019973633451346147, 0.00019973633451346147], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38320 loss: 0.7605 iter time (s): 4.063 samples/sec: 31.504
g0198:  iteration    38320/10000000 | consumed samples:      4904960 | consumed tokens:  10045358080 | elapsed time per iteration (ms): 4095.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.528093E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.253 | tokens per gpu per second (tgs): 2000.174 | TFLOPs: 16.10 |
g0184: [2024-08-11 08:40:49,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=38330, skipped=58, lr=[0.0001997361383510088, 0.0001997361383510088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38330 loss: 0.7541 iter time (s): 4.067 samples/sec: 31.471
g0198:  iteration    38330/10000000 | consumed samples:      4906240 | consumed tokens:  10047979520 | elapsed time per iteration (ms): 4099.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.522038E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.221 | tokens per gpu per second (tgs): 1998.120 | TFLOPs: 16.08 |
g0184: [2024-08-11 08:41:30,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=38340, skipped=58, lr=[0.000199735942115714, 0.000199735942115714], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38340 loss: 0.7693 iter time (s): 4.079 samples/sec: 31.381
g0198:  iteration    38340/10000000 | consumed samples:      4907520 | consumed tokens:  10050600960 | elapsed time per iteration (ms): 4111.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.608537E-01 | loss scale: 8192.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.132 | tokens per gpu per second (tgs): 1992.445 | TFLOPs: 16.03 |
g0184: [2024-08-11 08:42:12,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=38350, skipped=58, lr=[0.0001997357458075773, 0.0001997357458075773], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38350 loss: 0.7668 iter time (s): 4.174 samples/sec: 30.666
g0198:  iteration    38350/10000000 | consumed samples:      4908800 | consumed tokens:  10053222400 | elapsed time per iteration (ms): 4207.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.601309E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1946.993 | TFLOPs: 15.67 |
g0184: [2024-08-11 08:42:54,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=38360, skipped=58, lr=[0.00019973554942659882, 0.00019973554942659882], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38360 loss: 0.7761 iter time (s): 4.135 samples/sec: 30.959
g0198:  iteration    38360/10000000 | consumed samples:      4910080 | consumed tokens:  10055843840 | elapsed time per iteration (ms): 4167.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.534804E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.716 | tokens per gpu per second (tgs): 1965.828 | TFLOPs: 15.82 |
g0184: [2024-08-11 08:43:36,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=38370, skipped=58, lr=[0.00019973535297277868, 0.00019973535297277868], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38370 loss: 0.7635 iter time (s): 4.185 samples/sec: 30.589
g0198:  iteration    38370/10000000 | consumed samples:      4911360 | consumed tokens:  10058465280 | elapsed time per iteration (ms): 4218.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.554499E-01 | loss scale: 8192.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.345 | tokens per gpu per second (tgs): 1942.099 | TFLOPs: 15.63 |
g0184: [2024-08-11 08:44:18,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=38380, skipped=58, lr=[0.00019973515644611709, 0.00019973515644611709], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38380 loss: 0.7343 iter time (s): 4.216 samples/sec: 30.360
g0198:  iteration    38380/10000000 | consumed samples:      4912640 | consumed tokens:  10061086720 | elapsed time per iteration (ms): 4248.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.387734E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.126 | tokens per gpu per second (tgs): 1928.071 | TFLOPs: 15.52 |
g0184: [2024-08-11 08:45:02,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=38390, skipped=58, lr=[0.00019973495984661416, 0.00019973495984661416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38390 loss: 0.7244 iter time (s): 4.287 samples/sec: 29.860
g0198:  iteration    38390/10000000 | consumed samples:      4913920 | consumed tokens:  10063708160 | elapsed time per iteration (ms): 4319.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.506431E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.634 | tokens per gpu per second (tgs): 1896.549 | TFLOPs: 15.26 |
g0184: [2024-08-11 08:45:45,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=38400, skipped=58, lr=[0.00019973476317427006, 0.00019973476317427006], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38400 loss: 0.7563 iter time (s): 4.364 samples/sec: 29.333
g0198:  iteration    38400/10000000 | consumed samples:      4915200 | consumed tokens:  10066329600 | elapsed time per iteration (ms): 4396.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.527652E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.115 | tokens per gpu per second (tgs): 1863.387 | TFLOPs: 14.99 |
g0184: [2024-08-11 08:46:27,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=38410, skipped=58, lr=[0.00019973456642908492, 0.00019973456642908492], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38410 loss: 0.7563 iter time (s): 4.108 samples/sec: 31.162
g0198:  iteration    38410/10000000 | consumed samples:      4916480 | consumed tokens:  10068951040 | elapsed time per iteration (ms): 4140.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.403976E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.913 | tokens per gpu per second (tgs): 1978.445 | TFLOPs: 15.92 |
g0184: [2024-08-11 08:47:08,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=38420, skipped=58, lr=[0.0001997343696110589, 0.0001997343696110589], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38420 loss: 0.7639 iter time (s): 4.107 samples/sec: 31.168
g0198:  iteration    38420/10000000 | consumed samples:      4917760 | consumed tokens:  10071572480 | elapsed time per iteration (ms): 4139.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.596707E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.921 | tokens per gpu per second (tgs): 1978.927 | TFLOPs: 15.92 |
g0184: [2024-08-11 08:47:50,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=38430, skipped=58, lr=[0.00019973417272019212, 0.00019973417272019212], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38430 loss: 0.8336 iter time (s): 4.140 samples/sec: 30.921
g0198:  iteration    38430/10000000 | consumed samples:      4919040 | consumed tokens:  10074193920 | elapsed time per iteration (ms): 4172.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.670343E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.679 | tokens per gpu per second (tgs): 1963.464 | TFLOPs: 15.80 |
g0184: [2024-08-11 08:48:32,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=38440, skipped=58, lr=[0.00019973397575648481, 0.00019973397575648481], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38440 loss: 0.7536 iter time (s): 4.155 samples/sec: 30.803
g0198:  iteration    38440/10000000 | consumed samples:      4920320 | consumed tokens:  10076815360 | elapsed time per iteration (ms): 4188.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.594542E-01 | loss scale: 8192.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.557 | tokens per gpu per second (tgs): 1955.637 | TFLOPs: 15.74 |
g0184: [2024-08-11 08:49:14,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=38450, skipped=58, lr=[0.0001997337787199371, 0.0001997337787199371], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38450 loss: 0.7727 iter time (s): 4.153 samples/sec: 30.818
g0198:  iteration    38450/10000000 | consumed samples:      4921600 | consumed tokens:  10079436800 | elapsed time per iteration (ms): 4186.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.536134E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.576 | tokens per gpu per second (tgs): 1956.893 | TFLOPs: 15.75 |
g0184: [2024-08-11 08:49:57,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=38460, skipped=58, lr=[0.0001997335816105491, 0.0001997335816105491], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38460 loss: 0.7433 iter time (s): 4.329 samples/sec: 29.565
g0198:  iteration    38460/10000000 | consumed samples:      4922880 | consumed tokens:  10082058240 | elapsed time per iteration (ms): 4362.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.506543E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.343 | tokens per gpu per second (tgs): 1877.970 | TFLOPs: 15.11 |
g0184: [2024-08-11 08:50:40,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=38470, skipped=58, lr=[0.00019973338442832094, 0.00019973338442832094], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38470 loss: 0.7474 iter time (s): 4.199 samples/sec: 30.482
g0198:  iteration    38470/10000000 | consumed samples:      4924160 | consumed tokens:  10084679680 | elapsed time per iteration (ms): 4232.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.520923E-01 | loss scale: 8192.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.242 | tokens per gpu per second (tgs): 1935.489 | TFLOPs: 15.58 |
g0184: [2024-08-11 08:51:23,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=38480, skipped=58, lr=[0.00019973318717325284, 0.00019973318717325284], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38480 loss: 0.7451 iter time (s): 4.285 samples/sec: 29.873
g0198:  iteration    38480/10000000 | consumed samples:      4925440 | consumed tokens:  10087301120 | elapsed time per iteration (ms): 4318.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.581155E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.642 | tokens per gpu per second (tgs): 1897.119 | TFLOPs: 15.27 |
g0184: [2024-08-11 08:52:04,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=38490, skipped=58, lr=[0.00019973298984534492, 0.00019973298984534492], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38490 loss: 0.7333 iter time (s): 4.034 samples/sec: 31.729
g0198:  iteration    38490/10000000 | consumed samples:      4926720 | consumed tokens:  10089922560 | elapsed time per iteration (ms): 4067.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.487683E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.472 | tokens per gpu per second (tgs): 2014.230 | TFLOPs: 16.21 |
g0184: [2024-08-11 08:52:45,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=38500, skipped=58, lr=[0.00019973279244459737, 0.00019973279244459737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38500 loss: 0.7871 iter time (s): 4.089 samples/sec: 31.305
g0198:  iteration    38500/10000000 | consumed samples:      4928000 | consumed tokens:  10092544000 | elapsed time per iteration (ms): 4121.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.555141E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.056 | tokens per gpu per second (tgs): 1987.567 | TFLOPs: 15.99 |
g0184: [2024-08-11 08:53:26,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=38510, skipped=58, lr=[0.0001997325949710103, 0.0001997325949710103], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38510 loss: 0.7416 iter time (s): 4.137 samples/sec: 30.940
g0198:  iteration    38510/10000000 | consumed samples:      4929280 | consumed tokens:  10095165440 | elapsed time per iteration (ms): 4170.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.450992E-01 | loss scale: 8192.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.694 | tokens per gpu per second (tgs): 1964.430 | TFLOPs: 15.81 |
g0184: [2024-08-11 08:54:10,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=38520, skipped=58, lr=[0.00019973239742458385, 0.00019973239742458385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38520 loss: 0.7163 iter time (s): 4.278 samples/sec: 29.922
g0198:  iteration    38520/10000000 | consumed samples:      4930560 | consumed tokens:  10097786880 | elapsed time per iteration (ms): 4310.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.495302E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.694 | tokens per gpu per second (tgs): 1900.393 | TFLOPs: 15.29 |
g0184: [2024-08-11 08:54:53,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=38530, skipped=58, lr=[0.00019973219980531817, 0.00019973219980531817], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38530 loss: 0.7450 iter time (s): 4.307 samples/sec: 29.720
g0198:  iteration    38530/10000000 | consumed samples:      4931840 | consumed tokens:  10100408320 | elapsed time per iteration (ms): 4340.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.420140E-01 | loss scale: 8192.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.493 | tokens per gpu per second (tgs): 1887.557 | TFLOPs: 15.19 |
g0184: [2024-08-11 08:55:35,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=38540, skipped=58, lr=[0.00019973200211321346, 0.00019973200211321346], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38540 loss: 0.7788 iter time (s): 4.183 samples/sec: 30.598
g0198:  iteration    38540/10000000 | consumed samples:      4933120 | consumed tokens:  10103029760 | elapsed time per iteration (ms): 4215.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.492096E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.362 | tokens per gpu per second (tgs): 1943.146 | TFLOPs: 15.64 |
g0184: [2024-08-11 08:56:15,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=38550, skipped=58, lr=[0.00019973180434826985, 0.00019973180434826985], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38550 loss: 0.7266 iter time (s): 3.980 samples/sec: 32.165
g0198:  iteration    38550/10000000 | consumed samples:      4934400 | consumed tokens:  10105651200 | elapsed time per iteration (ms): 4012.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.527991E-01 | loss scale: 8192.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.899 | tokens per gpu per second (tgs): 2041.564 | TFLOPs: 16.43 |
g0184: [2024-08-11 08:56:58,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=38560, skipped=58, lr=[0.00019973160651048745, 0.00019973160651048745], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38560 loss: 0.7443 iter time (s): 4.236 samples/sec: 30.218
g0198:  iteration    38560/10000000 | consumed samples:      4935680 | consumed tokens:  10108272640 | elapsed time per iteration (ms): 4268.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.468234E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.984 | tokens per gpu per second (tgs): 1918.999 | TFLOPs: 15.44 |
g0184: [2024-08-11 08:57:41,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=38570, skipped=58, lr=[0.0001997314085998665, 0.0001997314085998665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38570 loss: 0.7599 iter time (s): 4.264 samples/sec: 30.019
g0198:  iteration    38570/10000000 | consumed samples:      4936960 | consumed tokens:  10110894080 | elapsed time per iteration (ms): 4297.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.463608E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.785 | tokens per gpu per second (tgs): 1906.258 | TFLOPs: 15.34 |
g0184: [2024-08-11 08:58:23,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=38580, skipped=58, lr=[0.00019973121061640708, 0.00019973121061640708], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38580 loss: 0.7217 iter time (s): 4.190 samples/sec: 30.553
g0198:  iteration    38580/10000000 | consumed samples:      4938240 | consumed tokens:  10113515520 | elapsed time per iteration (ms): 4225.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.499611E-01 | loss scale: 8192.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.294 | tokens per gpu per second (tgs): 1938.825 | TFLOPs: 15.60 |
g0184: [2024-08-11 08:59:05,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=38590, skipped=58, lr=[0.00019973101256010934, 0.00019973101256010934], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38590 loss: 0.7833 iter time (s): 4.108 samples/sec: 31.161
g0198:  iteration    38590/10000000 | consumed samples:      4939520 | consumed tokens:  10116136960 | elapsed time per iteration (ms): 4140.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.594821E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.913 | tokens per gpu per second (tgs): 1978.413 | TFLOPs: 15.92 |
g0184: [2024-08-11 08:59:48,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=38600, skipped=58, lr=[0.0001997308144309735, 0.0001997308144309735], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38600 loss: 0.7830 iter time (s): 4.283 samples/sec: 29.888
g0198:  iteration    38600/10000000 | consumed samples:      4940800 | consumed tokens:  10118758400 | elapsed time per iteration (ms): 4317.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.531394E-01 | loss scale: 8192.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.646 | tokens per gpu per second (tgs): 1897.336 | TFLOPs: 15.27 |
g0184: [2024-08-11 09:00:31,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=38610, skipped=58, lr=[0.00019973061622899965, 0.00019973061622899965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38610 loss: 0.7323 iter time (s): 4.250 samples/sec: 30.120
g0198:  iteration    38610/10000000 | consumed samples:      4942080 | consumed tokens:  10121379840 | elapsed time per iteration (ms): 4282.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.510876E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.890 | tokens per gpu per second (tgs): 1912.932 | TFLOPs: 15.39 |
g0184: [2024-08-11 09:01:12,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=38620, skipped=58, lr=[0.00019973041795418796, 0.00019973041795418796], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38620 loss: 0.7618 iter time (s): 4.148 samples/sec: 30.862
g0198:  iteration    38620/10000000 | consumed samples:      4943360 | consumed tokens:  10124001280 | elapsed time per iteration (ms): 4182.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.574769E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.605 | tokens per gpu per second (tgs): 1958.720 | TFLOPs: 15.76 |
g0184: [2024-08-11 09:01:56,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=38630, skipped=58, lr=[0.0001997302196065386, 0.0001997302196065386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38630 loss: 0.7410 iter time (s): 4.340 samples/sec: 29.492
g0198:  iteration    38630/10000000 | consumed samples:      4944640 | consumed tokens:  10126622720 | elapsed time per iteration (ms): 4373.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.530030E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.269 | tokens per gpu per second (tgs): 1873.239 | TFLOPs: 15.07 |
g0184: [2024-08-11 09:02:38,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=38640, skipped=58, lr=[0.00019973002118605165, 0.00019973002118605165], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38640 loss: 0.7572 iter time (s): 4.144 samples/sec: 30.885
g0198:  iteration    38640/10000000 | consumed samples:      4945920 | consumed tokens:  10129244160 | elapsed time per iteration (ms): 4177.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.566241E-01 | loss scale: 8192.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.640 | tokens per gpu per second (tgs): 1960.976 | TFLOPs: 15.78 |
g0184: [2024-08-11 09:03:19,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=38650, skipped=58, lr=[0.0001997298226927274, 0.0001997298226927274], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38650 loss: 0.7722 iter time (s): 4.059 samples/sec: 31.533
g0198:  iteration    38650/10000000 | consumed samples:      4947200 | consumed tokens:  10131865600 | elapsed time per iteration (ms): 4091.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.581325E-01 | loss scale: 8192.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.283 | tokens per gpu per second (tgs): 2002.088 | TFLOPs: 16.11 |
g0184: [2024-08-11 09:04:00,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=38660, skipped=58, lr=[0.00019972962412656586, 0.00019972962412656586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38660 loss: 0.7611 iter time (s): 4.103 samples/sec: 31.199
g0198:  iteration    38660/10000000 | consumed samples:      4948480 | consumed tokens:  10134487040 | elapsed time per iteration (ms): 4135.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.460008E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.951 | tokens per gpu per second (tgs): 1980.876 | TFLOPs: 15.94 |
g0184: [2024-08-11 09:04:43,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=38670, skipped=58, lr=[0.00019972942548756727, 0.00019972942548756727], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38670 loss: 0.7699 iter time (s): 4.282 samples/sec: 29.892
g0198:  iteration    38670/10000000 | consumed samples:      4949760 | consumed tokens:  10137108480 | elapsed time per iteration (ms): 4315.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.579086E-01 | loss scale: 8192.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.661 | tokens per gpu per second (tgs): 1898.330 | TFLOPs: 15.28 |
g0184: [2024-08-11 09:05:24,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=38680, skipped=58, lr=[0.00019972922677573174, 0.00019972922677573174], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38680 loss: 0.7729 iter time (s): 4.024 samples/sec: 31.812
g0198:  iteration    38680/10000000 | consumed samples:      4951040 | consumed tokens:  10139729920 | elapsed time per iteration (ms): 4056.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.527562E-01 | loss scale: 8192.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.552 | tokens per gpu per second (tgs): 2019.299 | TFLOPs: 16.25 |
g0184: [2024-08-11 09:06:06,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=38690, skipped=58, lr=[0.00019972902799105945, 0.00019972902799105945], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38690 loss: 0.7744 iter time (s): 4.172 samples/sec: 30.679
g0198:  iteration    38690/10000000 | consumed samples:      4952320 | consumed tokens:  10142351360 | elapsed time per iteration (ms): 4205.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.568133E-01 | loss scale: 8192.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.435 | tokens per gpu per second (tgs): 1947.837 | TFLOPs: 15.67 |
g0184: [2024-08-11 09:06:47,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=38700, skipped=58, lr=[0.00019972882913355054, 0.00019972882913355054], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38700 loss: 0.7485 iter time (s): 4.076 samples/sec: 31.404
g0198:  iteration    38700/10000000 | consumed samples:      4953600 | consumed tokens:  10144972800 | elapsed time per iteration (ms): 4109.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.522268E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.144 | tokens per gpu per second (tgs): 1993.244 | TFLOPs: 16.04 |
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 09:07:03,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 09:07:03,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 09:07:29,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=38710, skipped=58, lr=[0.00019972863020320514, 0.00019972863020320514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38710 loss: 0.7314 iter time (s): 4.161 samples/sec: 30.763
g0198:  iteration    38710/10000000 | consumed samples:      4954880 | consumed tokens:  10147594240 | elapsed time per iteration (ms): 4193.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.530904E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.522 | tokens per gpu per second (tgs): 1953.394 | TFLOPs: 15.72 |
g0184: [2024-08-11 09:08:11,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=38720, skipped=58, lr=[0.00019972843120002345, 0.00019972843120002345], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38720 loss: 0.7651 iter time (s): 4.140 samples/sec: 30.917
g0198:  iteration    38720/10000000 | consumed samples:      4956160 | consumed tokens:  10150215680 | elapsed time per iteration (ms): 4172.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.548331E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.676 | tokens per gpu per second (tgs): 1963.239 | TFLOPs: 15.80 |
g0184: [2024-08-11 09:08:53,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=38730, skipped=58, lr=[0.00019972823212400563, 0.00019972823212400563], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38730 loss: 0.7191 iter time (s): 4.182 samples/sec: 30.604
g0198:  iteration    38730/10000000 | consumed samples:      4957440 | consumed tokens:  10152837120 | elapsed time per iteration (ms): 4215.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.538603E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.364 | tokens per gpu per second (tgs): 1943.317 | TFLOPs: 15.64 |
g0184: [2024-08-11 09:09:35,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=38740, skipped=58, lr=[0.00019972803297515175, 0.00019972803297515175], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38740 loss: 0.7649 iter time (s): 4.197 samples/sec: 30.499
g0198:  iteration    38740/10000000 | consumed samples:      4958720 | consumed tokens:  10155458560 | elapsed time per iteration (ms): 4229.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.485067E-01 | loss scale: 16384.0 | grad norm: 1.036 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.262 | tokens per gpu per second (tgs): 1936.778 | TFLOPs: 15.59 |
g0184: [2024-08-11 09:10:17,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=38750, skipped=58, lr=[0.00019972783375346204, 0.00019972783375346204], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38750 loss: 0.7384 iter time (s): 4.171 samples/sec: 30.689
g0198:  iteration    38750/10000000 | consumed samples:      4960000 | consumed tokens:  10158080000 | elapsed time per iteration (ms): 4204.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.571152E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.447 | tokens per gpu per second (tgs): 1948.636 | TFLOPs: 15.68 |
g0184: [2024-08-11 09:10:59,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=38760, skipped=58, lr=[0.00019972763445893664, 0.00019972763445893664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38760 loss: 0.7527 iter time (s): 4.179 samples/sec: 30.629
g0198:  iteration    38760/10000000 | consumed samples:      4961280 | consumed tokens:  10160701440 | elapsed time per iteration (ms): 4211.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.554010E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.391 | tokens per gpu per second (tgs): 1945.017 | TFLOPs: 15.65 |
g0184: [2024-08-11 09:11:41,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=38770, skipped=58, lr=[0.0001997274350915757, 0.0001997274350915757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38770 loss: 0.7546 iter time (s): 4.175 samples/sec: 30.658
g0198:  iteration    38770/10000000 | consumed samples:      4962560 | consumed tokens:  10163322880 | elapsed time per iteration (ms): 4207.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.516863E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.420 | tokens per gpu per second (tgs): 1946.904 | TFLOPs: 15.67 |
g0184: [2024-08-11 09:12:24,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=38780, skipped=58, lr=[0.00019972723565137934, 0.00019972723565137934], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38780 loss: 0.7256 iter time (s): 4.193 samples/sec: 30.527
g0198:  iteration    38780/10000000 | consumed samples:      4963840 | consumed tokens:  10165944320 | elapsed time per iteration (ms): 4226.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.440044E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.285 | tokens per gpu per second (tgs): 1938.262 | TFLOPs: 15.60 |
g0184: [2024-08-11 09:13:06,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=38790, skipped=58, lr=[0.0001997270361383478, 0.0001997270361383478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38790 loss: 0.7391 iter time (s): 4.219 samples/sec: 30.337
g0198:  iteration    38790/10000000 | consumed samples:      4965120 | consumed tokens:  10168565760 | elapsed time per iteration (ms): 4253.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.560375E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.094 | tokens per gpu per second (tgs): 1926.026 | TFLOPs: 15.50 |
g0184: [2024-08-11 09:13:49,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=38800, skipped=58, lr=[0.0001997268365524811, 0.0001997268365524811], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38800 loss: 0.7552 iter time (s): 4.194 samples/sec: 30.518
g0198:  iteration    38800/10000000 | consumed samples:      4966400 | consumed tokens:  10171187200 | elapsed time per iteration (ms): 4227.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.509885E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.276 | tokens per gpu per second (tgs): 1937.674 | TFLOPs: 15.59 |
g0184: [2024-08-11 09:14:33,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=38810, skipped=58, lr=[0.00019972663689377947, 0.00019972663689377947], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38810 loss: 0.7301 iter time (s): 4.395 samples/sec: 29.121
g0198:  iteration    38810/10000000 | consumed samples:      4967680 | consumed tokens:  10173808640 | elapsed time per iteration (ms): 4429.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.505826E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.898 | tokens per gpu per second (tgs): 1849.448 | TFLOPs: 14.88 |
g0184: [2024-08-11 09:15:16,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=38820, skipped=58, lr=[0.0001997264371622431, 0.0001997264371622431], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38820 loss: 0.7251 iter time (s): 4.269 samples/sec: 29.985
g0198:  iteration    38820/10000000 | consumed samples:      4968960 | consumed tokens:  10176430080 | elapsed time per iteration (ms): 4301.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.427170E-01 | loss scale: 16384.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.755 | tokens per gpu per second (tgs): 1904.299 | TFLOPs: 15.32 |
g0184: [2024-08-11 09:15:58,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=38830, skipped=58, lr=[0.00019972623735787208, 0.00019972623735787208], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38830 loss: 0.7831 iter time (s): 4.214 samples/sec: 30.373
g0198:  iteration    38830/10000000 | consumed samples:      4970240 | consumed tokens:  10179051520 | elapsed time per iteration (ms): 4247.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.596974E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.135 | tokens per gpu per second (tgs): 1928.620 | TFLOPs: 15.52 |
g0184: [2024-08-11 09:16:41,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=38840, skipped=58, lr=[0.0001997260374806666, 0.0001997260374806666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38840 loss: 0.7370 iter time (s): 4.204 samples/sec: 30.449
g0198:  iteration    38840/10000000 | consumed samples:      4971520 | consumed tokens:  10181672960 | elapsed time per iteration (ms): 4236.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.603004E-01 | loss scale: 16384.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.216 | tokens per gpu per second (tgs): 1933.816 | TFLOPs: 15.56 |
g0184: [2024-08-11 09:17:22,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=38850, skipped=58, lr=[0.00019972583753062682, 0.00019972583753062682], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38850 loss: 0.7598 iter time (s): 4.079 samples/sec: 31.380
g0198:  iteration    38850/10000000 | consumed samples:      4972800 | consumed tokens:  10184294400 | elapsed time per iteration (ms): 4111.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.518831E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.133 | tokens per gpu per second (tgs): 1992.516 | TFLOPs: 16.03 |
g0184: [2024-08-11 09:18:05,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=38860, skipped=58, lr=[0.00019972563750775285, 0.00019972563750775285], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38860 loss: 0.7537 iter time (s): 4.264 samples/sec: 30.016
g0198:  iteration    38860/10000000 | consumed samples:      4974080 | consumed tokens:  10186915840 | elapsed time per iteration (ms): 4297.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.509296E-01 | loss scale: 16384.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.785 | tokens per gpu per second (tgs): 1906.247 | TFLOPs: 15.34 |
g0184: [2024-08-11 09:18:47,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=38870, skipped=58, lr=[0.00019972543741204486, 0.00019972543741204486], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38870 loss: 0.7296 iter time (s): 4.229 samples/sec: 30.269
g0198:  iteration    38870/10000000 | consumed samples:      4975360 | consumed tokens:  10189537280 | elapsed time per iteration (ms): 4261.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.466807E-01 | loss scale: 16384.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.037 | tokens per gpu per second (tgs): 1922.376 | TFLOPs: 15.47 |
g0184: [2024-08-11 09:19:29,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=38880, skipped=58, lr=[0.00019972523724350304, 0.00019972523724350304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38880 loss: 0.7297 iter time (s): 4.152 samples/sec: 30.830
g0198:  iteration    38880/10000000 | consumed samples:      4976640 | consumed tokens:  10192158720 | elapsed time per iteration (ms): 4184.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.402453E-01 | loss scale: 16384.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.589 | tokens per gpu per second (tgs): 1957.710 | TFLOPs: 15.75 |
g0184: [2024-08-11 09:20:11,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=38890, skipped=58, lr=[0.0001997250370021275, 0.0001997250370021275], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38890 loss: 0.7541 iter time (s): 4.139 samples/sec: 30.924
g0198:  iteration    38890/10000000 | consumed samples:      4977920 | consumed tokens:  10194780160 | elapsed time per iteration (ms): 4171.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.494611E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.682 | tokens per gpu per second (tgs): 1963.660 | TFLOPs: 15.80 |
g0184: [2024-08-11 09:20:54,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=38900, skipped=58, lr=[0.00019972483668791844, 0.00019972483668791844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38900 loss: 0.7676 iter time (s): 4.285 samples/sec: 29.872
g0198:  iteration    38900/10000000 | consumed samples:      4979200 | consumed tokens:  10197401600 | elapsed time per iteration (ms): 4317.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.612619E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.647 | tokens per gpu per second (tgs): 1897.388 | TFLOPs: 15.27 |
g0184: [2024-08-11 09:21:36,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=38910, skipped=58, lr=[0.00019972463630087595, 0.00019972463630087595], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38910 loss: 0.7857 iter time (s): 4.142 samples/sec: 30.900
g0198:  iteration    38910/10000000 | consumed samples:      4980480 | consumed tokens:  10200023040 | elapsed time per iteration (ms): 4175.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.506825E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.656 | tokens per gpu per second (tgs): 1961.997 | TFLOPs: 15.79 |
g0184: [2024-08-11 09:22:19,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=38920, skipped=58, lr=[0.0001997244358410002, 0.0001997244358410002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38920 loss: 0.7208 iter time (s): 4.318 samples/sec: 29.645
g0198:  iteration    38920/10000000 | consumed samples:      4981760 | consumed tokens:  10202644480 | elapsed time per iteration (ms): 4351.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.447162E-01 | loss scale: 16384.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.417 | tokens per gpu per second (tgs): 1882.714 | TFLOPs: 15.15 |
g0184: [2024-08-11 09:23:02,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=38930, skipped=58, lr=[0.0001997242353082914, 0.0001997242353082914], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38930 loss: 0.7572 iter time (s): 4.252 samples/sec: 30.106
g0198:  iteration    38930/10000000 | consumed samples:      4983040 | consumed tokens:  10205265920 | elapsed time per iteration (ms): 4292.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.428288E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.817 | tokens per gpu per second (tgs): 1908.314 | TFLOPs: 15.36 |
g0184: [2024-08-11 09:23:46,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=38940, skipped=58, lr=[0.00019972403470274965, 0.00019972403470274965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38940 loss: 0.7415 iter time (s): 4.344 samples/sec: 29.469
g0198:  iteration    38940/10000000 | consumed samples:      4984320 | consumed tokens:  10207887360 | elapsed time per iteration (ms): 4394.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.491149E-01 | loss scale: 16384.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.129 | tokens per gpu per second (tgs): 1864.284 | TFLOPs: 15.00 |
g0184: [2024-08-11 09:24:29,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=38950, skipped=58, lr=[0.00019972383402437513, 0.00019972383402437513], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38950 loss: 0.7404 iter time (s): 4.272 samples/sec: 29.965
g0198:  iteration    38950/10000000 | consumed samples:      4985600 | consumed tokens:  10210508800 | elapsed time per iteration (ms): 4304.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.532466E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.738 | tokens per gpu per second (tgs): 1903.256 | TFLOPs: 15.32 |
g0184: [2024-08-11 09:25:11,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=38960, skipped=58, lr=[0.000199723633273168, 0.000199723633273168], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38960 loss: 0.7514 iter time (s): 4.112 samples/sec: 31.125
g0198:  iteration    38960/10000000 | consumed samples:      4986880 | consumed tokens:  10213130240 | elapsed time per iteration (ms): 4145.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.476814E-01 | loss scale: 16384.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.880 | tokens per gpu per second (tgs): 1976.316 | TFLOPs: 15.90 |
g0184: [2024-08-11 09:25:53,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=38970, skipped=58, lr=[0.00019972343244912839, 0.00019972343244912839], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38970 loss: 0.7444 iter time (s): 4.169 samples/sec: 30.703
g0198:  iteration    38970/10000000 | consumed samples:      4988160 | consumed tokens:  10215751680 | elapsed time per iteration (ms): 4201.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.567925E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.465 | tokens per gpu per second (tgs): 1949.735 | TFLOPs: 15.69 |
g0184: [2024-08-11 09:26:36,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=38980, skipped=58, lr=[0.00019972323155225647, 0.00019972323155225647], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38980 loss: 0.7679 iter time (s): 4.296 samples/sec: 29.793
g0198:  iteration    38980/10000000 | consumed samples:      4989440 | consumed tokens:  10218373120 | elapsed time per iteration (ms): 4329.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.512215E-01 | loss scale: 16384.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.566 | tokens per gpu per second (tgs): 1892.213 | TFLOPs: 15.23 |
g0184: [2024-08-11 09:27:18,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=38990, skipped=58, lr=[0.00019972303058255237, 0.00019972303058255237], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 38990 loss: 0.7205 iter time (s): 4.157 samples/sec: 30.789
g0198:  iteration    38990/10000000 | consumed samples:      4990720 | consumed tokens:  10220994560 | elapsed time per iteration (ms): 4190.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.553080E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.546 | tokens per gpu per second (tgs): 1954.914 | TFLOPs: 15.73 |
g0184: [2024-08-11 09:28:00,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=58, lr=[0.00019972282954001624, 0.00019972282954001624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39000 loss: 0.7455 iter time (s): 4.207 samples/sec: 30.429
g0198:  iteration    39000/10000000 | consumed samples:      4992000 | consumed tokens:  10223616000 | elapsed time per iteration (ms): 4239.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.495035E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.194 | tokens per gpu per second (tgs): 1932.389 | TFLOPs: 15.55 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 39000 | lm loss value: 7.512360E-01 | lm loss PPL: 2.119618E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   39000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 09:34:41,876] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step39000 is about to be saved!
g0184: [2024-08-11 09:34:41,886] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0184: [2024-08-11 09:34:41,886] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0184: [2024-08-11 09:34:41,886] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0198: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0198: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0198: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0197: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0197: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0197: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0187: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0187: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0187: [2024-08-11 09:34:41,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0185: [2024-08-11 09:34:41,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0194: [2024-08-11 09:34:41,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0194: [2024-08-11 09:34:41,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0194: [2024-08-11 09:34:41,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0185: [2024-08-11 09:34:41,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0185: [2024-08-11 09:34:41,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0188: [2024-08-11 09:34:41,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0188: [2024-08-11 09:34:41,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0188: [2024-08-11 09:34:41,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0195: [2024-08-11 09:34:41,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0195: [2024-08-11 09:34:41,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0195: [2024-08-11 09:34:41,892] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0198: [2024-08-11 09:34:41,919] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_23-model_00-model_states.pt...
g0194: [2024-08-11 09:34:41,920] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_14-model_00-model_states.pt...
g0188: [2024-08-11 09:34:41,922] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_11-model_00-model_states.pt...
g0185: [2024-08-11 09:34:41,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_05-model_00-model_states.pt...
g0197: [2024-08-11 09:34:41,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_20-model_00-model_states.pt...
g0187: [2024-08-11 09:34:41,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_08-model_00-model_states.pt...
g0195: [2024-08-11 09:34:41,930] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_17-model_00-model_states.pt...
g0184: [2024-08-11 09:34:41,940] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_01-model_00-model_states.pt...
g0197: [2024-08-11 09:34:42,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_20-model_00-model_states.pt.
g0194: [2024-08-11 09:34:42,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_14-model_00-model_states.pt.
g0188: [2024-08-11 09:34:42,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_11-model_00-model_states.pt.
g0187: [2024-08-11 09:34:42,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_08-model_00-model_states.pt.
g0195: [2024-08-11 09:34:42,059] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_17-model_00-model_states.pt.
g0185: [2024-08-11 09:34:42,061] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_05-model_00-model_states.pt.
g0198: [2024-08-11 09:34:42,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 09:34:42,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 09:34:42,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_24-model_00-model_states.pt.
g0197: [2024-08-11 09:34:42,080] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_21-model_00-model_states.pt...
g0188: [2024-08-11 09:34:42,082] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_12-model_00-model_states.pt...
g0194: [2024-08-11 09:34:42,083] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_15-model_00-model_states.pt...
g0187: [2024-08-11 09:34:42,094] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_09-model_00-model_states.pt...
g0185: [2024-08-11 09:34:42,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_06-model_00-model_states.pt...
g0195: [2024-08-11 09:34:42,098] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_18-model_00-model_states.pt...
g0198: [2024-08-11 09:34:42,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_25-model_00-model_states.pt...
g0184: [2024-08-11 09:34:42,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 09:34:42,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_02-model_00-model_states.pt...
g0185: [2024-08-11 09:34:42,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_06-model_00-model_states.pt.
g0187: [2024-08-11 09:34:42,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_09-model_00-model_states.pt.
g0188: [2024-08-11 09:34:42,205] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_12-model_00-model_states.pt.
g0197: [2024-08-11 09:34:42,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_21-model_00-model_states.pt.
g0195: [2024-08-11 09:34:42,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_18-model_00-model_states.pt.
g0185: [2024-08-11 09:34:42,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_07-model_00-model_states.pt...
g0187: [2024-08-11 09:34:42,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_10-model_00-model_states.pt...
g0188: [2024-08-11 09:34:42,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_13-model_00-model_states.pt...
g0194: [2024-08-11 09:34:42,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_15-model_00-model_states.pt.
g0197: [2024-08-11 09:34:42,247] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_22-model_00-model_states.pt...
g0195: [2024-08-11 09:34:42,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_19-model_00-model_states.pt...
g0194: [2024-08-11 09:34:42,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_16-model_00-model_states.pt...
g0184: [2024-08-11 09:34:42,278] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 09:34:42,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_03-model_00-model_states.pt...
g0188: [2024-08-11 09:34:42,354] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 09:34:42,355] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_03_model_states.pt...
g0195: [2024-08-11 09:34:42,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 09:34:42,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_05_model_states.pt...
g0185: [2024-08-11 09:34:42,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_07-model_00-model_states.pt.
g0194: [2024-08-11 09:34:42,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_16-model_00-model_states.pt.
g0185: [2024-08-11 09:34:42,403] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_01_model_states.pt...
g0194: [2024-08-11 09:34:42,404] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_04_model_states.pt...
g0197: [2024-08-11 09:34:42,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 09:34:42,407] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_06_model_states.pt...
g0187: [2024-08-11 09:34:42,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_10-model_00-model_states.pt.
g0198: [2024-08-11 09:34:42,425] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_25-model_00-model_states.pt.
g0187: [2024-08-11 09:34:42,426] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_02_model_states.pt...
g0198: [2024-08-11 09:34:42,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_07_model_states.pt...
g0184: [2024-08-11 09:34:42,426] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 09:34:42,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 09:34:42,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 09:34:42,585] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_00_model_states.pt
g0184: [2024-08-11 09:34:42,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 09:34:44,439] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 09:34:44,440] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0188: [2024-08-11 09:34:44,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 09:34:44,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0185: [2024-08-11 09:34:44,755] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 09:34:44,756] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0195: [2024-08-11 09:34:44,801] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 09:34:44,801] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0187: [2024-08-11 09:34:44,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 09:34:44,817] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0194: [2024-08-11 09:34:44,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 09:34:44,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0197: [2024-08-11 09:34:44,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 09:34:44,975] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0184: [2024-08-11 09:34:46,060] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 09:34:46,061] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0184:   successfully saved checkpoint at iteration   39000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.31, Latency(second): 4.243
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4241.51, 4243.14)
g0184: [2024-08-11 09:35:28,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=39010, skipped=58, lr=[0.0001997226284246483, 0.0001997226284246483], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39010 loss: 0.7759 iter time (s): 4.224 samples/sec: 30.303
g0198:  iteration    39010/10000000 | consumed samples:      4993280 | consumed tokens:  10226237440 | elapsed time per iteration (ms): 44781.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.645239E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.858 | tokens per gpu per second (tgs): 182.933 | TFLOPs: 1.47 |
g0184: [2024-08-11 09:36:11,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=39020, skipped=58, lr=[0.00019972242723644866, 0.00019972242723644866], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39020 loss: 0.7404 iter time (s): 4.289 samples/sec: 29.840
g0198:  iteration    39020/10000000 | consumed samples:      4994560 | consumed tokens:  10228858880 | elapsed time per iteration (ms): 4322.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.555370E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.610 | tokens per gpu per second (tgs): 1895.029 | TFLOPs: 15.25 |
g0184: [2024-08-11 09:36:53,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=39030, skipped=58, lr=[0.00019972222597541747, 0.00019972222597541747], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39030 loss: 0.7553 iter time (s): 4.165 samples/sec: 30.735
g0198:  iteration    39030/10000000 | consumed samples:      4995840 | consumed tokens:  10231480320 | elapsed time per iteration (ms): 4197.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.563980E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.495 | tokens per gpu per second (tgs): 1951.652 | TFLOPs: 15.71 |
g0184: [2024-08-11 09:37:36,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=39040, skipped=58, lr=[0.0001997220246415549, 0.0001997220246415549], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39040 loss: 0.7441 iter time (s): 4.200 samples/sec: 30.474
g0198:  iteration    39040/10000000 | consumed samples:      4997120 | consumed tokens:  10234101760 | elapsed time per iteration (ms): 4233.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.441329E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.239 | tokens per gpu per second (tgs): 1935.289 | TFLOPs: 15.57 |
g0184: [2024-08-11 09:38:18,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=39050, skipped=58, lr=[0.00019972182323486107, 0.00019972182323486107], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39050 loss: 0.7437 iter time (s): 4.165 samples/sec: 30.730
g0198:  iteration    39050/10000000 | consumed samples:      4998400 | consumed tokens:  10236723200 | elapsed time per iteration (ms): 4198.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.414749E-01 | loss scale: 16384.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1950.974 | TFLOPs: 15.70 |
g0184: [2024-08-11 09:38:59,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=39060, skipped=58, lr=[0.0001997216217553362, 0.0001997216217553362], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39060 loss: 0.7651 iter time (s): 4.136 samples/sec: 30.950
g0198:  iteration    39060/10000000 | consumed samples:      4999680 | consumed tokens:  10239344640 | elapsed time per iteration (ms): 4168.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.445329E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.705 | tokens per gpu per second (tgs): 1965.118 | TFLOPs: 15.81 |
g0184: [2024-08-11 09:39:40,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=39070, skipped=58, lr=[0.00019972142020298038, 0.00019972142020298038], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39070 loss: 0.7848 iter time (s): 3.993 samples/sec: 32.060
g0198:  iteration    39070/10000000 | consumed samples:      5000960 | consumed tokens:  10241966080 | elapsed time per iteration (ms): 4025.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.479834E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.798 | tokens per gpu per second (tgs): 2035.073 | TFLOPs: 16.38 |
g0184: [2024-08-11 09:40:22,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=39080, skipped=58, lr=[0.00019972121857779378, 0.00019972121857779378], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39080 loss: 0.7496 iter time (s): 4.165 samples/sec: 30.732
g0198:  iteration    39080/10000000 | consumed samples:      5002240 | consumed tokens:  10244587520 | elapsed time per iteration (ms): 4198.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.426054E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.486 | tokens per gpu per second (tgs): 1951.096 | TFLOPs: 15.70 |
g0184: [2024-08-11 09:41:03,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=39090, skipped=58, lr=[0.0001997210168797766, 0.0001997210168797766], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39090 loss: 0.7293 iter time (s): 4.094 samples/sec: 31.265
g0198:  iteration    39090/10000000 | consumed samples:      5003520 | consumed tokens:  10247208960 | elapsed time per iteration (ms): 4127.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.475308E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.013 | tokens per gpu per second (tgs): 1984.827 | TFLOPs: 15.97 |
g0184: [2024-08-11 09:41:43,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=39100, skipped=58, lr=[0.00019972081510892897, 0.00019972081510892897], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39100 loss: 0.7287 iter time (s): 3.968 samples/sec: 32.256
g0198:  iteration    39100/10000000 | consumed samples:      5004800 | consumed tokens:  10249830400 | elapsed time per iteration (ms): 4001.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.419276E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.990 | tokens per gpu per second (tgs): 2047.346 | TFLOPs: 16.48 |
g0184: [2024-08-11 09:42:25,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=39110, skipped=58, lr=[0.00019972061326525102, 0.00019972061326525102], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39110 loss: 0.7287 iter time (s): 4.179 samples/sec: 30.629
g0198:  iteration    39110/10000000 | consumed samples:      5006080 | consumed tokens:  10252451840 | elapsed time per iteration (ms): 4213.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.464351E-01 | loss scale: 16384.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.377 | tokens per gpu per second (tgs): 1944.117 | TFLOPs: 15.64 |
g0184: [2024-08-11 09:43:07,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=39120, skipped=58, lr=[0.0001997204113487429, 0.0001997204113487429], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39120 loss: 0.7458 iter time (s): 4.192 samples/sec: 30.533
g0198:  iteration    39120/10000000 | consumed samples:      5007360 | consumed tokens:  10255073280 | elapsed time per iteration (ms): 4225.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.532171E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.295 | tokens per gpu per second (tgs): 1938.876 | TFLOPs: 15.60 |
g0184: [2024-08-11 09:43:49,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=39130, skipped=58, lr=[0.0001997202093594048, 0.0001997202093594048], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39130 loss: 0.7792 iter time (s): 4.126 samples/sec: 31.025
g0198:  iteration    39130/10000000 | consumed samples:      5008640 | consumed tokens:  10257694720 | elapsed time per iteration (ms): 4158.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.601781E-01 | loss scale: 16384.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.779 | tokens per gpu per second (tgs): 1969.886 | TFLOPs: 15.85 |
g0184: [2024-08-11 09:44:30,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=39140, skipped=58, lr=[0.00019972000729723686, 0.00019972000729723686], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39140 loss: 0.7184 iter time (s): 4.128 samples/sec: 31.010
g0198:  iteration    39140/10000000 | consumed samples:      5009920 | consumed tokens:  10260316160 | elapsed time per iteration (ms): 4160.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.475234E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.765 | tokens per gpu per second (tgs): 1968.987 | TFLOPs: 15.84 |
g0184: [2024-08-11 09:45:11,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=39150, skipped=58, lr=[0.00019971980516223927, 0.00019971980516223927], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39150 loss: 0.7465 iter time (s): 4.064 samples/sec: 31.493
g0198:  iteration    39150/10000000 | consumed samples:      5011200 | consumed tokens:  10262937600 | elapsed time per iteration (ms): 4097.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.409745E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.241 | tokens per gpu per second (tgs): 1999.393 | TFLOPs: 16.09 |
g0184: [2024-08-11 09:45:56,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=39160, skipped=58, lr=[0.0001997196029544121, 0.0001997196029544121], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39160 loss: 0.7432 iter time (s): 4.414 samples/sec: 28.999
g0198:  iteration    39160/10000000 | consumed samples:      5012480 | consumed tokens:  10265559040 | elapsed time per iteration (ms): 4446.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.501578E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.785 | tokens per gpu per second (tgs): 1842.265 | TFLOPs: 14.83 |
g0184: [2024-08-11 09:46:44,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=39170, skipped=58, lr=[0.0001997194006737556, 0.0001997194006737556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39170 loss: 0.7567 iter time (s): 4.752 samples/sec: 26.933
g0198:  iteration    39170/10000000 | consumed samples:      5013760 | consumed tokens:  10268180480 | elapsed time per iteration (ms): 4784.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.630184E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.751 | tokens per gpu per second (tgs): 1712.038 | TFLOPs: 13.78 |
g0184: [2024-08-11 09:47:28,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=39180, skipped=58, lr=[0.00019971919832026985, 0.00019971919832026985], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39180 loss: 0.7737 iter time (s): 4.357 samples/sec: 29.378
g0198:  iteration    39180/10000000 | consumed samples:      5015040 | consumed tokens:  10270801920 | elapsed time per iteration (ms): 4389.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.502420E-01 | loss scale: 16384.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.159 | tokens per gpu per second (tgs): 1866.201 | TFLOPs: 15.02 |
g0184: [2024-08-11 09:48:10,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=39190, skipped=58, lr=[0.00019971899589395507, 0.00019971899589395507], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39190 loss: 0.7306 iter time (s): 4.246 samples/sec: 30.144
g0198:  iteration    39190/10000000 | consumed samples:      5016320 | consumed tokens:  10273423360 | elapsed time per iteration (ms): 4279.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.402258E-01 | loss scale: 16384.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.909 | tokens per gpu per second (tgs): 1914.196 | TFLOPs: 15.40 |
g0184: [2024-08-11 09:48:57,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=39200, skipped=58, lr=[0.00019971879339481137, 0.00019971879339481137], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39200 loss: 0.7430 iter time (s): 4.663 samples/sec: 27.451
g0198:  iteration    39200/10000000 | consumed samples:      5017600 | consumed tokens:  10276044800 | elapsed time per iteration (ms): 4695.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.451470E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.259 | tokens per gpu per second (tgs): 1744.585 | TFLOPs: 14.04 |
g0184: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 09:49:16,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 09:49:16,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 09:49:45,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=39210, skipped=58, lr=[0.00019971859082283894, 0.00019971859082283894], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39210 loss: 0.7305 iter time (s): 4.732 samples/sec: 27.049
g0198:  iteration    39210/10000000 | consumed samples:      5018880 | consumed tokens:  10278666240 | elapsed time per iteration (ms): 4765.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.449724E-01 | loss scale: 32768.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.860 | tokens per gpu per second (tgs): 1719.063 | TFLOPs: 13.83 |
g0184: [2024-08-11 09:50:36,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=39220, skipped=58, lr=[0.0001997183881780379, 0.0001997183881780379], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39220 loss: 0.7653 iter time (s): 5.088 samples/sec: 25.157
g0198:  iteration    39220/10000000 | consumed samples:      5020160 | consumed tokens:  10281287680 | elapsed time per iteration (ms): 5121.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.517495E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.995 | tokens per gpu per second (tgs): 1599.692 | TFLOPs: 12.87 |
g0184: [2024-08-11 09:51:29,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=39230, skipped=58, lr=[0.0001997181854604084, 0.0001997181854604084], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39230 loss: 0.7621 iter time (s): 5.259 samples/sec: 24.340
g0198:  iteration    39230/10000000 | consumed samples:      5021440 | consumed tokens:  10283909120 | elapsed time per iteration (ms): 5291.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.556356E-01 | loss scale: 32768.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.188 | tokens per gpu per second (tgs): 1548.036 | TFLOPs: 12.46 |
g0184: [2024-08-11 09:52:18,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=39240, skipped=58, lr=[0.00019971798266995067, 0.00019971798266995067], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39240 loss: 0.7440 iter time (s): 4.832 samples/sec: 26.488
g0198:  iteration    39240/10000000 | consumed samples:      5022720 | consumed tokens:  10286530560 | elapsed time per iteration (ms): 4865.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.460507E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.309 | tokens per gpu per second (tgs): 1683.801 | TFLOPs: 13.55 |
g0184: [2024-08-11 09:53:03,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=39250, skipped=58, lr=[0.0001997177798066648, 0.0001997177798066648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39250 loss: 0.7364 iter time (s): 4.438 samples/sec: 28.842
g0198:  iteration    39250/10000000 | consumed samples:      5024000 | consumed tokens:  10289152000 | elapsed time per iteration (ms): 4471.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.468384E-01 | loss scale: 32768.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.628 | tokens per gpu per second (tgs): 1832.199 | TFLOPs: 14.74 |
g0184: [2024-08-11 09:53:46,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=39260, skipped=58, lr=[0.00019971757687055097, 0.00019971757687055097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39260 loss: 0.7376 iter time (s): 4.266 samples/sec: 30.003
g0198:  iteration    39260/10000000 | consumed samples:      5025280 | consumed tokens:  10291773440 | elapsed time per iteration (ms): 4299.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.492508E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.773 | tokens per gpu per second (tgs): 1905.472 | TFLOPs: 15.33 |
g0184: [2024-08-11 09:54:28,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=39270, skipped=58, lr=[0.0001997173738616093, 0.0001997173738616093], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39270 loss: 0.7350 iter time (s): 4.233 samples/sec: 30.237
g0198:  iteration    39270/10000000 | consumed samples:      5026560 | consumed tokens:  10294394880 | elapsed time per iteration (ms): 4266.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.513602E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.003 | tokens per gpu per second (tgs): 1920.186 | TFLOPs: 15.45 |
g0184: [2024-08-11 09:55:11,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=39280, skipped=58, lr=[0.00019971717077983995, 0.00019971717077983995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39280 loss: 0.7627 iter time (s): 4.233 samples/sec: 30.237
g0198:  iteration    39280/10000000 | consumed samples:      5027840 | consumed tokens:  10297016320 | elapsed time per iteration (ms): 4266.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.548113E-01 | loss scale: 32768.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.999 | tokens per gpu per second (tgs): 1919.960 | TFLOPs: 15.45 |
g0184: [2024-08-11 09:55:53,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=39290, skipped=58, lr=[0.00019971696762524315, 0.00019971696762524315], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39290 loss: 0.7220 iter time (s): 4.164 samples/sec: 30.741
g0198:  iteration    39290/10000000 | consumed samples:      5029120 | consumed tokens:  10299637760 | elapsed time per iteration (ms): 4196.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.431288E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.499 | tokens per gpu per second (tgs): 1951.967 | TFLOPs: 15.71 |
g0184: [2024-08-11 09:56:35,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=39300, skipped=58, lr=[0.000199716764397819, 0.000199716764397819], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39300 loss: 0.8087 iter time (s): 4.198 samples/sec: 30.488
g0198:  iteration    39300/10000000 | consumed samples:      5030400 | consumed tokens:  10302259200 | elapsed time per iteration (ms): 4231.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.657763E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.250 | tokens per gpu per second (tgs): 1936.002 | TFLOPs: 15.58 |
g0184: [2024-08-11 09:57:23,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=39310, skipped=58, lr=[0.00019971656109756762, 0.00019971656109756762], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39310 loss: 0.7474 iter time (s): 4.717 samples/sec: 27.135
g0198:  iteration    39310/10000000 | consumed samples:      5031680 | consumed tokens:  10304880640 | elapsed time per iteration (ms): 4749.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.461057E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.948 | tokens per gpu per second (tgs): 1724.669 | TFLOPs: 13.88 |
g0184: [2024-08-11 09:58:17,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=39320, skipped=58, lr=[0.00019971635772448928, 0.00019971635772448928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39320 loss: 0.7282 iter time (s): 5.377 samples/sec: 23.804
g0198:  iteration    39320/10000000 | consumed samples:      5032960 | consumed tokens:  10307502080 | elapsed time per iteration (ms): 5410.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.360187E-01 | loss scale: 32768.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.658 | tokens per gpu per second (tgs): 1514.138 | TFLOPs: 12.18 |
g0184: [2024-08-11 09:59:07,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=39330, skipped=58, lr=[0.00019971615427858398, 0.00019971615427858398], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39330 loss: 0.7613 iter time (s): 4.998 samples/sec: 25.610
g0198:  iteration    39330/10000000 | consumed samples:      5034240 | consumed tokens:  10310123520 | elapsed time per iteration (ms): 5030.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.496017E-01 | loss scale: 32768.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.444 | tokens per gpu per second (tgs): 1628.427 | TFLOPs: 13.10 |
g0184: [2024-08-11 09:59:51,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=39340, skipped=58, lr=[0.00019971595075985203, 0.00019971595075985203], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39340 loss: 0.7259 iter time (s): 4.350 samples/sec: 29.425
g0198:  iteration    39340/10000000 | consumed samples:      5035520 | consumed tokens:  10312744960 | elapsed time per iteration (ms): 4384.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.569341E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.196 | tokens per gpu per second (tgs): 1868.531 | TFLOPs: 15.04 |
g0184: [2024-08-11 10:00:37,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=39350, skipped=58, lr=[0.00019971574716829348, 0.00019971574716829348], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39350 loss: 0.7590 iter time (s): 4.576 samples/sec: 27.971
g0198:  iteration    39350/10000000 | consumed samples:      5036800 | consumed tokens:  10315366400 | elapsed time per iteration (ms): 4608.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.544091E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.772 | tokens per gpu per second (tgs): 1777.421 | TFLOPs: 14.30 |
g0184: [2024-08-11 10:01:22,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=39360, skipped=58, lr=[0.00019971554350390854, 0.00019971554350390854], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39360 loss: 0.7707 iter time (s): 4.448 samples/sec: 28.780
g0198:  iteration    39360/10000000 | consumed samples:      5038080 | consumed tokens:  10317987840 | elapsed time per iteration (ms): 4480.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.493464E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.567 | tokens per gpu per second (tgs): 1828.284 | TFLOPs: 14.71 |
g0184: [2024-08-11 10:02:04,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=39370, skipped=58, lr=[0.00019971533976669733, 0.00019971533976669733], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39370 loss: 0.7410 iter time (s): 4.151 samples/sec: 30.835
g0198:  iteration    39370/10000000 | consumed samples:      5039360 | consumed tokens:  10320609280 | elapsed time per iteration (ms): 4184.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.434930E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.592 | tokens per gpu per second (tgs): 1957.865 | TFLOPs: 15.76 |
g0184: [2024-08-11 10:02:47,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=39380, skipped=58, lr=[0.00019971513595666004, 0.00019971513595666004], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39380 loss: 0.7366 iter time (s): 4.258 samples/sec: 30.058
g0198:  iteration    39380/10000000 | consumed samples:      5040640 | consumed tokens:  10323230720 | elapsed time per iteration (ms): 4291.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.476651E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.830 | tokens per gpu per second (tgs): 1909.099 | TFLOPs: 15.36 |
g0184: [2024-08-11 10:03:28,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=39390, skipped=58, lr=[0.0001997149320737968, 0.0001997149320737968], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39390 loss: 0.7308 iter time (s): 4.084 samples/sec: 31.340
g0198:  iteration    39390/10000000 | consumed samples:      5041920 | consumed tokens:  10325852160 | elapsed time per iteration (ms): 4117.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.503453E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.089 | tokens per gpu per second (tgs): 1989.681 | TFLOPs: 16.01 |
g0184: [2024-08-11 10:04:11,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=39400, skipped=58, lr=[0.0001997147281181078, 0.0001997147281181078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39400 loss: 0.7337 iter time (s): 4.289 samples/sec: 29.841
g0198:  iteration    39400/10000000 | consumed samples:      5043200 | consumed tokens:  10328473600 | elapsed time per iteration (ms): 4322.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.589230E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.615 | tokens per gpu per second (tgs): 1895.340 | TFLOPs: 15.25 |
g0184: [2024-08-11 10:05:00,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=39410, skipped=58, lr=[0.00019971452408959318, 0.00019971452408959318], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39410 loss: 0.7523 iter time (s): 4.832 samples/sec: 26.492
g0198:  iteration    39410/10000000 | consumed samples:      5044480 | consumed tokens:  10331095040 | elapsed time per iteration (ms): 4864.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.522631E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.313 | tokens per gpu per second (tgs): 1684.015 | TFLOPs: 13.55 |
g0184: [2024-08-11 10:05:45,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=39420, skipped=58, lr=[0.00019971431998825308, 0.00019971431998825308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39420 loss: 0.7410 iter time (s): 4.545 samples/sec: 28.165
g0198:  iteration    39420/10000000 | consumed samples:      5045760 | consumed tokens:  10333716480 | elapsed time per iteration (ms): 4577.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.467042E-01 | loss scale: 32768.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.963 | tokens per gpu per second (tgs): 1789.634 | TFLOPs: 14.40 |
g0184: [2024-08-11 10:06:30,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=39430, skipped=58, lr=[0.00019971411581408768, 0.00019971411581408768], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39430 loss: 0.7651 iter time (s): 4.439 samples/sec: 28.836
g0198:  iteration    39430/10000000 | consumed samples:      5047040 | consumed tokens:  10336337920 | elapsed time per iteration (ms): 4471.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.537014E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.625 | tokens per gpu per second (tgs): 1831.988 | TFLOPs: 14.74 |
g0184: [2024-08-11 10:07:14,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=39440, skipped=58, lr=[0.00019971391156709713, 0.00019971391156709713], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39440 loss: 0.7491 iter time (s): 4.380 samples/sec: 29.224
g0198:  iteration    39440/10000000 | consumed samples:      5048320 | consumed tokens:  10338959360 | elapsed time per iteration (ms): 4412.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.465983E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.009 | tokens per gpu per second (tgs): 1856.566 | TFLOPs: 14.94 |
g0184: [2024-08-11 10:07:58,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=39450, skipped=58, lr=[0.00019971370724728156, 0.00019971370724728156], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39450 loss: 0.7483 iter time (s): 4.322 samples/sec: 29.619
g0198:  iteration    39450/10000000 | consumed samples:      5049600 | consumed tokens:  10341580800 | elapsed time per iteration (ms): 4354.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.463386E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.394 | tokens per gpu per second (tgs): 1881.187 | TFLOPs: 15.14 |
g0184: [2024-08-11 10:08:42,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=39460, skipped=58, lr=[0.00019971350285464116, 0.00019971350285464116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39460 loss: 0.7229 iter time (s): 4.416 samples/sec: 28.987
g0198:  iteration    39460/10000000 | consumed samples:      5050880 | consumed tokens:  10344202240 | elapsed time per iteration (ms): 4448.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.524257E-01 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.772 | tokens per gpu per second (tgs): 1841.408 | TFLOPs: 14.82 |
g0184: [2024-08-11 10:09:24,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=39470, skipped=58, lr=[0.0001997132983891761, 0.0001997132983891761], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39470 loss: 0.7869 iter time (s): 4.149 samples/sec: 30.848
g0198:  iteration    39470/10000000 | consumed samples:      5052160 | consumed tokens:  10346823680 | elapsed time per iteration (ms): 4182.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.483815E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.604 | tokens per gpu per second (tgs): 1958.629 | TFLOPs: 15.76 |
g0184: [2024-08-11 10:10:07,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=39480, skipped=58, lr=[0.0001997130938508865, 0.0001997130938508865], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39480 loss: 0.7510 iter time (s): 4.211 samples/sec: 30.393
g0198:  iteration    39480/10000000 | consumed samples:      5053440 | consumed tokens:  10349445120 | elapsed time per iteration (ms): 4244.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.501848E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.158 | tokens per gpu per second (tgs): 1930.125 | TFLOPs: 15.53 |
g0184: [2024-08-11 10:10:49,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=39490, skipped=58, lr=[0.00019971288923977256, 0.00019971288923977256], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39490 loss: 0.7771 iter time (s): 4.184 samples/sec: 30.590
g0198:  iteration    39490/10000000 | consumed samples:      5054720 | consumed tokens:  10352066560 | elapsed time per iteration (ms): 4216.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.492130E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.354 | tokens per gpu per second (tgs): 1942.649 | TFLOPs: 15.63 |
g0184: [2024-08-11 10:11:34,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=39500, skipped=58, lr=[0.00019971268455583438, 0.00019971268455583438], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39500 loss: 0.7734 iter time (s): 4.526 samples/sec: 28.279
g0198:  iteration    39500/10000000 | consumed samples:      5056000 | consumed tokens:  10354688000 | elapsed time per iteration (ms): 4559.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.585769E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.076 | tokens per gpu per second (tgs): 1796.893 | TFLOPs: 14.46 |
g0184: [2024-08-11 10:12:16,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=39510, skipped=58, lr=[0.00019971247979907216, 0.00019971247979907216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39510 loss: 0.7455 iter time (s): 4.163 samples/sec: 30.748
g0198:  iteration    39510/10000000 | consumed samples:      5057280 | consumed tokens:  10357309440 | elapsed time per iteration (ms): 4195.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.524757E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.508 | tokens per gpu per second (tgs): 1952.538 | TFLOPs: 15.71 |
g0184: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 39511
g0188: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 39511
g0187: Grad overflow on iteration 39511
g0197: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: Grad overflow on iteration 39511
g0187: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: Grad overflow on iteration 39511
g0187: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 39511
g0187: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 39511
g0187: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 39511
g0187: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: Grad overflow on iteration 39511
g0194: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 39511
g0185: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 39511
g0185: Grad overflow on iteration 39511
g0184: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 39511
g0184: Grad overflow on iteration 39511
g0184: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 39511
g0184: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: Grad overflow on iteration 39511
g0184: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 39511
g0197: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: Grad overflow on iteration 39511
g0187: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0197: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: Grad overflow on iteration 39511
g0195: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: Grad overflow on iteration 39511
g0198: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 39511
g0185: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 39511
g0198: Grad overflow on iteration 39511
g0194: Grad overflow on iteration 39511
g0198: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 39511
g0184: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0185: Grad overflow on iteration 39511
g0198: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 39511
g0188: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 10:12:24,929] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0185: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 39511
g0185: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: Grad overflow on iteration 39511
g0194: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 39511
g0194: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0194: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: Grad overflow on iteration 39511
g0187: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 39511
g0195: [2024-08-11 10:12:24,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0188: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0195: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0198: [2024-08-11 10:12:24,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0184: [2024-08-11 10:12:59,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=39520, skipped=59, lr=[0.00019971227496948604, 0.00019971227496948604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39520 loss: 0.7327 iter time (s): 4.205 samples/sec: 30.439
g0198:  iteration    39520/10000000 | consumed samples:      5058560 | consumed tokens:  10359930880 | elapsed time per iteration (ms): 4237.9 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.204 | tokens per gpu per second (tgs): 1933.038 | TFLOPs: 15.56 |
g0184: [2024-08-11 10:13:38,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=39530, skipped=59, lr=[0.0001997120700670762, 0.0001997120700670762], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39530 loss: 0.7489 iter time (s): 3.944 samples/sec: 32.454
g0198:  iteration    39530/10000000 | consumed samples:      5059840 | consumed tokens:  10362552320 | elapsed time per iteration (ms): 3976.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.586969E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.189 | tokens per gpu per second (tgs): 2060.088 | TFLOPs: 16.58 |
g0184: [2024-08-11 10:14:20,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=39540, skipped=59, lr=[0.00019971186509184276, 0.00019971186509184276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39540 loss: 0.7562 iter time (s): 4.158 samples/sec: 30.784
g0198:  iteration    39540/10000000 | consumed samples:      5061120 | consumed tokens:  10365173760 | elapsed time per iteration (ms): 4192.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.520740E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.769 | TFLOPs: 15.72 |
g0184: [2024-08-11 10:15:03,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=39550, skipped=59, lr=[0.0001997116600437859, 0.0001997116600437859], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39550 loss: 0.7308 iter time (s): 4.197 samples/sec: 30.497
g0198:  iteration    39550/10000000 | consumed samples:      5062400 | consumed tokens:  10367795200 | elapsed time per iteration (ms): 4230.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.490110E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.254 | tokens per gpu per second (tgs): 1936.254 | TFLOPs: 15.58 |
g0184: [2024-08-11 10:15:42,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=39560, skipped=59, lr=[0.00019971145492290578, 0.00019971145492290578], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39560 loss: 0.7270 iter time (s): 3.949 samples/sec: 32.411
g0198:  iteration    39560/10000000 | consumed samples:      5063680 | consumed tokens:  10370416640 | elapsed time per iteration (ms): 3983.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.624399E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.131 | tokens per gpu per second (tgs): 2056.377 | TFLOPs: 16.55 |
g0184: [2024-08-11 10:16:24,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=39570, skipped=59, lr=[0.00019971124972920255, 0.00019971124972920255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39570 loss: 0.7336 iter time (s): 4.166 samples/sec: 30.728
g0198:  iteration    39570/10000000 | consumed samples:      5064960 | consumed tokens:  10373038080 | elapsed time per iteration (ms): 4198.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.408245E-01 | loss scale: 16384.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1951.001 | TFLOPs: 15.70 |
g0184: [2024-08-11 10:17:06,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=39580, skipped=59, lr=[0.00019971104446267642, 0.00019971104446267642], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39580 loss: 0.7433 iter time (s): 4.094 samples/sec: 31.267
g0198:  iteration    39580/10000000 | consumed samples:      5066240 | consumed tokens:  10375659520 | elapsed time per iteration (ms): 4127.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.446521E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.012 | tokens per gpu per second (tgs): 1984.737 | TFLOPs: 15.97 |
g0184: [2024-08-11 10:17:48,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=39590, skipped=59, lr=[0.00019971083912332743, 0.00019971083912332743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39590 loss: 0.7353 iter time (s): 4.243 samples/sec: 30.164
g0198:  iteration    39590/10000000 | consumed samples:      5067520 | consumed tokens:  10378280960 | elapsed time per iteration (ms): 4276.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.544942E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.934 | tokens per gpu per second (tgs): 1915.760 | TFLOPs: 15.42 |
g0184: [2024-08-11 10:18:30,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=39600, skipped=59, lr=[0.00019971063371115584, 0.00019971063371115584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39600 loss: 0.7595 iter time (s): 4.087 samples/sec: 31.315
g0198:  iteration    39600/10000000 | consumed samples:      5068800 | consumed tokens:  10380902400 | elapsed time per iteration (ms): 4120.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.573659E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.064 | tokens per gpu per second (tgs): 1988.124 | TFLOPs: 16.00 |
g0184: [2024-08-11 10:19:11,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=39610, skipped=59, lr=[0.00019971042822616178, 0.00019971042822616178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39610 loss: 0.7424 iter time (s): 4.133 samples/sec: 30.971
g0198:  iteration    39610/10000000 | consumed samples:      5070080 | consumed tokens:  10383523840 | elapsed time per iteration (ms): 4165.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.498984E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.726 | tokens per gpu per second (tgs): 1966.486 | TFLOPs: 15.82 |
g0184: [2024-08-11 10:19:54,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=39620, skipped=59, lr=[0.00019971022266834537, 0.00019971022266834537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39620 loss: 0.7633 iter time (s): 4.225 samples/sec: 30.299
g0198:  iteration    39620/10000000 | consumed samples:      5071360 | consumed tokens:  10386145280 | elapsed time per iteration (ms): 4257.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.443131E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.066 | tokens per gpu per second (tgs): 1924.240 | TFLOPs: 15.48 |
g0184: [2024-08-11 10:20:35,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=39630, skipped=59, lr=[0.00019971001703770685, 0.00019971001703770685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39630 loss: 0.7342 iter time (s): 4.033 samples/sec: 31.741
g0198:  iteration    39630/10000000 | consumed samples:      5072640 | consumed tokens:  10388766720 | elapsed time per iteration (ms): 4065.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.438110E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.487 | tokens per gpu per second (tgs): 2015.181 | TFLOPs: 16.22 |
g0184: [2024-08-11 10:21:17,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=39640, skipped=59, lr=[0.0001997098113342463, 0.0001997098113342463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39640 loss: 0.7405 iter time (s): 4.234 samples/sec: 30.228
g0198:  iteration    39640/10000000 | consumed samples:      5073920 | consumed tokens:  10391388160 | elapsed time per iteration (ms): 4268.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.461154E-01 | loss scale: 16384.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.984 | tokens per gpu per second (tgs): 1918.981 | TFLOPs: 15.44 |
g0184: [2024-08-11 10:21:59,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=39650, skipped=59, lr=[0.0001997096055579639, 0.0001997096055579639], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39650 loss: 0.7407 iter time (s): 4.174 samples/sec: 30.663
g0198:  iteration    39650/10000000 | consumed samples:      5075200 | consumed tokens:  10394009600 | elapsed time per iteration (ms): 4207.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.512552E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1947.010 | TFLOPs: 15.67 |
g0184: [2024-08-11 10:22:41,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=39660, skipped=59, lr=[0.00019970939970885985, 0.00019970939970885985], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39660 loss: 0.7461 iter time (s): 4.132 samples/sec: 30.978
g0198:  iteration    39660/10000000 | consumed samples:      5076480 | consumed tokens:  10396631040 | elapsed time per iteration (ms): 4164.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.469399E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.733 | tokens per gpu per second (tgs): 1966.912 | TFLOPs: 15.83 |
g0184: [2024-08-11 10:23:23,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=39670, skipped=59, lr=[0.00019970919378693423, 0.00019970919378693423], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39670 loss: 0.7627 iter time (s): 4.169 samples/sec: 30.705
g0198:  iteration    39670/10000000 | consumed samples:      5077760 | consumed tokens:  10399252480 | elapsed time per iteration (ms): 4201.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.512820E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.465 | tokens per gpu per second (tgs): 1949.744 | TFLOPs: 15.69 |
g0184: [2024-08-11 10:24:05,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=39680, skipped=59, lr=[0.00019970898779218726, 0.00019970898779218726], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39680 loss: 0.7309 iter time (s): 4.205 samples/sec: 30.442
g0198:  iteration    39680/10000000 | consumed samples:      5079040 | consumed tokens:  10401873920 | elapsed time per iteration (ms): 4237.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.485597E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.205 | tokens per gpu per second (tgs): 1933.147 | TFLOPs: 15.56 |
g0184: [2024-08-11 10:24:47,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=39690, skipped=59, lr=[0.0001997087817246191, 0.0001997087817246191], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39690 loss: 0.7915 iter time (s): 4.085 samples/sec: 31.336
g0198:  iteration    39690/10000000 | consumed samples:      5080320 | consumed tokens:  10404495360 | elapsed time per iteration (ms): 4117.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.504187E-01 | loss scale: 16384.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.085 | tokens per gpu per second (tgs): 1989.465 | TFLOPs: 16.01 |
g0184: [2024-08-11 10:25:29,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=39700, skipped=59, lr=[0.00019970857558422987, 0.00019970857558422987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39700 loss: 0.7116 iter time (s): 4.172 samples/sec: 30.678
g0198:  iteration    39700/10000000 | consumed samples:      5081600 | consumed tokens:  10407116800 | elapsed time per iteration (ms): 4204.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.432863E-01 | loss scale: 16384.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.441 | tokens per gpu per second (tgs): 1948.223 | TFLOPs: 15.68 |
g0184: [2024-08-11 10:26:09,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=39710, skipped=59, lr=[0.00019970836937101977, 0.00019970836937101977], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39710 loss: 0.7554 iter time (s): 3.982 samples/sec: 32.148
g0198:  iteration    39710/10000000 | consumed samples:      5082880 | consumed tokens:  10409738240 | elapsed time per iteration (ms): 4014.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.566647E-01 | loss scale: 16384.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.883 | tokens per gpu per second (tgs): 2040.523 | TFLOPs: 16.42 |
g0184: [2024-08-11 10:26:51,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=39720, skipped=59, lr=[0.00019970816308498892, 0.00019970816308498892], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39720 loss: 0.7653 iter time (s): 4.176 samples/sec: 30.653
g0198:  iteration    39720/10000000 | consumed samples:      5084160 | consumed tokens:  10412359680 | elapsed time per iteration (ms): 4208.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.487860E-01 | loss scale: 16384.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.417 | tokens per gpu per second (tgs): 1946.657 | TFLOPs: 15.67 |
g0184: [2024-08-11 10:27:32,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=39730, skipped=59, lr=[0.0001997079567261375, 0.0001997079567261375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39730 loss: 0.7699 iter time (s): 4.110 samples/sec: 31.145
g0198:  iteration    39730/10000000 | consumed samples:      5085440 | consumed tokens:  10414981120 | elapsed time per iteration (ms): 4143.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.603743E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.894 | tokens per gpu per second (tgs): 1977.235 | TFLOPs: 15.91 |
g0184: [2024-08-11 10:28:14,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=39740, skipped=59, lr=[0.00019970775029446564, 0.00019970775029446564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39740 loss: 0.7545 iter time (s): 4.177 samples/sec: 30.643
g0198:  iteration    39740/10000000 | consumed samples:      5086720 | consumed tokens:  10417602560 | elapsed time per iteration (ms): 4217.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.478413E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.351 | tokens per gpu per second (tgs): 1942.432 | TFLOPs: 15.63 |
g0184: [2024-08-11 10:28:57,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=39750, skipped=59, lr=[0.00019970754378997356, 0.00019970754378997356], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39750 loss: 0.7691 iter time (s): 4.246 samples/sec: 30.149
g0198:  iteration    39750/10000000 | consumed samples:      5088000 | consumed tokens:  10420224000 | elapsed time per iteration (ms): 4280.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.560694E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.903 | tokens per gpu per second (tgs): 1913.778 | TFLOPs: 15.40 |
g0184: [2024-08-11 10:29:43,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=39760, skipped=59, lr=[0.00019970733721266136, 0.00019970733721266136], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39760 loss: 0.7749 iter time (s): 4.513 samples/sec: 28.364
g0198:  iteration    39760/10000000 | consumed samples:      5089280 | consumed tokens:  10422845440 | elapsed time per iteration (ms): 4546.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.520824E-01 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.156 | tokens per gpu per second (tgs): 1801.979 | TFLOPs: 14.50 |
g0184: [2024-08-11 10:30:26,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=39770, skipped=59, lr=[0.00019970713056252922, 0.00019970713056252922], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39770 loss: 0.7396 iter time (s): 4.272 samples/sec: 29.962
g0198:  iteration    39770/10000000 | consumed samples:      5090560 | consumed tokens:  10425466880 | elapsed time per iteration (ms): 4307.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.489782E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.719 | tokens per gpu per second (tgs): 1902.016 | TFLOPs: 15.31 |
g0184: [2024-08-11 10:31:09,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=39780, skipped=59, lr=[0.0001997069238395773, 0.0001997069238395773], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39780 loss: 0.7480 iter time (s): 4.251 samples/sec: 30.113
g0198:  iteration    39780/10000000 | consumed samples:      5091840 | consumed tokens:  10428088320 | elapsed time per iteration (ms): 4283.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.493131E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.881 | tokens per gpu per second (tgs): 1912.404 | TFLOPs: 15.39 |
g0184: [2024-08-11 10:31:54,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=39790, skipped=59, lr=[0.00019970671704380575, 0.00019970671704380575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39790 loss: 0.7231 iter time (s): 4.471 samples/sec: 28.627
g0198:  iteration    39790/10000000 | consumed samples:      5093120 | consumed tokens:  10430709760 | elapsed time per iteration (ms): 4505.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.406114E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.411 | tokens per gpu per second (tgs): 1818.303 | TFLOPs: 14.63 |
g0184: [2024-08-11 10:32:37,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=39800, skipped=59, lr=[0.00019970651017521475, 0.00019970651017521475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39800 loss: 0.7414 iter time (s): 4.293 samples/sec: 29.815
g0198:  iteration    39800/10000000 | consumed samples:      5094400 | consumed tokens:  10433331200 | elapsed time per iteration (ms): 4326.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.439577E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.583 | tokens per gpu per second (tgs): 1893.338 | TFLOPs: 15.24 |
g0184: [2024-08-11 10:33:19,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=39810, skipped=59, lr=[0.00019970630323380443, 0.00019970630323380443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39810 loss: 0.7553 iter time (s): 4.197 samples/sec: 30.498
g0198:  iteration    39810/10000000 | consumed samples:      5095680 | consumed tokens:  10435952640 | elapsed time per iteration (ms): 4234.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.447165E-01 | loss scale: 16384.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.230 | tokens per gpu per second (tgs): 1934.702 | TFLOPs: 15.57 |
g0184: [2024-08-11 10:34:01,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=39820, skipped=59, lr=[0.000199706096219575, 0.000199706096219575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39820 loss: 0.7455 iter time (s): 4.139 samples/sec: 30.926
g0198:  iteration    39820/10000000 | consumed samples:      5096960 | consumed tokens:  10438574080 | elapsed time per iteration (ms): 4184.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.424176E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.587 | tokens per gpu per second (tgs): 1957.540 | TFLOPs: 15.75 |
g0184: [2024-08-11 10:34:42,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=39830, skipped=59, lr=[0.00019970588913252656, 0.00019970588913252656], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39830 loss: 0.7652 iter time (s): 4.090 samples/sec: 31.299
g0198:  iteration    39830/10000000 | consumed samples:      5098240 | consumed tokens:  10441195520 | elapsed time per iteration (ms): 4122.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.602350E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.051 | tokens per gpu per second (tgs): 1987.276 | TFLOPs: 15.99 |
g0184: [2024-08-11 10:35:24,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=39840, skipped=59, lr=[0.00019970568197265928, 0.00019970568197265928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39840 loss: 0.7450 iter time (s): 4.084 samples/sec: 31.344
g0198:  iteration    39840/10000000 | consumed samples:      5099520 | consumed tokens:  10443816960 | elapsed time per iteration (ms): 4116.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.458988E-01 | loss scale: 16384.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.096 | tokens per gpu per second (tgs): 1990.124 | TFLOPs: 16.01 |
g0184: [2024-08-11 10:36:05,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=39850, skipped=59, lr=[0.00019970547473997335, 0.00019970547473997335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39850 loss: 0.7687 iter time (s): 4.162 samples/sec: 30.757
g0198:  iteration    39850/10000000 | consumed samples:      5100800 | consumed tokens:  10446438400 | elapsed time per iteration (ms): 4195.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.414231E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.512 | tokens per gpu per second (tgs): 1952.746 | TFLOPs: 15.71 |
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 39857
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 39857
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 39857
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 39857
g0188: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 39857
g0188: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 39857
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 39857
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: Grad overflow on iteration 39857
g0185: Grad overflow on iteration 39857
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 39857
g0197: Grad overflow on iteration 39857
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: Grad overflow on iteration 39857
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 39857
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: Grad overflow on iteration 39857
g0195: Grad overflow on iteration 39857
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0187: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0188: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: Grad overflow on iteration 39857
g0194: Grad overflow on iteration 39857
g0187: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: Grad overflow on iteration 39857
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: Grad overflow on iteration 39857
g0198: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: Grad overflow on iteration 39857
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: Grad overflow on iteration 39857
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 39857
g0184: [2024-08-11 10:36:38,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: Grad overflow on iteration 39857
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0197: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 39857
g0185: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0194: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 39857
g0198: [2024-08-11 10:36:38,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0198: [2024-08-11 10:36:38,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0184: [2024-08-11 10:36:46,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=39860, skipped=60, lr=[0.0001997052674344689, 0.0001997052674344689], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39860 loss: 0.7729 iter time (s): 4.039 samples/sec: 31.691
g0198:  iteration    39860/10000000 | consumed samples:      5102080 | consumed tokens:  10449059840 | elapsed time per iteration (ms): 4071.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.560963E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.437 | tokens per gpu per second (tgs): 2011.944 | TFLOPs: 16.19 |
g0184: [2024-08-11 10:37:27,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=39870, skipped=60, lr=[0.00019970506005614612, 0.00019970506005614612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39870 loss: 0.7319 iter time (s): 4.069 samples/sec: 31.457
g0198:  iteration    39870/10000000 | consumed samples:      5103360 | consumed tokens:  10451681280 | elapsed time per iteration (ms): 4102.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.545795E-01 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.201 | tokens per gpu per second (tgs): 1996.894 | TFLOPs: 16.07 |
g0184: [2024-08-11 10:38:09,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=39880, skipped=60, lr=[0.00019970485260500516, 0.00019970485260500516], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39880 loss: 0.7591 iter time (s): 4.168 samples/sec: 30.709
g0198:  iteration    39880/10000000 | consumed samples:      5104640 | consumed tokens:  10454302720 | elapsed time per iteration (ms): 4203.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.465371E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.450 | tokens per gpu per second (tgs): 1948.778 | TFLOPs: 15.68 |
g0184: [2024-08-11 10:38:50,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=39890, skipped=60, lr=[0.00019970464508104613, 0.00019970464508104613], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39890 loss: 0.7670 iter time (s): 4.090 samples/sec: 31.293
g0198:  iteration    39890/10000000 | consumed samples:      5105920 | consumed tokens:  10456924160 | elapsed time per iteration (ms): 4123.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.543512E-01 | loss scale: 8192.0 | grad norm: 0.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.040 | tokens per gpu per second (tgs): 1986.579 | TFLOPs: 15.99 |
g0184: [2024-08-11 10:39:33,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=39900, skipped=60, lr=[0.00019970443748426925, 0.00019970443748426925], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39900 loss: 0.7269 iter time (s): 4.233 samples/sec: 30.241
g0198:  iteration    39900/10000000 | consumed samples:      5107200 | consumed tokens:  10459545600 | elapsed time per iteration (ms): 4266.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.431217E-01 | loss scale: 8192.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.004 | tokens per gpu per second (tgs): 1920.228 | TFLOPs: 15.45 |
g0184: [2024-08-11 10:40:15,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=39910, skipped=60, lr=[0.00019970422981467468, 0.00019970422981467468], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39910 loss: 0.7505 iter time (s): 4.132 samples/sec: 30.980
g0198:  iteration    39910/10000000 | consumed samples:      5108480 | consumed tokens:  10462167040 | elapsed time per iteration (ms): 4165.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.593462E-01 | loss scale: 8192.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.727 | tokens per gpu per second (tgs): 1966.557 | TFLOPs: 15.83 |
g0184: [2024-08-11 10:40:57,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=39920, skipped=60, lr=[0.00019970402207226254, 0.00019970402207226254], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39920 loss: 0.7698 iter time (s): 4.169 samples/sec: 30.704
g0198:  iteration    39920/10000000 | consumed samples:      5109760 | consumed tokens:  10464788480 | elapsed time per iteration (ms): 4201.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.559764E-01 | loss scale: 8192.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.464 | tokens per gpu per second (tgs): 1949.711 | TFLOPs: 15.69 |
g0184: [2024-08-11 10:41:37,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=39930, skipped=60, lr=[0.000199703814257033, 0.000199703814257033], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39930 loss: 0.7119 iter time (s): 4.025 samples/sec: 31.804
g0198:  iteration    39930/10000000 | consumed samples:      5111040 | consumed tokens:  10467409920 | elapsed time per iteration (ms): 4057.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.458416E-01 | loss scale: 8192.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.547 | tokens per gpu per second (tgs): 2018.991 | TFLOPs: 16.25 |
g0184: [2024-08-11 10:42:18,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=39940, skipped=60, lr=[0.00019970360636898624, 0.00019970360636898624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39940 loss: 0.7324 iter time (s): 4.026 samples/sec: 31.796
g0198:  iteration    39940/10000000 | consumed samples:      5112320 | consumed tokens:  10470031360 | elapsed time per iteration (ms): 4059.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.554455E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.532 | tokens per gpu per second (tgs): 2018.058 | TFLOPs: 16.24 |
g0184: [2024-08-11 10:43:01,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=39950, skipped=60, lr=[0.0001997033984081224, 0.0001997033984081224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39950 loss: 0.7625 iter time (s): 4.272 samples/sec: 29.963
g0198:  iteration    39950/10000000 | consumed samples:      5113600 | consumed tokens:  10472652800 | elapsed time per iteration (ms): 4304.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.547120E-01 | loss scale: 8192.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.735 | tokens per gpu per second (tgs): 1903.055 | TFLOPs: 15.31 |
g0184: [2024-08-11 10:43:43,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=39960, skipped=60, lr=[0.00019970319037444166, 0.00019970319037444166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39960 loss: 0.7502 iter time (s): 4.164 samples/sec: 30.740
g0198:  iteration    39960/10000000 | consumed samples:      5114880 | consumed tokens:  10475274240 | elapsed time per iteration (ms): 4197.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.355246E-01 | loss scale: 8192.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.498 | tokens per gpu per second (tgs): 1951.850 | TFLOPs: 15.71 |
g0184: [2024-08-11 10:44:27,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=39970, skipped=60, lr=[0.00019970298226794415, 0.00019970298226794415], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39970 loss: 0.7620 iter time (s): 4.379 samples/sec: 29.233
g0198:  iteration    39970/10000000 | consumed samples:      5116160 | consumed tokens:  10477895680 | elapsed time per iteration (ms): 4411.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.546945E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.016 | tokens per gpu per second (tgs): 1856.997 | TFLOPs: 14.94 |
g0184: [2024-08-11 10:45:11,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=39980, skipped=60, lr=[0.00019970277408863006, 0.00019970277408863006], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39980 loss: 0.7584 iter time (s): 4.365 samples/sec: 29.323
g0198:  iteration    39980/10000000 | consumed samples:      5117440 | consumed tokens:  10480517120 | elapsed time per iteration (ms): 4397.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.583776E-01 | loss scale: 8192.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.107 | tokens per gpu per second (tgs): 1862.874 | TFLOPs: 14.99 |
g0184: [2024-08-11 10:45:55,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=39990, skipped=60, lr=[0.00019970256583649954, 0.00019970256583649954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 39990 loss: 0.7821 iter time (s): 4.403 samples/sec: 29.070
g0198:  iteration    39990/10000000 | consumed samples:      5118720 | consumed tokens:  10483138560 | elapsed time per iteration (ms): 4436.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.504999E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.852 | tokens per gpu per second (tgs): 1846.526 | TFLOPs: 14.86 |
g0184: [2024-08-11 10:46:39,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=60, lr=[0.00019970235751155276, 0.00019970235751155276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40000 loss: 0.7509 iter time (s): 4.294 samples/sec: 29.811
g0198:  iteration    40000/10000000 | consumed samples:      5120000 | consumed tokens:  10485760000 | elapsed time per iteration (ms): 4327.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.429425E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.580 | tokens per gpu per second (tgs): 1893.123 | TFLOPs: 15.23 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 40000 | lm loss value: 7.483363E-01 | lm loss PPL: 2.113481E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   40000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 10:53:25,798] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step40000 is about to be saved!
g0197: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0197: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0198: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0197: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0198: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0198: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0184: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0184: [2024-08-11 10:53:25,806] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0188: [2024-08-11 10:53:25,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0188: [2024-08-11 10:53:25,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0188: [2024-08-11 10:53:25,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0184: [2024-08-11 10:53:25,808] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0194: [2024-08-11 10:53:25,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0194: [2024-08-11 10:53:25,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0194: [2024-08-11 10:53:25,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0187: [2024-08-11 10:53:25,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0187: [2024-08-11 10:53:25,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0187: [2024-08-11 10:53:25,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0195: [2024-08-11 10:53:25,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0195: [2024-08-11 10:53:25,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0195: [2024-08-11 10:53:25,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0185: [2024-08-11 10:53:25,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0185: [2024-08-11 10:53:25,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0185: [2024-08-11 10:53:25,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0198: [2024-08-11 10:53:25,835] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_23-model_00-model_states.pt...
g0197: [2024-08-11 10:53:25,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_20-model_00-model_states.pt...
g0194: [2024-08-11 10:53:25,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_14-model_00-model_states.pt...
g0188: [2024-08-11 10:53:25,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_11-model_00-model_states.pt...
g0185: [2024-08-11 10:53:25,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_05-model_00-model_states.pt...
g0195: [2024-08-11 10:53:25,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_17-model_00-model_states.pt...
g0187: [2024-08-11 10:53:25,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_08-model_00-model_states.pt...
g0184: [2024-08-11 10:53:25,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_01-model_00-model_states.pt...
g0187: [2024-08-11 10:53:25,997] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_08-model_00-model_states.pt.
g0198: [2024-08-11 10:53:26,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 10:53:26,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 10:53:26,009] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_24-model_00-model_states.pt.
g0197: [2024-08-11 10:53:26,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_20-model_00-model_states.pt.
g0188: [2024-08-11 10:53:26,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_11-model_00-model_states.pt.
g0185: [2024-08-11 10:53:26,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_05-model_00-model_states.pt.
g0187: [2024-08-11 10:53:26,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_09-model_00-model_states.pt...
g0188: [2024-08-11 10:53:26,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_12-model_00-model_states.pt...
g0185: [2024-08-11 10:53:26,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_06-model_00-model_states.pt...
g0197: [2024-08-11 10:53:26,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_21-model_00-model_states.pt...
g0198: [2024-08-11 10:53:26,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_25-model_00-model_states.pt...
g0194: [2024-08-11 10:53:26,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_14-model_00-model_states.pt.
g0184: [2024-08-11 10:53:26,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 10:53:26,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_02-model_00-model_states.pt...
g0194: [2024-08-11 10:53:26,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_15-model_00-model_states.pt...
g0195: [2024-08-11 10:53:26,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_17-model_00-model_states.pt.
g0187: [2024-08-11 10:53:26,154] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_09-model_00-model_states.pt.
g0185: [2024-08-11 10:53:26,173] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_06-model_00-model_states.pt.
g0197: [2024-08-11 10:53:26,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_21-model_00-model_states.pt.
g0195: [2024-08-11 10:53:26,185] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_18-model_00-model_states.pt...
g0187: [2024-08-11 10:53:26,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_10-model_00-model_states.pt...
g0185: [2024-08-11 10:53:26,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_07-model_00-model_states.pt...
g0197: [2024-08-11 10:53:26,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_22-model_00-model_states.pt...
g0184: [2024-08-11 10:53:26,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_02-model_00-model_states.pt.
g0188: [2024-08-11 10:53:26,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_12-model_00-model_states.pt.
g0184: [2024-08-11 10:53:26,247] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_03-model_00-model_states.pt...
g0194: [2024-08-11 10:53:26,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_15-model_00-model_states.pt.
g0188: [2024-08-11 10:53:26,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_13-model_00-model_states.pt...
g0194: [2024-08-11 10:53:26,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_16-model_00-model_states.pt...
g0195: [2024-08-11 10:53:26,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_18-model_00-model_states.pt.
g0197: [2024-08-11 10:53:26,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_22-model_00-model_states.pt.
g0187: [2024-08-11 10:53:26,320] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_10-model_00-model_states.pt.
g0197: [2024-08-11 10:53:26,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_06_model_states.pt...
g0187: [2024-08-11 10:53:26,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_02_model_states.pt...
g0185: [2024-08-11 10:53:26,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 10:53:26,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_01_model_states.pt...
g0195: [2024-08-11 10:53:26,347] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_19-model_00-model_states.pt...
g0184: [2024-08-11 10:53:26,366] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_03-model_00-model_states.pt.
g0188: [2024-08-11 10:53:26,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 10:53:26,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_03_model_states.pt...
g0184: [2024-08-11 10:53:26,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_04-model_00-model_states.pt...
g0194: [2024-08-11 10:53:26,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 10:53:26,421] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_04_model_states.pt...
g0195: [2024-08-11 10:53:26,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 10:53:26,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 10:53:26,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 10:53:26,529] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_00_model_states.pt
g0184: [2024-08-11 10:53:26,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 10:53:26,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 10:53:26,806] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_07_model_states.pt...
g0185: [2024-08-11 10:53:28,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 10:53:28,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0197: [2024-08-11 10:53:28,668] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 10:53:28,668] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0198: [2024-08-11 10:53:28,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 10:53:28,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0187: [2024-08-11 10:53:28,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 10:53:28,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0194: [2024-08-11 10:53:28,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 10:53:28,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0195: [2024-08-11 10:53:28,864] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 10:53:28,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0188: [2024-08-11 10:53:28,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 10:53:28,911] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0184: [2024-08-11 10:53:29,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 10:53:29,914] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0184:   successfully saved checkpoint at iteration   40000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 5.39, Latency(second): 4.175
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4175.13, 4175.44)
g0184: [2024-08-11 10:54:11,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=40010, skipped=60, lr=[0.00019970214911378987, 0.00019970214911378987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40010 loss: 0.7303 iter time (s): 4.118 samples/sec: 31.080
g0198:  iteration    40010/10000000 | consumed samples:      5121280 | consumed tokens:  10488381440 | elapsed time per iteration (ms): 45220.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.547459E-01 | loss scale: 8192.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.831 | tokens per gpu per second (tgs): 181.157 | TFLOPs: 1.46 |
g0184: [2024-08-11 10:54:52,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=40020, skipped=60, lr=[0.00019970194064321102, 0.00019970194064321102], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40020 loss: 0.7361 iter time (s): 4.094 samples/sec: 31.263
g0198:  iteration    40020/10000000 | consumed samples:      5122560 | consumed tokens:  10491002880 | elapsed time per iteration (ms): 4127.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.456478E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.015 | tokens per gpu per second (tgs): 1984.978 | TFLOPs: 15.97 |
g0184: [2024-08-11 10:55:34,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=40030, skipped=60, lr=[0.00019970173209981636, 0.00019970173209981636], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40030 loss: 0.7296 iter time (s): 4.104 samples/sec: 31.188
g0198:  iteration    40030/10000000 | consumed samples:      5123840 | consumed tokens:  10493624320 | elapsed time per iteration (ms): 4136.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.462253E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.942 | tokens per gpu per second (tgs): 1980.315 | TFLOPs: 15.94 |
g0184: [2024-08-11 10:56:16,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=40040, skipped=60, lr=[0.00019970152348360612, 0.00019970152348360612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40040 loss: 0.7852 iter time (s): 4.235 samples/sec: 30.224
g0198:  iteration    40040/10000000 | consumed samples:      5125120 | consumed tokens:  10496245760 | elapsed time per iteration (ms): 4268.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.574839E-01 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.987 | tokens per gpu per second (tgs): 1919.172 | TFLOPs: 15.44 |
g0184: [2024-08-11 10:56:57,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=40050, skipped=60, lr=[0.00019970131479458037, 0.00019970131479458037], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40050 loss: 0.7624 iter time (s): 4.087 samples/sec: 31.322
g0198:  iteration    40050/10000000 | consumed samples:      5126400 | consumed tokens:  10498867200 | elapsed time per iteration (ms): 4119.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.476163E-01 | loss scale: 8192.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.069 | tokens per gpu per second (tgs): 1988.428 | TFLOPs: 16.00 |
g0184: [2024-08-11 10:57:38,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=40060, skipped=60, lr=[0.0001997011060327393, 0.0001997011060327393], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40060 loss: 0.7757 iter time (s): 3.990 samples/sec: 32.077
g0198:  iteration    40060/10000000 | consumed samples:      5127680 | consumed tokens:  10501488640 | elapsed time per iteration (ms): 4023.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.493304E-01 | loss scale: 8192.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.815 | tokens per gpu per second (tgs): 2036.186 | TFLOPs: 16.39 |
g0184: [2024-08-11 10:58:19,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=40070, skipped=60, lr=[0.0001997008971980831, 0.0001997008971980831], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40070 loss: 0.7284 iter time (s): 4.118 samples/sec: 31.085
g0198:  iteration    40070/10000000 | consumed samples:      5128960 | consumed tokens:  10504110080 | elapsed time per iteration (ms): 4150.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.472397E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.841 | tokens per gpu per second (tgs): 1973.835 | TFLOPs: 15.88 |
g0184: [2024-08-11 10:59:01,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=40080, skipped=60, lr=[0.00019970068829061193, 0.00019970068829061193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40080 loss: 0.7672 iter time (s): 4.144 samples/sec: 30.887
g0198:  iteration    40080/10000000 | consumed samples:      5130240 | consumed tokens:  10506731520 | elapsed time per iteration (ms): 4176.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.504390E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.645 | tokens per gpu per second (tgs): 1961.288 | TFLOPs: 15.78 |
g0184: [2024-08-11 10:59:44,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=40090, skipped=60, lr=[0.0001997004793103259, 0.0001997004793103259], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40090 loss: 0.7233 iter time (s): 4.225 samples/sec: 30.298
g0198:  iteration    40090/10000000 | consumed samples:      5131520 | consumed tokens:  10509352960 | elapsed time per iteration (ms): 4257.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.491732E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.066 | tokens per gpu per second (tgs): 1924.209 | TFLOPs: 15.48 |
g0184: [2024-08-11 11:00:25,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=40100, skipped=60, lr=[0.0001997002702572252, 0.0001997002702572252], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40100 loss: 0.7319 iter time (s): 4.119 samples/sec: 31.079
g0198:  iteration    40100/10000000 | consumed samples:      5132800 | consumed tokens:  10511974400 | elapsed time per iteration (ms): 4151.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.608680E-01 | loss scale: 8192.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.836 | tokens per gpu per second (tgs): 1973.493 | TFLOPs: 15.88 |
g0184: [2024-08-11 11:01:07,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=40110, skipped=60, lr=[0.00019970006113131003, 0.00019970006113131003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40110 loss: 0.7344 iter time (s): 4.180 samples/sec: 30.619
g0198:  iteration    40110/10000000 | consumed samples:      5134080 | consumed tokens:  10514595840 | elapsed time per iteration (ms): 4213.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.363551E-01 | loss scale: 8192.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.381 | tokens per gpu per second (tgs): 1944.383 | TFLOPs: 15.65 |
g0184: [2024-08-11 11:01:49,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=40120, skipped=60, lr=[0.0001996998519325805, 0.0001996998519325805], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40120 loss: 0.7244 iter time (s): 4.158 samples/sec: 30.787
g0198:  iteration    40120/10000000 | consumed samples:      5135360 | consumed tokens:  10517217280 | elapsed time per iteration (ms): 4191.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.421264E-01 | loss scale: 8192.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.541 | tokens per gpu per second (tgs): 1954.627 | TFLOPs: 15.73 |
g0184: [2024-08-11 11:02:31,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=40130, skipped=60, lr=[0.00019969964266103676, 0.00019969964266103676], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40130 loss: 0.7534 iter time (s): 4.142 samples/sec: 30.904
g0198:  iteration    40130/10000000 | consumed samples:      5136640 | consumed tokens:  10519838720 | elapsed time per iteration (ms): 4175.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.452580E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.652 | tokens per gpu per second (tgs): 1961.737 | TFLOPs: 15.79 |
g0184: [2024-08-11 11:03:13,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=40140, skipped=60, lr=[0.00019969943331667902, 0.00019969943331667902], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40140 loss: 0.7340 iter time (s): 4.201 samples/sec: 30.468
g0198:  iteration    40140/10000000 | consumed samples:      5137920 | consumed tokens:  10522460160 | elapsed time per iteration (ms): 4233.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.444586E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.232 | tokens per gpu per second (tgs): 1934.879 | TFLOPs: 15.57 |
g0184: [2024-08-11 11:03:56,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=40150, skipped=60, lr=[0.0001996992238995074, 0.0001996992238995074], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40150 loss: 0.7550 iter time (s): 4.238 samples/sec: 30.203
g0198:  iteration    40150/10000000 | consumed samples:      5139200 | consumed tokens:  10525081600 | elapsed time per iteration (ms): 4270.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.475439E-01 | loss scale: 8192.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.970 | tokens per gpu per second (tgs): 1918.107 | TFLOPs: 15.44 |
g0184: [2024-08-11 11:04:38,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=40160, skipped=60, lr=[0.00019969901440952212, 0.00019969901440952212], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40160 loss: 0.7525 iter time (s): 4.212 samples/sec: 30.386
g0198:  iteration    40160/10000000 | consumed samples:      5140480 | consumed tokens:  10527703040 | elapsed time per iteration (ms): 4245.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.504363E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.152 | tokens per gpu per second (tgs): 1929.731 | TFLOPs: 15.53 |
g0184: [2024-08-11 11:05:21,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=40170, skipped=60, lr=[0.00019969880484672326, 0.00019969880484672326], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40170 loss: 0.7314 iter time (s): 4.245 samples/sec: 30.155
g0198:  iteration    40170/10000000 | consumed samples:      5141760 | consumed tokens:  10530324480 | elapsed time per iteration (ms): 4278.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.494136E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.915 | tokens per gpu per second (tgs): 1914.543 | TFLOPs: 15.41 |
g0184: [2024-08-11 11:06:02,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=40180, skipped=60, lr=[0.00019969859521111103, 0.00019969859521111103], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40180 loss: 0.7334 iter time (s): 4.065 samples/sec: 31.485
g0198:  iteration    40180/10000000 | consumed samples:      5143040 | consumed tokens:  10532945920 | elapsed time per iteration (ms): 4098.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.522589E-01 | loss scale: 8192.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.232 | tokens per gpu per second (tgs): 1998.856 | TFLOPs: 16.09 |
g0184: [2024-08-11 11:06:44,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=40190, skipped=60, lr=[0.00019969838550268557, 0.00019969838550268557], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40190 loss: 0.7356 iter time (s): 4.168 samples/sec: 30.710
g0198:  iteration    40190/10000000 | consumed samples:      5144320 | consumed tokens:  10535567360 | elapsed time per iteration (ms): 4200.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.465116E-01 | loss scale: 8192.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.472 | tokens per gpu per second (tgs): 1950.231 | TFLOPs: 15.69 |
g0184: [2024-08-11 11:07:26,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=40200, skipped=60, lr=[0.00019969817572144708, 0.00019969817572144708], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40200 loss: 0.7115 iter time (s): 4.159 samples/sec: 30.779
g0198:  iteration    40200/10000000 | consumed samples:      5145600 | consumed tokens:  10538188800 | elapsed time per iteration (ms): 4191.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.457597E-01 | loss scale: 8192.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.535 | tokens per gpu per second (tgs): 1954.264 | TFLOPs: 15.73 |
g0184: [2024-08-11 11:08:08,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=40210, skipped=60, lr=[0.00019969796586739565, 0.00019969796586739565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40210 loss: 0.7426 iter time (s): 4.191 samples/sec: 30.539
g0198:  iteration    40210/10000000 | consumed samples:      5146880 | consumed tokens:  10540810240 | elapsed time per iteration (ms): 4224.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.395876E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.301 | tokens per gpu per second (tgs): 1939.237 | TFLOPs: 15.61 |
g0184: [2024-08-11 11:08:50,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=40220, skipped=60, lr=[0.00019969775594053154, 0.00019969775594053154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40220 loss: 0.7479 iter time (s): 4.110 samples/sec: 31.141
g0198:  iteration    40220/10000000 | consumed samples:      5148160 | consumed tokens:  10543431680 | elapsed time per iteration (ms): 4142.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.466251E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.898 | tokens per gpu per second (tgs): 1977.504 | TFLOPs: 15.91 |
g0184: [2024-08-11 11:09:30,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=40230, skipped=60, lr=[0.00019969754594085482, 0.00019969754594085482], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40230 loss: 0.7343 iter time (s): 3.984 samples/sec: 32.130
g0198:  iteration    40230/10000000 | consumed samples:      5149440 | consumed tokens:  10546053120 | elapsed time per iteration (ms): 4016.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.415184E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.866 | tokens per gpu per second (tgs): 2039.414 | TFLOPs: 16.41 |
g0184: [2024-08-11 11:10:13,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=40240, skipped=60, lr=[0.0001996973358683657, 0.0001996973358683657], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40240 loss: 0.7560 iter time (s): 4.273 samples/sec: 29.959
g0198:  iteration    40240/10000000 | consumed samples:      5150720 | consumed tokens:  10548674560 | elapsed time per iteration (ms): 4310.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.511790E-01 | loss scale: 8192.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.695 | tokens per gpu per second (tgs): 1900.505 | TFLOPs: 15.29 |
g0184: [2024-08-11 11:10:57,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=40250, skipped=60, lr=[0.00019969712572306435, 0.00019969712572306435], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40250 loss: 0.7319 iter time (s): 4.363 samples/sec: 29.338
g0198:  iteration    40250/10000000 | consumed samples:      5152000 | consumed tokens:  10551296000 | elapsed time per iteration (ms): 4395.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.466665E-01 | loss scale: 8192.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.120 | tokens per gpu per second (tgs): 1863.679 | TFLOPs: 15.00 |
g0184: [2024-08-11 11:11:41,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=40260, skipped=60, lr=[0.0001996969155049509, 0.0001996969155049509], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40260 loss: 0.7502 iter time (s): 4.337 samples/sec: 29.515
g0198:  iteration    40260/10000000 | consumed samples:      5153280 | consumed tokens:  10553917440 | elapsed time per iteration (ms): 4369.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.566680E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.294 | tokens per gpu per second (tgs): 1874.842 | TFLOPs: 15.09 |
g0184: [2024-08-11 11:12:24,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=40270, skipped=60, lr=[0.00019969670521402552, 0.00019969670521402552], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40270 loss: 0.7410 iter time (s): 4.266 samples/sec: 30.002
g0198:  iteration    40270/10000000 | consumed samples:      5154560 | consumed tokens:  10556538880 | elapsed time per iteration (ms): 4299.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.365777E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.774 | tokens per gpu per second (tgs): 1905.532 | TFLOPs: 15.33 |
g0184: [2024-08-11 11:13:07,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=40280, skipped=60, lr=[0.00019969649485028834, 0.00019969649485028834], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40280 loss: 0.7464 iter time (s): 4.259 samples/sec: 30.057
g0198:  iteration    40280/10000000 | consumed samples:      5155840 | consumed tokens:  10559160320 | elapsed time per iteration (ms): 4291.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.490495E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.824 | tokens per gpu per second (tgs): 1908.713 | TFLOPs: 15.36 |
g0184: [2024-08-11 11:13:48,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=40290, skipped=60, lr=[0.0001996962844137396, 0.0001996962844137396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40290 loss: 0.7783 iter time (s): 4.135 samples/sec: 30.958
g0198:  iteration    40290/10000000 | consumed samples:      5157120 | consumed tokens:  10561781760 | elapsed time per iteration (ms): 4168.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.518234E-01 | loss scale: 8192.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.708 | tokens per gpu per second (tgs): 1965.322 | TFLOPs: 15.82 |
g0184: [2024-08-11 11:14:29,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=40300, skipped=60, lr=[0.0001996960739043794, 0.0001996960739043794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40300 loss: 0.7690 iter time (s): 4.054 samples/sec: 31.572
g0198:  iteration    40300/10000000 | consumed samples:      5158400 | consumed tokens:  10564403200 | elapsed time per iteration (ms): 4087.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.596241E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.317 | tokens per gpu per second (tgs): 2004.290 | TFLOPs: 16.13 |
g0184: [2024-08-11 11:15:11,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=40310, skipped=60, lr=[0.00019969586332220794, 0.00019969586332220794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40310 loss: 0.7434 iter time (s): 4.170 samples/sec: 30.698
g0198:  iteration    40310/10000000 | consumed samples:      5159680 | consumed tokens:  10567024640 | elapsed time per iteration (ms): 4202.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.470905E-01 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.457 | tokens per gpu per second (tgs): 1949.255 | TFLOPs: 15.69 |
g0184: [2024-08-11 11:15:53,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=40320, skipped=60, lr=[0.00019969565266722534, 0.00019969565266722534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40320 loss: 0.7361 iter time (s): 4.185 samples/sec: 30.585
g0198:  iteration    40320/10000000 | consumed samples:      5160960 | consumed tokens:  10569646080 | elapsed time per iteration (ms): 4221.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.414245E-01 | loss scale: 8192.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.322 | tokens per gpu per second (tgs): 1940.609 | TFLOPs: 15.62 |
g0184: [2024-08-11 11:16:35,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=40330, skipped=60, lr=[0.00019969544193943178, 0.00019969544193943178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40330 loss: 0.7491 iter time (s): 4.145 samples/sec: 30.882
g0198:  iteration    40330/10000000 | consumed samples:      5162240 | consumed tokens:  10572267520 | elapsed time per iteration (ms): 4177.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.427809E-01 | loss scale: 8192.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.640 | tokens per gpu per second (tgs): 1960.945 | TFLOPs: 15.78 |
g0184: [2024-08-11 11:17:17,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=40340, skipped=60, lr=[0.00019969523113882746, 0.00019969523113882746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40340 loss: 0.7089 iter time (s): 4.160 samples/sec: 30.766
g0198:  iteration    40340/10000000 | consumed samples:      5163520 | consumed tokens:  10574888960 | elapsed time per iteration (ms): 4194.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.405343E-01 | loss scale: 8192.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.519 | tokens per gpu per second (tgs): 1953.225 | TFLOPs: 15.72 |
g0184: [2024-08-11 11:18:00,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=40350, skipped=60, lr=[0.00019969502026541245, 0.00019969502026541245], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40350 loss: 0.7364 iter time (s): 4.223 samples/sec: 30.314
g0198:  iteration    40350/10000000 | consumed samples:      5164800 | consumed tokens:  10577510400 | elapsed time per iteration (ms): 4257.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.436483E-01 | loss scale: 8192.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.067 | tokens per gpu per second (tgs): 1924.265 | TFLOPs: 15.48 |
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0187: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0185: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0194: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0197: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0195: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0188: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0198: [2024-08-11 11:18:37,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
g0184: [2024-08-11 11:18:41,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=40360, skipped=60, lr=[0.00019969480931918701, 0.00019969480931918701], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40360 loss: 0.7235 iter time (s): 4.137 samples/sec: 30.940
g0198:  iteration    40360/10000000 | consumed samples:      5166080 | consumed tokens:  10580131840 | elapsed time per iteration (ms): 4169.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.436827E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.697 | tokens per gpu per second (tgs): 1964.614 | TFLOPs: 15.81 |
g0184: [2024-08-11 11:19:24,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=40370, skipped=60, lr=[0.0001996945983001513, 0.0001996945983001513], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40370 loss: 0.7285 iter time (s): 4.194 samples/sec: 30.520
g0198:  iteration    40370/10000000 | consumed samples:      5167360 | consumed tokens:  10582753280 | elapsed time per iteration (ms): 4226.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.485251E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.284 | tokens per gpu per second (tgs): 1938.151 | TFLOPs: 15.60 |
g0184: [2024-08-11 11:20:05,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=40380, skipped=60, lr=[0.00019969438720830537, 0.00019969438720830537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40380 loss: 0.7815 iter time (s): 4.143 samples/sec: 30.896
g0198:  iteration    40380/10000000 | consumed samples:      5168640 | consumed tokens:  10585374720 | elapsed time per iteration (ms): 4175.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.580282E-01 | loss scale: 16384.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.777 | TFLOPs: 15.79 |
g0184: [2024-08-11 11:20:49,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=40390, skipped=60, lr=[0.00019969417604364948, 0.00019969417604364948], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40390 loss: 0.7331 iter time (s): 4.296 samples/sec: 29.798
g0198:  iteration    40390/10000000 | consumed samples:      5169920 | consumed tokens:  10587996160 | elapsed time per iteration (ms): 4328.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.481648E-01 | loss scale: 16384.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.572 | tokens per gpu per second (tgs): 1892.636 | TFLOPs: 15.23 |
g0184: [2024-08-11 11:21:30,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=40400, skipped=60, lr=[0.0001996939648061838, 0.0001996939648061838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40400 loss: 0.7430 iter time (s): 4.098 samples/sec: 31.238
g0198:  iteration    40400/10000000 | consumed samples:      5171200 | consumed tokens:  10590617600 | elapsed time per iteration (ms): 4130.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.542959E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.990 | tokens per gpu per second (tgs): 1983.340 | TFLOPs: 15.96 |
g0184: [2024-08-11 11:22:12,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=40410, skipped=60, lr=[0.00019969375349590846, 0.00019969375349590846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40410 loss: 0.7586 iter time (s): 4.145 samples/sec: 30.884
g0198:  iteration    40410/10000000 | consumed samples:      5172480 | consumed tokens:  10593239040 | elapsed time per iteration (ms): 4177.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.549381E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.644 | tokens per gpu per second (tgs): 1961.199 | TFLOPs: 15.78 |
g0184: [2024-08-11 11:22:55,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=40420, skipped=60, lr=[0.0001996935421128236, 0.0001996935421128236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40420 loss: 0.7705 iter time (s): 4.262 samples/sec: 30.036
g0198:  iteration    40420/10000000 | consumed samples:      5173760 | consumed tokens:  10595860480 | elapsed time per iteration (ms): 4294.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.526523E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.807 | tokens per gpu per second (tgs): 1907.654 | TFLOPs: 15.35 |
g0184: [2024-08-11 11:23:38,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=40430, skipped=60, lr=[0.0001996933306569294, 0.0001996933306569294], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40430 loss: 0.7370 iter time (s): 4.286 samples/sec: 29.865
g0198:  iteration    40430/10000000 | consumed samples:      5175040 | consumed tokens:  10598481920 | elapsed time per iteration (ms): 4318.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.457761E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.638 | tokens per gpu per second (tgs): 1896.815 | TFLOPs: 15.26 |
g0184: [2024-08-11 11:24:19,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=40440, skipped=60, lr=[0.00019969311912822606, 0.00019969311912822606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40440 loss: 0.7577 iter time (s): 4.085 samples/sec: 31.333
g0198:  iteration    40440/10000000 | consumed samples:      5176320 | consumed tokens:  10601103360 | elapsed time per iteration (ms): 4118.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.407069E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.079 | tokens per gpu per second (tgs): 1989.081 | TFLOPs: 16.01 |
g0184: [2024-08-11 11:25:02,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=40450, skipped=60, lr=[0.00019969290752671368, 0.00019969290752671368], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40450 loss: 0.7707 iter time (s): 4.245 samples/sec: 30.152
g0198:  iteration    40450/10000000 | consumed samples:      5177600 | consumed tokens:  10603724800 | elapsed time per iteration (ms): 4278.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.589426E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.919 | tokens per gpu per second (tgs): 1914.789 | TFLOPs: 15.41 |
g0184: [2024-08-11 11:25:44,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=40460, skipped=60, lr=[0.00019969269585239248, 0.00019969269585239248], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40460 loss: 0.7706 iter time (s): 4.219 samples/sec: 30.337
g0198:  iteration    40460/10000000 | consumed samples:      5178880 | consumed tokens:  10606346240 | elapsed time per iteration (ms): 4253.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.532633E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.096 | tokens per gpu per second (tgs): 1926.148 | TFLOPs: 15.50 |
g0184: [2024-08-11 11:26:27,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=40470, skipped=60, lr=[0.0001996924841052626, 0.0001996924841052626], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40470 loss: 0.7669 iter time (s): 4.185 samples/sec: 30.582
g0198:  iteration    40470/10000000 | consumed samples:      5180160 | consumed tokens:  10608967680 | elapsed time per iteration (ms): 4218.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.526688E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.344 | tokens per gpu per second (tgs): 1942.000 | TFLOPs: 15.63 |
g0184: [2024-08-11 11:27:09,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=40480, skipped=60, lr=[0.00019969227228532416, 0.00019969227228532416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40480 loss: 0.7235 iter time (s): 4.238 samples/sec: 30.203
g0198:  iteration    40480/10000000 | consumed samples:      5181440 | consumed tokens:  10611589120 | elapsed time per iteration (ms): 4271.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.458062E-01 | loss scale: 16384.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.968 | tokens per gpu per second (tgs): 1917.947 | TFLOPs: 15.43 |
g0184: [2024-08-11 11:27:52,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=40490, skipped=60, lr=[0.00019969206039257743, 0.00019969206039257743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40490 loss: 0.7561 iter time (s): 4.210 samples/sec: 30.401
g0198:  iteration    40490/10000000 | consumed samples:      5182720 | consumed tokens:  10614210560 | elapsed time per iteration (ms): 4243.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.552242E-01 | loss scale: 16384.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.164 | tokens per gpu per second (tgs): 1930.507 | TFLOPs: 15.54 |
g0184: [2024-08-11 11:28:34,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=40500, skipped=60, lr=[0.00019969184842702245, 0.00019969184842702245], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40500 loss: 0.7504 iter time (s): 4.224 samples/sec: 30.306
g0198:  iteration    40500/10000000 | consumed samples:      5184000 | consumed tokens:  10616832000 | elapsed time per iteration (ms): 4256.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.416569E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.070 | tokens per gpu per second (tgs): 1924.469 | TFLOPs: 15.49 |
g0184: [2024-08-11 11:29:16,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=40510, skipped=60, lr=[0.00019969163638865946, 0.00019969163638865946], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40510 loss: 0.7408 iter time (s): 4.108 samples/sec: 31.158
g0198:  iteration    40510/10000000 | consumed samples:      5185280 | consumed tokens:  10619453440 | elapsed time per iteration (ms): 4142.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.467865E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.900 | tokens per gpu per second (tgs): 1977.623 | TFLOPs: 15.91 |
g0184: [2024-08-11 11:29:57,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=40520, skipped=60, lr=[0.0001996914242774886, 0.0001996914242774886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40520 loss: 0.7510 iter time (s): 4.071 samples/sec: 31.444
g0198:  iteration    40520/10000000 | consumed samples:      5186560 | consumed tokens:  10622074880 | elapsed time per iteration (ms): 4105.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.546191E-01 | loss scale: 16384.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.182 | tokens per gpu per second (tgs): 1995.626 | TFLOPs: 16.06 |
g0184: [2024-08-11 11:30:39,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=40530, skipped=60, lr=[0.00019969121209351001, 0.00019969121209351001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40530 loss: 0.7608 iter time (s): 4.175 samples/sec: 30.657
g0198:  iteration    40530/10000000 | consumed samples:      5187840 | consumed tokens:  10624696320 | elapsed time per iteration (ms): 4208.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.485531E-01 | loss scale: 16384.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.418 | tokens per gpu per second (tgs): 1946.782 | TFLOPs: 15.67 |
g0184: [2024-08-11 11:31:22,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=40540, skipped=60, lr=[0.00019969099983672392, 0.00019969099983672392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40540 loss: 0.7339 iter time (s): 4.284 samples/sec: 29.881
g0198:  iteration    40540/10000000 | consumed samples:      5189120 | consumed tokens:  10627317760 | elapsed time per iteration (ms): 4316.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.448067E-01 | loss scale: 16384.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.654 | tokens per gpu per second (tgs): 1897.856 | TFLOPs: 15.27 |
g0184: [2024-08-11 11:32:04,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=40550, skipped=60, lr=[0.00019969078750713045, 0.00019969078750713045], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40550 loss: 0.7377 iter time (s): 4.185 samples/sec: 30.585
g0198:  iteration    40550/10000000 | consumed samples:      5190400 | consumed tokens:  10629939200 | elapsed time per iteration (ms): 4217.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.405569E-01 | loss scale: 16384.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.347 | tokens per gpu per second (tgs): 1942.228 | TFLOPs: 15.63 |
g0184: [2024-08-11 11:32:46,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=40560, skipped=60, lr=[0.00019969057510472974, 0.00019969057510472974], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40560 loss: 0.7525 iter time (s): 4.115 samples/sec: 31.106
g0198:  iteration    40560/10000000 | consumed samples:      5191680 | consumed tokens:  10632560640 | elapsed time per iteration (ms): 4149.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.501999E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.845 | tokens per gpu per second (tgs): 1974.100 | TFLOPs: 15.89 |
g0184: [2024-08-11 11:33:27,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=40570, skipped=60, lr=[0.000199690362629522, 0.000199690362629522], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40570 loss: 0.7039 iter time (s): 4.102 samples/sec: 31.202
g0198:  iteration    40570/10000000 | consumed samples:      5192960 | consumed tokens:  10635182080 | elapsed time per iteration (ms): 4140.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.559126E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.913 | tokens per gpu per second (tgs): 1978.427 | TFLOPs: 15.92 |
g0184: [2024-08-11 11:34:09,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=40580, skipped=60, lr=[0.00019969015008150737, 0.00019969015008150737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40580 loss: 0.7387 iter time (s): 4.118 samples/sec: 31.083
g0198:  iteration    40580/10000000 | consumed samples:      5194240 | consumed tokens:  10637803520 | elapsed time per iteration (ms): 4150.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.544998E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.837 | tokens per gpu per second (tgs): 1973.594 | TFLOPs: 15.88 |
g0184: [2024-08-11 11:34:51,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=40590, skipped=60, lr=[0.000199689937460686, 0.000199689937460686], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40590 loss: 0.7582 iter time (s): 4.188 samples/sec: 30.562
g0198:  iteration    40590/10000000 | consumed samples:      5195520 | consumed tokens:  10640424960 | elapsed time per iteration (ms): 4220.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.497481E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.327 | tokens per gpu per second (tgs): 1940.909 | TFLOPs: 15.62 |
g0184: [2024-08-11 11:35:32,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=40600, skipped=60, lr=[0.00019968972476705807, 0.00019968972476705807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40600 loss: 0.7333 iter time (s): 4.074 samples/sec: 31.419
g0198:  iteration    40600/10000000 | consumed samples:      5196800 | consumed tokens:  10643046400 | elapsed time per iteration (ms): 4106.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.488495E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.170 | tokens per gpu per second (tgs): 1994.901 | TFLOPs: 16.05 |
g0184: [2024-08-11 11:36:14,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=40610, skipped=60, lr=[0.00019968951200062375, 0.00019968951200062375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40610 loss: 0.7379 iter time (s): 4.138 samples/sec: 30.929
g0198:  iteration    40610/10000000 | consumed samples:      5198080 | consumed tokens:  10645667840 | elapsed time per iteration (ms): 4171.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.463333E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.686 | tokens per gpu per second (tgs): 1963.925 | TFLOPs: 15.80 |
g0184: [2024-08-11 11:36:56,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=40620, skipped=60, lr=[0.00019968929916138318, 0.00019968929916138318], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40620 loss: 0.7618 iter time (s): 4.193 samples/sec: 30.527
g0198:  iteration    40620/10000000 | consumed samples:      5199360 | consumed tokens:  10648289280 | elapsed time per iteration (ms): 4225.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.448726E-01 | loss scale: 16384.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.291 | tokens per gpu per second (tgs): 1938.642 | TFLOPs: 15.60 |
g0184: [2024-08-11 11:37:38,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=40630, skipped=60, lr=[0.00019968908624933658, 0.00019968908624933658], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40630 loss: 0.7276 iter time (s): 4.201 samples/sec: 30.469
g0198:  iteration    40630/10000000 | consumed samples:      5200640 | consumed tokens:  10650910720 | elapsed time per iteration (ms): 4233.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.427584E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.235 | tokens per gpu per second (tgs): 1935.019 | TFLOPs: 15.57 |
g0184: [2024-08-11 11:38:22,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=40640, skipped=60, lr=[0.00019968887326448403, 0.00019968887326448403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40640 loss: 0.7425 iter time (s): 4.370 samples/sec: 29.291
g0198:  iteration    40640/10000000 | consumed samples:      5201920 | consumed tokens:  10653532160 | elapsed time per iteration (ms): 4405.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.453170E-01 | loss scale: 16384.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.055 | tokens per gpu per second (tgs): 1859.548 | TFLOPs: 14.96 |
g0184: [2024-08-11 11:39:04,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=40650, skipped=60, lr=[0.00019968866020682576, 0.00019968866020682576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40650 loss: 0.7184 iter time (s): 4.169 samples/sec: 30.705
g0198:  iteration    40650/10000000 | consumed samples:      5203200 | consumed tokens:  10656153600 | elapsed time per iteration (ms): 4215.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.444818E-01 | loss scale: 16384.0 | grad norm: 0.153 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.362 | tokens per gpu per second (tgs): 1943.176 | TFLOPs: 15.64 |
g0184: [2024-08-11 11:39:45,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=40660, skipped=60, lr=[0.0001996884470763619, 0.0001996884470763619], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40660 loss: 0.7418 iter time (s): 4.054 samples/sec: 31.574
g0198:  iteration    40660/10000000 | consumed samples:      5204480 | consumed tokens:  10658775040 | elapsed time per iteration (ms): 4087.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.460662E-01 | loss scale: 16384.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.318 | tokens per gpu per second (tgs): 2004.320 | TFLOPs: 16.13 |
g0184: [2024-08-11 11:40:28,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=40670, skipped=60, lr=[0.00019968823387309263, 0.00019968823387309263], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40670 loss: 0.7367 iter time (s): 4.199 samples/sec: 30.487
g0198:  iteration    40670/10000000 | consumed samples:      5205760 | consumed tokens:  10661396480 | elapsed time per iteration (ms): 4231.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.528396E-01 | loss scale: 16384.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.253 | tokens per gpu per second (tgs): 1936.193 | TFLOPs: 15.58 |
g0184: [2024-08-11 11:41:10,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=40680, skipped=60, lr=[0.00019968802059701807, 0.00019968802059701807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40680 loss: 0.7356 iter time (s): 4.226 samples/sec: 30.292
g0198:  iteration    40680/10000000 | consumed samples:      5207040 | consumed tokens:  10664017920 | elapsed time per iteration (ms): 4258.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.385073E-01 | loss scale: 16384.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.056 | tokens per gpu per second (tgs): 1923.575 | TFLOPs: 15.48 |
g0184: [2024-08-11 11:41:53,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=40690, skipped=60, lr=[0.00019968780724813847, 0.00019968780724813847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40690 loss: 0.7517 iter time (s): 4.273 samples/sec: 29.953
g0198:  iteration    40690/10000000 | consumed samples:      5208320 | consumed tokens:  10666639360 | elapsed time per iteration (ms): 4305.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.446860E-01 | loss scale: 16384.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.727 | tokens per gpu per second (tgs): 1902.559 | TFLOPs: 15.31 |
g0184: [2024-08-11 11:42:36,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=40700, skipped=60, lr=[0.00019968759382645395, 0.00019968759382645395], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40700 loss: 0.7596 iter time (s): 4.272 samples/sec: 29.966
g0198:  iteration    40700/10000000 | consumed samples:      5209600 | consumed tokens:  10669260800 | elapsed time per iteration (ms): 4303.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.422931E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.741 | tokens per gpu per second (tgs): 1903.413 | TFLOPs: 15.32 |
g0184: [2024-08-11 11:43:18,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=40710, skipped=60, lr=[0.00019968738033196467, 0.00019968738033196467], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40710 loss: 0.7373 iter time (s): 4.134 samples/sec: 30.962
g0198:  iteration    40710/10000000 | consumed samples:      5210880 | consumed tokens:  10671882240 | elapsed time per iteration (ms): 4169.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.382469E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.703 | tokens per gpu per second (tgs): 1964.987 | TFLOPs: 15.81 |
g0184: [2024-08-11 11:44:00,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=40720, skipped=60, lr=[0.00019968716676467074, 0.00019968716676467074], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40720 loss: 0.7457 iter time (s): 4.206 samples/sec: 30.436
g0198:  iteration    40720/10000000 | consumed samples:      5212160 | consumed tokens:  10674503680 | elapsed time per iteration (ms): 4244.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.505077E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.160 | tokens per gpu per second (tgs): 1930.216 | TFLOPs: 15.53 |
g0184: [2024-08-11 11:44:42,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=40730, skipped=60, lr=[0.00019968695312457243, 0.00019968695312457243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40730 loss: 0.7164 iter time (s): 4.150 samples/sec: 30.843
g0198:  iteration    40730/10000000 | consumed samples:      5213440 | consumed tokens:  10677125120 | elapsed time per iteration (ms): 4183.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.468535E-01 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.594 | tokens per gpu per second (tgs): 1958.026 | TFLOPs: 15.76 |
g0184: [2024-08-11 11:45:23,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=40740, skipped=60, lr=[0.00019968673941166983, 0.00019968673941166983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40740 loss: 0.7295 iter time (s): 4.009 samples/sec: 31.926
g0198:  iteration    40740/10000000 | consumed samples:      5214720 | consumed tokens:  10679746560 | elapsed time per iteration (ms): 4043.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.456724E-01 | loss scale: 16384.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.655 | tokens per gpu per second (tgs): 2025.949 | TFLOPs: 16.30 |
g0184: [2024-08-11 11:46:05,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=40750, skipped=60, lr=[0.00019968652562596316, 0.00019968652562596316], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40750 loss: 0.7800 iter time (s): 4.201 samples/sec: 30.470
g0198:  iteration    40750/10000000 | consumed samples:      5216000 | consumed tokens:  10682368000 | elapsed time per iteration (ms): 4234.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.560911E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.227 | tokens per gpu per second (tgs): 1934.507 | TFLOPs: 15.57 |
g0184: [2024-08-11 11:46:49,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=40760, skipped=60, lr=[0.00019968631176745254, 0.00019968631176745254], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40760 loss: 0.7362 iter time (s): 4.341 samples/sec: 29.489
g0198:  iteration    40760/10000000 | consumed samples:      5217280 | consumed tokens:  10684989440 | elapsed time per iteration (ms): 4373.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.467690E-01 | loss scale: 16384.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.269 | tokens per gpu per second (tgs): 1873.213 | TFLOPs: 15.07 |
g0184: [2024-08-11 11:47:30,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=40770, skipped=60, lr=[0.00019968609783613812, 0.00019968609783613812], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40770 loss: 0.7771 iter time (s): 4.111 samples/sec: 31.135
g0198:  iteration    40770/10000000 | consumed samples:      5218560 | consumed tokens:  10687610880 | elapsed time per iteration (ms): 4144.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.454688E-01 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.884 | tokens per gpu per second (tgs): 1976.607 | TFLOPs: 15.91 |
g0184: [2024-08-11 11:48:12,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=40780, skipped=60, lr=[0.00019968588383202011, 0.00019968588383202011], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40780 loss: 0.6981 iter time (s): 4.189 samples/sec: 30.558
g0198:  iteration    40780/10000000 | consumed samples:      5219840 | consumed tokens:  10690232320 | elapsed time per iteration (ms): 4222.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.334208E-01 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.317 | tokens per gpu per second (tgs): 1940.275 | TFLOPs: 15.61 |
g0184: [2024-08-11 11:48:54,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=40790, skipped=60, lr=[0.00019968566975509867, 0.00019968566975509867], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40790 loss: 0.7529 iter time (s): 4.133 samples/sec: 30.968
g0198:  iteration    40790/10000000 | consumed samples:      5221120 | consumed tokens:  10692853760 | elapsed time per iteration (ms): 4167.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.416620E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.714 | tokens per gpu per second (tgs): 1965.670 | TFLOPs: 15.82 |
g0184: [2024-08-11 11:49:37,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=40800, skipped=60, lr=[0.0001996854556053739, 0.0001996854556053739], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40800 loss: 0.7328 iter time (s): 4.224 samples/sec: 30.303
g0198:  iteration    40800/10000000 | consumed samples:      5222400 | consumed tokens:  10695475200 | elapsed time per iteration (ms): 4257.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.496707E-01 | loss scale: 16384.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.066 | tokens per gpu per second (tgs): 1924.230 | TFLOPs: 15.48 |
g0184: [2024-08-11 11:50:18,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=40810, skipped=60, lr=[0.00019968524138284607, 0.00019968524138284607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40810 loss: 0.7351 iter time (s): 4.121 samples/sec: 31.062
g0198:  iteration    40810/10000000 | consumed samples:      5223680 | consumed tokens:  10698096640 | elapsed time per iteration (ms): 4153.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.422261E-01 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.817 | tokens per gpu per second (tgs): 1972.306 | TFLOPs: 15.87 |
g0184: [2024-08-11 11:51:00,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=40820, skipped=60, lr=[0.00019968502708751524, 0.00019968502708751524], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40820 loss: 0.7241 iter time (s): 4.196 samples/sec: 30.508
g0198:  iteration    40820/10000000 | consumed samples:      5224960 | consumed tokens:  10700718080 | elapsed time per iteration (ms): 4230.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.437691E-01 | loss scale: 16384.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.259 | tokens per gpu per second (tgs): 1936.572 | TFLOPs: 15.58 |
g0184: [2024-08-11 11:51:43,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=40830, skipped=60, lr=[0.00019968481271938167, 0.00019968481271938167], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40830 loss: 0.7369 iter time (s): 4.176 samples/sec: 30.653
g0198:  iteration    40830/10000000 | consumed samples:      5226240 | consumed tokens:  10703339520 | elapsed time per iteration (ms): 4210.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.522452E-01 | loss scale: 16384.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.404 | tokens per gpu per second (tgs): 1945.846 | TFLOPs: 15.66 |
g0184: [2024-08-11 11:52:25,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=40840, skipped=60, lr=[0.00019968459827844547, 0.00019968459827844547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40840 loss: 0.7578 iter time (s): 4.177 samples/sec: 30.642
g0198:  iteration    40840/10000000 | consumed samples:      5227520 | consumed tokens:  10705960960 | elapsed time per iteration (ms): 4211.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.552517E-01 | loss scale: 16384.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.396 | tokens per gpu per second (tgs): 1945.360 | TFLOPs: 15.65 |
g0184: [2024-08-11 11:53:06,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=40850, skipped=60, lr=[0.0001996843837647068, 0.0001996843837647068], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40850 loss: 0.7291 iter time (s): 4.133 samples/sec: 30.969
g0198:  iteration    40850/10000000 | consumed samples:      5228800 | consumed tokens:  10708582400 | elapsed time per iteration (ms): 4165.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.404013E-01 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.726 | tokens per gpu per second (tgs): 1966.475 | TFLOPs: 15.82 |
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0188: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0185: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0195: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0187: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 11:53:44,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0197: [2024-08-11 11:53:44,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0198: [2024-08-11 11:53:44,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0184: [2024-08-11 11:53:48,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=40860, skipped=60, lr=[0.00019968416917816583, 0.00019968416917816583], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40860 loss: 0.7370 iter time (s): 4.130 samples/sec: 30.993
g0198:  iteration    40860/10000000 | consumed samples:      5230080 | consumed tokens:  10711203840 | elapsed time per iteration (ms): 4163.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.501945E-01 | loss scale: 32768.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.745 | tokens per gpu per second (tgs): 1967.682 | TFLOPs: 15.83 |
g0184: [2024-08-11 11:54:30,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=40870, skipped=60, lr=[0.00019968395451882277, 0.00019968395451882277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40870 loss: 0.7496 iter time (s): 4.143 samples/sec: 30.893
g0198:  iteration    40870/10000000 | consumed samples:      5231360 | consumed tokens:  10713825280 | elapsed time per iteration (ms): 4175.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.459794E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.818 | TFLOPs: 15.79 |
g0184: [2024-08-11 11:55:13,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=40880, skipped=60, lr=[0.00019968373978667772, 0.00019968373978667772], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40880 loss: 0.7929 iter time (s): 4.258 samples/sec: 30.058
g0198:  iteration    40880/10000000 | consumed samples:      5232640 | consumed tokens:  10716446720 | elapsed time per iteration (ms): 4290.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.505939E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.832 | tokens per gpu per second (tgs): 1909.220 | TFLOPs: 15.36 |
g0184: [2024-08-11 11:55:55,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=40890, skipped=60, lr=[0.00019968352498173086, 0.00019968352498173086], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40890 loss: 0.7469 iter time (s): 4.208 samples/sec: 30.422
g0198:  iteration    40890/10000000 | consumed samples:      5233920 | consumed tokens:  10719068160 | elapsed time per iteration (ms): 4240.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.342925E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.187 | tokens per gpu per second (tgs): 1931.991 | TFLOPs: 15.55 |
g0184: [2024-08-11 11:56:36,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=40900, skipped=60, lr=[0.0001996833101039824, 0.0001996833101039824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40900 loss: 0.7448 iter time (s): 4.031 samples/sec: 31.750
g0198:  iteration    40900/10000000 | consumed samples:      5235200 | consumed tokens:  10721689600 | elapsed time per iteration (ms): 4063.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.441661E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.498 | tokens per gpu per second (tgs): 2015.891 | TFLOPs: 16.22 |
g0184: [2024-08-11 11:57:18,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=40910, skipped=60, lr=[0.00019968309515343248, 0.00019968309515343248], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40910 loss: 0.7710 iter time (s): 4.215 samples/sec: 30.364
g0198:  iteration    40910/10000000 | consumed samples:      5236480 | consumed tokens:  10724311040 | elapsed time per iteration (ms): 4248.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.449321E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.131 | tokens per gpu per second (tgs): 1928.412 | TFLOPs: 15.52 |
g0184: [2024-08-11 11:58:00,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=40920, skipped=60, lr=[0.00019968288013008125, 0.00019968288013008125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40920 loss: 0.7486 iter time (s): 4.198 samples/sec: 30.492
g0198:  iteration    40920/10000000 | consumed samples:      5237760 | consumed tokens:  10726932480 | elapsed time per iteration (ms): 4230.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.384497E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.257 | tokens per gpu per second (tgs): 1936.432 | TFLOPs: 15.58 |
g0184: [2024-08-11 11:58:43,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=40930, skipped=60, lr=[0.00019968266503392885, 0.00019968266503392885], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40930 loss: 0.7396 iter time (s): 4.237 samples/sec: 30.209
g0198:  iteration    40930/10000000 | consumed samples:      5239040 | consumed tokens:  10729553920 | elapsed time per iteration (ms): 4269.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.449526E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.982 | tokens per gpu per second (tgs): 1918.847 | TFLOPs: 15.44 |
g0184: [2024-08-11 11:59:27,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=40940, skipped=60, lr=[0.0001996824498649755, 0.0001996824498649755], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40940 loss: 0.7434 iter time (s): 4.315 samples/sec: 29.667
g0198:  iteration    40940/10000000 | consumed samples:      5240320 | consumed tokens:  10732175360 | elapsed time per iteration (ms): 4347.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.460135E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.446 | tokens per gpu per second (tgs): 1884.527 | TFLOPs: 15.17 |
g0184: [2024-08-11 12:00:11,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=40950, skipped=60, lr=[0.00019968223462322137, 0.00019968223462322137], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40950 loss: 0.7027 iter time (s): 4.369 samples/sec: 29.299
g0198:  iteration    40950/10000000 | consumed samples:      5241600 | consumed tokens:  10734796800 | elapsed time per iteration (ms): 4401.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.465292E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.083 | tokens per gpu per second (tgs): 1861.338 | TFLOPs: 14.98 |
g0184: [2024-08-11 12:00:52,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=40960, skipped=60, lr=[0.00019968201930866658, 0.00019968201930866658], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40960 loss: 0.7718 iter time (s): 4.107 samples/sec: 31.168
g0198:  iteration    40960/10000000 | consumed samples:      5242880 | consumed tokens:  10737418240 | elapsed time per iteration (ms): 4139.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.496400E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.918 | tokens per gpu per second (tgs): 1978.782 | TFLOPs: 15.92 |
g0184: [2024-08-11 12:01:34,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=40970, skipped=60, lr=[0.00019968180392131133, 0.00019968180392131133], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40970 loss: 0.7466 iter time (s): 4.168 samples/sec: 30.709
g0198:  iteration    40970/10000000 | consumed samples:      5244160 | consumed tokens:  10740039680 | elapsed time per iteration (ms): 4200.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.449506E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.472 | tokens per gpu per second (tgs): 1950.209 | TFLOPs: 15.69 |
g0184: [2024-08-11 12:02:17,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=40980, skipped=60, lr=[0.00019968158846115575, 0.00019968158846115575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40980 loss: 0.7360 iter time (s): 4.225 samples/sec: 30.294
g0198:  iteration    40980/10000000 | consumed samples:      5245440 | consumed tokens:  10742661120 | elapsed time per iteration (ms): 4257.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.458020E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.063 | tokens per gpu per second (tgs): 1924.041 | TFLOPs: 15.48 |
g0184: [2024-08-11 12:02:58,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=40990, skipped=60, lr=[0.00019968137292820007, 0.00019968137292820007], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 40990 loss: 0.7604 iter time (s): 4.122 samples/sec: 31.057
g0198:  iteration    40990/10000000 | consumed samples:      5246720 | consumed tokens:  10745282560 | elapsed time per iteration (ms): 4153.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.405044E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.815 | tokens per gpu per second (tgs): 1972.150 | TFLOPs: 15.87 |
g0184: [2024-08-11 12:03:41,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=60, lr=[0.00019968115732244435, 0.00019968115732244435], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41000 loss: 0.7383 iter time (s): 4.209 samples/sec: 30.412
g0198:  iteration    41000/10000000 | consumed samples:      5248000 | consumed tokens:  10747904000 | elapsed time per iteration (ms): 4241.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.434709E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.180 | tokens per gpu per second (tgs): 1931.552 | TFLOPs: 15.54 |
g0198: ----------------------------------------------------------------------------------------
g0198:  validation loss at iteration 41000 | lm loss value: NAN | lm loss PPL: 4.851652E+08 | 
g0198: ----------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   41000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 12:10:22,696] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step41000 is about to be saved!
g0184: [2024-08-11 12:10:22,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0184: [2024-08-11 12:10:22,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0184: [2024-08-11 12:10:22,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0198: [2024-08-11 12:10:22,702] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0198: [2024-08-11 12:10:22,702] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0198: [2024-08-11 12:10:22,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0188: [2024-08-11 12:10:22,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0188: [2024-08-11 12:10:22,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0188: [2024-08-11 12:10:22,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0187: [2024-08-11 12:10:22,705] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0187: [2024-08-11 12:10:22,705] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0187: [2024-08-11 12:10:22,705] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0197: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0197: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0194: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0194: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0195: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0195: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0195: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0197: [2024-08-11 12:10:22,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0194: [2024-08-11 12:10:22,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0185: [2024-08-11 12:10:22,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0185: [2024-08-11 12:10:22,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0185: [2024-08-11 12:10:22,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0198: [2024-08-11 12:10:22,731] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_23-model_00-model_states.pt...
g0188: [2024-08-11 12:10:22,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_11-model_00-model_states.pt...
g0194: [2024-08-11 12:10:22,738] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_14-model_00-model_states.pt...
g0185: [2024-08-11 12:10:22,740] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_05-model_00-model_states.pt...
g0187: [2024-08-11 12:10:22,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_08-model_00-model_states.pt...
g0195: [2024-08-11 12:10:22,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_17-model_00-model_states.pt...
g0197: [2024-08-11 12:10:22,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_20-model_00-model_states.pt...
g0184: [2024-08-11 12:10:22,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_01-model_00-model_states.pt...
g0198: [2024-08-11 12:10:22,841] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 12:10:22,841] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 12:10:22,844] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_24-model_00-model_states.pt.
g0185: [2024-08-11 12:10:22,847] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_05-model_00-model_states.pt.
g0194: [2024-08-11 12:10:22,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_14-model_00-model_states.pt.
g0197: [2024-08-11 12:10:22,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_20-model_00-model_states.pt.
g0185: [2024-08-11 12:10:22,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_06-model_00-model_states.pt...
g0195: [2024-08-11 12:10:22,892] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_17-model_00-model_states.pt.
g0194: [2024-08-11 12:10:22,894] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_15-model_00-model_states.pt...
g0198: [2024-08-11 12:10:22,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_25-model_00-model_states.pt...
g0188: [2024-08-11 12:10:22,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_11-model_00-model_states.pt.
g0197: [2024-08-11 12:10:22,921] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_21-model_00-model_states.pt...
g0195: [2024-08-11 12:10:22,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_18-model_00-model_states.pt...
g0187: [2024-08-11 12:10:22,934] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_08-model_00-model_states.pt.
g0188: [2024-08-11 12:10:22,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_12-model_00-model_states.pt...
g0184: [2024-08-11 12:10:22,963] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_01-model_00-model_states.pt.
g0187: [2024-08-11 12:10:22,975] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_09-model_00-model_states.pt...
g0184: [2024-08-11 12:10:22,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_02-model_00-model_states.pt...
g0185: [2024-08-11 12:10:23,009] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_06-model_00-model_states.pt.
g0194: [2024-08-11 12:10:23,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_15-model_00-model_states.pt.
g0197: [2024-08-11 12:10:23,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_21-model_00-model_states.pt.
g0185: [2024-08-11 12:10:23,035] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_07-model_00-model_states.pt...
g0194: [2024-08-11 12:10:23,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_16-model_00-model_states.pt...
g0198: [2024-08-11 12:10:23,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 12:10:23,054] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_07_model_states.pt...
g0195: [2024-08-11 12:10:23,055] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_18-model_00-model_states.pt.
g0197: [2024-08-11 12:10:23,070] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_22-model_00-model_states.pt...
g0195: [2024-08-11 12:10:23,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_19-model_00-model_states.pt...
g0187: [2024-08-11 12:10:23,091] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_09-model_00-model_states.pt.
g0184: [2024-08-11 12:10:23,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_02-model_00-model_states.pt.
g0187: [2024-08-11 12:10:23,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_10-model_00-model_states.pt...
g0184: [2024-08-11 12:10:23,132] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_03-model_00-model_states.pt...
g0197: [2024-08-11 12:10:23,199] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_22-model_00-model_states.pt.
g0188: [2024-08-11 12:10:23,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_12-model_00-model_states.pt.
g0197: [2024-08-11 12:10:23,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_06_model_states.pt...
g0185: [2024-08-11 12:10:23,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 12:10:23,221] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_01_model_states.pt...
g0194: [2024-08-11 12:10:23,222] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 12:10:23,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_04_model_states.pt...
g0184: [2024-08-11 12:10:23,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_03-model_00-model_states.pt.
g0188: [2024-08-11 12:10:23,231] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_13-model_00-model_states.pt...
g0195: [2024-08-11 12:10:23,234] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 12:10:23,235] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 12:10:23,251] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_04-model_00-model_states.pt...
g0187: [2024-08-11 12:10:23,285] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 12:10:23,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_02_model_states.pt...
g0188: [2024-08-11 12:10:23,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 12:10:23,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_03_model_states.pt...
g0184: [2024-08-11 12:10:23,340] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 12:10:23,341] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_00_model_states.pt
g0184: [2024-08-11 12:10:23,341] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 12:10:25,017] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 12:10:25,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0185: [2024-08-11 12:10:25,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 12:10:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0197: [2024-08-11 12:10:25,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 12:10:25,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0194: [2024-08-11 12:10:25,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 12:10:25,584] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0195: [2024-08-11 12:10:25,648] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 12:10:25,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0188: [2024-08-11 12:10:25,687] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 12:10:25,688] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0187: [2024-08-11 12:10:25,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 12:10:25,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0184: [2024-08-11 12:10:27,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 12:10:27,331] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0184:   successfully saved checkpoint at iteration   41000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.8, Latency(second): 4.69
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (4690.14, 4690.44)
g0184: [2024-08-11 12:11:10,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=41010, skipped=60, lr=[0.00019968094164388885, 0.00019968094164388885], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41010 loss: 0.7552 iter time (s): 4.294 samples/sec: 29.808
g0198:  iteration    41010/10000000 | consumed samples:      5249280 | consumed tokens:  10750525440 | elapsed time per iteration (ms): 44956.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.566107E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.847 | tokens per gpu per second (tgs): 182.221 | TFLOPs: 1.47 |
g0184: [2024-08-11 12:11:53,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=41020, skipped=60, lr=[0.00019968072589253373, 0.00019968072589253373], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41020 loss: 0.7576 iter time (s): 4.215 samples/sec: 30.370
g0198:  iteration    41020/10000000 | consumed samples:      5250560 | consumed tokens:  10753146880 | elapsed time per iteration (ms): 4247.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.505669E-01 | loss scale: 32768.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.137 | tokens per gpu per second (tgs): 1928.793 | TFLOPs: 15.52 |
g0184: [2024-08-11 12:12:33,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=41030, skipped=60, lr=[0.0001996805100683791, 0.0001996805100683791], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41030 loss: 0.7317 iter time (s): 4.024 samples/sec: 31.810
g0198:  iteration    41030/10000000 | consumed samples:      5251840 | consumed tokens:  10755768320 | elapsed time per iteration (ms): 4056.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.541350E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.556 | tokens per gpu per second (tgs): 2019.610 | TFLOPs: 16.25 |
g0184: [2024-08-11 12:13:16,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=41040, skipped=60, lr=[0.0001996802941714252, 0.0001996802941714252], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41040 loss: 0.7332 iter time (s): 4.284 samples/sec: 29.879
g0198:  iteration    41040/10000000 | consumed samples:      5253120 | consumed tokens:  10758389760 | elapsed time per iteration (ms): 4316.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.444705E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.654 | tokens per gpu per second (tgs): 1897.872 | TFLOPs: 15.27 |
g0184: [2024-08-11 12:13:59,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=41050, skipped=60, lr=[0.0001996800782016721, 0.0001996800782016721], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41050 loss: 0.7367 iter time (s): 4.257 samples/sec: 30.068
g0198:  iteration    41050/10000000 | consumed samples:      5254400 | consumed tokens:  10761011200 | elapsed time per iteration (ms): 4290.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.421627E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.836 | tokens per gpu per second (tgs): 1909.513 | TFLOPs: 15.37 |
g0184: [2024-08-11 12:14:40,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=41060, skipped=60, lr=[0.00019967986215912005, 0.00019967986215912005], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41060 loss: 0.7723 iter time (s): 4.064 samples/sec: 31.498
g0198:  iteration    41060/10000000 | consumed samples:      5255680 | consumed tokens:  10763632640 | elapsed time per iteration (ms): 4096.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.470081E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.248 | tokens per gpu per second (tgs): 1999.891 | TFLOPs: 16.09 |
g0184: [2024-08-11 12:15:23,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=41070, skipped=60, lr=[0.00019967964604376918, 0.00019967964604376918], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41070 loss: 0.7349 iter time (s): 4.249 samples/sec: 30.124
g0198:  iteration    41070/10000000 | consumed samples:      5256960 | consumed tokens:  10766254080 | elapsed time per iteration (ms): 4287.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.385102E-01 | loss scale: 32768.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.858 | tokens per gpu per second (tgs): 1910.889 | TFLOPs: 15.38 |
g0184: [2024-08-11 12:16:06,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=41080, skipped=60, lr=[0.00019967942985561966, 0.00019967942985561966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41080 loss: 0.7518 iter time (s): 4.246 samples/sec: 30.146
g0198:  iteration    41080/10000000 | consumed samples:      5258240 | consumed tokens:  10768875520 | elapsed time per iteration (ms): 4279.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.417760E-01 | loss scale: 32768.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.912 | tokens per gpu per second (tgs): 1914.378 | TFLOPs: 15.41 |
g0184: [2024-08-11 12:16:49,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=41090, skipped=60, lr=[0.00019967921359467165, 0.00019967921359467165], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41090 loss: 0.7660 iter time (s): 4.238 samples/sec: 30.203
g0198:  iteration    41090/10000000 | consumed samples:      5259520 | consumed tokens:  10771496960 | elapsed time per iteration (ms): 4271.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.582490E-01 | loss scale: 32768.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.963 | tokens per gpu per second (tgs): 1917.653 | TFLOPs: 15.43 |
g0184: [2024-08-11 12:17:31,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=41100, skipped=60, lr=[0.00019967899726092532, 0.00019967899726092532], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41100 loss: 0.7471 iter time (s): 4.210 samples/sec: 30.407
g0198:  iteration    41100/10000000 | consumed samples:      5260800 | consumed tokens:  10774118400 | elapsed time per iteration (ms): 4242.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.495691E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.173 | tokens per gpu per second (tgs): 1931.062 | TFLOPs: 15.54 |
g0184: [2024-08-11 12:18:14,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=41110, skipped=60, lr=[0.00019967878085438084, 0.00019967878085438084], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41110 loss: 0.7485 iter time (s): 4.253 samples/sec: 30.094
g0198:  iteration    41110/10000000 | consumed samples:      5262080 | consumed tokens:  10776739840 | elapsed time per iteration (ms): 4286.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.307220E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.860 | tokens per gpu per second (tgs): 1911.048 | TFLOPs: 15.38 |
g0184: [2024-08-11 12:18:57,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=41120, skipped=60, lr=[0.0001996785643750384, 0.0001996785643750384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41120 loss: 0.7353 iter time (s): 4.281 samples/sec: 29.898
g0198:  iteration    41120/10000000 | consumed samples:      5263360 | consumed tokens:  10779361280 | elapsed time per iteration (ms): 4314.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.389704E-01 | loss scale: 32768.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.669 | tokens per gpu per second (tgs): 1898.802 | TFLOPs: 15.28 |
g0184: [2024-08-11 12:19:41,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=41130, skipped=60, lr=[0.00019967834782289812, 0.00019967834782289812], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41130 loss: 0.7150 iter time (s): 4.405 samples/sec: 29.058
g0198:  iteration    41130/10000000 | consumed samples:      5264640 | consumed tokens:  10781982720 | elapsed time per iteration (ms): 4437.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.332799E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.845 | tokens per gpu per second (tgs): 1846.049 | TFLOPs: 14.86 |
g0184: [2024-08-11 12:20:24,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=41140, skipped=60, lr=[0.00019967813119796022, 0.00019967813119796022], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41140 loss: 0.7296 iter time (s): 4.244 samples/sec: 30.161
g0198:  iteration    41140/10000000 | consumed samples:      5265920 | consumed tokens:  10784604160 | elapsed time per iteration (ms): 4276.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.425061E-01 | loss scale: 32768.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.931 | tokens per gpu per second (tgs): 1915.591 | TFLOPs: 15.42 |
g0184: [2024-08-11 12:21:09,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=41150, skipped=60, lr=[0.0001996779145002248, 0.0001996779145002248], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41150 loss: 0.7289 iter time (s): 4.481 samples/sec: 28.566
g0198:  iteration    41150/10000000 | consumed samples:      5267200 | consumed tokens:  10787225600 | elapsed time per iteration (ms): 4513.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.494846E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.360 | tokens per gpu per second (tgs): 1815.071 | TFLOPs: 14.61 |
g0184: [2024-08-11 12:21:52,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=41160, skipped=60, lr=[0.0001996776977296921, 0.0001996776977296921], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41160 loss: 0.7710 iter time (s): 4.253 samples/sec: 30.099
g0198:  iteration    41160/10000000 | consumed samples:      5268480 | consumed tokens:  10789847040 | elapsed time per iteration (ms): 4285.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.611121E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.869 | tokens per gpu per second (tgs): 1911.644 | TFLOPs: 15.38 |
g0184: [2024-08-11 12:22:35,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=41170, skipped=60, lr=[0.00019967748088636222, 0.00019967748088636222], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41170 loss: 0.7493 iter time (s): 4.270 samples/sec: 29.977
g0198:  iteration    41170/10000000 | consumed samples:      5269760 | consumed tokens:  10792468480 | elapsed time per iteration (ms): 4302.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.463656E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.748 | tokens per gpu per second (tgs): 1903.868 | TFLOPs: 15.32 |
g0184: [2024-08-11 12:23:20,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=41180, skipped=60, lr=[0.0001996772639702354, 0.0001996772639702354], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41180 loss: 0.7522 iter time (s): 4.498 samples/sec: 28.458
g0198:  iteration    41180/10000000 | consumed samples:      5271040 | consumed tokens:  10795089920 | elapsed time per iteration (ms): 4530.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.496545E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.251 | tokens per gpu per second (tgs): 1808.043 | TFLOPs: 14.55 |
g0184: [2024-08-11 12:24:03,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=41190, skipped=60, lr=[0.00019967704698131168, 0.00019967704698131168], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41190 loss: 0.7399 iter time (s): 4.226 samples/sec: 30.288
g0198:  iteration    41190/10000000 | consumed samples:      5272320 | consumed tokens:  10797711360 | elapsed time per iteration (ms): 4258.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.491019E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.055 | tokens per gpu per second (tgs): 1923.531 | TFLOPs: 15.48 |
g0184: [2024-08-11 12:24:45,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=41200, skipped=60, lr=[0.00019967682991959136, 0.00019967682991959136], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41200 loss: 0.7074 iter time (s): 4.194 samples/sec: 30.518
g0198:  iteration    41200/10000000 | consumed samples:      5273600 | consumed tokens:  10800332800 | elapsed time per iteration (ms): 4226.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.363282E-01 | loss scale: 32768.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.283 | tokens per gpu per second (tgs): 1938.107 | TFLOPs: 15.60 |
g0184: [2024-08-11 12:25:28,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=41210, skipped=60, lr=[0.00019967661278507459, 0.00019967661278507459], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41210 loss: 0.7547 iter time (s): 4.206 samples/sec: 30.433
g0198:  iteration    41210/10000000 | consumed samples:      5274880 | consumed tokens:  10802954240 | elapsed time per iteration (ms): 4238.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.456366E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.198 | tokens per gpu per second (tgs): 1932.693 | TFLOPs: 15.55 |
g0184: [2024-08-11 12:26:10,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=41220, skipped=60, lr=[0.00019967639557776144, 0.00019967639557776144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41220 loss: 0.7488 iter time (s): 4.157 samples/sec: 30.791
g0198:  iteration    41220/10000000 | consumed samples:      5276160 | consumed tokens:  10805575680 | elapsed time per iteration (ms): 4190.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.463552E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.549 | tokens per gpu per second (tgs): 1955.134 | TFLOPs: 15.73 |
g0184: [2024-08-11 12:26:53,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=41230, skipped=60, lr=[0.0001996761782976522, 0.0001996761782976522], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41230 loss: 0.7498 iter time (s): 4.284 samples/sec: 29.875
g0198:  iteration    41230/10000000 | consumed samples:      5277440 | consumed tokens:  10808197120 | elapsed time per iteration (ms): 4317.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.448472E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.649 | tokens per gpu per second (tgs): 1897.546 | TFLOPs: 15.27 |
g0184: [2024-08-11 12:27:35,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=41240, skipped=60, lr=[0.00019967596094474694, 0.00019967596094474694], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41240 loss: 0.7764 iter time (s): 4.220 samples/sec: 30.330
g0198:  iteration    41240/10000000 | consumed samples:      5278720 | consumed tokens:  10810818560 | elapsed time per iteration (ms): 4252.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.442962E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.098 | tokens per gpu per second (tgs): 1926.290 | TFLOPs: 15.50 |
g0184: [2024-08-11 12:28:19,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=41250, skipped=60, lr=[0.00019967574351904588, 0.00019967574351904588], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41250 loss: 0.7269 iter time (s): 4.336 samples/sec: 29.521
g0198:  iteration    41250/10000000 | consumed samples:      5280000 | consumed tokens:  10813440000 | elapsed time per iteration (ms): 4369.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.437891E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.298 | tokens per gpu per second (tgs): 1875.047 | TFLOPs: 15.09 |
g0184: [2024-08-11 12:29:02,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=41260, skipped=60, lr=[0.00019967552602054916, 0.00019967552602054916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41260 loss: 0.7540 iter time (s): 4.273 samples/sec: 29.954
g0198:  iteration    41260/10000000 | consumed samples:      5281280 | consumed tokens:  10816061440 | elapsed time per iteration (ms): 4307.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.500634E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.719 | tokens per gpu per second (tgs): 1901.997 | TFLOPs: 15.31 |
g0184: [2024-08-11 12:29:43,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=41270, skipped=60, lr=[0.00019967530844925696, 0.00019967530844925696], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41270 loss: 0.7568 iter time (s): 4.100 samples/sec: 31.219
g0198:  iteration    41270/10000000 | consumed samples:      5282560 | consumed tokens:  10818682880 | elapsed time per iteration (ms): 4132.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.450040E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.973 | tokens per gpu per second (tgs): 1982.246 | TFLOPs: 15.95 |
g0184: [2024-08-11 12:30:23,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=41280, skipped=60, lr=[0.00019967509080516944, 0.00019967509080516944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41280 loss: 0.7160 iter time (s): 3.953 samples/sec: 32.377
g0198:  iteration    41280/10000000 | consumed samples:      5283840 | consumed tokens:  10821304320 | elapsed time per iteration (ms): 3986.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.404127E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.112 | tokens per gpu per second (tgs): 2055.152 | TFLOPs: 16.54 |
g0184: [2024-08-11 12:31:07,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=41290, skipped=60, lr=[0.0001996748730882868, 0.0001996748730882868], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41290 loss: 0.7353 iter time (s): 4.340 samples/sec: 29.490
g0198:  iteration    41290/10000000 | consumed samples:      5285120 | consumed tokens:  10823925760 | elapsed time per iteration (ms): 4374.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.286422E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.260 | tokens per gpu per second (tgs): 1872.623 | TFLOPs: 15.07 |
g0184: [2024-08-11 12:31:49,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=41300, skipped=60, lr=[0.00019967465529860916, 0.00019967465529860916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41300 loss: 0.7631 iter time (s): 4.155 samples/sec: 30.804
g0198:  iteration    41300/10000000 | consumed samples:      5286400 | consumed tokens:  10826547200 | elapsed time per iteration (ms): 4189.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.505479E-01 | loss scale: 32768.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.556 | tokens per gpu per second (tgs): 1955.601 | TFLOPs: 15.74 |
g0184: [2024-08-11 12:32:31,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=41310, skipped=60, lr=[0.0001996744374361367, 0.0001996744374361367], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41310 loss: 0.7141 iter time (s): 4.134 samples/sec: 30.962
g0198:  iteration    41310/10000000 | consumed samples:      5287680 | consumed tokens:  10829168640 | elapsed time per iteration (ms): 4166.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.446531E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.718 | tokens per gpu per second (tgs): 1965.967 | TFLOPs: 15.82 |
g0184: [2024-08-11 12:33:13,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=41320, skipped=60, lr=[0.00019967421950086963, 0.00019967421950086963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41320 loss: 0.7577 iter time (s): 4.195 samples/sec: 30.512
g0198:  iteration    41320/10000000 | consumed samples:      5288960 | consumed tokens:  10831790080 | elapsed time per iteration (ms): 4227.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.500439E-01 | loss scale: 32768.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.278 | tokens per gpu per second (tgs): 1937.797 | TFLOPs: 15.59 |
g0184: [2024-08-11 12:33:54,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=41330, skipped=60, lr=[0.00019967400149280807, 0.00019967400149280807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41330 loss: 0.7322 iter time (s): 4.076 samples/sec: 31.401
g0198:  iteration    41330/10000000 | consumed samples:      5290240 | consumed tokens:  10834411520 | elapsed time per iteration (ms): 4109.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.474472E-01 | loss scale: 32768.0 | grad norm: 0.153 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.150 | tokens per gpu per second (tgs): 1993.632 | TFLOPs: 16.04 |
g0184: [2024-08-11 12:34:38,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=41340, skipped=60, lr=[0.0001996737834119522, 0.0001996737834119522], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41340 loss: 0.7582 iter time (s): 4.389 samples/sec: 29.162
g0198:  iteration    41340/10000000 | consumed samples:      5291520 | consumed tokens:  10837032960 | elapsed time per iteration (ms): 4421.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.484031E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.949 | tokens per gpu per second (tgs): 1852.719 | TFLOPs: 14.91 |
g0184: [2024-08-11 12:35:20,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=41350, skipped=60, lr=[0.00019967356525830217, 0.00019967356525830217], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41350 loss: 0.7717 iter time (s): 4.108 samples/sec: 31.157
g0198:  iteration    41350/10000000 | consumed samples:      5292800 | consumed tokens:  10839654400 | elapsed time per iteration (ms): 4141.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.473967E-01 | loss scale: 32768.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.909 | tokens per gpu per second (tgs): 1978.186 | TFLOPs: 15.92 |
g0188: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 12:36:00,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 12:36:00,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 12:36:04,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=41360, skipped=60, lr=[0.0001996733470318582, 0.0001996733470318582], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41360 loss: 0.7360 iter time (s): 4.418 samples/sec: 28.972
g0198:  iteration    41360/10000000 | consumed samples:      5294080 | consumed tokens:  10842275840 | elapsed time per iteration (ms): 4450.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.500093E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.760 | tokens per gpu per second (tgs): 1840.616 | TFLOPs: 14.81 |
g0184: [2024-08-11 12:36:46,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=41370, skipped=60, lr=[0.0001996731287326204, 0.0001996731287326204], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41370 loss: 0.7355 iter time (s): 4.166 samples/sec: 30.722
g0198:  iteration    41370/10000000 | consumed samples:      5295360 | consumed tokens:  10844897280 | elapsed time per iteration (ms): 4199.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.398651E-01 | loss scale: 65536.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1950.961 | TFLOPs: 15.70 |
g0184: [2024-08-11 12:37:28,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=41380, skipped=60, lr=[0.000199672910360589, 0.000199672910360589], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41380 loss: 0.7776 iter time (s): 4.143 samples/sec: 30.899
g0198:  iteration    41380/10000000 | consumed samples:      5296640 | consumed tokens:  10847518720 | elapsed time per iteration (ms): 4175.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.527271E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.657 | tokens per gpu per second (tgs): 1962.077 | TFLOPs: 15.79 |
g0184: [2024-08-11 12:38:12,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=41390, skipped=60, lr=[0.00019967269191576407, 0.00019967269191576407], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41390 loss: 0.7442 iter time (s): 4.344 samples/sec: 29.467
g0198:  iteration    41390/10000000 | consumed samples:      5297920 | consumed tokens:  10850140160 | elapsed time per iteration (ms): 4383.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.490177E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.203 | tokens per gpu per second (tgs): 1868.989 | TFLOPs: 15.04 |
g0184: [2024-08-11 12:38:54,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=41400, skipped=60, lr=[0.0001996724733981459, 0.0001996724733981459], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41400 loss: 0.7805 iter time (s): 4.169 samples/sec: 30.702
g0198:  iteration    41400/10000000 | consumed samples:      5299200 | consumed tokens:  10852761600 | elapsed time per iteration (ms): 4203.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.438257E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.454 | tokens per gpu per second (tgs): 1949.052 | TFLOPs: 15.68 |
g0184: [2024-08-11 12:39:36,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=41410, skipped=60, lr=[0.00019967225480773456, 0.00019967225480773456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41410 loss: 0.7776 iter time (s): 4.158 samples/sec: 30.783
g0198:  iteration    41410/10000000 | consumed samples:      5300480 | consumed tokens:  10855383040 | elapsed time per iteration (ms): 4190.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.454014E-01 | loss scale: 65536.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.543 | tokens per gpu per second (tgs): 1954.741 | TFLOPs: 15.73 |
g0184: [2024-08-11 12:40:18,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=41420, skipped=60, lr=[0.00019967203614453026, 0.00019967203614453026], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41420 loss: 0.7657 iter time (s): 4.170 samples/sec: 30.698
g0198:  iteration    41420/10000000 | consumed samples:      5301760 | consumed tokens:  10858004480 | elapsed time per iteration (ms): 4202.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.337707E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.461 | tokens per gpu per second (tgs): 1949.495 | TFLOPs: 15.69 |
g0184: [2024-08-11 12:41:01,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=41430, skipped=60, lr=[0.00019967181740853317, 0.00019967181740853317], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41430 loss: 0.7440 iter time (s): 4.328 samples/sec: 29.572
g0198:  iteration    41430/10000000 | consumed samples:      5303040 | consumed tokens:  10860625920 | elapsed time per iteration (ms): 4361.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.463834E-01 | loss scale: 65536.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.349 | tokens per gpu per second (tgs): 1878.340 | TFLOPs: 15.12 |
g0184: [2024-08-11 12:41:43,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=41440, skipped=60, lr=[0.00019967159859974346, 0.00019967159859974346], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41440 loss: 0.7306 iter time (s): 4.184 samples/sec: 30.593
g0198:  iteration    41440/10000000 | consumed samples:      5304320 | consumed tokens:  10863247360 | elapsed time per iteration (ms): 4216.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.471190E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.355 | tokens per gpu per second (tgs): 1942.699 | TFLOPs: 15.63 |
g0184: [2024-08-11 12:42:26,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=41450, skipped=60, lr=[0.00019967137971816127, 0.00019967137971816127], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41450 loss: 0.7490 iter time (s): 4.230 samples/sec: 30.259
g0198:  iteration    41450/10000000 | consumed samples:      5305600 | consumed tokens:  10865868800 | elapsed time per iteration (ms): 4264.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.487324E-01 | loss scale: 65536.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.019 | tokens per gpu per second (tgs): 1921.198 | TFLOPs: 15.46 |
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 41458
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 41458
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 41458
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 41458
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 41458
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 41458
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 41458
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: Grad overflow on iteration 41458
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 41458
g0194: Grad overflow on iteration 41458
g0185: Grad overflow on iteration 41458
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 41458
g0185: Grad overflow on iteration 41458
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 41458
g0194: Grad overflow on iteration 41458
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: Grad overflow on iteration 41458
g0188: Grad overflow on iteration 41458
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 41458
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 41458
g0197: Grad overflow on iteration 41458
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 41458
g0198: Grad overflow on iteration 41458
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 41458
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 41458
g0198: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: Grad overflow on iteration 41458
g0187: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0185: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: Grad overflow on iteration 41458
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: Grad overflow on iteration 41458
g0198: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: Grad overflow on iteration 41458
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-11 12:43:05,224] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 41458
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 41458
g0184: Grad overflow on iteration 41458
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 41458
g0188: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0195: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0194: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0197: [2024-08-11 12:43:05,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0198: [2024-08-11 12:43:05,225] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0184: [2024-08-11 12:43:09,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=41460, skipped=61, lr=[0.00019967116076378676, 0.00019967116076378676], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41460 loss: 0.7376 iter time (s): 4.280 samples/sec: 29.905
g0198:  iteration    41460/10000000 | consumed samples:      5306880 | consumed tokens:  10868490240 | elapsed time per iteration (ms): 4313.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.507828E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.678 | tokens per gpu per second (tgs): 1899.372 | TFLOPs: 15.28 |
g0184: [2024-08-11 12:43:51,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=41470, skipped=61, lr=[0.00019967094173662018, 0.00019967094173662018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41470 loss: 0.7385 iter time (s): 4.149 samples/sec: 30.851
g0198:  iteration    41470/10000000 | consumed samples:      5308160 | consumed tokens:  10871111680 | elapsed time per iteration (ms): 4187.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.455406E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.567 | tokens per gpu per second (tgs): 1956.288 | TFLOPs: 15.74 |
g0184: [2024-08-11 12:44:32,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=41480, skipped=61, lr=[0.00019967072263666163, 0.00019967072263666163], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41480 loss: 0.7404 iter time (s): 4.102 samples/sec: 31.203
g0198:  iteration    41480/10000000 | consumed samples:      5309440 | consumed tokens:  10873733120 | elapsed time per iteration (ms): 4134.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.521060E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.957 | tokens per gpu per second (tgs): 1981.223 | TFLOPs: 15.94 |
g0184: [2024-08-11 12:45:15,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=41490, skipped=61, lr=[0.00019967050346391126, 0.00019967050346391126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41490 loss: 0.7179 iter time (s): 4.194 samples/sec: 30.521
g0198:  iteration    41490/10000000 | consumed samples:      5310720 | consumed tokens:  10876354560 | elapsed time per iteration (ms): 4226.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.452897E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.285 | tokens per gpu per second (tgs): 1938.211 | TFLOPs: 15.60 |
g0184: [2024-08-11 12:45:57,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=41500, skipped=61, lr=[0.00019967028421836932, 0.00019967028421836932], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41500 loss: 0.7268 iter time (s): 4.222 samples/sec: 30.318
g0198:  iteration    41500/10000000 | consumed samples:      5312000 | consumed tokens:  10878976000 | elapsed time per iteration (ms): 4255.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.446651E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.080 | tokens per gpu per second (tgs): 1925.136 | TFLOPs: 15.49 |
g0184: [2024-08-11 12:46:39,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=41510, skipped=61, lr=[0.0001996700649000359, 0.0001996700649000359], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41510 loss: 0.7619 iter time (s): 4.126 samples/sec: 31.025
g0198:  iteration    41510/10000000 | consumed samples:      5313280 | consumed tokens:  10881597440 | elapsed time per iteration (ms): 4159.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.408010E-01 | loss scale: 32768.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.776 | tokens per gpu per second (tgs): 1969.653 | TFLOPs: 15.85 |
g0184: [2024-08-11 12:47:20,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=41520, skipped=61, lr=[0.00019966984550891123, 0.00019966984550891123], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41520 loss: 0.7689 iter time (s): 4.120 samples/sec: 31.068
g0198:  iteration    41520/10000000 | consumed samples:      5314560 | consumed tokens:  10884218880 | elapsed time per iteration (ms): 4153.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.496273E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.819 | tokens per gpu per second (tgs): 1972.416 | TFLOPs: 15.87 |
g0184: [2024-08-11 12:48:03,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=41530, skipped=61, lr=[0.0001996696260449954, 0.0001996696260449954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41530 loss: 0.7643 iter time (s): 4.234 samples/sec: 30.234
g0198:  iteration    41530/10000000 | consumed samples:      5315840 | consumed tokens:  10886840320 | elapsed time per iteration (ms): 4267.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.473256E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.991 | tokens per gpu per second (tgs): 1919.423 | TFLOPs: 15.45 |
g0184: [2024-08-11 12:48:44,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=41540, skipped=61, lr=[0.00019966940650828865, 0.00019966940650828865], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41540 loss: 0.7298 iter time (s): 4.092 samples/sec: 31.284
g0198:  iteration    41540/10000000 | consumed samples:      5317120 | consumed tokens:  10889461760 | elapsed time per iteration (ms): 4128.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.534508E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.005 | tokens per gpu per second (tgs): 1984.289 | TFLOPs: 15.97 |
g0184: [2024-08-11 12:49:26,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=41550, skipped=61, lr=[0.00019966918689879115, 0.00019966918689879115], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41550 loss: 0.7469 iter time (s): 4.102 samples/sec: 31.207
g0198:  iteration    41550/10000000 | consumed samples:      5318400 | consumed tokens:  10892083200 | elapsed time per iteration (ms): 4134.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.406900E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.959 | tokens per gpu per second (tgs): 1981.374 | TFLOPs: 15.94 |
g0184: [2024-08-11 12:50:07,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=41560, skipped=61, lr=[0.000199668967216503, 0.000199668967216503], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41560 loss: 0.7371 iter time (s): 4.099 samples/sec: 31.224
g0198:  iteration    41560/10000000 | consumed samples:      5319680 | consumed tokens:  10894704640 | elapsed time per iteration (ms): 4131.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.267891E-01 | loss scale: 32768.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.979 | tokens per gpu per second (tgs): 1982.643 | TFLOPs: 15.95 |
g0184: [2024-08-11 12:50:50,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=41570, skipped=61, lr=[0.00019966874746142445, 0.00019966874746142445], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41570 loss: 0.7176 iter time (s): 4.249 samples/sec: 30.126
g0198:  iteration    41570/10000000 | consumed samples:      5320960 | consumed tokens:  10897326080 | elapsed time per iteration (ms): 4281.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.410758E-01 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.898 | tokens per gpu per second (tgs): 1913.460 | TFLOPs: 15.40 |
g0184: [2024-08-11 12:51:32,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=41580, skipped=61, lr=[0.0001996685276335556, 0.0001996685276335556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41580 loss: 0.7494 iter time (s): 4.203 samples/sec: 30.458
g0198:  iteration    41580/10000000 | consumed samples:      5322240 | consumed tokens:  10899947520 | elapsed time per iteration (ms): 4235.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.440868E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.220 | tokens per gpu per second (tgs): 1934.111 | TFLOPs: 15.56 |
g0184: [2024-08-11 12:52:15,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=41590, skipped=61, lr=[0.00019966830773289663, 0.00019966830773289663], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41590 loss: 0.7551 iter time (s): 4.225 samples/sec: 30.298
g0198:  iteration    41590/10000000 | consumed samples:      5323520 | consumed tokens:  10902568960 | elapsed time per iteration (ms): 4257.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.472311E-01 | loss scale: 32768.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.067 | tokens per gpu per second (tgs): 1924.277 | TFLOPs: 15.48 |
g0184: [2024-08-11 12:52:55,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=41600, skipped=61, lr=[0.00019966808775944778, 0.00019966808775944778], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41600 loss: 0.7536 iter time (s): 3.993 samples/sec: 32.054
g0198:  iteration    41600/10000000 | consumed samples:      5324800 | consumed tokens:  10905190400 | elapsed time per iteration (ms): 4026.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.421235E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.792 | tokens per gpu per second (tgs): 2034.693 | TFLOPs: 16.37 |
g0184: [2024-08-11 12:53:37,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=41610, skipped=61, lr=[0.00019966786771320917, 0.00019966786771320917], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41610 loss: 0.7722 iter time (s): 4.219 samples/sec: 30.339
g0198:  iteration    41610/10000000 | consumed samples:      5326080 | consumed tokens:  10907811840 | elapsed time per iteration (ms): 4252.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.484940E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.104 | tokens per gpu per second (tgs): 1926.643 | TFLOPs: 15.50 |
g0184: [2024-08-11 12:54:20,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=41620, skipped=61, lr=[0.00019966764759418094, 0.00019966764759418094], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41620 loss: 0.7480 iter time (s): 4.212 samples/sec: 30.388
g0198:  iteration    41620/10000000 | consumed samples:      5327360 | consumed tokens:  10910433280 | elapsed time per iteration (ms): 4244.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.485635E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.154 | tokens per gpu per second (tgs): 1929.848 | TFLOPs: 15.53 |
g0184: [2024-08-11 12:55:03,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=41630, skipped=61, lr=[0.00019966742740236328, 0.00019966742740236328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41630 loss: 0.7575 iter time (s): 4.281 samples/sec: 29.896
g0198:  iteration    41630/10000000 | consumed samples:      5328640 | consumed tokens:  10913054720 | elapsed time per iteration (ms): 4314.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.426340E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.666 | tokens per gpu per second (tgs): 1898.603 | TFLOPs: 15.28 |
g0184: [2024-08-11 12:55:45,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=41640, skipped=61, lr=[0.00019966720713775638, 0.00019966720713775638], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41640 loss: 0.7319 iter time (s): 4.170 samples/sec: 30.698
g0198:  iteration    41640/10000000 | consumed samples:      5329920 | consumed tokens:  10915676160 | elapsed time per iteration (ms): 4202.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.408031E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.456 | tokens per gpu per second (tgs): 1949.175 | TFLOPs: 15.69 |
g0184: [2024-08-11 12:56:25,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=41650, skipped=61, lr=[0.0001996669868003604, 0.0001996669868003604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41650 loss: 0.7667 iter time (s): 3.953 samples/sec: 32.384
g0198:  iteration    41650/10000000 | consumed samples:      5331200 | consumed tokens:  10918297600 | elapsed time per iteration (ms): 3986.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.468798E-01 | loss scale: 32768.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.107 | tokens per gpu per second (tgs): 2054.877 | TFLOPs: 16.54 |
g0184: [2024-08-11 12:57:07,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=41660, skipped=61, lr=[0.0001996667663901755, 0.0001996667663901755], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41660 loss: 0.7510 iter time (s): 4.141 samples/sec: 30.909
g0198:  iteration    41660/10000000 | consumed samples:      5332480 | consumed tokens:  10920919040 | elapsed time per iteration (ms): 4174.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.510790E-01 | loss scale: 32768.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.664 | tokens per gpu per second (tgs): 1962.516 | TFLOPs: 15.79 |
g0184: [2024-08-11 12:57:46,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=41670, skipped=61, lr=[0.00019966654590720186, 0.00019966654590720186], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41670 loss: 0.7368 iter time (s): 3.879 samples/sec: 32.997
g0198:  iteration    41670/10000000 | consumed samples:      5333760 | consumed tokens:  10923540480 | elapsed time per iteration (ms): 3912.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.312036E-01 | loss scale: 32768.0 | grad norm: 0.154 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.719 | tokens per gpu per second (tgs): 2094.028 | TFLOPs: 16.85 |
g0184: [2024-08-11 12:58:27,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=41680, skipped=61, lr=[0.00019966632535143965, 0.00019966632535143965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41680 loss: 0.7207 iter time (s): 4.127 samples/sec: 31.015
g0198:  iteration    41680/10000000 | consumed samples:      5335040 | consumed tokens:  10926161920 | elapsed time per iteration (ms): 4160.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.381567E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.768 | tokens per gpu per second (tgs): 1969.135 | TFLOPs: 15.85 |
g0184: [2024-08-11 12:59:08,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=41690, skipped=61, lr=[0.000199666104722889, 0.000199666104722889], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41690 loss: 0.7431 iter time (s): 4.074 samples/sec: 31.416
g0198:  iteration    41690/10000000 | consumed samples:      5336320 | consumed tokens:  10928783360 | elapsed time per iteration (ms): 4108.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.458679E-01 | loss scale: 32768.0 | grad norm: 0.718 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.159 | tokens per gpu per second (tgs): 1994.172 | TFLOPs: 16.05 |
g0184: [2024-08-11 12:59:50,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=41700, skipped=61, lr=[0.00019966588402155016, 0.00019966588402155016], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41700 loss: 0.7340 iter time (s): 4.137 samples/sec: 30.939
g0198:  iteration    41700/10000000 | consumed samples:      5337600 | consumed tokens:  10931404800 | elapsed time per iteration (ms): 4169.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.430531E-01 | loss scale: 32768.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.696 | tokens per gpu per second (tgs): 1964.571 | TFLOPs: 15.81 |
g0184: [2024-08-11 13:00:30,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=41710, skipped=61, lr=[0.00019966566324742323, 0.00019966566324742323], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41710 loss: 0.7370 iter time (s): 3.995 samples/sec: 32.037
g0198:  iteration    41710/10000000 | consumed samples:      5338880 | consumed tokens:  10934026240 | elapsed time per iteration (ms): 4028.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.398828E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.777 | tokens per gpu per second (tgs): 2033.711 | TFLOPs: 16.37 |
g0184: [2024-08-11 13:01:13,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=41720, skipped=61, lr=[0.0001996654424005084, 0.0001996654424005084], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41720 loss: 0.7639 iter time (s): 4.177 samples/sec: 30.642
g0198:  iteration    41720/10000000 | consumed samples:      5340160 | consumed tokens:  10936647680 | elapsed time per iteration (ms): 4211.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.462253E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.395 | tokens per gpu per second (tgs): 1945.273 | TFLOPs: 15.65 |
g0184: [2024-08-11 13:01:54,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=41730, skipped=61, lr=[0.00019966522148080586, 0.00019966522148080586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41730 loss: 0.7323 iter time (s): 4.103 samples/sec: 31.194
g0198:  iteration    41730/10000000 | consumed samples:      5341440 | consumed tokens:  10939269120 | elapsed time per iteration (ms): 4135.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.395687E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.948 | tokens per gpu per second (tgs): 1980.696 | TFLOPs: 15.94 |
g0184: [2024-08-11 13:02:36,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=41740, skipped=61, lr=[0.00019966500048831573, 0.00019966500048831573], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41740 loss: 0.7592 iter time (s): 4.129 samples/sec: 31.002
g0198:  iteration    41740/10000000 | consumed samples:      5342720 | consumed tokens:  10941890560 | elapsed time per iteration (ms): 4161.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.424793E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.759 | tokens per gpu per second (tgs): 1968.554 | TFLOPs: 15.84 |
g0184: [2024-08-11 13:03:18,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=41750, skipped=61, lr=[0.00019966477942303824, 0.00019966477942303824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41750 loss: 0.7201 iter time (s): 4.218 samples/sec: 30.349
g0198:  iteration    41750/10000000 | consumed samples:      5344000 | consumed tokens:  10944512000 | elapsed time per iteration (ms): 4250.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.389987E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.114 | tokens per gpu per second (tgs): 1927.296 | TFLOPs: 15.51 |
g0184: [2024-08-11 13:03:59,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=41760, skipped=61, lr=[0.00019966455828497354, 0.00019966455828497354], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41760 loss: 0.7628 iter time (s): 4.073 samples/sec: 31.429
g0198:  iteration    41760/10000000 | consumed samples:      5345280 | consumed tokens:  10947133440 | elapsed time per iteration (ms): 4105.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.465220E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.179 | tokens per gpu per second (tgs): 1995.455 | TFLOPs: 16.06 |
g0184: [2024-08-11 13:04:41,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=41770, skipped=61, lr=[0.00019966433707412174, 0.00019966433707412174], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41770 loss: 0.7594 iter time (s): 4.137 samples/sec: 30.941
g0198:  iteration    41770/10000000 | consumed samples:      5346560 | consumed tokens:  10949754880 | elapsed time per iteration (ms): 4170.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.476716E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.692 | tokens per gpu per second (tgs): 1964.308 | TFLOPs: 15.81 |
g0184: [2024-08-11 13:05:24,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=41780, skipped=61, lr=[0.0001996641157904831, 0.0001996641157904831], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41780 loss: 0.7414 iter time (s): 4.245 samples/sec: 30.150
g0198:  iteration    41780/10000000 | consumed samples:      5347840 | consumed tokens:  10952376320 | elapsed time per iteration (ms): 4278.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.392012E-01 | loss scale: 32768.0 | grad norm: 0.154 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.916 | tokens per gpu per second (tgs): 1914.653 | TFLOPs: 15.41 |
g0184: [2024-08-11 13:06:06,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=41790, skipped=61, lr=[0.00019966389443405776, 0.00019966389443405776], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41790 loss: 0.7371 iter time (s): 4.200 samples/sec: 30.473
g0198:  iteration    41790/10000000 | consumed samples:      5349120 | consumed tokens:  10954997760 | elapsed time per iteration (ms): 4233.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.443789E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.235 | tokens per gpu per second (tgs): 1935.029 | TFLOPs: 15.57 |
g0184: [2024-08-11 13:06:49,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=41800, skipped=61, lr=[0.00019966367300484588, 0.00019966367300484588], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41800 loss: 0.7483 iter time (s): 4.271 samples/sec: 29.970
g0198:  iteration    41800/10000000 | consumed samples:      5350400 | consumed tokens:  10957619200 | elapsed time per iteration (ms): 4304.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.579771E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.738 | tokens per gpu per second (tgs): 1903.205 | TFLOPs: 15.32 |
g0184: [2024-08-11 13:07:31,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=41810, skipped=61, lr=[0.00019966345150284763, 0.00019966345150284763], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41810 loss: 0.7355 iter time (s): 4.166 samples/sec: 30.722
g0198:  iteration    41810/10000000 | consumed samples:      5351680 | consumed tokens:  10960240640 | elapsed time per iteration (ms): 4199.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.495422E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.480 | tokens per gpu per second (tgs): 1950.728 | TFLOPs: 15.70 |
g0184: [2024-08-11 13:08:12,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=41820, skipped=61, lr=[0.0001996632299280632, 0.0001996632299280632], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41820 loss: 0.7551 iter time (s): 4.108 samples/sec: 31.160
g0198:  iteration    41820/10000000 | consumed samples:      5352960 | consumed tokens:  10962862080 | elapsed time per iteration (ms): 4141.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.419653E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.911 | tokens per gpu per second (tgs): 1978.273 | TFLOPs: 15.92 |
g0184: [2024-08-11 13:08:54,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=41830, skipped=61, lr=[0.0001996630082804927, 0.0001996630082804927], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41830 loss: 0.7456 iter time (s): 4.146 samples/sec: 30.872
g0198:  iteration    41830/10000000 | consumed samples:      5354240 | consumed tokens:  10965483520 | elapsed time per iteration (ms): 4179.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.420494E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.627 | tokens per gpu per second (tgs): 1960.135 | TFLOPs: 15.77 |
g0184: [2024-08-11 13:09:36,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=41840, skipped=61, lr=[0.0001996627865601364, 0.0001996627865601364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41840 loss: 0.7260 iter time (s): 4.190 samples/sec: 30.547
g0198:  iteration    41840/10000000 | consumed samples:      5355520 | consumed tokens:  10968104960 | elapsed time per iteration (ms): 4229.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.397304E-01 | loss scale: 32768.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.267 | tokens per gpu per second (tgs): 1937.083 | TFLOPs: 15.59 |
g0184: [2024-08-11 13:10:19,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=41850, skipped=61, lr=[0.00019966256476699437, 0.00019966256476699437], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41850 loss: 0.7796 iter time (s): 4.237 samples/sec: 30.210
g0198:  iteration    41850/10000000 | consumed samples:      5356800 | consumed tokens:  10970726400 | elapsed time per iteration (ms): 4270.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.397701E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.977 | tokens per gpu per second (tgs): 1918.510 | TFLOPs: 15.44 |
g0184: [2024-08-11 13:11:00,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=41860, skipped=61, lr=[0.00019966234290106683, 0.00019966234290106683], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41860 loss: 0.7309 iter time (s): 4.069 samples/sec: 31.454
g0198:  iteration    41860/10000000 | consumed samples:      5358080 | consumed tokens:  10973347840 | elapsed time per iteration (ms): 4102.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.367967E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.204 | tokens per gpu per second (tgs): 1997.029 | TFLOPs: 16.07 |
g0184: [2024-08-11 13:11:42,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=41870, skipped=61, lr=[0.00019966212096235396, 0.00019966212096235396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41870 loss: 0.7585 iter time (s): 4.128 samples/sec: 31.010
g0198:  iteration    41870/10000000 | consumed samples:      5359360 | consumed tokens:  10975969280 | elapsed time per iteration (ms): 4160.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.460873E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.764 | tokens per gpu per second (tgs): 1968.924 | TFLOPs: 15.84 |
g0184: [2024-08-11 13:12:24,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=41880, skipped=61, lr=[0.00019966189895085593, 0.00019966189895085593], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41880 loss: 0.7599 iter time (s): 4.216 samples/sec: 30.358
g0198:  iteration    41880/10000000 | consumed samples:      5360640 | consumed tokens:  10978590720 | elapsed time per iteration (ms): 4249.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.425865E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.122 | tokens per gpu per second (tgs): 1927.795 | TFLOPs: 15.51 |
g0184: [2024-08-11 13:13:06,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=41890, skipped=61, lr=[0.00019966167686657287, 0.00019966167686657287], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41890 loss: 0.7713 iter time (s): 4.179 samples/sec: 30.628
g0198:  iteration    41890/10000000 | consumed samples:      5361920 | consumed tokens:  10981212160 | elapsed time per iteration (ms): 4212.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.465687E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.388 | tokens per gpu per second (tgs): 1944.812 | TFLOPs: 15.65 |
g0184: [2024-08-11 13:13:47,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=41900, skipped=61, lr=[0.00019966145470950498, 0.00019966145470950498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41900 loss: 0.7504 iter time (s): 4.076 samples/sec: 31.405
g0198:  iteration    41900/10000000 | consumed samples:      5363200 | consumed tokens:  10983833600 | elapsed time per iteration (ms): 4108.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.445770E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.156 | tokens per gpu per second (tgs): 1993.971 | TFLOPs: 16.05 |
g0184: [2024-08-11 13:14:30,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=41910, skipped=61, lr=[0.00019966123247965243, 0.00019966123247965243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41910 loss: 0.7471 iter time (s): 4.225 samples/sec: 30.294
g0198:  iteration    41910/10000000 | consumed samples:      5364480 | consumed tokens:  10986455040 | elapsed time per iteration (ms): 4258.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.349700E-01 | loss scale: 32768.0 | grad norm: 0.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.055 | tokens per gpu per second (tgs): 1923.518 | TFLOPs: 15.48 |
g0184: [2024-08-11 13:15:13,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=41920, skipped=61, lr=[0.0001996610101770154, 0.0001996610101770154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41920 loss: 0.7678 iter time (s): 4.228 samples/sec: 30.272
g0198:  iteration    41920/10000000 | consumed samples:      5365760 | consumed tokens:  10989076480 | elapsed time per iteration (ms): 4261.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.446232E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.037 | tokens per gpu per second (tgs): 1922.376 | TFLOPs: 15.47 |
g0184: [2024-08-11 13:15:55,484] [INFO] [logging.py:96:log_dist] [Rank 0] step=41930, skipped=61, lr=[0.00019966078780159403, 0.00019966078780159403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41930 loss: 0.7068 iter time (s): 4.196 samples/sec: 30.505
g0198:  iteration    41930/10000000 | consumed samples:      5367040 | consumed tokens:  10991697920 | elapsed time per iteration (ms): 4229.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.367980E-01 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.265 | tokens per gpu per second (tgs): 1936.928 | TFLOPs: 15.59 |
g0184: [2024-08-11 13:16:36,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=41940, skipped=61, lr=[0.00019966056535338853, 0.00019966056535338853], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41940 loss: 0.7431 iter time (s): 4.113 samples/sec: 31.124
g0198:  iteration    41940/10000000 | consumed samples:      5368320 | consumed tokens:  10994319360 | elapsed time per iteration (ms): 4145.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.451668E-01 | loss scale: 32768.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.880 | tokens per gpu per second (tgs): 1976.299 | TFLOPs: 15.90 |
g0184: [2024-08-11 13:17:18,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=41950, skipped=61, lr=[0.00019966034283239908, 0.00019966034283239908], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41950 loss: 0.7227 iter time (s): 4.124 samples/sec: 31.039
g0198:  iteration    41950/10000000 | consumed samples:      5369600 | consumed tokens:  10996940800 | elapsed time per iteration (ms): 4156.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.404700E-01 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.794 | tokens per gpu per second (tgs): 1970.788 | TFLOPs: 15.86 |
g0194: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0194: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0188: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:18:01,286] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0187: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0197: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0185: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0195: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0198: [2024-08-11 13:18:01,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0184: [2024-08-11 13:18:01,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=41960, skipped=61, lr=[0.00019966012023862577, 0.00019966012023862577], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41960 loss: 0.7600 iter time (s): 4.246 samples/sec: 30.148
g0198:  iteration    41960/10000000 | consumed samples:      5370880 | consumed tokens:  10999562240 | elapsed time per iteration (ms): 4278.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.496843E-01 | loss scale: 65536.0 | grad norm: 0.091 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.915 | tokens per gpu per second (tgs): 1914.574 | TFLOPs: 15.41 |
g0184: [2024-08-11 13:18:41,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=41970, skipped=61, lr=[0.00019965989757206884, 0.00019965989757206884], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41970 loss: 0.7596 iter time (s): 4.022 samples/sec: 31.823
g0198:  iteration    41970/10000000 | consumed samples:      5372160 | consumed tokens:  11002183680 | elapsed time per iteration (ms): 4055.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.401000E-01 | loss scale: 65536.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.566 | tokens per gpu per second (tgs): 2020.229 | TFLOPs: 16.26 |
g0184: [2024-08-11 13:19:25,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=41980, skipped=61, lr=[0.00019965967483272847, 0.00019965967483272847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41980 loss: 0.7412 iter time (s): 4.328 samples/sec: 29.576
g0198:  iteration    41980/10000000 | consumed samples:      5373440 | consumed tokens:  11004805120 | elapsed time per iteration (ms): 4361.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.537013E-01 | loss scale: 65536.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.347 | tokens per gpu per second (tgs): 1878.208 | TFLOPs: 15.11 |
g0184: [2024-08-11 13:20:06,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=41990, skipped=61, lr=[0.0001996594520206048, 0.0001996594520206048], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 41990 loss: 0.7126 iter time (s): 4.107 samples/sec: 31.166
g0198:  iteration    41990/10000000 | consumed samples:      5374720 | consumed tokens:  11007426560 | elapsed time per iteration (ms): 4145.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.442454E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.880 | tokens per gpu per second (tgs): 1976.296 | TFLOPs: 15.90 |
g0184: [2024-08-11 13:20:49,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=61, lr=[0.000199659229135698, 0.000199659229135698], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42000 loss: 0.7604 iter time (s): 4.241 samples/sec: 30.181
g0198:  iteration    42000/10000000 | consumed samples:      5376000 | consumed tokens:  11010048000 | elapsed time per iteration (ms): 4273.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.369390E-01 | loss scale: 65536.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.950 | tokens per gpu per second (tgs): 1916.809 | TFLOPs: 15.42 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 42000 | lm loss value: 7.418089E-01 | lm loss PPL: 2.099730E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   42000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 13:27:25,838] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step42000 is about to be saved!
g0184: [2024-08-11 13:27:25,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0184: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0198: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0198: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0198: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0184: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0188: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0188: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0188: [2024-08-11 13:27:25,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0187: [2024-08-11 13:27:25,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0187: [2024-08-11 13:27:25,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0187: [2024-08-11 13:27:25,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0197: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0197: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0197: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0185: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0185: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0185: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0194: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0194: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0194: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0195: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0195: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0195: [2024-08-11 13:27:25,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0198: [2024-08-11 13:27:25,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0188: [2024-08-11 13:27:25,878] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0185: [2024-08-11 13:27:25,879] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0194: [2024-08-11 13:27:25,880] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0187: [2024-08-11 13:27:25,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0195: [2024-08-11 13:27:25,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0197: [2024-08-11 13:27:25,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0184: [2024-08-11 13:27:25,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0195: [2024-08-11 13:27:25,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0185: [2024-08-11 13:27:26,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0195: [2024-08-11 13:27:26,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0197: [2024-08-11 13:27:26,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0185: [2024-08-11 13:27:26,058] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0187: [2024-08-11 13:27:26,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0197: [2024-08-11 13:27:26,079] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0187: [2024-08-11 13:27:26,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0188: [2024-08-11 13:27:26,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0198: [2024-08-11 13:27:26,118] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 13:27:26,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 13:27:26,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0194: [2024-08-11 13:27:26,133] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0195: [2024-08-11 13:27:26,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0188: [2024-08-11 13:27:26,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0194: [2024-08-11 13:27:26,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0195: [2024-08-11 13:27:26,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0198: [2024-08-11 13:27:26,173] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0185: [2024-08-11 13:27:26,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0187: [2024-08-11 13:27:26,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0197: [2024-08-11 13:27:26,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0185: [2024-08-11 13:27:26,238] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0184: [2024-08-11 13:27:26,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0187: [2024-08-11 13:27:26,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0184: [2024-08-11 13:27:26,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0197: [2024-08-11 13:27:26,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0188: [2024-08-11 13:27:26,282] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0195: [2024-08-11 13:27:26,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 13:27:26,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0194: [2024-08-11 13:27:26,310] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0188: [2024-08-11 13:27:26,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0194: [2024-08-11 13:27:26,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0198: [2024-08-11 13:27:26,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0185: [2024-08-11 13:27:26,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0198: [2024-08-11 13:27:26,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0185: [2024-08-11 13:27:26,352] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0184: [2024-08-11 13:27:26,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0187: [2024-08-11 13:27:26,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 13:27:26,369] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0197: [2024-08-11 13:27:26,378] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0184: [2024-08-11 13:27:26,380] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0197: [2024-08-11 13:27:26,380] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0188: [2024-08-11 13:27:26,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 13:27:26,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0194: [2024-08-11 13:27:26,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 13:27:26,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0198: [2024-08-11 13:27:28,363] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 13:27:28,364] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0185: [2024-08-11 13:27:28,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 13:27:28,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0195: [2024-08-11 13:27:28,664] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 13:27:28,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0187: [2024-08-11 13:27:28,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 13:27:28,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0197: [2024-08-11 13:27:28,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 13:27:28,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0194: [2024-08-11 13:27:28,791] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 13:27:28,792] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0188: [2024-08-11 13:27:28,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 13:27:28,874] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0184: [2024-08-11 13:27:29,534] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 13:27:29,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 13:27:29,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 13:27:29,668] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt
g0184: [2024-08-11 13:27:29,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0184: [2024-08-11 13:27:33,368] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 13:27:33,369] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0184:   successfully saved checkpoint at iteration   42000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 2.97, Latency(second): 7.593
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (7592.47, 7592.65)
g0184: [2024-08-11 13:28:15,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=42010, skipped=61, lr=[0.00019965900617800823, 0.00019965900617800823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42010 loss: 0.7481 iter time (s): 4.163 samples/sec: 30.750
g0198:  iteration    42010/10000000 | consumed samples:      5377280 | consumed tokens:  11012669440 | elapsed time per iteration (ms): 44568.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.449222E-01 | loss scale: 65536.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.872 | tokens per gpu per second (tgs): 183.807 | TFLOPs: 1.48 |
g0184: [2024-08-11 13:28:57,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=42020, skipped=61, lr=[0.00019965878314753572, 0.00019965878314753572], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42020 loss: 0.7468 iter time (s): 4.155 samples/sec: 30.805
g0198:  iteration    42020/10000000 | consumed samples:      5378560 | consumed tokens:  11015290880 | elapsed time per iteration (ms): 4188.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.462724E-01 | loss scale: 65536.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.562 | tokens per gpu per second (tgs): 1955.978 | TFLOPs: 15.74 |
g0184: [2024-08-11 13:29:38,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=42030, skipped=61, lr=[0.00019965856004428058, 0.00019965856004428058], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42030 loss: 0.7564 iter time (s): 4.132 samples/sec: 30.976
g0198:  iteration    42030/10000000 | consumed samples:      5379840 | consumed tokens:  11017912320 | elapsed time per iteration (ms): 4165.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.402082E-01 | loss scale: 65536.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.731 | tokens per gpu per second (tgs): 1966.787 | TFLOPs: 15.83 |
g0184: [2024-08-11 13:30:21,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=42040, skipped=61, lr=[0.000199658336868243, 0.000199658336868243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42040 loss: 0.7333 iter time (s): 4.204 samples/sec: 30.447
g0198:  iteration    42040/10000000 | consumed samples:      5381120 | consumed tokens:  11020533760 | elapsed time per iteration (ms): 4236.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.448524E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.211 | tokens per gpu per second (tgs): 1933.530 | TFLOPs: 15.56 |
g0184: [2024-08-11 13:31:03,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=42050, skipped=61, lr=[0.0001996581136194232, 0.0001996581136194232], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42050 loss: 0.7346 iter time (s): 4.174 samples/sec: 30.667
g0198:  iteration    42050/10000000 | consumed samples:      5382400 | consumed tokens:  11023155200 | elapsed time per iteration (ms): 4207.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.410028E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.424 | tokens per gpu per second (tgs): 1947.137 | TFLOPs: 15.67 |
g0184: [2024-08-11 13:31:44,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=42060, skipped=61, lr=[0.00019965789029782125, 0.00019965789029782125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42060 loss: 0.7725 iter time (s): 4.125 samples/sec: 31.031
g0198:  iteration    42060/10000000 | consumed samples:      5383680 | consumed tokens:  11025776640 | elapsed time per iteration (ms): 4158.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.446959E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.780 | tokens per gpu per second (tgs): 1969.946 | TFLOPs: 15.85 |
g0184: [2024-08-11 13:32:27,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=42070, skipped=61, lr=[0.00019965766690343742, 0.00019965766690343742], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42070 loss: 0.7467 iter time (s): 4.180 samples/sec: 30.620
g0198:  iteration    42070/10000000 | consumed samples:      5384960 | consumed tokens:  11028398080 | elapsed time per iteration (ms): 4214.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.403805E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.372 | tokens per gpu per second (tgs): 1943.793 | TFLOPs: 15.64 |
g0184: [2024-08-11 13:33:09,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=42080, skipped=61, lr=[0.00019965744343627184, 0.00019965744343627184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42080 loss: 0.7539 iter time (s): 4.163 samples/sec: 30.744
g0198:  iteration    42080/10000000 | consumed samples:      5386240 | consumed tokens:  11031019520 | elapsed time per iteration (ms): 4196.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.380703E-01 | loss scale: 65536.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.500 | tokens per gpu per second (tgs): 1951.990 | TFLOPs: 15.71 |
g0184: [2024-08-11 13:33:52,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=42090, skipped=61, lr=[0.00019965721989632465, 0.00019965721989632465], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42090 loss: 0.7365 iter time (s): 4.288 samples/sec: 29.848
g0198:  iteration    42090/10000000 | consumed samples:      5387520 | consumed tokens:  11033640960 | elapsed time per iteration (ms): 4321.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.395346E-01 | loss scale: 65536.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.619 | tokens per gpu per second (tgs): 1895.630 | TFLOPs: 15.25 |
g0184: [2024-08-11 13:34:34,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=42100, skipped=61, lr=[0.00019965699628359606, 0.00019965699628359606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42100 loss: 0.7099 iter time (s): 4.219 samples/sec: 30.340
g0198:  iteration    42100/10000000 | consumed samples:      5388800 | consumed tokens:  11036262400 | elapsed time per iteration (ms): 4251.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.418602E-01 | loss scale: 65536.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.105 | tokens per gpu per second (tgs): 1926.717 | TFLOPs: 15.50 |
g0184: [2024-08-11 13:35:16,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=42110, skipped=61, lr=[0.00019965677259808624, 0.00019965677259808624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42110 loss: 0.7448 iter time (s): 4.143 samples/sec: 30.898
g0198:  iteration    42110/10000000 | consumed samples:      5390080 | consumed tokens:  11038883840 | elapsed time per iteration (ms): 4175.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.390356E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.774 | TFLOPs: 15.79 |
g0184: [2024-08-11 13:35:57,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=42120, skipped=61, lr=[0.00019965654883979538, 0.00019965654883979538], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42120 loss: 0.7519 iter time (s): 4.084 samples/sec: 31.342
g0198:  iteration    42120/10000000 | consumed samples:      5391360 | consumed tokens:  11041505280 | elapsed time per iteration (ms): 4117.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.401393E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.086 | tokens per gpu per second (tgs): 1989.526 | TFLOPs: 16.01 |
g0184: [2024-08-11 13:36:39,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=42130, skipped=61, lr=[0.00019965632500872362, 0.00019965632500872362], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42130 loss: 0.7109 iter time (s): 4.175 samples/sec: 30.660
g0198:  iteration    42130/10000000 | consumed samples:      5392640 | consumed tokens:  11044126720 | elapsed time per iteration (ms): 4207.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.494076E-01 | loss scale: 65536.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1947.000 | TFLOPs: 15.67 |
g0184: [2024-08-11 13:37:21,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=42140, skipped=61, lr=[0.00019965610110487113, 0.00019965610110487113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42140 loss: 0.7480 iter time (s): 4.138 samples/sec: 30.930
g0198:  iteration    42140/10000000 | consumed samples:      5393920 | consumed tokens:  11046748160 | elapsed time per iteration (ms): 4171.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.489282E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.684 | tokens per gpu per second (tgs): 1963.771 | TFLOPs: 15.80 |
g0184: [2024-08-11 13:38:04,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=42150, skipped=61, lr=[0.00019965587712823812, 0.00019965587712823812], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42150 loss: 0.7890 iter time (s): 4.296 samples/sec: 29.797
g0198:  iteration    42150/10000000 | consumed samples:      5395200 | consumed tokens:  11049369600 | elapsed time per iteration (ms): 4328.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.515184E-01 | loss scale: 65536.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.569 | tokens per gpu per second (tgs): 1892.412 | TFLOPs: 15.23 |
g0184: [2024-08-11 13:38:46,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=42160, skipped=61, lr=[0.00019965565307882471, 0.00019965565307882471], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42160 loss: 0.7418 iter time (s): 4.174 samples/sec: 30.666
g0198:  iteration    42160/10000000 | consumed samples:      5396480 | consumed tokens:  11051991040 | elapsed time per iteration (ms): 4207.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.406496E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.425 | tokens per gpu per second (tgs): 1947.178 | TFLOPs: 15.67 |
g0184: [2024-08-11 13:39:28,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=42170, skipped=61, lr=[0.00019965542895663115, 0.00019965542895663115], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42170 loss: 0.7630 iter time (s): 4.170 samples/sec: 30.698
g0198:  iteration    42170/10000000 | consumed samples:      5397760 | consumed tokens:  11054612480 | elapsed time per iteration (ms): 4202.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.527811E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.457 | tokens per gpu per second (tgs): 1949.254 | TFLOPs: 15.69 |
g0184: [2024-08-11 13:40:11,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=42180, skipped=61, lr=[0.0001996552047616575, 0.0001996552047616575], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42180 loss: 0.7562 iter time (s): 4.195 samples/sec: 30.511
g0198:  iteration    42180/10000000 | consumed samples:      5399040 | consumed tokens:  11057233920 | elapsed time per iteration (ms): 4228.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.438911E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.270 | tokens per gpu per second (tgs): 1937.284 | TFLOPs: 15.59 |
g0184: [2024-08-11 13:40:53,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=42190, skipped=61, lr=[0.00019965498049390404, 0.00019965498049390404], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42190 loss: 0.7448 iter time (s): 4.215 samples/sec: 30.369
g0198:  iteration    42190/10000000 | consumed samples:      5400320 | consumed tokens:  11059855360 | elapsed time per iteration (ms): 4247.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.363185E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.132 | tokens per gpu per second (tgs): 1928.473 | TFLOPs: 15.52 |
g0184: [2024-08-11 13:41:34,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=42200, skipped=61, lr=[0.00019965475615337086, 0.00019965475615337086], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42200 loss: 0.7546 iter time (s): 4.100 samples/sec: 31.219
g0198:  iteration    42200/10000000 | consumed samples:      5401600 | consumed tokens:  11062476800 | elapsed time per iteration (ms): 4133.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.352578E-01 | loss scale: 65536.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.969 | tokens per gpu per second (tgs): 1981.994 | TFLOPs: 15.95 |
g0184: [2024-08-11 13:42:16,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=42210, skipped=61, lr=[0.0001996545317400582, 0.0001996545317400582], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42210 loss: 0.7503 iter time (s): 4.170 samples/sec: 30.696
g0198:  iteration    42210/10000000 | consumed samples:      5402880 | consumed tokens:  11065098240 | elapsed time per iteration (ms): 4202.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.362400E-01 | loss scale: 65536.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.459 | tokens per gpu per second (tgs): 1949.349 | TFLOPs: 15.69 |
g0184: [2024-08-11 13:42:58,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=42220, skipped=61, lr=[0.00019965430725396618, 0.00019965430725396618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42220 loss: 0.7022 iter time (s): 4.113 samples/sec: 31.124
g0198:  iteration    42220/10000000 | consumed samples:      5404160 | consumed tokens:  11067719680 | elapsed time per iteration (ms): 4157.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.427033E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.791 | tokens per gpu per second (tgs): 1970.649 | TFLOPs: 15.86 |
g0184: [2024-08-11 13:43:40,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=42230, skipped=61, lr=[0.000199654082695095, 0.000199654082695095], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42230 loss: 0.7411 iter time (s): 4.169 samples/sec: 30.700
g0198:  iteration    42230/10000000 | consumed samples:      5405440 | consumed tokens:  11070341120 | elapsed time per iteration (ms): 4208.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.391946E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.414 | tokens per gpu per second (tgs): 1946.513 | TFLOPs: 15.66 |
g0184: [2024-08-11 13:44:22,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=42240, skipped=61, lr=[0.00019965385806344484, 0.00019965385806344484], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42240 loss: 0.7542 iter time (s): 4.170 samples/sec: 30.693
g0198:  iteration    42240/10000000 | consumed samples:      5406720 | consumed tokens:  11072962560 | elapsed time per iteration (ms): 4204.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.483119E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.448 | tokens per gpu per second (tgs): 1948.642 | TFLOPs: 15.68 |
g0184: [2024-08-11 13:45:05,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=42250, skipped=61, lr=[0.00019965363335901584, 0.00019965363335901584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42250 loss: 0.7662 iter time (s): 4.272 samples/sec: 29.965
g0198:  iteration    42250/10000000 | consumed samples:      5408000 | consumed tokens:  11075584000 | elapsed time per iteration (ms): 4304.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.405720E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.736 | tokens per gpu per second (tgs): 1903.134 | TFLOPs: 15.31 |
g0184: [2024-08-11 13:45:50,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=42260, skipped=61, lr=[0.00019965340858180822, 0.00019965340858180822], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42260 loss: 0.7153 iter time (s): 4.406 samples/sec: 29.051
g0198:  iteration    42260/10000000 | consumed samples:      5409280 | consumed tokens:  11078205440 | elapsed time per iteration (ms): 4438.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.371633E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.836 | tokens per gpu per second (tgs): 1845.496 | TFLOPs: 14.85 |
g0184: [2024-08-11 13:46:32,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=42270, skipped=61, lr=[0.0001996531837318221, 0.0001996531837318221], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42270 loss: 0.7815 iter time (s): 4.209 samples/sec: 30.408
g0198:  iteration    42270/10000000 | consumed samples:      5410560 | consumed tokens:  11080826880 | elapsed time per iteration (ms): 4242.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.512969E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.174 | tokens per gpu per second (tgs): 1931.135 | TFLOPs: 15.54 |
g0184: [2024-08-11 13:47:14,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=42280, skipped=61, lr=[0.00019965295880905765, 0.00019965295880905765], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42280 loss: 0.7469 iter time (s): 4.116 samples/sec: 31.097
g0198:  iteration    42280/10000000 | consumed samples:      5411840 | consumed tokens:  11083448320 | elapsed time per iteration (ms): 4149.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.418003E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.850 | tokens per gpu per second (tgs): 1974.396 | TFLOPs: 15.89 |
g0184: [2024-08-11 13:47:54,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=42290, skipped=61, lr=[0.00019965273381351514, 0.00019965273381351514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42290 loss: 0.7456 iter time (s): 4.057 samples/sec: 31.551
g0198:  iteration    42290/10000000 | consumed samples:      5413120 | consumed tokens:  11086069760 | elapsed time per iteration (ms): 4089.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.492165E-01 | loss scale: 65536.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.298 | tokens per gpu per second (tgs): 2003.089 | TFLOPs: 16.12 |
g0184: [2024-08-11 13:48:37,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=42300, skipped=61, lr=[0.00019965250874519463, 0.00019965250874519463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42300 loss: 0.7305 iter time (s): 4.166 samples/sec: 30.728
g0198:  iteration    42300/10000000 | consumed samples:      5414400 | consumed tokens:  11088691200 | elapsed time per iteration (ms): 4219.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.288888E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.335 | tokens per gpu per second (tgs): 1941.450 | TFLOPs: 15.62 |
g0184: [2024-08-11 13:49:18,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=42310, skipped=61, lr=[0.00019965228360409635, 0.00019965228360409635], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42310 loss: 0.7627 iter time (s): 4.063 samples/sec: 31.504
g0198:  iteration    42310/10000000 | consumed samples:      5415680 | consumed tokens:  11091312640 | elapsed time per iteration (ms): 4095.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.411091E-01 | loss scale: 65536.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.251 | tokens per gpu per second (tgs): 2000.083 | TFLOPs: 16.10 |
g0184: [2024-08-11 13:49:58,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=42320, skipped=61, lr=[0.00019965205839022045, 0.00019965205839022045], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42320 loss: 0.7410 iter time (s): 3.995 samples/sec: 32.043
g0198:  iteration    42320/10000000 | consumed samples:      5416960 | consumed tokens:  11093934080 | elapsed time per iteration (ms): 4028.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.476829E-01 | loss scale: 65536.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.774 | tokens per gpu per second (tgs): 2033.566 | TFLOPs: 16.36 |
g0184: [2024-08-11 13:50:40,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=42330, skipped=61, lr=[0.00019965183310356713, 0.00019965183310356713], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42330 loss: 0.7854 iter time (s): 4.193 samples/sec: 30.525
g0198:  iteration    42330/10000000 | consumed samples:      5418240 | consumed tokens:  11096555520 | elapsed time per iteration (ms): 4226.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.472506E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.285 | tokens per gpu per second (tgs): 1938.237 | TFLOPs: 15.60 |
g0184: [2024-08-11 13:51:22,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=42340, skipped=61, lr=[0.00019965160774413653, 0.00019965160774413653], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42340 loss: 0.7308 iter time (s): 4.115 samples/sec: 31.105
g0198:  iteration    42340/10000000 | consumed samples:      5419520 | consumed tokens:  11099176960 | elapsed time per iteration (ms): 4148.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.397259E-01 | loss scale: 65536.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.857 | tokens per gpu per second (tgs): 1974.847 | TFLOPs: 15.89 |
g0184: [2024-08-11 13:52:06,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=42350, skipped=61, lr=[0.00019965138231192885, 0.00019965138231192885], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42350 loss: 0.7634 iter time (s): 4.375 samples/sec: 29.256
g0198:  iteration    42350/10000000 | consumed samples:      5420800 | consumed tokens:  11101798400 | elapsed time per iteration (ms): 4408.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.348944E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.035 | tokens per gpu per second (tgs): 1858.234 | TFLOPs: 14.95 |
g0184: [2024-08-11 13:52:47,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=42360, skipped=61, lr=[0.00019965115680694426, 0.00019965115680694426], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42360 loss: 0.7274 iter time (s): 4.078 samples/sec: 31.385
g0198:  iteration    42360/10000000 | consumed samples:      5422080 | consumed tokens:  11104419840 | elapsed time per iteration (ms): 4114.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.369119E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.112 | tokens per gpu per second (tgs): 1991.160 | TFLOPs: 16.02 |
g0184: [2024-08-11 13:53:28,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=42370, skipped=61, lr=[0.00019965093122918293, 0.00019965093122918293], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42370 loss: 0.7574 iter time (s): 4.086 samples/sec: 31.330
g0198:  iteration    42370/10000000 | consumed samples:      5423360 | consumed tokens:  11107041280 | elapsed time per iteration (ms): 4120.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.385824E-01 | loss scale: 65536.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.061 | tokens per gpu per second (tgs): 1987.932 | TFLOPs: 16.00 |
g0184: [2024-08-11 13:54:09,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=42380, skipped=61, lr=[0.00019965070557864502, 0.00019965070557864502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42380 loss: 0.7460 iter time (s): 4.096 samples/sec: 31.250
g0198:  iteration    42380/10000000 | consumed samples:      5424640 | consumed tokens:  11109662720 | elapsed time per iteration (ms): 4129.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.388441E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.999 | tokens per gpu per second (tgs): 1983.939 | TFLOPs: 15.97 |
g0184: [2024-08-11 13:54:52,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=42390, skipped=61, lr=[0.0001996504798553307, 0.0001996504798553307], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42390 loss: 0.7284 iter time (s): 4.203 samples/sec: 30.457
g0198:  iteration    42390/10000000 | consumed samples:      5425920 | consumed tokens:  11112284160 | elapsed time per iteration (ms): 4235.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.424648E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.224 | tokens per gpu per second (tgs): 1934.313 | TFLOPs: 15.57 |
g0184: [2024-08-11 13:55:34,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=42400, skipped=61, lr=[0.00019965025405924017, 0.00019965025405924017], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42400 loss: 0.7459 iter time (s): 4.175 samples/sec: 30.658
g0198:  iteration    42400/10000000 | consumed samples:      5427200 | consumed tokens:  11114905600 | elapsed time per iteration (ms): 4208.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.455809E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.416 | tokens per gpu per second (tgs): 1946.622 | TFLOPs: 15.66 |
g0184: [2024-08-11 13:56:15,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=42410, skipped=61, lr=[0.0001996500281903736, 0.0001996500281903736], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42410 loss: 0.7818 iter time (s): 4.123 samples/sec: 31.048
g0198:  iteration    42410/10000000 | consumed samples:      5428480 | consumed tokens:  11117527040 | elapsed time per iteration (ms): 4155.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 7.589284E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.802 | tokens per gpu per second (tgs): 1971.319 | TFLOPs: 15.86 |
g0184: [2024-08-11 13:56:57,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=42420, skipped=61, lr=[0.00019964980224873114, 0.00019964980224873114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42420 loss: 0.7583 iter time (s): 4.112 samples/sec: 31.127
g0198:  iteration    42420/10000000 | consumed samples:      5429760 | consumed tokens:  11120148480 | elapsed time per iteration (ms): 4145.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.425081E-01 | loss scale: 65536.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.880 | tokens per gpu per second (tgs): 1976.312 | TFLOPs: 15.90 |
g0184: [2024-08-11 13:57:39,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=42430, skipped=61, lr=[0.000199649576234313, 0.000199649576234313], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42430 loss: 0.7427 iter time (s): 4.215 samples/sec: 30.371
g0198:  iteration    42430/10000000 | consumed samples:      5431040 | consumed tokens:  11122769920 | elapsed time per iteration (ms): 4247.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.396277E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.136 | tokens per gpu per second (tgs): 1928.725 | TFLOPs: 15.52 |
g0184: [2024-08-11 13:58:21,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=42440, skipped=61, lr=[0.00019964935014711932, 0.00019964935014711932], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42440 loss: 0.7516 iter time (s): 4.186 samples/sec: 30.581
g0198:  iteration    42440/10000000 | consumed samples:      5432320 | consumed tokens:  11125391360 | elapsed time per iteration (ms): 4218.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.414242E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.344 | tokens per gpu per second (tgs): 1941.987 | TFLOPs: 15.63 |
g0184: [2024-08-11 13:59:04,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=42450, skipped=61, lr=[0.00019964912398715028, 0.00019964912398715028], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42450 loss: 0.7446 iter time (s): 4.272 samples/sec: 29.966
g0198:  iteration    42450/10000000 | consumed samples:      5433600 | consumed tokens:  11128012800 | elapsed time per iteration (ms): 4304.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.390006E-01 | loss scale: 65536.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.738 | tokens per gpu per second (tgs): 1903.243 | TFLOPs: 15.32 |
g0184: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0198: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0188: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0188: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0195: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0187: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0195: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0194: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0194: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0187: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0198: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0197: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-11 13:59:46,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0197: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0185: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0185: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0184: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-11 13:59:46,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0184: [2024-08-11 13:59:46,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=42460, skipped=61, lr=[0.00019964889775440605, 0.00019964889775440605], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42460 loss: 0.7308 iter time (s): 4.117 samples/sec: 31.087
g0198:  iteration    42460/10000000 | consumed samples:      5434880 | consumed tokens:  11130634240 | elapsed time per iteration (ms): 4150.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.414717E-01 | loss scale: 131072.0 | grad norm: 0.085 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.842 | tokens per gpu per second (tgs): 1973.893 | TFLOPs: 15.88 |
g0184: [2024-08-11 14:00:28,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=42470, skipped=61, lr=[0.00019964867144888683, 0.00019964867144888683], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42470 loss: 0.7590 iter time (s): 4.212 samples/sec: 30.387
g0198:  iteration    42470/10000000 | consumed samples:      5436160 | consumed tokens:  11133255680 | elapsed time per iteration (ms): 4245.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.435212E-01 | loss scale: 131072.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.153 | tokens per gpu per second (tgs): 1929.763 | TFLOPs: 15.53 |
g0184: [2024-08-11 14:01:10,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=42480, skipped=61, lr=[0.00019964844507059276, 0.00019964844507059276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42480 loss: 0.7458 iter time (s): 4.148 samples/sec: 30.860
g0198:  iteration    42480/10000000 | consumed samples:      5437440 | consumed tokens:  11135877120 | elapsed time per iteration (ms): 4180.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.519996E-01 | loss scale: 131072.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.620 | tokens per gpu per second (tgs): 1959.688 | TFLOPs: 15.77 |
g0184: [2024-08-11 14:01:55,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=42490, skipped=61, lr=[0.00019964821861952406, 0.00019964821861952406], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42490 loss: 0.7800 iter time (s): 4.434 samples/sec: 28.868
g0198:  iteration    42490/10000000 | consumed samples:      5438720 | consumed tokens:  11138498560 | elapsed time per iteration (ms): 4466.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.491905E-01 | loss scale: 131072.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.657 | tokens per gpu per second (tgs): 1834.027 | TFLOPs: 14.76 |
g0184: [2024-08-11 14:02:38,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=42500, skipped=61, lr=[0.00019964799209568087, 0.00019964799209568087], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42500 loss: 0.7522 iter time (s): 4.317 samples/sec: 29.653
g0198:  iteration    42500/10000000 | consumed samples:      5440000 | consumed tokens:  11141120000 | elapsed time per iteration (ms): 4349.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.402678E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.431 | tokens per gpu per second (tgs): 1883.592 | TFLOPs: 15.16 |
g0184: [2024-08-11 14:03:21,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=42510, skipped=61, lr=[0.00019964776549906337, 0.00019964776549906337], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42510 loss: 0.7655 iter time (s): 4.217 samples/sec: 30.354
g0198:  iteration    42510/10000000 | consumed samples:      5441280 | consumed tokens:  11143741440 | elapsed time per iteration (ms): 4249.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.414805E-01 | loss scale: 131072.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.119 | tokens per gpu per second (tgs): 1927.620 | TFLOPs: 15.51 |
g0184: [2024-08-11 14:04:03,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=42520, skipped=61, lr=[0.0001996475388296717, 0.0001996475388296717], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42520 loss: 0.7104 iter time (s): 4.134 samples/sec: 30.964
g0198:  iteration    42520/10000000 | consumed samples:      5442560 | consumed tokens:  11146362880 | elapsed time per iteration (ms): 4166.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.387997E-01 | loss scale: 131072.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.722 | tokens per gpu per second (tgs): 1966.183 | TFLOPs: 15.82 |
g0184: [2024-08-11 14:04:44,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=42530, skipped=61, lr=[0.0001996473120875061, 0.0001996473120875061], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42530 loss: 0.7494 iter time (s): 4.121 samples/sec: 31.061
g0198:  iteration    42530/10000000 | consumed samples:      5443840 | consumed tokens:  11148984320 | elapsed time per iteration (ms): 4153.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.465762E-01 | loss scale: 131072.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.815 | tokens per gpu per second (tgs): 1972.151 | TFLOPs: 15.87 |
g0184: [2024-08-11 14:05:26,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=42540, skipped=61, lr=[0.0001996470852725667, 0.0001996470852725667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42540 loss: 0.7456 iter time (s): 4.176 samples/sec: 30.652
g0198:  iteration    42540/10000000 | consumed samples:      5445120 | consumed tokens:  11151605760 | elapsed time per iteration (ms): 4208.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.485813E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.416 | tokens per gpu per second (tgs): 1946.616 | TFLOPs: 15.66 |
g0184: [2024-08-11 14:06:07,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=42550, skipped=61, lr=[0.00019964685838485372, 0.00019964685838485372], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42550 loss: 0.7440 iter time (s): 4.092 samples/sec: 31.277
g0198:  iteration    42550/10000000 | consumed samples:      5446400 | consumed tokens:  11154227200 | elapsed time per iteration (ms): 4125.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.433625E-01 | loss scale: 131072.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.028 | tokens per gpu per second (tgs): 1985.769 | TFLOPs: 15.98 |
g0184: [2024-08-11 14:06:49,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=42560, skipped=61, lr=[0.00019964663142436727, 0.00019964663142436727], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42560 loss: 0.7576 iter time (s): 4.159 samples/sec: 30.775
g0198:  iteration    42560/10000000 | consumed samples:      5447680 | consumed tokens:  11156848640 | elapsed time per iteration (ms): 4192.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.474034E-01 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.535 | tokens per gpu per second (tgs): 1954.208 | TFLOPs: 15.73 |
g0184: [2024-08-11 14:07:30,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=42570, skipped=61, lr=[0.00019964640439110756, 0.00019964640439110756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42570 loss: 0.7261 iter time (s): 4.048 samples/sec: 31.622
g0198:  iteration    42570/10000000 | consumed samples:      5448960 | consumed tokens:  11159470080 | elapsed time per iteration (ms): 4080.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.242980E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.369 | tokens per gpu per second (tgs): 2007.607 | TFLOPs: 16.16 |
g0184: [2024-08-11 14:08:12,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=42580, skipped=61, lr=[0.00019964617728507477, 0.00019964617728507477], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42580 loss: 0.7586 iter time (s): 4.189 samples/sec: 30.557
g0198:  iteration    42580/10000000 | consumed samples:      5450240 | consumed tokens:  11162091520 | elapsed time per iteration (ms): 4221.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.501953E-01 | loss scale: 131072.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.321 | tokens per gpu per second (tgs): 1940.517 | TFLOPs: 15.62 |
g0184: [2024-08-11 14:08:53,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=42590, skipped=61, lr=[0.00019964595010626905, 0.00019964595010626905], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42590 loss: 0.7435 iter time (s): 4.061 samples/sec: 31.519
g0198:  iteration    42590/10000000 | consumed samples:      5451520 | consumed tokens:  11164712960 | elapsed time per iteration (ms): 4094.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.419976E-01 | loss scale: 131072.0 | grad norm: 0.151 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.263 | tokens per gpu per second (tgs): 2000.850 | TFLOPs: 16.10 |
g0184: [2024-08-11 14:09:36,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=42600, skipped=61, lr=[0.0001996457228546906, 0.0001996457228546906], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42600 loss: 0.7489 iter time (s): 4.224 samples/sec: 30.302
g0198:  iteration    42600/10000000 | consumed samples:      5452800 | consumed tokens:  11167334400 | elapsed time per iteration (ms): 4257.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.482345E-01 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.068 | tokens per gpu per second (tgs): 1924.361 | TFLOPs: 15.49 |
g0184: [2024-08-11 14:10:18,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=42610, skipped=61, lr=[0.00019964549553033958, 0.00019964549553033958], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42610 loss: 0.7358 iter time (s): 4.154 samples/sec: 30.811
g0198:  iteration    42610/10000000 | consumed samples:      5454080 | consumed tokens:  11169955840 | elapsed time per iteration (ms): 4187.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.398256E-01 | loss scale: 131072.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.567 | tokens per gpu per second (tgs): 1956.312 | TFLOPs: 15.74 |
g0184: [2024-08-11 14:11:00,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=42620, skipped=61, lr=[0.00019964526813321618, 0.00019964526813321618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42620 loss: 0.7208 iter time (s): 4.167 samples/sec: 30.714
g0198:  iteration    42620/10000000 | consumed samples:      5455360 | consumed tokens:  11172577280 | elapsed time per iteration (ms): 4215.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.390594E-01 | loss scale: 131072.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.364 | tokens per gpu per second (tgs): 1943.301 | TFLOPs: 15.64 |
g0184: [2024-08-11 14:11:42,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=42630, skipped=61, lr=[0.00019964504066332056, 0.00019964504066332056], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42630 loss: 0.7474 iter time (s): 4.177 samples/sec: 30.643
g0198:  iteration    42630/10000000 | consumed samples:      5456640 | consumed tokens:  11175198720 | elapsed time per iteration (ms): 4209.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.422171E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.407 | tokens per gpu per second (tgs): 1946.057 | TFLOPs: 15.66 |
g0184: [2024-08-11 14:12:24,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=42640, skipped=61, lr=[0.0001996448131206529, 0.0001996448131206529], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42640 loss: 0.7146 iter time (s): 4.166 samples/sec: 30.723
g0198:  iteration    42640/10000000 | consumed samples:      5457920 | consumed tokens:  11177820160 | elapsed time per iteration (ms): 4199.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.458804E-01 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.480 | tokens per gpu per second (tgs): 1950.730 | TFLOPs: 15.70 |
g0184: [2024-08-11 14:13:07,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=42650, skipped=61, lr=[0.00019964458550521337, 0.00019964458550521337], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42650 loss: 0.7405 iter time (s): 4.219 samples/sec: 30.336
g0198:  iteration    42650/10000000 | consumed samples:      5459200 | consumed tokens:  11180441600 | elapsed time per iteration (ms): 4251.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.395068E-01 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.104 | tokens per gpu per second (tgs): 1926.669 | TFLOPs: 15.50 |
g0184: [2024-08-11 14:13:49,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=42660, skipped=61, lr=[0.00019964435781700214, 0.00019964435781700214], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42660 loss: 0.7241 iter time (s): 4.212 samples/sec: 30.386
g0198:  iteration    42660/10000000 | consumed samples:      5460480 | consumed tokens:  11183063040 | elapsed time per iteration (ms): 4245.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.429553E-01 | loss scale: 131072.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.152 | tokens per gpu per second (tgs): 1929.757 | TFLOPs: 15.53 |
g0184: [2024-08-11 14:14:31,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=42670, skipped=61, lr=[0.00019964413005601944, 0.00019964413005601944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42670 loss: 0.7390 iter time (s): 4.203 samples/sec: 30.454
g0198:  iteration    42670/10000000 | consumed samples:      5461760 | consumed tokens:  11185684480 | elapsed time per iteration (ms): 4235.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.402190E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.221 | tokens per gpu per second (tgs): 1934.154 | TFLOPs: 15.56 |
g0184: [2024-08-11 14:15:13,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=42680, skipped=61, lr=[0.00019964390222226535, 0.00019964390222226535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42680 loss: 0.7359 iter time (s): 4.125 samples/sec: 31.030
g0198:  iteration    42680/10000000 | consumed samples:      5463040 | consumed tokens:  11188305920 | elapsed time per iteration (ms): 4157.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.386189E-01 | loss scale: 131072.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.789 | tokens per gpu per second (tgs): 1970.473 | TFLOPs: 15.86 |
g0184: [2024-08-11 14:15:55,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=42690, skipped=61, lr=[0.0001996436743157401, 0.0001996436743157401], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42690 loss: 0.7340 iter time (s): 4.164 samples/sec: 30.737
g0198:  iteration    42690/10000000 | consumed samples:      5464320 | consumed tokens:  11190927360 | elapsed time per iteration (ms): 4196.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.411465E-01 | loss scale: 131072.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.499 | tokens per gpu per second (tgs): 1951.962 | TFLOPs: 15.71 |
g0184: [2024-08-11 14:16:37,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=42700, skipped=61, lr=[0.00019964344633644388, 0.00019964344633644388], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42700 loss: 0.7390 iter time (s): 4.211 samples/sec: 30.400
g0198:  iteration    42700/10000000 | consumed samples:      5465600 | consumed tokens:  11193548800 | elapsed time per iteration (ms): 4243.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.376605E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.167 | tokens per gpu per second (tgs): 1930.665 | TFLOPs: 15.54 |
g0184: [2024-08-11 14:17:19,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=42710, skipped=61, lr=[0.00019964321828437685, 0.00019964321828437685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42710 loss: 0.7521 iter time (s): 4.166 samples/sec: 30.726
g0198:  iteration    42710/10000000 | consumed samples:      5466880 | consumed tokens:  11196170240 | elapsed time per iteration (ms): 4198.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.496886E-01 | loss scale: 131072.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.487 | tokens per gpu per second (tgs): 1951.144 | TFLOPs: 15.70 |
g0184: [2024-08-11 14:18:00,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=42720, skipped=61, lr=[0.00019964299015953914, 0.00019964299015953914], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42720 loss: 0.7464 iter time (s): 4.087 samples/sec: 31.315
g0198:  iteration    42720/10000000 | consumed samples:      5468160 | consumed tokens:  11198791680 | elapsed time per iteration (ms): 4119.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.385876E-01 | loss scale: 131072.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.068 | tokens per gpu per second (tgs): 1988.381 | TFLOPs: 16.00 |
g0184: [2024-08-11 14:18:43,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=42730, skipped=61, lr=[0.00019964276196193103, 0.00019964276196193103], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42730 loss: 0.7794 iter time (s): 4.241 samples/sec: 30.180
g0198:  iteration    42730/10000000 | consumed samples:      5469440 | consumed tokens:  11201413120 | elapsed time per iteration (ms): 4273.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.472599E-01 | loss scale: 131072.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.951 | tokens per gpu per second (tgs): 1916.858 | TFLOPs: 15.43 |
g0184: [2024-08-11 14:19:25,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=42740, skipped=61, lr=[0.0001996425336915526, 0.0001996425336915526], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42740 loss: 0.7410 iter time (s): 4.156 samples/sec: 30.797
g0198:  iteration    42740/10000000 | consumed samples:      5470720 | consumed tokens:  11204034560 | elapsed time per iteration (ms): 4189.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.399895E-01 | loss scale: 131072.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.553 | tokens per gpu per second (tgs): 1955.420 | TFLOPs: 15.74 |
g0184: [2024-08-11 14:20:07,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=42750, skipped=61, lr=[0.00019964230534840402, 0.00019964230534840402], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42750 loss: 0.7425 iter time (s): 4.160 samples/sec: 30.766
g0198:  iteration    42750/10000000 | consumed samples:      5472000 | consumed tokens:  11206656000 | elapsed time per iteration (ms): 4192.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.493570E-01 | loss scale: 131072.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.529 | tokens per gpu per second (tgs): 1953.884 | TFLOPs: 15.72 |
g0184: [2024-08-11 14:20:47,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=42760, skipped=61, lr=[0.00019964207693248557, 0.00019964207693248557], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42760 loss: 0.7250 iter time (s): 3.960 samples/sec: 32.324
g0198:  iteration    42760/10000000 | consumed samples:      5473280 | consumed tokens:  11209277440 | elapsed time per iteration (ms): 3992.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.506351E-01 | loss scale: 131072.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.060 | tokens per gpu per second (tgs): 2051.867 | TFLOPs: 16.51 |
g0184: [2024-08-11 14:21:29,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=42770, skipped=61, lr=[0.00019964184844379728, 0.00019964184844379728], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42770 loss: 0.7463 iter time (s): 4.143 samples/sec: 30.893
g0198:  iteration    42770/10000000 | consumed samples:      5474560 | consumed tokens:  11211898880 | elapsed time per iteration (ms): 4175.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.441489E-01 | loss scale: 131072.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.654 | tokens per gpu per second (tgs): 1961.841 | TFLOPs: 15.79 |
g0184: [2024-08-11 14:22:11,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=42780, skipped=61, lr=[0.0001996416198823395, 0.0001996416198823395], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42780 loss: 0.7533 iter time (s): 4.217 samples/sec: 30.352
g0198:  iteration    42780/10000000 | consumed samples:      5475840 | consumed tokens:  11214520320 | elapsed time per iteration (ms): 4249.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.537062E-01 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.122 | tokens per gpu per second (tgs): 1927.809 | TFLOPs: 15.51 |
g0184: [2024-08-11 14:22:53,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=42790, skipped=61, lr=[0.00019964139124811225, 0.00019964139124811225], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42790 loss: 0.7521 iter time (s): 4.173 samples/sec: 30.677
g0198:  iteration    42790/10000000 | consumed samples:      5477120 | consumed tokens:  11217141760 | elapsed time per iteration (ms): 4204.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.457102E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.441 | tokens per gpu per second (tgs): 1948.213 | TFLOPs: 15.68 |
g0184: [2024-08-11 14:23:36,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=42800, skipped=61, lr=[0.00019964116254111573, 0.00019964116254111573], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42800 loss: 0.7428 iter time (s): 4.258 samples/sec: 30.062
g0198:  iteration    42800/10000000 | consumed samples:      5478400 | consumed tokens:  11219763200 | elapsed time per iteration (ms): 4291.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.486601E-01 | loss scale: 131072.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.827 | tokens per gpu per second (tgs): 1908.912 | TFLOPs: 15.36 |
g0184: [2024-08-11 14:24:18,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=42810, skipped=61, lr=[0.0001996409337613502, 0.0001996409337613502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42810 loss: 0.7427 iter time (s): 4.193 samples/sec: 30.527
g0198:  iteration    42810/10000000 | consumed samples:      5479680 | consumed tokens:  11222384640 | elapsed time per iteration (ms): 4226.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.347819E-01 | loss scale: 131072.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.289 | tokens per gpu per second (tgs): 1938.466 | TFLOPs: 15.60 |
g0184: [2024-08-11 14:25:00,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=42820, skipped=61, lr=[0.0001996407049088158, 0.0001996407049088158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42820 loss: 0.7411 iter time (s): 4.167 samples/sec: 30.718
g0198:  iteration    42820/10000000 | consumed samples:      5480960 | consumed tokens:  11225006080 | elapsed time per iteration (ms): 4202.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.383481E-01 | loss scale: 131072.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.458 | tokens per gpu per second (tgs): 1949.311 | TFLOPs: 15.69 |
g0184: [2024-08-11 14:25:42,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=42830, skipped=61, lr=[0.00019964047598351267, 0.00019964047598351267], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42830 loss: 0.7078 iter time (s): 4.143 samples/sec: 30.899
g0198:  iteration    42830/10000000 | consumed samples:      5482240 | consumed tokens:  11227627520 | elapsed time per iteration (ms): 4175.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.344979E-01 | loss scale: 131072.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.657 | tokens per gpu per second (tgs): 1962.051 | TFLOPs: 15.79 |
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 42831
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 42831
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 42831
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: Grad overflow on iteration 42831
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0184: Grad overflow on iteration 42831
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 42831
g0187: Grad overflow on iteration 42831
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 42831
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: Grad overflow on iteration 42831
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 42831
g0197: Grad overflow on iteration 42831
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 42831
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0195: Grad overflow on iteration 42831
g0194: Grad overflow on iteration 42831
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: Grad overflow on iteration 42831
g0188: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0187: Grad overflow on iteration 42831
g0188: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 42831
g0188: Grad overflow on iteration 42831
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 42831
g0194: Grad overflow on iteration 42831
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: Grad overflow on iteration 42831
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 42831
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0185: Grad overflow on iteration 42831
g0187: Grad overflow on iteration 42831
g0188: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-11 14:25:51,161] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: Grad overflow on iteration 42831
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0194: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 42831
g0187: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: Grad overflow on iteration 42831
g0198: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0188: Grad overflow on iteration 42831
g0198: Grad overflow on iteration 42831
g0198: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: Grad overflow on iteration 42831
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0197: Grad overflow on iteration 42831
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0197: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0195: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:344:_update_scale] 
g0198: Grad overflow on iteration 42831
g0184: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0185: [2024-08-11 14:25:51,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0188: [2024-08-11 14:25:51,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0198: [2024-08-11 14:25:51,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0184: [2024-08-11 14:26:24,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=42840, skipped=62, lr=[0.000199640246985441, 0.000199640246985441], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42840 loss: 0.7150 iter time (s): 4.170 samples/sec: 30.697
g0198:  iteration    42840/10000000 | consumed samples:      5483520 | consumed tokens:  11230248960 | elapsed time per iteration (ms): 4202.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.421093E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.458 | tokens per gpu per second (tgs): 1949.293 | TFLOPs: 15.69 |
g0184: [2024-08-11 14:27:06,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=42850, skipped=62, lr=[0.000199640017914601, 0.000199640017914601], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42850 loss: 0.7405 iter time (s): 4.119 samples/sec: 31.076
g0198:  iteration    42850/10000000 | consumed samples:      5484800 | consumed tokens:  11232870400 | elapsed time per iteration (ms): 4151.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.400083E-01 | loss scale: 65536.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.831 | tokens per gpu per second (tgs): 1973.164 | TFLOPs: 15.88 |
g0184: [2024-08-11 14:27:46,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=42860, skipped=62, lr=[0.0001996397887709928, 0.0001996397887709928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42860 loss: 0.7547 iter time (s): 4.005 samples/sec: 31.958
g0198:  iteration    42860/10000000 | consumed samples:      5486080 | consumed tokens:  11235491840 | elapsed time per iteration (ms): 4038.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.398815E-01 | loss scale: 65536.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.698 | tokens per gpu per second (tgs): 2028.643 | TFLOPs: 16.32 |
g0184: [2024-08-11 14:28:26,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=42870, skipped=62, lr=[0.00019963955955461663, 0.00019963955955461663], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42870 loss: 0.7265 iter time (s): 3.984 samples/sec: 32.132
g0198:  iteration    42870/10000000 | consumed samples:      5487360 | consumed tokens:  11238113280 | elapsed time per iteration (ms): 4016.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.444838E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.870 | tokens per gpu per second (tgs): 2039.672 | TFLOPs: 16.41 |
g0184: [2024-08-11 14:29:09,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=42880, skipped=62, lr=[0.0001996393302654726, 0.0001996393302654726], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42880 loss: 0.7289 iter time (s): 4.214 samples/sec: 30.375
g0198:  iteration    42880/10000000 | consumed samples:      5488640 | consumed tokens:  11240734720 | elapsed time per iteration (ms): 4247.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.400056E-01 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.136 | tokens per gpu per second (tgs): 1928.715 | TFLOPs: 15.52 |
g0184: [2024-08-11 14:29:51,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=42890, skipped=62, lr=[0.00019963910090356094, 0.00019963910090356094], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42890 loss: 0.7309 iter time (s): 4.177 samples/sec: 30.642
g0198:  iteration    42890/10000000 | consumed samples:      5489920 | consumed tokens:  11243356160 | elapsed time per iteration (ms): 4210.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.454465E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.401 | tokens per gpu per second (tgs): 1945.679 | TFLOPs: 15.66 |
g0184: [2024-08-11 14:30:32,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=42900, skipped=62, lr=[0.0001996388714688818, 0.0001996388714688818], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42900 loss: 0.7273 iter time (s): 4.083 samples/sec: 31.346
g0198:  iteration    42900/10000000 | consumed samples:      5491200 | consumed tokens:  11245977600 | elapsed time per iteration (ms): 4116.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.507669E-01 | loss scale: 65536.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.097 | tokens per gpu per second (tgs): 1990.179 | TFLOPs: 16.02 |
g0184: [2024-08-11 14:31:14,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=42910, skipped=62, lr=[0.00019963864196143537, 0.00019963864196143537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42910 loss: 0.7602 iter time (s): 4.171 samples/sec: 30.687
g0198:  iteration    42910/10000000 | consumed samples:      5492480 | consumed tokens:  11248599040 | elapsed time per iteration (ms): 4204.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.526988E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.446 | tokens per gpu per second (tgs): 1948.560 | TFLOPs: 15.68 |
g0184: [2024-08-11 14:31:56,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=42920, skipped=62, lr=[0.00019963841238122178, 0.00019963841238122178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42920 loss: 0.7169 iter time (s): 4.191 samples/sec: 30.541
g0198:  iteration    42920/10000000 | consumed samples:      5493760 | consumed tokens:  11251220480 | elapsed time per iteration (ms): 4223.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.452286E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.577 | TFLOPs: 15.61 |
g0184: [2024-08-11 14:32:38,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=42930, skipped=62, lr=[0.00019963818272824129, 0.00019963818272824129], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42930 loss: 0.7695 iter time (s): 4.128 samples/sec: 31.006
g0198:  iteration    42930/10000000 | consumed samples:      5495040 | consumed tokens:  11253841920 | elapsed time per iteration (ms): 4160.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.424772E-01 | loss scale: 65536.0 | grad norm: 0.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.764 | tokens per gpu per second (tgs): 1968.891 | TFLOPs: 15.84 |
g0184: [2024-08-11 14:33:19,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=42940, skipped=62, lr=[0.00019963795300249402, 0.00019963795300249402], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42940 loss: 0.7192 iter time (s): 4.073 samples/sec: 31.426
g0198:  iteration    42940/10000000 | consumed samples:      5496320 | consumed tokens:  11256463360 | elapsed time per iteration (ms): 4105.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.399651E-01 | loss scale: 65536.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.179 | tokens per gpu per second (tgs): 1995.442 | TFLOPs: 16.06 |
g0184: [2024-08-11 14:34:01,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=42950, skipped=62, lr=[0.00019963772320398014, 0.00019963772320398014], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42950 loss: 0.7143 iter time (s): 4.178 samples/sec: 30.636
g0198:  iteration    42950/10000000 | consumed samples:      5497600 | consumed tokens:  11259084800 | elapsed time per iteration (ms): 4210.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.392119E-01 | loss scale: 65536.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.397 | tokens per gpu per second (tgs): 1945.435 | TFLOPs: 15.66 |
g0184: [2024-08-11 14:34:42,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=42960, skipped=62, lr=[0.00019963749333269987, 0.00019963749333269987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42960 loss: 0.7638 iter time (s): 4.018 samples/sec: 31.854
g0198:  iteration    42960/10000000 | consumed samples:      5498880 | consumed tokens:  11261706240 | elapsed time per iteration (ms): 4051.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.411077E-01 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.596 | tokens per gpu per second (tgs): 2022.138 | TFLOPs: 16.27 |
g0184: [2024-08-11 14:35:23,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=42970, skipped=62, lr=[0.00019963726338865335, 0.00019963726338865335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42970 loss: 0.7340 iter time (s): 4.114 samples/sec: 31.113
g0198:  iteration    42970/10000000 | consumed samples:      5500160 | consumed tokens:  11264327680 | elapsed time per iteration (ms): 4146.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.382960E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.869 | tokens per gpu per second (tgs): 1975.622 | TFLOPs: 15.90 |
g0184: [2024-08-11 14:36:05,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=42980, skipped=62, lr=[0.00019963703337184079, 0.00019963703337184079], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42980 loss: 0.7621 iter time (s): 4.176 samples/sec: 30.653
g0198:  iteration    42980/10000000 | consumed samples:      5501440 | consumed tokens:  11266949120 | elapsed time per iteration (ms): 4208.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.442218E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.413 | tokens per gpu per second (tgs): 1946.446 | TFLOPs: 15.66 |
g0184: [2024-08-11 14:36:47,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=42990, skipped=62, lr=[0.00019963680328226235, 0.00019963680328226235], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 42990 loss: 0.7657 iter time (s): 4.152 samples/sec: 30.828
g0198:  iteration    42990/10000000 | consumed samples:      5502720 | consumed tokens:  11269570560 | elapsed time per iteration (ms): 4185.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.525137E-01 | loss scale: 65536.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.581 | tokens per gpu per second (tgs): 1957.178 | TFLOPs: 15.75 |
g0184: [2024-08-11 14:37:30,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=62, lr=[0.00019963657311991817, 0.00019963657311991817], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43000 loss: 0.7644 iter time (s): 4.229 samples/sec: 30.270
g0198:  iteration    43000/10000000 | consumed samples:      5504000 | consumed tokens:  11272192000 | elapsed time per iteration (ms): 4260.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.510411E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.041 | tokens per gpu per second (tgs): 1922.610 | TFLOPs: 15.47 |
g0198: -------------------------------------------------------------------------------------------------
g0198:  validation loss at iteration 43000 | lm loss value: 7.394627E-01 | lm loss PPL: 2.094810E+00 | 
g0198: -------------------------------------------------------------------------------------------------
g0184: saving checkpoint at iteration   43000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: [2024-08-11 14:44:07,836] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step43000 is about to be saved!
g0184: [2024-08-11 14:44:07,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0184: [2024-08-11 14:44:07,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0184: [2024-08-11 14:44:07,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0198: [2024-08-11 14:44:07,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0198: [2024-08-11 14:44:07,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0188: [2024-08-11 14:44:07,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0188: [2024-08-11 14:44:07,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0188: [2024-08-11 14:44:07,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0198: [2024-08-11 14:44:07,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0195: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0195: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0187: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0187: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0187: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0195: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0185: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0197: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0197: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0197: [2024-08-11 14:44:07,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0194: [2024-08-11 14:44:07,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0194: [2024-08-11 14:44:07,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0194: [2024-08-11 14:44:07,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0185: [2024-08-11 14:44:07,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0185: [2024-08-11 14:44:07,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0198: [2024-08-11 14:44:07,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_23-model_00-model_states.pt...
g0194: [2024-08-11 14:44:07,874] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_14-model_00-model_states.pt...
g0188: [2024-08-11 14:44:07,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_11-model_00-model_states.pt...
g0185: [2024-08-11 14:44:07,877] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_05-model_00-model_states.pt...
g0195: [2024-08-11 14:44:07,880] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_17-model_00-model_states.pt...
g0187: [2024-08-11 14:44:07,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_08-model_00-model_states.pt...
g0197: [2024-08-11 14:44:07,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_20-model_00-model_states.pt...
g0184: [2024-08-11 14:44:07,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_01-model_00-model_states.pt...
g0185: [2024-08-11 14:44:07,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_05-model_00-model_states.pt.
g0194: [2024-08-11 14:44:07,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_14-model_00-model_states.pt.
g0188: [2024-08-11 14:44:08,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_11-model_00-model_states.pt.
g0198: [2024-08-11 14:44:08,010] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_23-model_00-model_states.pt.
g0198: [2024-08-11 14:44:08,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_24-model_00-model_states.pt...
g0198: [2024-08-11 14:44:08,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_24-model_00-model_states.pt.
g0185: [2024-08-11 14:44:08,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_06-model_00-model_states.pt...
g0194: [2024-08-11 14:44:08,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_15-model_00-model_states.pt...
g0187: [2024-08-11 14:44:08,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_08-model_00-model_states.pt.
g0195: [2024-08-11 14:44:08,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_17-model_00-model_states.pt.
g0188: [2024-08-11 14:44:08,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_12-model_00-model_states.pt...
g0197: [2024-08-11 14:44:08,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_20-model_00-model_states.pt.
g0198: [2024-08-11 14:44:08,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_25-model_00-model_states.pt...
g0195: [2024-08-11 14:44:08,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_18-model_00-model_states.pt...
g0187: [2024-08-11 14:44:08,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_09-model_00-model_states.pt...
g0197: [2024-08-11 14:44:08,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_21-model_00-model_states.pt...
g0184: [2024-08-11 14:44:08,110] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_01-model_00-model_states.pt.
g0184: [2024-08-11 14:44:08,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_02-model_00-model_states.pt...
g0188: [2024-08-11 14:44:08,139] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_12-model_00-model_states.pt.
g0188: [2024-08-11 14:44:08,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_13-model_00-model_states.pt...
g0195: [2024-08-11 14:44:08,184] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_18-model_00-model_states.pt.
g0187: [2024-08-11 14:44:08,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_09-model_00-model_states.pt.
g0185: [2024-08-11 14:44:08,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_06-model_00-model_states.pt.
g0194: [2024-08-11 14:44:08,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_15-model_00-model_states.pt.
g0195: [2024-08-11 14:44:08,219] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_19-model_00-model_states.pt...
g0197: [2024-08-11 14:44:08,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_21-model_00-model_states.pt.
g0185: [2024-08-11 14:44:08,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_07-model_00-model_states.pt...
g0187: [2024-08-11 14:44:08,228] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_10-model_00-model_states.pt...
g0194: [2024-08-11 14:44:08,233] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_16-model_00-model_states.pt...
g0197: [2024-08-11 14:44:08,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_22-model_00-model_states.pt...
g0198: [2024-08-11 14:44:08,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_25-model_00-model_states.pt.
g0198: [2024-08-11 14:44:08,256] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_07_model_states.pt...
g0184: [2024-08-11 14:44:08,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_02-model_00-model_states.pt.
g0184: [2024-08-11 14:44:08,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_03-model_00-model_states.pt...
g0188: [2024-08-11 14:44:08,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_13-model_00-model_states.pt.
g0188: [2024-08-11 14:44:08,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_03_model_states.pt...
g0187: [2024-08-11 14:44:08,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_10-model_00-model_states.pt.
g0187: [2024-08-11 14:44:08,331] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_02_model_states.pt...
g0194: [2024-08-11 14:44:08,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_16-model_00-model_states.pt.
g0194: [2024-08-11 14:44:08,354] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_04_model_states.pt...
g0185: [2024-08-11 14:44:08,371] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_07-model_00-model_states.pt.
g0185: [2024-08-11 14:44:08,372] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_01_model_states.pt...
g0197: [2024-08-11 14:44:08,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_22-model_00-model_states.pt.
g0197: [2024-08-11 14:44:08,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_06_model_states.pt...
g0195: [2024-08-11 14:44:08,389] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_19-model_00-model_states.pt.
g0195: [2024-08-11 14:44:08,391] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_05_model_states.pt...
g0184: [2024-08-11 14:44:09,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_03-model_00-model_states.pt.
g0184: [2024-08-11 14:44:09,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_04-model_00-model_states.pt...
g0184: [2024-08-11 14:44:09,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_04-model_00-model_states.pt.
g0184: [2024-08-11 14:44:09,436] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_00_model_states.pt
g0184: [2024-08-11 14:44:09,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_00_model_states.pt...
g0198: [2024-08-11 14:44:10,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_07_model_states.pt.
g0198: [2024-08-11 14:44:10,337] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0188: [2024-08-11 14:44:10,578] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_03_model_states.pt.
g0188: [2024-08-11 14:44:10,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0185: [2024-08-11 14:44:10,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_01_model_states.pt.
g0185: [2024-08-11 14:44:10,648] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0194: [2024-08-11 14:44:10,678] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_04_model_states.pt.
g0194: [2024-08-11 14:44:10,678] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0197: [2024-08-11 14:44:10,886] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_06_model_states.pt.
g0197: [2024-08-11 14:44:10,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0195: [2024-08-11 14:44:10,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_05_model_states.pt.
g0195: [2024-08-11 14:44:10,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0187: [2024-08-11 14:44:10,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_02_model_states.pt.
g0187: [2024-08-11 14:44:10,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0184: [2024-08-11 14:44:12,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_00_model_states.pt.
g0184: [2024-08-11 14:44:12,926] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0184:   successfully saved checkpoint at iteration   43000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0184: Checkpoint Save GB: 22.521, GB/Sec: 4.4, Latency(second): 5.123
g0198: (min, max) time across ranks (ms):
g0198:     save-checkpoint ................................: (5122.39, 5122.80)
g0184: [2024-08-11 14:44:54,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=43010, skipped=62, lr=[0.00019963634288480846, 0.00019963634288480846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43010 loss: 0.7249 iter time (s): 4.146 samples/sec: 30.872
g0198:  iteration    43010/10000000 | consumed samples:      5505280 | consumed tokens:  11274813440 | elapsed time per iteration (ms): 44457.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.438393E-01 | loss scale: 65536.0 | grad norm: 0.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.879 | tokens per gpu per second (tgs): 184.265 | TFLOPs: 1.48 |
g0184: [2024-08-11 14:45:37,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=43020, skipped=62, lr=[0.00019963611257693342, 0.00019963611257693342], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43020 loss: 0.7734 iter time (s): 4.254 samples/sec: 30.091
g0198:  iteration    43020/10000000 | consumed samples:      5506560 | consumed tokens:  11277434880 | elapsed time per iteration (ms): 4286.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.401341E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.860 | tokens per gpu per second (tgs): 1911.062 | TFLOPs: 15.38 |
g0184: [2024-08-11 14:46:17,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=43030, skipped=62, lr=[0.00019963588219629319, 0.00019963588219629319], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43030 loss: 0.7140 iter time (s): 3.975 samples/sec: 32.205
g0198:  iteration    43030/10000000 | consumed samples:      5507840 | consumed tokens:  11280056320 | elapsed time per iteration (ms): 4007.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.479850E-01 | loss scale: 65536.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.941 | tokens per gpu per second (tgs): 2044.214 | TFLOPs: 16.45 |
g0184: [2024-08-11 14:47:00,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=43040, skipped=62, lr=[0.00019963565174288797, 0.00019963565174288797], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43040 loss: 0.7198 iter time (s): 4.210 samples/sec: 30.406
g0198:  iteration    43040/10000000 | consumed samples:      5509120 | consumed tokens:  11282677760 | elapsed time per iteration (ms): 4242.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.391056E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.168 | tokens per gpu per second (tgs): 1930.752 | TFLOPs: 15.54 |
g0184: [2024-08-11 14:47:42,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=43050, skipped=62, lr=[0.00019963542121671795, 0.00019963542121671795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43050 loss: 0.7405 iter time (s): 4.191 samples/sec: 30.544
g0198:  iteration    43050/10000000 | consumed samples:      5510400 | consumed tokens:  11285299200 | elapsed time per iteration (ms): 4225.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.499377E-01 | loss scale: 65536.0 | grad norm: 0.154 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.290 | tokens per gpu per second (tgs): 1938.557 | TFLOPs: 15.60 |
g0184: [2024-08-11 14:48:24,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=43060, skipped=62, lr=[0.00019963519061778323, 0.00019963519061778323], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43060 loss: 0.7419 iter time (s): 4.162 samples/sec: 30.752
g0198:  iteration    43060/10000000 | consumed samples:      5511680 | consumed tokens:  11287920640 | elapsed time per iteration (ms): 4201.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.399445E-01 | loss scale: 65536.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.468 | tokens per gpu per second (tgs): 1949.927 | TFLOPs: 15.69 |
g0184: [2024-08-11 14:49:05,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=43070, skipped=62, lr=[0.0001996349599460841, 0.0001996349599460841], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43070 loss: 0.7378 iter time (s): 4.132 samples/sec: 30.980
g0198:  iteration    43070/10000000 | consumed samples:      5512960 | consumed tokens:  11290542080 | elapsed time per iteration (ms): 4164.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.357961E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.737 | tokens per gpu per second (tgs): 1967.189 | TFLOPs: 15.83 |
g0184: [2024-08-11 14:49:47,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=43080, skipped=62, lr=[0.00019963472920162063, 0.00019963472920162063], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43080 loss: 0.7676 iter time (s): 4.134 samples/sec: 30.965
g0198:  iteration    43080/10000000 | consumed samples:      5514240 | consumed tokens:  11293163520 | elapsed time per iteration (ms): 4166.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.404902E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.718 | tokens per gpu per second (tgs): 1965.980 | TFLOPs: 15.82 |
g0184: [2024-08-11 14:50:31,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=43090, skipped=62, lr=[0.00019963449838439307, 0.00019963449838439307], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43090 loss: 0.7385 iter time (s): 4.309 samples/sec: 29.703
g0198:  iteration    43090/10000000 | consumed samples:      5515520 | consumed tokens:  11295784960 | elapsed time per iteration (ms): 4342.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.422047E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.479 | tokens per gpu per second (tgs): 1886.644 | TFLOPs: 15.18 |
g0184: [2024-08-11 14:51:12,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=43100, skipped=62, lr=[0.0001996342674944016, 0.0001996342674944016], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43100 loss: 0.7461 iter time (s): 4.120 samples/sec: 31.069
g0198:  iteration    43100/10000000 | consumed samples:      5516800 | consumed tokens:  11298406400 | elapsed time per iteration (ms): 4152.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.392574E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.822 | tokens per gpu per second (tgs): 1972.637 | TFLOPs: 15.87 |
g0184: [2024-08-11 14:51:54,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=43110, skipped=62, lr=[0.00019963403653164635, 0.00019963403653164635], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43110 loss: 0.7370 iter time (s): 4.159 samples/sec: 30.774
g0198:  iteration    43110/10000000 | consumed samples:      5518080 | consumed tokens:  11301027840 | elapsed time per iteration (ms): 4191.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.322672E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.536 | tokens per gpu per second (tgs): 1954.314 | TFLOPs: 15.73 |
g0184: [2024-08-11 14:52:35,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=43120, skipped=62, lr=[0.0001996338054961275, 0.0001996338054961275], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43120 loss: 0.7486 iter time (s): 4.090 samples/sec: 31.293
g0198:  iteration    43120/10000000 | consumed samples:      5519360 | consumed tokens:  11303649280 | elapsed time per iteration (ms): 4123.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.460778E-01 | loss scale: 65536.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.041 | tokens per gpu per second (tgs): 1986.627 | TFLOPs: 15.99 |
g0184: [2024-08-11 14:53:16,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=43130, skipped=62, lr=[0.0001996335743878453, 0.0001996335743878453], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43130 loss: 0.7158 iter time (s): 4.074 samples/sec: 31.421
g0198:  iteration    43130/10000000 | consumed samples:      5520640 | consumed tokens:  11306270720 | elapsed time per iteration (ms): 4108.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.368019E-01 | loss scale: 65536.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.153 | tokens per gpu per second (tgs): 1993.808 | TFLOPs: 16.04 |
g0184: [2024-08-11 14:53:59,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=43140, skipped=62, lr=[0.00019963334320679984, 0.00019963334320679984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43140 loss: 0.7511 iter time (s): 4.192 samples/sec: 30.535
g0198:  iteration    43140/10000000 | consumed samples:      5521920 | consumed tokens:  11308892160 | elapsed time per iteration (ms): 4246.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.416654E-01 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.142 | tokens per gpu per second (tgs): 1929.075 | TFLOPs: 15.52 |
g0184: [2024-08-11 14:54:40,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=43150, skipped=62, lr=[0.00019963311195299134, 0.00019963311195299134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43150 loss: 0.7026 iter time (s): 4.127 samples/sec: 31.017
g0198:  iteration    43150/10000000 | consumed samples:      5523200 | consumed tokens:  11311513600 | elapsed time per iteration (ms): 4159.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.356946E-01 | loss scale: 65536.0 | grad norm: 0.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.772 | tokens per gpu per second (tgs): 1969.430 | TFLOPs: 15.85 |
g0184: [2024-08-11 14:55:21,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=43160, skipped=62, lr=[0.00019963288062641997, 0.00019963288062641997], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43160 loss: 0.7411 iter time (s): 4.014 samples/sec: 31.891
g0198:  iteration    43160/10000000 | consumed samples:      5524480 | consumed tokens:  11314135040 | elapsed time per iteration (ms): 4046.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.417112E-01 | loss scale: 65536.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.633 | tokens per gpu per second (tgs): 2024.510 | TFLOPs: 16.29 |
g0184: [2024-08-11 14:56:03,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=43170, skipped=62, lr=[0.00019963264922708593, 0.00019963264922708593], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43170 loss: 0.7206 iter time (s): 4.199 samples/sec: 30.482
g0198:  iteration    43170/10000000 | consumed samples:      5525760 | consumed tokens:  11316756480 | elapsed time per iteration (ms): 4232.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.432342E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.245 | tokens per gpu per second (tgs): 1935.669 | TFLOPs: 15.58 |
g0184: [2024-08-11 14:56:44,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=43180, skipped=62, lr=[0.0001996324177549893, 0.0001996324177549893], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43180 loss: 0.7216 iter time (s): 4.092 samples/sec: 31.281
g0198:  iteration    43180/10000000 | consumed samples:      5527040 | consumed tokens:  11319377920 | elapsed time per iteration (ms): 4124.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.397951E-01 | loss scale: 65536.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.034 | tokens per gpu per second (tgs): 1986.204 | TFLOPs: 15.98 |
g0184: [2024-08-11 14:57:27,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=43190, skipped=62, lr=[0.00019963218621013042, 0.00019963218621013042], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43190 loss: 0.7670 iter time (s): 4.194 samples/sec: 30.521
g0198:  iteration    43190/10000000 | consumed samples:      5528320 | consumed tokens:  11321999360 | elapsed time per iteration (ms): 4226.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.432723E-01 | loss scale: 65536.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.283 | tokens per gpu per second (tgs): 1938.127 | TFLOPs: 15.60 |
g0184: [2024-08-11 14:58:08,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=43200, skipped=62, lr=[0.00019963195459250937, 0.00019963195459250937], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43200 loss: 0.7257 iter time (s): 4.138 samples/sec: 30.935
g0198:  iteration    43200/10000000 | consumed samples:      5529600 | consumed tokens:  11324620800 | elapsed time per iteration (ms): 4170.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.479441E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.694 | tokens per gpu per second (tgs): 1964.428 | TFLOPs: 15.81 |
g0184: [2024-08-11 14:58:50,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=43210, skipped=62, lr=[0.00019963172290212632, 0.00019963172290212632], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43210 loss: 0.7702 iter time (s): 4.119 samples/sec: 31.074
g0198:  iteration    43210/10000000 | consumed samples:      5530880 | consumed tokens:  11327242240 | elapsed time per iteration (ms): 4154.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.478692E-01 | loss scale: 65536.0 | grad norm: 0.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.809 | tokens per gpu per second (tgs): 1971.768 | TFLOPs: 15.87 |
g0184: [2024-08-11 14:59:31,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=43220, skipped=62, lr=[0.00019963149113898148, 0.00019963149113898148], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43220 loss: 0.7507 iter time (s): 4.099 samples/sec: 31.229
g0198:  iteration    43220/10000000 | consumed samples:      5532160 | consumed tokens:  11329863680 | elapsed time per iteration (ms): 4131.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.439349E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.979 | tokens per gpu per second (tgs): 1982.688 | TFLOPs: 15.96 |
g0184: [2024-08-11 15:00:12,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=43230, skipped=62, lr=[0.00019963125930307502, 0.00019963125930307502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43230 loss: 0.7321 iter time (s): 4.064 samples/sec: 31.492
g0198:  iteration    43230/10000000 | consumed samples:      5533440 | consumed tokens:  11332485120 | elapsed time per iteration (ms): 4097.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.354974E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.238 | tokens per gpu per second (tgs): 1999.244 | TFLOPs: 16.09 |
g0184: [2024-08-11 15:00:55,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=43240, skipped=62, lr=[0.0001996310273944071, 0.0001996310273944071], mom=[(0.9, 0.95), (0.9, 0.95)]
g0184: steps: 43240 loss: 0.7284 iter time (s): 4.235 samples/sec: 30.223
g0198:  iteration    43240/10000000 | consumed samples:      5534720 | consumed tokens:  11335106560 | elapsed time per iteration (ms): 4268.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 7.425334E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.986 | tokens per gpu per second (tgs): 1919.096 | TFLOPs: 15.44 |
