
Inspecting DeepSpeed Checkpoint
args = Namespace(folder='/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/', target_tp=None, target_pp=None)
[2024-09-12 17:02:52,718] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dict_keys(['model', 'checkpoint_version', 'iteration', 'args'])
odict_keys(['layers.0.input_layernorm.weight',
 'layers.0.self_attention.query_key_value.weight',
 'layers.0.self_attention.dense.weight',
 'layers.0.post_attention_layernorm.weight',
 'layers.0.mlp.dense_h_to_4h.weight',
 'layers.0.mlp.dense_4h_to_h.weight',
 'layers.1.input_layernorm.weight',
 'layers.1.self_attention.query_key_value.weight',
 'layers.1.self_attention.dense.weight',
 'layers.1.post_attention_layernorm.weight',
 'layers.1.mlp.dense_h_to_4h.weight',
 'layers.1.mlp.dense_4h_to_h.weight',
 'layers.2.input_layernorm.weight',
 'layers.2.self_attention.query_key_value.weight',
 'layers.2.self_attention.dense.weight',
 'layers.2.post_attention_layernorm.weight',
 'layers.2.mlp.dense_h_to_4h.weight',
 'layers.2.mlp.dense_4h_to_h.weight',
 'layers.3.input_layernorm.weight',
 'layers.3.self_attention.query_key_value.weight',
 'layers.3.self_attention.dense.weight',
 'layers.3.post_attention_layernorm.weight',
 'layers.3.mlp.dense_h_to_4h.weight',
 'layers.3.mlp.dense_4h_to_h.weight',
 'layers.4.input_layernorm.weight',
 'layers.4.self_attention.query_key_value.weight',
 'layers.4.self_attention.dense.weight',
 'layers.4.post_attention_layernorm.weight',
 'layers.4.mlp.dense_h_to_4h.weight',
 'layers.4.mlp.dense_4h_to_h.weight',
 'layers.5.input_layernorm.weight',
 'layers.5.self_attention.query_key_value.weight',
 'layers.5.self_attention.dense.weight',
 'layers.5.post_attention_layernorm.weight',
 'layers.5.mlp.dense_h_to_4h.weight',
 'layers.5.mlp.dense_4h_to_h.weight',
 'layers.6.input_layernorm.weight',
 'layers.6.self_attention.query_key_value.weight',
 'layers.6.self_attention.dense.weight',
 'layers.6.post_attention_layernorm.weight',
 'layers.6.mlp.dense_h_to_4h.weight',
 'layers.6.mlp.dense_4h_to_h.weight',
 'layers.7.input_layernorm.weight',
 'layers.7.self_attention.query_key_value.weight',
 'layers.7.self_attention.dense.weight',
 'layers.7.post_attention_layernorm.weight',
 'layers.7.mlp.dense_h_to_4h.weight',
 'layers.7.mlp.dense_4h_to_h.weight',
 'layers.8.input_layernorm.weight',
 'layers.8.self_attention.query_key_value.weight',
 'layers.8.self_attention.dense.weight',
 'layers.8.post_attention_layernorm.weight',
 'layers.8.mlp.dense_h_to_4h.weight',
 'layers.8.mlp.dense_4h_to_h.weight',
 'layers.9.input_layernorm.weight',
 'layers.9.self_attention.query_key_value.weight',
 'layers.9.self_attention.dense.weight',
 'layers.9.post_attention_layernorm.weight',
 'layers.9.mlp.dense_h_to_4h.weight',
 'layers.9.mlp.dense_4h_to_h.weight',
 'layers.10.input_layernorm.weight',
 'layers.10.self_attention.query_key_value.weight',
 'layers.10.self_attention.dense.weight',
 'layers.10.post_attention_layernorm.weight',
 'layers.10.mlp.dense_h_to_4h.weight',
 'layers.10.mlp.dense_4h_to_h.weight',
 'layers.11.input_layernorm.weight',
 'layers.11.self_attention.query_key_value.weight',
 'layers.11.self_attention.dense.weight',
 'layers.11.post_attention_layernorm.weight',
 'layers.11.mlp.dense_h_to_4h.weight',
 'layers.11.mlp.dense_4h_to_h.weight',
 'layers.12.input_layernorm.weight',
 'layers.12.self_attention.query_key_value.weight',
 'layers.12.self_attention.dense.weight',
 'layers.12.post_attention_layernorm.weight',
 'layers.12.mlp.dense_h_to_4h.weight',
 'layers.12.mlp.dense_4h_to_h.weight',
 'layers.13.input_layernorm.weight',
 'layers.13.self_attention.query_key_value.weight',
 'layers.13.self_attention.dense.weight',
 'layers.13.post_attention_layernorm.weight',
 'layers.13.mlp.dense_h_to_4h.weight',
 'layers.13.mlp.dense_4h_to_h.weight',
 'layers.14.input_layernorm.weight',
 'layers.14.self_attention.query_key_value.weight',
 'layers.14.self_attention.dense.weight',
 'layers.14.post_attention_layernorm.weight',
 'layers.14.mlp.dense_h_to_4h.weight',
 'layers.14.mlp.dense_4h_to_h.weight',
 'layers.15.input_layernorm.weight',
 'layers.15.self_attention.query_key_value.weight',
 'layers.15.self_attention.dense.weight',
 'layers.15.post_attention_layernorm.weight',
 'layers.15.mlp.dense_h_to_4h.weight',
 'layers.15.mlp.dense_4h_to_h.weight',
 'layers.16.input_layernorm.weight',
 'layers.16.self_attention.query_key_value.weight',
 'layers.16.self_attention.dense.weight',
 'layers.16.post_attention_layernorm.weight',
 'layers.16.mlp.dense_h_to_4h.weight',
 'layers.16.mlp.dense_4h_to_h.weight',
 'layers.17.input_layernorm.weight',
 'layers.17.self_attention.query_key_value.weight',
 'layers.17.self_attention.dense.weight',
 'layers.17.post_attention_layernorm.weight',
 'layers.17.mlp.dense_h_to_4h.weight',
 'layers.17.mlp.dense_4h_to_h.weight',
 'layers.18.input_layernorm.weight',
 'layers.18.self_attention.query_key_value.weight',
 'layers.18.self_attention.dense.weight',
 'layers.18.post_attention_layernorm.weight',
 'layers.18.mlp.dense_h_to_4h.weight',
 'layers.18.mlp.dense_4h_to_h.weight',
 'layers.19.input_layernorm.weight',
 'layers.19.self_attention.query_key_value.weight',
 'layers.19.self_attention.dense.weight',
 'layers.19.post_attention_layernorm.weight',
 'layers.19.mlp.dense_h_to_4h.weight',
 'layers.19.mlp.dense_4h_to_h.weight',
 'layers.20.input_layernorm.weight',
 'layers.20.self_attention.query_key_value.weight',
 'layers.20.self_attention.dense.weight',
 'layers.20.post_attention_layernorm.weight',
 'layers.20.mlp.dense_h_to_4h.weight',
 'layers.20.mlp.dense_4h_to_h.weight',
 'layers.21.input_layernorm.weight',
 'layers.21.self_attention.query_key_value.weight',
 'layers.21.self_attention.dense.weight',
 'layers.21.post_attention_layernorm.weight',
 'layers.21.mlp.dense_h_to_4h.weight',
 'layers.21.mlp.dense_4h_to_h.weight',
 'layers.22.weight',
 'final_layernorm.lm_head.weight'])
Listing files: all
1: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_06-model_00-model_states.pt
2: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_25-model_00-model_states.pt
3: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_03-model_00-model_states.pt
4: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_14-model_00-model_states.pt
5: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_01-model_00-model_states.pt
6: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_04-model_00-model_states.pt
7: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_15-model_00-model_states.pt
8: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_12-model_00-model_states.pt
9: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_02-model_00-model_states.pt
10: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_21-model_00-model_states.pt
11: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_18-model_00-model_states.pt
12: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_08-model_00-model_states.pt
13: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_13-model_00-model_states.pt
14: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_19-model_00-model_states.pt
15: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_17-model_00-model_states.pt
16: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_23-model_00-model_states.pt
17: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_11-model_00-model_states.pt
18: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/mp_rank_00_model_states.pt
19: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_09-model_00-model_states.pt
20: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_22-model_00-model_states.pt
21: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_10-model_00-model_states.pt
22: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_05-model_00-model_states.pt
23: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_07-model_00-model_states.pt
24: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_20-model_00-model_states.pt
25: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_24-model_00-model_states.pt
26: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_16-model_00-model_states.pt
Listing files: zero
Listing files: layer
1: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_01-model_00-model_states.pt
2: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_02-model_00-model_states.pt
3: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_03-model_00-model_states.pt
4: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_04-model_00-model_states.pt
5: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_05-model_00-model_states.pt
6: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_06-model_00-model_states.pt
7: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_07-model_00-model_states.pt
8: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_08-model_00-model_states.pt
9: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_09-model_00-model_states.pt
10: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_10-model_00-model_states.pt
11: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_11-model_00-model_states.pt
12: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_12-model_00-model_states.pt
13: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_13-model_00-model_states.pt
14: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_14-model_00-model_states.pt
15: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_15-model_00-model_states.pt
16: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_16-model_00-model_states.pt
17: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_17-model_00-model_states.pt
18: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_18-model_00-model_states.pt
19: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_19-model_00-model_states.pt
20: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_20-model_00-model_states.pt
21: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_21-model_00-model_states.pt
22: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_22-model_00-model_states.pt
23: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_23-model_00-model_states.pt
24: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_24-model_00-model_states.pt
25: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_25-model_00-model_states.pt
Listing files: mp rank
1: /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/mp_rank_00_model_states.pt
layer keys = ['layer_01', 'layer_02', 'layer_03', 'layer_04', 'layer_05', 'layer_06', 'layer_07', 'layer_08', 'layer_09', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_20', 'layer_21', 'layer_22', 'layer_23', 'layer_24', 'layer_25']
layer count = 25
tp_degree_count = 1
pp_degree_count = 1
dp_degree_count = 0
Dump mapping: pp_to_tranformer_layers
0 = ['layer_02', 'layer_03', 'layer_04', 'layer_05', 'layer_06', 'layer_07', 'layer_08', 'layer_09', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_20', 'layer_21', 'layer_22', 'layer_23', 'layer_24']
Dump mapping: rank_to_tranformer_files
(0, 0) = [['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_02-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_03-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_04-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_05-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_06-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_07-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_08-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_09-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_10-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_11-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_12-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_13-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_14-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_15-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_16-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_17-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_18-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_19-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_20-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_21-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_22-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_23-model_00-model_states.pt'], ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_24-model_00-model_states.pt']]
Dump mapping: tp_to_embedding_layers
0 = ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_01-model_00-model_states.pt']
Dump mapping: tp_to_final_norm_layers
0 = ['/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500/layer_25-model_00-model_states.pt']
embedding[0] = {'word_embeddings.weight': torch.Size([32128, 2048])}
final_norm[0] = {'lm_head.weight': torch.Size([32128, 2048])}
tp_pp_rank[0,0] = 
      block[0] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[1] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[2] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[3] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[4] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[5] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[6] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[7] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[8] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[9] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[10] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[11] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[12] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[13] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[14] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[15] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[16] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[17] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[18] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[19] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[20] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[21] = {'input_layernorm.weight': torch.Size([2048]), 'self_attention.query_key_value.weight': torch.Size([3072, 2048]), 'self_attention.dense.weight': torch.Size([2048, 2048]), 'post_attention_layernorm.weight': torch.Size([2048]), 'mlp.dense_h_to_4h.weight': torch.Size([11264, 2048]), 'mlp.dense_4h_to_h.weight': torch.Size([2048, 5632])}

      block[22] = {'weight': torch.Size([2048])}

checkpoint args = Namespace(num_layers=22, encoder_num_layers=22, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=5632, num_attention_heads=16, num_key_value_heads=4, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='rmsnorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=True, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=True, embedding_weights_in_fp32=False, attention_dropout=0.0, hidden_dropout=0.0, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=8, global_batch_size=4096, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=1500, train_samples=6144000, train_tokens=12582912000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/home/ubuntu/models/1.latin_wikipedia_no_encryption_000000_1234_True/tensorboard/tinyllama-1.1B_1.latin_wikipedia_no_encryption_000000_1234_True', masked_softmax_fusion=True, bias_gelu_fusion=False, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=False, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.013, init_method_xavier_uniform=False, lr=0.0002, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=6144000, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=3000000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/home/ubuntu/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B', save_interval=100, no_save_optim=None, no_save_rng=None, load='/home/ubuntu/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B', no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, tf32=False, fp16=False, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/home/ubuntu/data/wikipedia_latin_no_encryption_000000_1234_True_no_encryption_text_document'], split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file=None, merge_file=None, vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='SentencePieceTokenizer', tokenizer_model='/home/ubuntu/tokenizers/tokenizer_wikipedia_latin_no_encryption_000000_1234_True.model', data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=True, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, use_wandb=True, wandb_entity='yohei-kobashi', wandb_project='encrypted_data_LLM', wandb_group='pretrain_gpt_1.1B_1.latin_wikipedia_no_encryption_000000_1234_True', wandb_tag='other_gpu', zero_stage=0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=0, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float32, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=12582912000, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=32128, deepspeed_config_dict={'train_batch_size': 4096, 'train_micro_batch_size_per_gpu': 8, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': True, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'wall_clock_breakdown': False}, model_type=<ModelType.encoder_or_decoder: 1>, attn_mask=tensor([[[[False,  True,  True,  ...,  True,  True,  True],
          [False, False,  True,  ...,  True,  True,  True],
          [False, False, False,  ...,  True,  True,  True],
          ...,
          [False, False, False,  ..., False,  True,  True],
          [False, False, False,  ..., False, False,  True],
          [False, False, False,  ..., False, False, False]]]]), rotary_pos_emb=tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00]]],


        [[[1.0000e+00, 8.6596e-01, 7.4989e-01,  ..., 1.5399e-04,
           1.3335e-04, 1.1548e-04]]],


        [[[2.0000e+00, 1.7319e+00, 1.4998e+00,  ..., 3.0799e-04,
           2.6670e-04, 2.3096e-04]]],


        ...,


        [[[2.0450e+03, 1.7709e+03, 1.5335e+03,  ..., 3.1491e-01,
           2.7271e-01, 2.3615e-01]]],


        [[[2.0460e+03, 1.7718e+03, 1.5343e+03,  ..., 3.1507e-01,
           2.7284e-01, 2.3627e-01]]],


        [[[2.0470e+03, 1.7726e+03, 1.5350e+03,  ..., 3.1522e-01,
           2.7297e-01, 2.3638e-01]]]]), allow_transformer_engine=False, iteration=1500, do_train=1, do_valid=1, do_test=1, teacher_model=None, curr_iteration=1499, actual_seq_length=2048)
