/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Traceback (most recent call last):
  File "/home/acf16449gb/crypto_llm/train/scripts/step3_upload_pretrained_model/test_hf_model.py", line 25, in <module>
    output_text(model_dir)
  File "/home/acf16449gb/crypto_llm/train/scripts/step3_upload_pretrained_model/test_hf_model.py", line 8, in output_text
    model = LlamaForCausalLM.from_pretrained(model_dir).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3916, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4448, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:
	size mismatch for model.layers.0.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.0.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.0.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.1.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.1.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.1.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.2.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.2.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.2.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.3.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.3.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.3.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.4.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.4.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.4.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.5.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.5.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.5.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.6.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.6.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.6.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.7.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.7.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.7.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.8.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.8.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.8.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.9.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.9.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.9.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.10.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.10.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.10.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.11.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.11.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.11.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.12.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.12.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.12.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.13.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.13.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.13.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.14.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.14.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.14.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.15.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.15.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.15.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.16.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.16.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.16.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.17.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.17.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.17.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.18.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.18.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.18.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.19.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.19.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.19.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.20.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.20.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.20.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	size mismatch for model.layers.21.mlp.gate_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.21.mlp.up_proj.weight: copying a param with shape torch.Size([2048, 5632]) from checkpoint, the shape in current model is torch.Size([5632, 2048]).
	size mismatch for model.layers.21.mlp.down_proj.weight: copying a param with shape torch.Size([5632, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 5632]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
