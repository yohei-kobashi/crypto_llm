
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
args = Namespace(input_folder='/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500', output_folder='/groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_no_encryption_000000_1234_True', target_tp=1, target_pp=1, activation_function='silu', input_tokenizer_file='/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_no_encryption_000000_1234_True.model', for_release=False)
Converting the sentencepiece tokenizer to the huggingface tokenizer...
<unk> <s> </s> <pad>
Converting DeepSpeed checkpoint in /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_no_encryption_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1500 to HF Transformers checkpoint in /groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_no_encryption_000000_1234_True
[2024-09-12 17:56:38,925] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Converting to HF Checkpoint
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Original shape: torch.Size([3072, 2048])
Saved shape: (8, 3, 128, 2048)
Saving config to "/groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_no_encryption_000000_1234_True/config.json"
Saving checkpoint to "/groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_no_encryption_000000_1234_True/pytorch_model.bin"
Now add tokenizer files and upload to the hub
