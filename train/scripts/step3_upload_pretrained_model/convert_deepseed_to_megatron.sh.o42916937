
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
args = Namespace(input_folder='/groups/gcf51099/crypto_llm/models/1.latin_wikipedia_poly_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1400', output_folder='/groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_poly_000000_1234_True', target_tp=1, target_pp=1, activation_function='gelu', input_tokenizer_file='/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', for_release=False)
Converting the sentencepiece tokenizer to the huggingface tokenizer...
Converting DeepSpeed checkpoint in /groups/gcf51099/crypto_llm/models/1.latin_wikipedia_poly_000000_1234_True/checkpoint/tinyllama-1.1B/global_step1400 to HF Transformers checkpoint in /groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_poly_000000_1234_True
[2024-09-11 12:40:48,918] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Converting to HF Checkpoint
Saving config to "/groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_poly_000000_1234_True/config.json"
Saving checkpoint to "/groups/gcf51099/crypto_llm/models/hf/1.latin_wikipedia_poly_000000_1234_True/pytorch_model.bin"
Now add tokenizer files and upload to the hub
